# 特征工程指南-朝闻道-51CTO博客
特征工程
数据科学最有创造力的方面。
要像其他任何有创造力的尝试一样对待它，就想写一个喜剧的秀。
坚持头脑风暴
创建模板或公式
检查/重新审视以前的工作
特征分类
一些预处理似乎永远都是必要的
很高的基数（即包含大量不同的值）会带来很稀疏的数据
填补缺失值是一个难点
Onehot 编码
One-of-K 给一个长度为 K 的数组编码
基本方法：大多使用一些线性的算法
去掉第一列避免共线性
稀疏格式是记忆友好的（便于存储）
目前对待缺失值的大部分实现方法都不够优雅，没有考虑变量。
哈希编码
对固定长度的数组进行 Onehot 编码
避免极度稀疏的数据
可能会引发冲突性
可用不同的哈希函数重复操作并 bagging 小凹凸精度的结果
冲突性可能会降低结果的准确度，也可能提升结果的准确度
优雅的处理新的变量（比如 new user-agents）
标签编码
给每个类一个独一无二的数字化 ID
对于非线性的基于树模型的算法很有用
不增加维度
完全打乱 cat_var --num_id 的映射并重新训练，平均小凹凸精度。
Count 编码
用训练集里的 count 替换对应的变量
对线性或非线性的算法都适用
对异常值敏感
可以加入对数变换，和 counts 一起使用时效果良好
用‘1’代替没有考虑的变量
可以主动给一些冲突性：相同的编码，不同的变量
LabelCount 编码
按训练集中的 counts 等级给变量分类
对线性或非线性算法都适用
对异常值不敏感
不会对不同的变量进行相同的编码
两全其美
目标编码
用目标的比例对类别变量编码（二元分类或回归）
注意避免过拟合
Stacking 的形式：输出目标的平均值的单变量模型
记得做交叉验证
加入平滑性避免出现编码为 0 的情况
加入随机噪声避免过拟合
当被正确的应用时，是最好的线性或非线性编码
嵌入类别
用神经网络来创建类别变量的稠密层
在函数近似问题中将类别变量映射到欧式空间
模型训练更快
更少的存储开销
比 Onehot 编码更精准
NaN 编码
给 NAN 值一个明确的值来代替其被忽略的情况
NAN 值可以保存信息
注意避免过拟合
仅当 NAN 值在训练集和测试集中一致或在本地验证了其独立性时方
可使用
多项式编码
为类别变量间的交互进行编码
没有交互的线性算法不能解决 XOR 问题（逻辑运算）
一个拥有多项式的内核可以解决 XOR 问题
探索特征空间，用：FS，Hashing and/or VW
扩张编码
从一个单变量创建多个类别变量
一些高基数的特征，比如 user-agents，里面有非常多的信息：
is_mobile?
Is_latest_version?
Operation_system
Browser_build
Etc
整合编码
将不同的类别变量映射到同一个变量中
拼写错误，稍有不同的工作描述，全名 vs 缩写
真实的数据是散乱的，自由文本尤其如此
找出特征是很难的，费时，需要专业的知识。应用机器学习基本上就
等于特征工程。
--------------Andrew Ng
数字化特征
使数据进入算法更容易
可由 floats,counts,numbers 组成
更易估算缺失值
凑整
数字化变量的凑整
有损耗的压缩方式：保存数据中大部分重要的特征
有时候太过精准反而会带来噪声
被凑整的变量可以当做类别变量
在凑整前可进行对数变换
装箱
把数字化变量放入箱中，并用 bin-ID 编码
用分位数装箱是很实用的，甚至可以用模型找出可选的箱
可以优雅的找到训练集范围外的变量
缩放
将数字化变量缩放到一个确定的范围内
标准化缩放
最大最小缩放
根缩放
对数缩放
填补
填补缺失值
用缺失值组合硬编码
求平均：相当基础
求中位数：应对离群点时更健壮
直接忽略：只是推迟问题
使用模型：可以暴露算法偏差
交互
数字化变量间具体的交互编码
尝试：减，加，乘法，除法
使用：通过统计检验进行特征选择，或通过训练集对特征重要性排序
易忽略：人的直觉；有时一些诡异的交互会带来出其不意的效果
机器学习项目众多，有点成功了，有的失败了。到底是什么造成了它
们 的 区 别 ？ 简 单 来 说 最 重 要 的 就 是 对 特 征 的 使 用 。
------------Predro Domingos
线性算法的非线性编码
用非线性硬编码来改进线性算法
多项式内核
叶编码（嵌入随机森林）
遗传算法
局部线性嵌入，谱嵌入，t-SNE
行统计
NAN 值数
零值数
负值数
平均值，最大值，最小值，偏度，等
时间变量
时间变量，比如日期，需要更好的本地验证方案（比如回溯测试）
这里很容易犯错
有很多机会获得重大突破
投影到圆内
将单个的特征，比如 day_of_week,投影到一个圆的两个坐标上
确保 max 和 min 间的距离等于 min 和 min+1 间的距离
使用 day_of_week,day_of_month,hour_of_day
趋势线
代替编码:总花费，对某件事编码要像：上周的花费，上个月的花费，
上一年的花费
给出一个算法趋势：两个顾客花费相当，但却有截然不同的表现方式
-------一个顾客可能开始花的更多，而另一个接下来可能会减少花费
重大事件的临近
硬编码类别特征，如：data_3_days_before_holidays:1
Try：国庆节，重大体育事件，周末，每个月的第一个星期六，等
这些因素可能会对花费行为产生重大影响
空间变量
空间变量是对本地空间编码的变量
示例：GPS，城市，国家，地址
将位置归类
克里金插值
K-means 分类
原经纬度
将城市转换为经纬度
给街名添加邮政编码
与中心的接近程度
确定当前位置与中心点的接近程度
小城镇传承了周边的大城市的文化和语言
手机的位置可映射到最近的商业中心或超市
空间的欺骗行为
本地事件的数据可以作为可疑行为的指示
不可能的旅行速度：在多个国家同时出现交易
在不同城镇的花费多过在家或送货地址
从不在同一地点花费
探索
数据探索可以发现数据的健壮程度，离群点，噪声，特征工程的想法，
以及特征清洗的想法
可用的有：控制台，pandas，notebook
试试简单的统计：min，max
合并目标，一遍找到项目间的相关性
迭代/调试
特征工程是一个迭代的过程：合理的规划以便快速迭代
使用次线性调试：输出过程中的信息，要做伪日志
使用工具进行快速实验
许多的想法会失败，更多的想法就会成功
标签工程
把标签/目标/依赖性变量当做数据的特征，反之亦然
对数变换：y---> log(y+1) | exp(y_pred)-1
平方变换
箱行变换
创建一个 score，来当做最初的回归目标函数
训练回归来预测测试集里没有的特征
自然语言处理
关于类别特征的所有想法都能用
深度学习（自动化特征工程）在这个领域越来越越来越吃香，但浅度
的学习和精细化特征设计仍然很有竞争力。
高度稀疏的数据可能会带来维数灾难
这部分有许多机会来做特征工程：
清洗：
大写变小写
解码
去掉符号
修复
标志法
编码标点来标记：硬编码“！”和“？”
标志：将句子切分成单词
N-Grams：连续的记号编码：“I like the Beatles”--->[“I like”,”like the”,”the
Beatles”]
Char-grams: 和 N-Grams 一 样 ， 但 是 是 字 母 级 的 ：“ Beatles ”
--->[“Bea”,”eat”,”atl”,”tle”,”les”]
词缀：和 char-grams 一样，但是只针对后缀和前缀
去除
停止词：去掉出现在停止词列表中的词/标记
特殊词：去掉只在训练集中出现过几次的单词。
常见词：去掉可能不在停止词列表中但却及其常见的词
根
拼写校正：改变正确拼写的标志
切分：只拿出一个单词中的前 8 个字母
词干：去掉单词/标记的词根 cars-->car
把文中的词按屈折变化形式进行归类：never be late--->never are late
相似性
标记相似性：两个文本中出现的标记个数
压缩距离：寻找比当前文本更好的压缩距离的另一个文本
编辑/汉明/杰卡德距离：检查两个字符串间的相似性
Word2Vec/Glove：检查两个平均向量间的余弦相似性
词频-逆文档词频
词频：减少长文本的偏好
逆文档词频：减少常见标记的偏好
词频-逆文档词频：用于识别文档中最重要的标记，来去除不重要的
标记，或预处理降维
降维
PCA：将文本的维度降到 50-100
SVD：将文本的维度降到 50-100
LDA: TF-IDF followed by SVD
LSA: 创建主题向量
外部模型
情绪分析：得出一个反应文本中积极或消极情绪的向量
主题模型：用另一个数据集来创建一个新数据集的主题向量
神经网络&深度学习
神经网络号称 end-to-end 的自动化特征工程
特征工程垂死的领域？
不！继续专注在建筑工程领域
尽管许下了承诺：计算机视觉像这样使用特征：HOG，SIFT，美白，
摄动，图像金字塔，旋转，缩放 z 缩放，日志，架克，外部语义数据，
等。
