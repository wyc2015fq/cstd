# 分类回归树什么时候不能反映数据的真实趋势-朝闻道-51CTO博客
本质上lightgbm和xgboost都是基于分类回归树集成的学习算法，所以他们也会有一些先天缺陷：
当训练数据的特征都集中在一个区间内，而测试数据偏离该区间较大时会出现不能拟合的情况，根本原因是因为分类回归树对于某一个叶子节点上的数据的目标值是采取该叶子节点的目标值均值做梯度提升最终确定预测值的。
比如我有个数据如下：
x,y
1,1
2,2
3,3
4,4
5,5
6,6
7,7
8,8
9,9
10,10
11,11
12,12
13,13
14,14
15,15
16,16
17,17
18,18
19,19
20,20
21,21
22,22
23,23
24,24
25,25
26,26
27,27
28,28
这个非常明显 是 y=x
如果输入测试数据 x =200 y应该是200
但是你用下面的程序测试，发现怎么调参数都不能得到200
因为分类回归树将这些数据分到若干个叶子节点上时候，采用的目标值最大只有28，他没有再根据特征做线性关系的拟合。程序如下：
```
import pandas as pd
import lightgbm as lgb
path_train = "data.csv"
train1 = pd.read_csv(path_train)
testlist = [[200]]
# 采用lgb回归预测模型，具体参数设置如下
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=28,
                              learning_rate=0.1, n_estimators=2000,
                              max_bin = 28, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =10, min_sum_hessian_in_leaf = 100
                              ,max_depth = 10)
# 训练、预测
model_lgb.fit(train1[['x']].fillna(-1), train1['y'])
test1 = pd.DataFrame(testlist)
test1.columns = ['x']
y_pred = model_lgb.predict(test1[['x']].fillna(-1))
print(y_pred)
print("lgb success")
```
套用一句话就是“没有见过星空的民族。怎会有遨游宇宙的梦想”
所以并不是什么数据都可以直接往lightgbm，xgboost里面灌的，要注意分析这个新的预测的数据的特征是否在训练数据集的特征的空间范围内。
不然挖掘的时候应该采用其他的方法分析。例如线性回归，或者将上面的博客的分类回归树底层加一个线性回归。
