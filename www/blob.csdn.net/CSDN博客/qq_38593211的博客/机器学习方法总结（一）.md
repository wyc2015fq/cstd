# 机器学习方法总结（一） - qq_38593211的博客 - CSDN博客





2018年07月18日 11:50:56[小屁猴多看书](https://me.csdn.net/qq_38593211)阅读数：335








# 机器学习方法概论

> 
   说明：本教程的主要目的是个人秋招复习，适用于一些有基础的同学进行复习，主要来自于对统计学习方法和西瓜书的整理，所以不适用于系统学习，详细内容大家可以看书。其中加入个人的理解和各个算法是实例，由于理解不够导致的错误还请各位指出。 


## 1.特点与分类

      1.统计学习方法是基于数据构建概率统计模型并运用模型进行预测 与分析。

   2.统计学习方法可分为监督学习、半监督学习、非监督学习和强化学习，监督学习是最常用的，它是从给定的有限训练数据集合出发，假设数据是独立同分布产生的，学习的模型是某个函数的集合，这个集合称为假设空间，从假设空间中选取一个最优的模型用于测试。

      3.三要素分别是：模型、策略、算法。

## 2.基本概念

     1.输入和输出所有可能取值的集合分别叫输入空间和输出空间，输入的实例通常用特征向量进行表示，所有的特征向量存在的空间成为特征空间。

      2.输入和输出均为连续变量的预测问题称为回归问题，输出变量为有限个离散变量的预测问题为分类问题，它们均为序列变量则是标注问题。

      3.X和Y具有联合概率分布是监督学习的基本假设。

## 3.学习策略

      1.损失函数：模型执行一次的好坏，由预测值f(x)和真实值Y之间的非负实值函数 L[Y,f(x)] 表示（这里简单的公式就不列出，百度很多）:
-        0-1损失函数
-        平方损失函数
-        绝对损失函数
-        对数似然损失函数

      2.风险函数：平均意义下模型预测的好坏，它的理想值是模型关于联合分布损失函数的期望Rexp，实际是风险函数是训练数据集的损失函数的均值，当训练样本越大时它们约接近。

   3.经验风险最小化能保证较好的学习效果，如极大似然估计（通过数据估计概率分布的参数）；结构风险最小化等价于正则化，如贝叶斯估计中的最大后验概率估计。

## 4.模型的估计与选择

      1.评估方法：交叉验证
- K折验证：将数据集D划分为k个互斥的子集，每次用k-1个子集的并集作为训练集，剩下的一个作为测试集。
- 简单交叉：将数据集分为不同的比例即可。
- 留一交叉验证：当数据量不足时（N），k=N就是留一交叉验证。

     2.准确率：正确信息/提取信息（查准率）；召回率：提取的正确信息/正确信息数（查全率）；一般P,R不可兼得，可以根据偏好制定模型的评估标准，如P,R的调和平均![F\beta =(1+\beta ^{2})RP/(\beta ^{2}P+R)](https://private.codecogs.com/gif.latex?F%5Cbeta%20%3D%281&plus;%5Cbeta%20%5E%7B2%7D%29RP/%28%5Cbeta%20%5E%7B2%7DP&plus;R%29),![\beta](https://private.codecogs.com/gif.latex?%5Cbeta)表示查全率相对于查准率的相对重要性，越高表示越重视查全率。一般反应它们的曲线有：PR曲线和ROC曲线，在展示模型的时候可以绘制这两个曲线。

    3.偏差（bias）度量的是学习方法的期望预测和真实结果之间的偏离程度；方差（variance）是刻画的数据扰动造成的噪声；一般偏差大时说明模型的泛化能力强，自然数据扰动的影响就会变小，而方差大时说明模型有可能过拟合了，偏差自然小。

##  5.过拟合与正则化

    1.过拟合的原因：一味的提高对训练数据的预测能力，有可能是所选模型的复杂度往往比真实复杂度高，也可能是训练数据不足导致不能全面反映总体特征或样本噪声过多。

      2.解决办法：
- Early Stopping（当发现有过拟合现象就停止训练）
- Penelizing Large Weight（loss function中加正则化项）
- Bagging思想（对同一样本用多个模型投票产生结果）
- Boosting思想（多个弱分类器增强分类能力，降低偏差）
- Dropconnection（神经网络全连接层中减少过拟合的发生）

       3.正则化：是结构风险最小化的实现，在经验风险函数后加入一个正则化项：
- L1正则化：![L(\omega )=\frac{1}{N}\sum (f(xi;\omega )-yi)^{2}+\lambda |\omega |](https://private.codecogs.com/gif.latex?L%28%5Comega%20%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum%20%28f%28xi%3B%5Comega%20%29-yi%29%5E%7B2%7D&plus;%5Clambda%20%7C%5Comega%20%7C)
- L2正则化：![L(\omega )=\frac{1}{N}\sum (f(xi;\omega )-yi)^{2}+\lambda/2 |\omega |^{2}](https://private.codecogs.com/gif.latex?L%28%5Comega%20%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum%20%28f%28xi%3B%5Comega%20%29-yi%29%5E%7B2%7D&plus;%5Clambda/2%20%7C%5Comega%20%7C%5E%7B2%7D)
- L1正则化是权重的绝对值之和，可以产生稀疏权重矩阵，L2正则化倾向于权重较小一些，会导致模型抗扰动能力强，当![\lambda](https://private.codecogs.com/gif.latex?%5Clambda)较小时，L1和L1很接近，都有防止过拟合的作用。

## 6.生成模型和判别模型

       1.生成模型：由数据学习联合概率分布P(X,Y),然和求P(Y|X)。

       2.判别模型：由数据直接拟合决策函数f(x)或P(Y|X)。

       3.当样本数量上升时，生成模型的收敛更快，准确率更高，当存在隐变量时，只有生成方法有效，如HMM。







