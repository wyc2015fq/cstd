# 机器学习方法总结（六）：集成学习 - qq_38593211的博客 - CSDN博客





2018年08月01日 14:51:34[小屁猴多看书](https://me.csdn.net/qq_38593211)阅读数：61标签：[learing machine																[random forest																[gbdt																[xgboost																[ensemble learning](https://so.csdn.net/so/search/s.do?q=ensemble learning&t=blog)
个人分类：[机器学习](https://blog.csdn.net/qq_38593211/article/category/7811819)





# 集成学习（Ensemble Learning）

## 1.集成学习的分类
- Bagging：Bagging是由多个弱学习器集成起来的强学习器，但弱学习器之间没有依赖关系，可以并行生成，简单来说就是每个弱学习器学习数据的一部分特征，然后测试的时候根据结合策略就能得到一个数据总体特征然后分类。Bagging是基于自助采样法（bootstrap sampling）：给定包含m个样本的数据集，先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现，然后基于每个采样集训练出一个基学习器，再集成。
- Boosting：Boosting是让整个数据集以放入到一个弱学习器中进行学习，每个样本和弱分类器都有一个初始权重，之后根据训练结果的好坏对权重进行调整，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

## 2.集成学习的结合策略
- 平均法：当预测问题是连续数值的回归问题时常用。
- 投票法：取单层分类器分类结果的众数，一般还有阈值，也就是说众数达到一定数量才会判断为正确。
- 学习法：从初始训练集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当做样例标记，得到的新学习器作为集成学习器。

## 3.随机森林
- 方法：从样本集（假设样本集N个数据点）中重采样选出n个样本（有放回的采样，样本数据点个数仍然不变为N，之所以要有放回就是怕过拟合），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART），重复以上两步m次，获得m个分类器，最后根据这m个分类器的结合策略，决定数据属于哪一类**。**
- 优点：样本的随机（从样本集中用Bootstrap随机选取n个样本）和特征的随机性（从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树）由于特征子集是随机选择的，所以它能够处理很高维度（feature很多）的数据，并且不用做特征选择；训练速度快，容易做成并行化方法；在训练完后，它能够给出哪些feature比较重要。
- 缺点：随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合；对同一特征去取值过多会降低效果；超参数的调节不便。
- 实现：这里需要调节的超参数主要有：树的棵树、深度、最大分裂、叶子节点最少样本数

```python
#这里只是sklearn中的调用方法示例，不是完整程序
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier
# 使用默认参数初始化算法
# n_estimators是需要构建的树的数量
# 叶子节点最少样本数min_samples_leaf
#min_samples_split 当前节点允许分裂的最小样本数
alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)
kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)
```

## 4.AdaBoost
- 思路：对于AdaBoost就是对Boosting思想的一种实现，那只需要解决两个问题，第一就是在每一轮训练前如何取改变训练数据的权重和概率分布；第二就是如何对弱分类器进行组合。对于第二个问题不需要再阐述，第一个问题是提高前一轮被训练器错误分类样本的权值，加大分类误差率小的弱分类器的权值。
- 优点：泛化错误率低，易编码，可用在大部分弱分类器上，无参数调整。缺点：对离群点敏感。
- 算法框架：

          输入：T（训练集），弱分类算法；

          输出：分类器G（x）

          (1).初始化权值分布  ![D_{1}={w_{11}...w_{1i}...w_{1N}}](https://private.codecogs.com/gif.latex?D_%7B1%7D%3D%7Bw_%7B11%7D...w_%7B1i%7D...w_%7B1N%7D%7D)

          (2).对m=1,2......N：

                 使用具有权值分布的Dm的数据进行学习结果为Gm(x)

                 计算Gm(x)在训练集上的分类误差率Em

                 更新权值分布

          (3).构建基本分类器的线性组合

## 5.GBDT(Grandient Bossting Decison Tree)

    思路：与Adaboost不同的是GBDT的弱分类器用的是CART分类回归树模型，同时迭代的思路也不同，GBDT是基于上一轮迭代的残差进行训练的，损失函数为![L(y,f_{m-1}(x))](https://private.codecogs.com/gif.latex?L%28y%2Cf_%7Bm-1%7D%28x%29%29)，此时的y就是残差，m轮迭代得到![h_{m}(x)](https://private.codecogs.com/gif.latex?h_%7Bm%7D%28x%29)，所以损失函数为![L(y,f_{m-1}(x)+h_{m}(x))](https://private.codecogs.com/gif.latex?L%28y%2Cf_%7Bm-1%7D%28x%29&plus;h_%7Bm%7D%28x%29%29)。

   这里残差指的是什么呢？一般指![f_{m}(x)-f_{m-1}(x)](https://private.codecogs.com/gif.latex?f_%7Bm%7D%28x%29-f_%7Bm-1%7D%28x%29)，但考虑一个问题，这样的残差是否可以解决分类问题？答案是不可以的，所以我们就会用损失函数的负梯度值取表示残差，因为我们想让你得到的值加上原本预测的值接近真实值，所以要用负梯度，这样只要损失函数可求导就能得到残差：

![\frac{\partial L(y,f_{m}(x))}{f_{m}(x)}](https://private.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%20L%28y%2Cf_%7Bm%7D%28x%29%29%7D%7Bf_%7Bm%7D%28x%29%7D)

    设每一轮产生的单个分类器为![T(x,\Theta _{m})](https://private.codecogs.com/gif.latex?T%28x%2C%5CTheta%20_%7Bm%7D%29)，一共训练M次，我们把模型描述M个弱分类器线性相加为：

![F_{m}(x)=\sum_{m=1}^{M} T(x,\Theta _{m})](https://private.codecogs.com/gif.latex?F_%7Bm%7D%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20T%28x%2C%5CTheta%20_%7Bm%7D%29)

    对于每一次训练，模型为：

![f_{m}(x)=f_{m-1}(x) +T(x,\Theta _{m})](https://private.codecogs.com/gif.latex?f_%7Bm%7D%28x%29%3Df_%7Bm-1%7D%28x%29%20&plus;T%28x%2C%5CTheta%20_%7Bm%7D%29)

    损失函数：

![L(f(x),y)=L(y,f_{m-1}(x) +T(x,\Theta _{m}))](https://private.codecogs.com/gif.latex?L%28f%28x%29%2Cy%29%3DL%28y%2Cf_%7Bm-1%7D%28x%29%20&plus;T%28x%2C%5CTheta%20_%7Bm%7D%29%29)

    其中![\Theta _{m}](https://private.codecogs.com/gif.latex?%5CTheta%20_%7Bm%7D) 是第m个分类器的参数，GBDT用到的是SGD策略进行训练，每次训练得到的就是这个参数。

  我们已经知道GBDT一般弱分类器采用了单层决策树，那单层决策树就需要一个特征，这个特征如何去取就涉及到上篇文章介绍的CART切分特征的知识，总结一下GBDT的优缺点：

    (1).可以灵活处理各种类型的数据，包括连续值和离散值。

　(2).相比于普通决策树泛化能力更强，不易过拟合。

    缺点即是不能达到并行训练的高效率的优点。

## 6.Xgboost

    这个算法其实是对GBDT的一个优化实现，总结一下它的优势：
- xgboost在目标函数中加入了正则化项（与叶节点的个数和值有关）：

![L(\Phi )=\sum l(\widehat{y},y)+\sum \Omega (f_{k})](https://private.codecogs.com/gif.latex?L%28%5CPhi%20%29%3D%5Csum%20l%28%5Cwidehat%7By%7D%2Cy%29&plus;%5Csum%20%5COmega%20%28f_%7Bk%7D%29)

![\Omega (f_{k})=rT+\frac{1}{2}|w|^{2}](https://private.codecogs.com/gif.latex?%5COmega%20%28f_%7Bk%7D%29%3DrT&plus;%5Cfrac%7B1%7D%7B2%7D%7Cw%7C%5E%7B2%7D)
- GBDT中生成Fm(x)用的是一阶梯度的负方向来替代残差的，而Xgboost用了二阶泰勒展开。
- GBDT中寻找最佳分割点（选取特征）的衡量标准是最小均方误差，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。
- 当训练数据为稀疏矩阵时，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。

    现在的xgboost都被封装成了工具包，想调用可直接用sklearn。](https://so.csdn.net/so/search/s.do?q=xgboost&t=blog)](https://so.csdn.net/so/search/s.do?q=gbdt&t=blog)](https://so.csdn.net/so/search/s.do?q=random forest&t=blog)](https://so.csdn.net/so/search/s.do?q=learing machine&t=blog)




