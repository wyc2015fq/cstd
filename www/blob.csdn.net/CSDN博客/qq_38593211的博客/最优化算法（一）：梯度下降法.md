# 最优化算法（一）：梯度下降法 - qq_38593211的博客 - CSDN博客





2018年08月05日 15:36:26[小屁猴多看书](https://me.csdn.net/qq_38593211)阅读数：263








## 1.介绍

  梯度下降法的目标是：![min_{x\in R^{n}}f(x)](https://private.codecogs.com/gif.latex?min_%7Bx%5Cin%20R%5E%7Bn%7D%7Df%28x%29)，要找到一个![x=x^{*}](https://private.codecogs.com/gif.latex?x%3Dx%5E%7B*%7D)使得目标成立，一般选取初值![x^{(0)}](https://private.codecogs.com/gif.latex?x%5E%7B%280%29%7D)然后不断迭代，更新x的值，直到收敛，那更新的方向就是函数的负梯度方向，因为这样函数下降的最快，这也就是为什么较梯度下降算法的原因。

![f(x)](https://private.codecogs.com/gif.latex?f%28x%29)必须具备一个条件，就是一阶连续可导，我们把它在第k次迭代![x^{(k)}](https://private.codecogs.com/gif.latex?x%5E%7B%28k%29%7D)方向上进行泰勒一节展开：

![f(x)=f(x^{(k)})+g_{k}^{T}(x-x^{(k)})](https://private.codecogs.com/gif.latex?f%28x%29%3Df%28x%5E%7B%28k%29%7D%29&plus;g_%7Bk%7D%5E%7BT%7D%28x-x%5E%7B%28k%29%7D%29)

       此时![g_{k}](https://private.codecogs.com/gif.latex?g_%7Bk%7D)就是函数在此点的梯度，然后可以求出k+1次的迭代值![x^{k+1}=\lambda _{k}p_{k}](https://private.codecogs.com/gif.latex?x%5E%7Bk&plus;1%7D%3D%5Clambda%20_%7Bk%7Dp_%7Bk%7D),其中lamda是步长，pk是负梯度方向。

## 2.流程

       输入：目标函数f(x)，梯度函数g(x)，精度e

       输出：f(x)的极小点![x=x^{*}](https://private.codecogs.com/gif.latex?x%3Dx%5E%7B*%7D)；

        （1）.取初始值![x^{(0)}](https://private.codecogs.com/gif.latex?x%5E%7B%280%29%7D)，k=0；

        （2）.计算![f(x^{(k)})](https://private.codecogs.com/gif.latex?f%28x%5E%7B%28k%29%7D%29)；

       （3）.计算梯度值![g_{k}](https://private.codecogs.com/gif.latex?g_%7Bk%7D)，当梯度值的绝对值小于精度或者迭代次数达到最大停止迭代，否则按![f(x)=f(x^{(k)})+g_{k}^{T}(x-x^{(k)})](https://private.codecogs.com/gif.latex?f%28x%29%3Df%28x%5E%7B%28k%29%7D%29&plus;g_%7Bk%7D%5E%7BT%7D%28x-x%5E%7B%28k%29%7D%29)        更新，k=k+1。

       注意：一般当目标函数为凸函数时，梯度下降法为全局最优解，对于复杂函数并不一定。

## 3.实现

         对于梯度下降有两种实现方法：

      批量梯度下降：每次遍历所有的样本取最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。

　 随机梯度下降：每次遍历选取一个样本，最小化这个样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。



