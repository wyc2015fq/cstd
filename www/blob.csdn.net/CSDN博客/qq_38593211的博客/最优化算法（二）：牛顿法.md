# 最优化算法（二）：牛顿法 - qq_38593211的博客 - CSDN博客





2018年08月05日 16:21:26[小屁猴多看书](https://me.csdn.net/qq_38593211)阅读数：620








## 1.推导

       牛顿法和拟牛顿法是求解无约束最优化问题的常用方法，它们比梯度下降收敛更快。考虑同样的一个无约束最优化问题：

![min_{x\in R^{n}}f(x)](https://private.codecogs.com/gif.latex?min_%7Bx%5Cin%20R%5E%7Bn%7D%7Df%28x%29)

       其中f(x)具有二阶连续偏导数的性质，如果k次迭代值为![x^{(k)}](https://private.codecogs.com/gif.latex?x%5E%7B%28k%29%7D)，则可进行二阶泰勒展开：

![f(x)=f(x^{(k)})+g_{k}^{T}(x-x^{(k)})+1/2H(x^{(k)})(x-x^{(k)})^{2}](https://private.codecogs.com/gif.latex?f%28x%29%3Df%28x%5E%7B%28k%29%7D%29&plus;g_%7Bk%7D%5E%7BT%7D%28x-x%5E%7B%28k%29%7D%29&plus;1/2H%28x%5E%7B%28k%29%7D%29%28x-x%5E%7B%28k%29%7D%29%5E%7B2%7D)

      上述公式里面的值不解释了，就是一阶导和二阶导（也称海塞矩阵![H(X)](https://private.codecogs.com/gif.latex?H%28X%29)在此点的值），函数有极值的必要条件就是在极值点      处一阶导数为0。牛顿法的每次迭代就是让一阶导为零，即满足：

![f^{'}(x^{(k+1)})=0](https://private.codecogs.com/gif.latex?f%5E%7B%27%7D%28x%5E%7B%28k&plus;1%29%7D%29%3D0)

       而上式根据泰勒一阶导等于：

![f^{'}(x^{(k+1)})=g_{k}+H(x-x^{(k)})=0](https://private.codecogs.com/gif.latex?f%5E%7B%27%7D%28x%5E%7B%28k&plus;1%29%7D%29%3Dg_%7Bk%7D&plus;H%28x-x%5E%7B%28k%29%7D%29%3D0)

       根据这一步就能得到迭代公式：

![x^{(k+1)}=x^{(k)}-H^{-1}_{k}g_{k}](https://private.codecogs.com/gif.latex?x%5E%7B%28k&plus;1%29%7D%3Dx%5E%7B%28k%29%7D-H%5E%7B-1%7D_%7Bk%7Dg_%7Bk%7D)

## 2.对比

     为什么牛顿法更快呢？我和网上其他的想的不太一样，我认为是因为每次迭代，牛顿法都能找到当前的极小值，而不是单纯找到当前下降最快的部分，直接走到当前能走的最低点，在下一次迭代中，换一个点继续求解。

## 3.拟牛顿法

    拟牛顿法就是对牛顿法的计算上加以简化，因为牛顿法每次会求海塞矩阵的逆矩阵，比较麻烦，所以它会用一个近似矩阵       G（x）替代H（x）的逆矩阵，所以拟牛顿法不需要二阶导数的信息，有时比牛顿法更为有效。 常用的拟牛顿实现方法有DFP和BFGS.。具体的推导有兴趣可以见统计学习方法P220，这个算法我用得不多，所以没有细究。



