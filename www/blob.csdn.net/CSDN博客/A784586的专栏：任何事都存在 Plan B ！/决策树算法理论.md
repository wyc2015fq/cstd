# 决策树算法理论 - A784586的专栏：任何事都存在 Plan B ！ - CSDN博客





2017年04月22日 16:29:03[QuJack](https://me.csdn.net/A784586)阅读数：552








**决策树算法理论**


0. 机器学习中**分类和预测算法**的评估：

- 准确率
- 速度
- 强壮行
- 可规模性
- 可解释性

1. 什么是**决策树/判定树（decision tree)**?
 判定树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。

下图展示根据天气等特征某天玩不玩某种运动？

![](https://img-blog.csdn.net/20170422161602139?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQTc4NDU4Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)








2.  机器学习中分类方法中的一个重要算法




3.  构造决策树的基本算法  【判断一个人是否会买电脑？】          

![](https://img-blog.csdn.net/20170422161737661?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQTc4NDU4Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)





     3.1 **熵（entropy）**概念：

信息和抽象，如何度量？

          1948年，香农提出了 ”**信息熵(entropy)**“的概念

          一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者          

          是我们一无所知的事情，需要了解大量信息==>**信息量的度量**就等于**不确定性的多少**

**例子：猜世界杯冠军，假如一无所知，猜多少次？**每个队夺冠的几率不是相等的；

**比特(bit)**来衡量信息的多少。

以下是熵的公式：

![](https://img-blog.csdn.net/20170422165704411?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQTc4NDU4Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


如上图公式，若各个概率相等则，最终值为6.

如果概率不同，值会小余6。猜32个球队，32个值，用二分法猜就是6次。

**变量的不确定性越大，熵也就越大**



     3.1 决策树归纳算法 （ID3）

**1970-1980， J.Ross.Quinlan, ID3算法**

          选择属性判断结点

**信息获取量(Information Gain)：Gain(A) = Info(D) - Infor_A(D)**

          通过A来作为节点分类获取了多少信息

![](https://img-blog.csdn.net/20170422165905172?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQTc4NDU4Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


           类似，Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048

          所以，选择age作为第一个根节点

![](https://img-blog.csdn.net/20170422170015912?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQTc4NDU4Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


          重复以上建树的过程即可。




**          算法描述：**

> - 树以代表训练样本的单个结点开始（步骤1）。
- 如果样本都在同一个类，则该结点成为树叶，并用该类标号（步骤2 和3）。
- 否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性（步骤6）。该属性成为该结点的“测试”或“判定”属性（步骤7）。在算法的该版本中，
- 所有的属性都是分类的，即离散值。连续属性必须离散化。
- 对测试属性的每个已知的值，创建一个分枝，并据此划分样本（步骤8-10）。
- 算法使用同样的过程，递归地形成每个划分上的样本判定树。一旦一个属性出现在一个结点上，就不必该结点的任何后代上考虑它（步骤13）。
- 递归划分步骤仅当下列条件之一成立停止：
- (a) 给定结点的所有样本属于同一类（步骤2 和3）。
- (b)没有剩余属性可以用来进一步划分样本（步骤4）。在此情况下，使用**多数表决**（步骤5）。
- 这涉及将给定的结点转换成树叶，并用样本中的多数所在的类标记它。替换地，可以存放结
- 点样本的类分布。
- (c) 分枝
- test_attribute=ai没有样本（步骤11）。在这种情况下，以samples中的多数类
- 创建一个树叶（步骤12）


     3.1 其他算法：

               C4.5:  Quinlan

               Classification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)

               共同点：**都是贪心算法，自上而下**(Top-down approach)

               区别：**属性选择度量方法不同**： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain)

     3.2 如何处理连续性变量的属性？ 




4. 树剪枝叶 （避免overfitting)

     4.1 先剪枝

     4.2 后剪枝

5. 决策树的**优点**：

     直观，便于理解，小规模数据集有效     

6. 决策树的**缺点**：

     处理连续变量不好

     类别较多时，错误增加的比较快

     可规模性一般

------------------------------------------------

算法实现请看下一篇博客！

[决策树算法具体实现【基于Python实现】【点我】](http://blog.csdn.net/a784586/article/details/70473523)







