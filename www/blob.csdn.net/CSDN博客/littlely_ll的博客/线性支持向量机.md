# 线性支持向量机 - littlely_ll的博客 - CSDN博客





2017年04月16日 14:37:30[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：2136








支持向量机可以分为：线性可分支持向量机，线性支持向量机和非线性支持向量机，本次内容只讲述线性可分支持向量机和线性支持向量机。支持持向量机是一种强大的分类学习算法，即可以解决二分类也可以解决多分类问题，本次只讲述二分类的问题。

# 线性可分支持向量机

线性可分支持向量机是要学习一个超平面，而这个超平面能把正例和负例完全分开，但是这样的超平面可能有无数多个，线性可分支持向量机利用间隔最大化求分离超平面，这时，解是唯一的。 

通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为： 


$w^*\cdot x+b^*=0$

以及相应的分类决策函数： 


$f(x)=sign(w^*\cdot x+b^*)$

称为线性可分支持向量机。
## 函数间隔和几何间隔

**函数间隔：**对于给定的训练集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为 


$\hat{\gamma_i}=y_i(w\cdot x_i+b)$

定义超平面$(w,b)$关于训练集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔只最小值，即 


$\hat{\gamma}=min_{i=1,...,N}\hat{\gamma_i}$

对分离超平面的法向量$w$进行规范化，就得到集合间隔 
**几何间隔：**对于给定的训练集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为 


$\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})$

定义超平面$(w,b)$关于训练集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔只最小值，即 


$\gamma=min_{i=1,...,N}\gamma_i$

从上述看出，几何间隔与函数间隔只差了一个乘积$\frac{1}{||w||}$。
要间隔最大化，只需要最大化间隔，可以表述为下面的约束最优化问题： 


$max_{w,b}\quad\gamma\\s.t.\quad y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})\ge \gamma, i=1,2,\ldots,N$

考虑几何间隔与函数间隔的关系，则可将问题改成 


$max_{w,b}\quad\frac{\hat{\gamma}}{||w||}\\s.t.\quad y_i(w\cdot x_i+b)\ge \hat{\gamma}, i=1,2,\ldots,N$

函数间隔$\hat{\gamma}$取值不影响最优化问题的求解，所以取$\hat{\gamma}=1$，而注意到最大化$\frac{1}{||w||}$和最小化$1/2||w||^2$等价，故得到线性可分支持向量机学习的最优化问题 


$min_{w,b}\quad \frac{1}{2}||w||^2\\s.t.\quad y_i(w\cdot x_i+b)-1\ge 0, i=1,\ldots,N$
## 线性可分支持向量机算法（最大间隔法）

输入：线性可分训练集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中，$x_i \in \chi=R^n,y_i\in Y=\{1,-1\},i=1,2,\cdots,N$； 

输出：最大间隔分离超平面和分类决策函数。 

（1）构造并求解约束最优化问题： 


$min_{w,b}\quad \frac{1}{2}||w||^2\\s.t.\quad y_i(w\cdot x_i+b)-1\ge 0, i=1,\ldots,N$

求得最优解$w^*,b^*$。 

（2）由此得到分离超平面： 


$w^*\cdot x+b^*=0$

分类决策函数为 


$f(x)=sign(w^*\cdot x+b^*)$

现在的问题是怎样求解$w^*,b^*$，只要把$w^*,b^*$求解出，那么自然而然就得到了分离超平面和分类决策函数。在求解$w^*,b^*$之前，先看看对偶问题
## 拉格朗日对偶性

在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转化为对偶问题，通过对偶问题而得到原始问题的解。 

1. 原始问题 

假设$f(x),c_i(x),h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题： 


$min_{x\in R^n}\quad f(x)\\s.t.\quad c_i(x)\le 0, i=1,2,...,k\\h_j(x)=0,j=1,2,...,l$

此约束最优化问题就是原始问题。 

引进拉格朗日函数 


$L(x,\alpha,\beta)=f(x)+\Sigma_{i=1}^k\alpha_ic_i(x)+\Sigma_{j=1}^l\beta_jh_j(x)$
$\alpha_i,\beta_j$为拉格朗日乘子，$\alpha_i\ge 0 $,考虑x的函数： 


$\theta_p(x)=max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)$

下标P表示原始问题。 

如果x满足KKT条件，则有上两式知$\theta_p(x)=f(x)$

考虑$f(x)$的极小化问题为 


$min_{x}\theta_p(x)=min_xmax_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)$

原始问题的最优值 


$p^*=min_x\theta_p(x)$

2. 对偶问题 

定义： 


$\theta_D(\alpha,\beta)=min_xL(x,\alpha,\beta)$

然后再极大化$\theta_D(\alpha,\beta)$，即 


$\theta_D(\alpha,\beta)=max_{\alpha,\beta:\alpha_i\ge0 }min_xL(x,\alpha,\beta)$

问题$max_{\alpha,\beta:\alpha_i\ge0 }min_xL(x,\alpha,\beta)$称为广义拉格朗日函数的极大极小问题。 

将广义拉格朗日函数的极大极小问题表示为约束最优化问题： 


$max_{\alpha,\beta}\theta_D(\alpha,\beta)=max_{\alpha,\beta:\alpha_i\ge0 }min_xL(x,\alpha,\beta)\\s.t. \alpha_i\ge0,i=1,2,...,k$

称为原始问题的对偶问题，对偶问题的最优值 


$d^*=max_{\alpha,\beta:\alpha_i\ge0}\theta_D(\alpha,\beta)$
**即由原始的极小极大值问题转化为极大极小的对偶问题**
**定理：**若原始问题和对偶问题都有最优值，则 


$d^*=p^*$
**定理：**对于原始问题和对偶问题，假设$f(x)和c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且不等式约束$c_i(x)$是严格可行的，则$x^*和\alpha^*,\beta^*$分别是原始问题和对偶问题的解的充分必要条件是它们满足KKT条件： 


$\nabla_xL(x^*,\alpha^*,\beta^*)=0\\\nabla_\alpha L(x^*,\alpha^*,\beta^*)=0\\\nabla_\beta L(x^*,\alpha^*,\beta^*)=0\\\alpha_i^*c_i(x^*)=0, i=1,2,...k\\c_i(x^*)\le0,i=1,2,...,k\\\alpha_i^*\ge0,i=1,2,...,k\\h_j(x^*),j=1,2,...,l$
## 学习的对偶算法

首先构建拉格朗日函数： 


$L(w,b,\alpha)=\frac{1}{2}||w||^2-\Sigma_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\Sigma_{i=1}^N\alpha_i$

其中$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)^T$为拉格朗日乘子。 

根据对偶性，原始问题的对偶问题为极大极小问题： 


$max_\alpha min_{w,b}L(w,b,\alpha)$

（1）求$min_{w,b}L(w,b,\alpha)$

将拉格朗日函数对$w,b$分别求导并等于0，得 


$w=\Sigma_{i=1}^N\alpha_iy_ix_i\\\Sigma_{i=1}^N\alpha_iy_i=0$

代入到拉格朗日函数中得： 


$L(w,b,\alpha)=-\frac{1}{2}\Sigma_{i=1}^N\Sigma_{i=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\Sigma_{i=1}^N\alpha_i$

（2）求$min_{w,b}L(w,b,\alpha)对\alpha$的极大 


$max_{\alpha}-\frac{1}{2}\Sigma_{i=1}^N\Sigma_{i=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\Sigma_{i=1}^N\alpha_i\\s.t.\quad \Sigma_{i=1}^N\alpha_iy_i=0\\   \alpha_i\ge0,i=1,2,...,N$

稍微转化一下 


$min_{\alpha}\frac{1}{2}\Sigma_{i=1}^N\Sigma_{i=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\Sigma_{i=1}^N\alpha_i\\s.t.\quad \Sigma_{i=1}^N\alpha_iy_i=0\\   \alpha_i\ge0,i=1,2,...,N$
最终求得$w^*,b^*$为： 


$w^*=\Sigma_{i=1}^N\alpha^*_iy_ix_i\\b^*=y_j-\Sigma_{i=1}^N\alpha^*_iy_i(x_i\cdot x_j)$

## 线性可分支持向量机学习算法

输入：线性可分训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中$x_i\in \chi =R^n,y\in Y=\{1,-1\},i=1,2,...,N$; 

输出：分离超平面和分类决策函数。 

（1）构造并求解约束最优化问题 


$min_{\alpha}\frac{1}{2}\Sigma_{i=1}^N\Sigma_{i=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\Sigma_{i=1}^N\alpha_i\\s.t.\quad \Sigma_{i=1}^N\alpha_iy_i=0\\   \alpha_i\ge0,i=1,2,...,N$

   求得最优解$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_N)^T$

（2）计算 


$w^*=\Sigma_{i=1}^N\alpha^*_iy_ix_i$

选择$\alpha^*的一个正分量\alpha^*\gt0$（因为是唯一解，所以只要选择其中一个$\alpha^*\gt0$就行），计算 


$b^*=y_j-\Sigma_{i=1}^N\alpha^*_iy_i(x_i\cdot x_j)$
# 线性支持向量机

而对于线性支持向量机，就是在线性可分支持向量机的基础上加了一个松弛变量$\xi$,这时约束条件为$y_i(w\cdot x_i+b)\ge 1-\xi_i$,而目标函数变为$1/2||w||^2+C\Sigma_{i=1}^N\xi_i$。 

而最终得到的对偶问题为： 


$max_{\alpha}-\frac{1}{2}\Sigma_{i=1}^N\Sigma_{i=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\Sigma_{i=1}^N\alpha_i\\s.t.\quad \Sigma_{i=1}^N\alpha_iy_i=0\\C-\alpha_i-\mu_i=0\\   \alpha_i\ge0\\\mu_i\ge0,i=1,2,...,N$

最后三式可写为$0\le \alpha_i\le C$
## 线性支持向量机算法

输入：线性可分训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中$x_i\in \chi =R^n,y\in Y=\{1,-1\},i=1,2,...,N$; 

输出：分离超平面和分类决策函数。 

（1）选择惩罚系数$C>0$，构造并求解凸二次规划问题 


$min_{\alpha}\frac{1}{2}\Sigma_{i=1}^N\Sigma_{i=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\Sigma_{i=1}^N\alpha_i\\s.t.\quad \Sigma_{i=1}^N\alpha_iy_i=0\\  C\ge \alpha_i\ge0,i=1,2,...,N$

 求得最优解$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_N)^T$

 （2）  计算 


$w^*=\Sigma_{i=1}^N\alpha^*_iy_ix_i$

选择$\alpha^*的一个分量\alpha_j^*适合条件0<\alpha_j^*<C$，计算 


$b^*=y_j-\Sigma_{i=1}^N\alpha^*_iy_i(x_i\cdot x_j)$

（3）求得分离超平面 


$w^*\cdot x+b^*=0$

分类决策函数为 


$f(x)=sign(w^*\cdot x+b^*)$

步骤（2）中求得b的值不唯一，故可以取在所有符合条件的样本点上的平均值。
# R语言实现

R有多种实现SVM的方法，例如`kernlab`包的`ksvm`，主要是基于核函数的；`svmadmm`包的`svm`主要是基于ADMM 和 IADMM算法的线性和非线性函数，另外还有最常用的`caret`包也有实现SVM方法的。

**参考**

[1] 李航 《统计学习方法》 

[2] 周志华 《机器学习》 

[3] 韩家伟《数据挖掘：概念与技术》













