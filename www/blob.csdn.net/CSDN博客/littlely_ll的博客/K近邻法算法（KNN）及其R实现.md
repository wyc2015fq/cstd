# K近邻法算法（KNN）及其R实现 - littlely_ll的博客 - CSDN博客





2016年12月21日 20:50:51[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：5509标签：[K近邻算法																[R语言](https://so.csdn.net/so/search/s.do?q=R语言&t=blog)](https://so.csdn.net/so/search/s.do?q=K近邻算法&t=blog)
个人分类：[机器学习](https://blog.csdn.net/littlely_ll/article/category/6601963)








## 1. K近邻算法

输入：训练数据集 


$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$

其中，$x_i \in \chi \subseteq R^n$为实例的特征向量，$y_i\in Y=\{c_1,c_2,\cdots,c_K\}$为实例的类别，$i=1,2,\cdots,N;$实例特征向量x； 

输出：实例x所属的类y。 

（1）根据给定的距离度量，在训练数据集T中找出与x最近邻的k个点，涵盖这k个点的x的邻域记做$N_k(x);$

（2）在$N_k(x)$中根据分类决策规则（如多数表决）决定x 的类别y： 


$y=arg \quad max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j),i=1,2,\cdots,N; j=1,2,\cdots,K$

上式中，I为指示函数，当$y_i=c_j$时I为1，否则为0.
## 2 距离度量

设特徵空间$\chi$是n维实数向量空间$R^n$，$x_i,x_j\in\chi,x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,$$x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T, x_i,x_j$的距离定义为 

闵可夫斯基距离（Minkowski distance）： 


$L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}， p\ge 1$

欧氏距离（Euclidean distance）: 


$L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$

曼哈顿距离（Manhattan distance）: 


$L_1(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|$

切比雪夫距离（Chebyshev distance）: 


$L_\inf(x_i,x_j)=max_{l=1}|x_i^{(l)}-x_j^{(l)}|$
## 3. 分类决策规则

分类损失函数为0-1损失函数，分类函数为 


$f: R^n\rightarrow\{c_1,c_2,\cdots,c_K\}$

那么误分类的概率为 


$P(Y\ne f(X))=1-P(Y=f(X))$

对给定的实例$x\in\chi$,其最近邻的k个训练实例点构成集合$N_k(x).$如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是 


$\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\ne c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i=c_j)$

要使经验风险最小，则使$\sum_{xi\in N_k(x)}I(y_i=c_j)$最大。
## 4. K近邻算法的实现：kd树

### 4.1 构造kd树

算法（构造平衡kd树）： 

输入：k维空间数据集$T=\{x_1,x_2,\cdots,x_N\}$, 

其中$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(k)})^T,i=1,2,\cdots,N;$

输出: kd树。 

（1）开始：构造根节点，根节点对应于包含T的k维空间的超矩形区域。选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根节点对应的超矩形区域切分成两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。 
$\quad$由根节点生成深度为1的左、右子节点：左子节点对应坐标$x^{(1)}$小于切分点的子区域，右子节点对应于坐标$x^{(1)}$大于切分点的子区域。 
$\quad$将落在切分超平面上的实例点保存在根节点。 

（2）重复：对深度为j的的节点，选择$x^{(l)}$为切分的坐标轴，$l=j(modk)+1$，以该节点的去榆中所有实例的$x^{(l)}$坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。 
$\quad$由该节点生成深度为j+1的左、右子节点：左子节点对应坐标$x^{(l)}$小于切分点的子区域，右子节点对应于坐标$x^{(l)}$大于切分点的子区域。将落在切分超平面上的实例点保存在该节点。 

（3）直到两个子区域没有实例存在时停止，从而形成kd树的区域划分。
### 4.2 搜索kd树

算法（用kd树的最近邻搜索）： 

输入：以构造的kd树；目标点x； 

输出：x的最近邻。 

（1）在kd树中找出包含目标点x的叶节点：从根节点出发，递归地乡下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到子节点为叶节点为止。 

（2）以此叶节点为“当前最近点”。 

（3）递归地向上回退，在每个节点进行以下操作： 
$\quad$（a）如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。 
$\quad$（b）当前最近点一定存在与该节点一个子节点对应的区域。检查该子节点的父节点的另一个子节点对应的区域是否有更近的点。具体地，检查另一子节点对应的区域是否与以目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子节点对应的区域内存在距离目标点更近的点，移动到另一个子节点。接着，递归进行最近邻搜索；如果不相交，向上回退。 

（4）当回退到根节点时，搜索结束，最后的“当前最近点”即为x的最近邻点。
## 5. KNN算法的优缺点

优点： 

 - 简单有效 

 - 对数据的分布没有要求 

 - 训练阶段很快 

缺点： 

 - 不产生模型，在发现特征之间关系上的能力有限 

 - 分类阶段很慢 

 - 需要大量的内存 

 - 名义变量（特征）和缺失数据需要额外处理
## 6. R语言中的KNN算法实现

**KNN分类语法**

应用class包中的函数knn()： 

创建分类器并进行预测： 
`p <- knn(train,test,class,k)`

 - train: 一个包含数值型训练数据的数据框 

 - test: 一个包含数值型测试数据的数据框 

 - class(cl): 包含训练数据每一行分类的一个因子向量 

 - k: 标识最近邻数目的一个整数 

该函数返回一个因子向量，该向量含有测试数据框中的每一行的预测分类。










