# 神经网络在文本分类中的应用 - littlely_ll的博客 - CSDN博客





2018年01月24日 15:26:48[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：2740
所属专栏：[自然语言处理](https://blog.csdn.net/column/details/18554.html)









在自然语言的文本分类中，主要使用两类模型，一类是使用传统的机器学习模型，如朴素贝叶斯，最大熵，支持向量机等，第二类就是使用神经网络模型，包括CNN和RNN。传统的机器模型在分类前首先要做特征工程，例如把文本转换成词袋，并转化为TF-IDF矩阵，然后再做分类。而使用神经网络模型可以使它自己提取特征并进行文本分类，并能获得优于传统机器学习模型的能力。

# CNN模型的文本分类

CNN原来是用于对图像分类，后来按照其形式用到了对自然语言处理上，处理原理相同，首先是对句子的每一个词生成一个实值的词向量，然后按照句子合并成一个词向量矩阵，这个词向量矩阵就相当于一个图像的像素，剩下的就如同图像处理一样，使用卷积核进行卷积以及进行池化等。

Kim使用了CNN对句子进行分类。具体模型形式如下：
![这里写图片描述](https://img-blog.csdn.net/20180123142853812?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
让$x_i\in R^k$为一个句子中第$i$个单词的词向量，向量维度为$k$，句子的长度为$n$，则整个句子的向量为：
$$(1)x_{1:n} = x_1\oplus x_2\oplus\cdots\oplus x_n\tag{1}$$

其中，$\oplus$为连接算子，则$x_{1:n}$的维度为$nk$。

让$x_{i:i+j}$为词向量$x_i,x_{i+1},\cdots,x_{i+j}$的连接，卷积核为$w\in R^{hk}$，其中，$h$为卷积的窗口大小，则从窗口中的词向量产生的特征$c_i$为：
$$(2)c_i=f(w\cdot x_{i:i+h}+b)\tag{2}$$

其中，$b\in R$为偏置，$f$为非线性函数，例如$tanh$等。然后，卷积核应用于句子的每个可能的窗口$\{x_{1:h},x_{2:h+1},\cdots,x_{n-h+1:n}\}$，产生一个特征图：
$$(3)c = [c_1,c_2,\cdots,c_{n-h+1}]\tag{3}$$

其中，$c\in R^{n-h+1}$。然后对特征图进行最大池化操作来获取最重要的特征$\hat c=max\{c\}$

为了获取多个特征可以使用多个卷积核。Kim使用了2个通道，每个通道用2个卷积核，这样共生成4个特征图。这两个通道中，一个是在训练中保持不变，即词向量是不变的，另一个通道在训练中通过后向传播对词向量进行修正。

Kim对最大池化层进行了dropout正则化，假设得到的最大池化层为$z=[\hat c_1, \cdots, \hat c_m]$，则前向传播中，dropout使用：
$$(4)y = w\cdot(z\circ r)+b\tag{4}$$

其中，$\circ$为按元素乘积，$r\in R^m$为以概率$p$为1的Bernnoulli随机变量，此向量又被称为“遮盖向量”，也就是在梯度后向传播中不经过这些遮盖住的单元。在测试的时候，学习权重变为$\hat w = pw$，这个$\hat w$用于预测新的句子 。Kim又对$w$做了约束为$||w||_2=s$，即在梯度下降中，当$||w||_2&gt;s$时把$||w||_2$设置为$s$。
**一些技巧：**
> - 使用多个卷积核以获取多个特征
- 使用dropout
- 使用word2vec初始化词向量（也可以使用GloVe）
- 梯度下降使用Adadelta或Adagrad（两个相差不大）


另一个具有相似的CNN模型是Zhang提出的。仍然是先把句子转化成单词向量组成的句子矩阵，每一行为一个词向量，可由word2vec或GloVe训练出，维度为$d$，则句子矩阵的维度为$s\times d$

与Kim不太一样的是，Zhang并没有把句子转化为一个向量，而是仍然按照句子矩阵的形式排列，这样就相当于一个图像，由于每一行代表一个离散的特征，所以在选卷积核的时候，对应核的长度就是这个词向量的维度$d$，而核的高度（region size）可由自己确定。假设卷积核$w$的高度为$h$，则$w$的维度为$h\times d$。句子矩阵为$A\in R^{s\times d}$，用$A[i:j]$表示行$i$至行$j$，则经过卷积之后的输出为：
$$(5)o_i=\Sigma (w\oplus A[i:i+h-1])\tag{5}$$

即矩阵对应元素乘积再求和。而整个卷积后输出序列为$o=[o_1,\cdots,o_{s-h+1}]$。

然后做了个变换，即：
$$(6)c_i = f(o_i+b)\tag{6}$$

得到特征图$c=[c_1,\cdots,c_{s-h+1}]$，然后再进行最大池化操作，具体可看下图：
![卷积展示](https://img-blog.csdn.net/20180123155830725?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

Zhang使用了3个卷积宽度（region size），分别是（2，3，4），每个region size使用了2个不同的卷积核，总共6个卷积核，然后用这些卷积核对句子矩阵做卷积得到6个特征图，然后对每个region size中的2个特征图分别取最大值，即做最大池化操作，再然后把每个region size中最大池化操作后的值拼接起来，再然后把不同的region size中拼接后的值再拼接起来，最后使用softmax函数进行分类。这样的CNN可以处理不同句子长度以及不同region size的情况。
**经验技巧：**
> - 词向量初始化很重要（word2vec或GloVe初始化）
- 卷积核高度对结果有影响
- 卷积核的个数对模型有较大影响，应多试几个
- 最大池化操作比其他池化操作要好
- 考虑不同的激活函数（这个是在卷积层后的那一步非线性转换）
- 考虑正则化（如dropout）
- 考虑方差（可以使用cross validation）


Kalchbrenner提出了另一种动态卷积神经网络（DCNN），主要的变化是在最大池化层，最大池化层使用的是Dynamic k-max-pooling，另外又加了一个folding层，这样能捕获相邻行的特征，具体可看A Convolutional Neural Network for Modelling Sentences

# RNN模型的文本分类

Lai使用了一种RNN模型进行文本分类，这个模型结构比较奇葩，一共分为三层，首先使用的是类似双向RNN的结构当做卷积层，第二层为最大池化层，最后一层是输出层，具体如下。

使用$c_l(w_i)$表示单词$w_i$的左边的上下文，使用$c_r(w_i)$表示$w_i$的右边的上下文，当然$c_l(w_i)$和$c_r(w_i)$都是实值向量，维度为$|c|$。单词$w_i$左边和右边的上下文向量分别有下式计算：
$$(1)c_l(w_i)=f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))\tag{1}$$
$$(2)c_r(w_i)=f(W^{(l)}c_r(w_{i+1})+W^{(sr)}e(w_{i+1}))\tag{2}$$

其中，$e(w_{i})$表示第$i$个词的词向量，$W^{(l)}$为从一个隐藏层（上下文）到下一个隐藏层的参数矩阵，W^{(sl)}为当前词与左边上下文的参数矩阵，$f$诶非线性激活函数，对于$c_r(w_i)$类似。第一个单词的左边上下文默认为$c_l(w_1)$，对于最后一个单词的右边上下文默认为$c_r(w_n)$。

那么，现在$w_i$的词向量可以表示为左右上下文和原来词向量的连接，即$x_i=[c_l(w_i);e(w_i);c_r(w_i)]$而不是$e(w_i)$了。然后再进行一个非线性转换：
$$(3)y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})\tag{3}$$

这个$y_i^{(2)}$称为潜在语义向量。

那么这整个的就算做第一层了，可以见下图：
![这里写图片描述](https://img-blog.csdn.net/20180124103018302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
接着第二层，使用了一个最大池化层：
$$(4)y^{(3)}=max_{i=1}^ny_i^{(2)}\tag{4}$$

即$y^{(3)}$的第$k$个元素为$y_i^{(2)}$的第$k$个元素的最大值。

最后，输出层为：
$$(5)y^{(4)}=W^{(4)}y^{(3)}+b^{(4)}\tag{5}$$

把输出转化为概率：
$$(6)p_i=\frac{exp(y_i^{(4)})}{\Sigma_{k=1}^nexp(y_k^{(4)})}\tag{6}$$

##添加注意力向量

另一种文本分类中，是使用注意力机制来捕获相关特征。我在[神经网络机器翻译总结](http://blog.csdn.net/littlely_ll/article/details/79026870)中详细说明了注意力机制的内容。Yang等人就是使用了双向GRU+注意力机制来进行文本分类的。这个模型主要分两大块：一是单词编码，在句子水平上获得词注意力向量；二是句子编码，在文档水平上获得句子注意力向量。最后解码输出，图示如下：
![这里写图片描述](https://img-blog.csdn.net/20180124105441128?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
首先来看单词编码。假设一个文档中第$i$个句子中的单词为$w_{it}， t\in[0,T]$，它的词向量为$x_ij$。然后把它输入到双向GRU：
$$\overrightarrow{h}_{it}=\overrightarrow{GRU}(x_{it}), t\in [1,T],\\\overleftarrow{h}_{it}=\overleftarrow{GRU}(x_{it}),t\in [T,1]$$

然后把得到的$\overrightarrow{h}_{it},\overleftarrow{h}_{it}$拼接起来就得到单词$w_{it}$的annotation$h_{it}=[\overrightarrow{h}_{it},\overleftarrow{h}_{it}]$，这样就能得到整个句子的信息。

在加入attention的时候与Bahdanau一样：
$$(6)u_{it}=tanh(W_wh_{it}+b_w)\\ \alpha_{it}=\frac{exp(u_{it}^{&#x27;}u_w)}{\Sigma_texp(u_{it}^{&#x27;}u_w)}\\ s_i = \Sigma_t\alpha_{it}h_{it}\tag{6}$$
上面三个式子中，首先对$h_{it}$进行非线性变换得到它的隐藏表达$u_{it}$，然后计算$u_{it}$与上下文向量$u_w$(随机初始化的，后期要训练)的相似性并标准化得到annotation的权重$\alpha_{it}$，最后计算句子向量$s_i$。至此，句子的解析就结束了，下一部分是句子的编码。

仍然是相同的操作，使用双向GRU编码句子，只是这里的输入为$s_i$：
$$\overrightarrow{h}_i=\overrightarrow{GRU}(s_i),i\in [1,L],\\\overleftarrow{h}_i=\overleftarrow{GRU}(s_i),t\in[L,1]$$

其中，$L$为某个文档中句子的个数。然后拼接起来作为第$i$个句子的annotation$h_i=[\overrightarrow{h}_i,\overleftarrow{h}_i]$，那么$h_i$就综合了该句子上下文句子的信息。

然后加入句子的attention：
$$(7)u_i=tanh(W_sh_i+b_s)\\\alpha_i=\frac{exp(u_i^{&#x27;}u_s)}{\Sigma_iexp(u_i^{&#x27;}u_s)}\\v=\Sigma_i\alpha_ih_i\tag{7}$$

意义与解析单词编码的一样。这个$v$就表示了该文档的文档向量。

最后一步就是对文档进行分类：
$$(8)p=softmax(W_cv+b_c)\tag{8}$$

用对数似然函数作为损失函数：
$$L=-\Sigma_dlogp_{dj}$$

其中，$d$为文档个数，$j$为文档$d$的标签。
## 使用多任务学习分类

Liu使用了多任务学习方法进行文本分类，文章中共使用三种模型结构。
![这里写图片描述](https://img-blog.csdn.net/20180124150216796?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

第一种是单层的结构，以两个任务为例，使用了共享向量$x_i^{(s)}$，把各个任务的输入向量与共享向量拼接在一起，通过共同的RNN隐藏层来达到学习不同分类的目的。
![这里写图片描述](https://img-blog.csdn.net/20180124150529971?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

第二种是双层结构，它没有使用一个共享向量$x_i^{(s)}$，而是每个任务有各自隐藏层，但是他们的隐藏层之间有交叉，即你可以用我的，我也可以用你的，最后得到不同的分类。
![](https://img-blog.csdn.net/20180124150813251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

第三种是共享层结构，这个结构是在第二种结构上又加了一个双向RNN，这个双向RNN的输入是两个不同任务的输入向量，而双向RNN的输出是两个不同任务的输入，从而达到信息共享的效果。具体内容可以参考Liu的论文。
**参考文献**

【Yoon Kim】Convolutional Neural Networks for Sentence Classification

【Ye Zhang, Byron C. Wallace】A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification

【Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom】A Convolutional Neural Network for Modelling Sentences

【Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao】Recurrent Convolutional Neural Networks for Text Classification

【Zichao Yang, Diyi Yang, Chris Dyer et al】Hierarchical Attention Networks for Document Classification

【Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio】NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE

【Pengfei Liu, Xipeng Qiu, Xuanjing Huang】Recurrent Neural Network for Text Classification with Multi-Task Learning










