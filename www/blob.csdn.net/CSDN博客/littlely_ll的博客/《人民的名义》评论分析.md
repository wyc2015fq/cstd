# 《人民的名义》评论分析 - littlely_ll的博客 - CSDN博客





2017年04月04日 13:45:30[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：2913








近日，反腐大剧《人民的名义》讲述了反腐天团与位高权重的贪腐分子之间斗智斗勇的故事。一经播出，响应非常强烈，在此，对与《人民的名义》网友的评论做一分析。[（《人民的名义》豆瓣评论链接）](https://movie.douban.com/subject/26727273/discussion/)

此次分析的主要工具为R，有两个主要的包要用到，一个是`Rwordseg`，这个主要是做中文分词的，另一个是`tm`包，是一个文本处理的框架，但是`tm`包有一个缺点，就是对中文支持不太好，会经常出现乱码的现象。然而现在有一个非常好的包`chinese.misc`，这个包对中文分词支持非常好，不用担心有乱码问题，而且处理非常简单，只需几行代码就可以解决问题，下面使用这两种方法进行简单分析。

截止直到现在，共有四百多条评论。

```
library(RCurl) ##craw the data from web
library(XML)##craw the data from web and parse the data
library(Rwordseg)##word segment
library(tm)
library(wordcloud2)##wordcloud plot

step <- seq(0,420,20)
contents <- vector()
for (i in step){
  url <- paste("https://movie.douban.com/subject/26727273/discussion/?start=",step ,"&sort_by=time",sep="") ##integrate the url
  html_form <- getURL(url=url)
  parsed_form <- htmlParse(html_form)
  tables <- readHTMLTable(parsed_form)##read HTML table
  contents <- c(contents,as.character(tables[[2]][,1]))##integrate the content from the website
}

write.table(contents,"contents.txt",row.names = F)
segmentCN("contents.txt",returnType = "tm")
mydoc <- readLines("content.txt")##transform the contents to content,the type of content should be ANSI

data_stw <- read.table("stopwords.txt",colClasses = "character")
stopwords_CN <- c(NULL)
for(i in 1:dim(data_stw)[1]){
       stopwords_CN <- c(stopwords_CN, data_stw[i,1])
}
write.table(stopwords_CN,"stopwords_CN.txt",row.names = F)
installDict("stopwords_CN.txt","stop")
installDict("mydict.txt","mydict")

mydoc.vec <- VectorSource(mydoc)
mydoc.corpus <- Corpus(mydoc.vec)
mydoc.corpus <- tm_map(mydoc.corpus,removeWords,stopwords_CN)##remove stopwords
mydoc.corpus <- tm_map(mydoc.corpus,stripWhitespace)##remove white space
control=list(removePunctuation=T,minDocFreq=5,wordLengths= c(1, Inf),weighting = weightTfIdf)
mydoc.tdm <- TermDocumentMatrix(mydoc.corpus,control = control)
mydoc.matrix <- as.matrix(mydoc.tdm)
mydoc.sum <- rowSums(mydoc.matrix)
mydoc.df <- data.frame(term=names(mydoc.sum), freq = as.numeric(mydoc.sum))##generate the data frame
```

在生成corpus时文档没有乱码，然而再生成词汇文档-矩阵时出现l乱码的问题，然而中间没有什么错误。可以参考博文[文本挖掘——词云图的操作](http://blog.csdn.net/littlely_ll/article/details/54866184)。

如果用`chinese.misc`包做文本分析，就非常简单了：

```
library(chinese.misc)
dtm <- corp_or_dtm("contents.txt",type="d",stop_word = "jibar",control = "auto2")
sort_tf(dtm,top=20)
df <- sort_tf(dtm,todf=T)
wordcloud2(df,backgroundColor = "black", shape = "circle")
```

单单几行代码，就可以生成词云图，而不用自己去去除停止词，空格，数字还有符号等。

![这里写图片描述](https://img-blog.csdn.net/20170404133817608?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20170404181149058?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20170404181310393?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

使用`wordcloud2`作图要比`wordcloud`作图美观的多，而且在你点击词汇的时候它可以给出该词的词频。由于评论较少，生成的词云比较少，而且还有一个问题，在使用`chinese.misc`包时去除的停止词好像比较多，两千多个不同的次最后只剩下99个词汇，不过，通过参数设置，还可以调整词汇水平。



