# 决策树（一） - littlely_ll的博客 - CSDN博客





2016年12月22日 16:54:27[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：374








## 1. 特征选择

### 1.1 信息增益

熵（entropy）是表示随机变量不确定性的度量，设x是一个取有限个值的离散随机变量，其概率分布为： 


$P(X=x_i)=p_i,i=1,2,\cdots,n$

则随机变量X的熵定义为 


$H(X)=-\sum_{i=1}^np_ilog p_i$

熵只依赖于X的分布，与X的取值无关，所以可将X的熵记做$H(p)$,即 


$H(p)=-\sum_{i=1}^np_ilogp_i$

条件熵$P(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下Y的条件熵为 


$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i),\\这里，p_i=P(X=x_i),i=1,2,\cdots,n.$

信息增益（information gain）表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 
**定义：**  特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A）之差，即 


$g(D,A)=H(D)-H(D|A)$

设：D为训练数据集，|D|为其样本容量，即样本个数。设有K个类$C_k, k=1,2,\cdots,K$, $|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$.设特征A有n个不同的取值$\{a_1,a_2,\cdots,a_n\}$,根据特征A的取值将D划分为n个子集$D_1,D_2,\cdots,D_n,|D_i|为D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|.$记子集$D_i$中属于属于类$C_k$的样本集合为$D_{ik}$,即$D_{ik}=D_i \bigcap C_k,|D_{ik}|为D_{ik}$的样本个数。则信息增益算法如下
### 1.2 信息增益的算法

输入：训练数据集D和特征A; 

输出：特征A对训练数据集D的信息增益g(D,A). 

（1）计算数据集D的经验熵$H(D)$


$H(D)=-\sum_{K=1}^k\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}$

（2）计算特征A对数据集D的经验条件熵H(D|A) 


$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_ik|}{|D_i|}$

（3）计算信息增益 


$g(D,A)=H(D)-H(D|A)$
### 1.3 信息增益比

**定义：**特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即 


$g_R(D,A)=g(D,A)/H_A(D)$

其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$，n是特征A取值的个数。
## 2. ID3算法

输入： 训练数据集D，特征集A，阈值$\epsilon$; 

输出： 决策树T。 

（1）若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$作为该节点的类标记，返回T； 

（2）若$A=\varnothing$,则T为单节点树，并将D中的实例数最大的类$C_k$作为该节点的类标记，返回T； 

（3）否则，按照算法1.2计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$； 

（4）如果$A_g$的信息增益小于阈值$\epsilon$，则置T为单节点树，并将D中的实例数最大的类$C_k$作为该节点的类标记，返回T； 

（5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T   ； 

（6）对第i个子节点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步（1）~（5），得到子树$T_i$,返回$T_i$.
## 3. C4.5算法

C4.5算法对ID3算法进行改进，在生成树过程中用信息增益比来选择特征。 

输入： 训练数据集D，特征集A，阈值$\epsilon$; 

输出： 决策树T。 

（1） 如果D中所有实例属于同一类$C_k$，则置T为单节点树，并将$C_k$作为该节点的类，返回T； 

（2） 如果$A=\varnothing$,则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T； 

（3） 否则，按信息增益比公式计算A中各特征对D的信息增益比，选择信息增益比的最大的特征$A_g;$

（4） 如果$A_g$的信息增益比小于阈值$\epsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T； 

（5） 否则，对$A_g$的每一个可能值$a_i$,依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T ； 

(6对第i个子节点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步（1）~（5），得到子树$T_i$,返回$T_i$.
## 4. 决策树的剪枝

决策树的剪枝是通过极小化决策树整体的损失函数来实现。设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，k=1,2,…,K,$H_t(T)$为叶结点t上的经验熵，$\alpha\ge0$为参数，则决策树学习的损失函数可以定义为 


$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T) +\alpha|T|$

其中，经验熵为 


$H_t(T)=-\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$

在损失函数中，右端第一项可记做C(T) 
**剪枝算法：**

输入： 生成算法产生的整个树T，参数$\alpha$; 

输出： 修剪后的子树$T_{\alpha}$. 

（1） 计算每个节点的经验熵； 

（2） 递归地从树的叶结点向上回缩； 
$\qquad$设一组叶结点回缩到其父节点之前与之后的整体树分别为$T_B和T_A$，其对应的损失函数值分别为$C_{\alpha}(T_B)与C_{\alpha}(T_A)$,如果 


$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$
$\qquad$则进行剪枝，即将父节点变为新的叶结点； 

（3） 返回（2），直至不能继续为止，得到损失函数最小的子树$T_\alpha$
## 5. C5.0算法R实现

C5.0是C4.5的后继版本，C5.0算法是销售给商业用户的，但是该算法的单线程版本的源代码是公开的。C5.0算法已成为生成决策树的行业标准。

### 5.1 C5.0的 优缺点

**优点：**

 - 一个适用于大多数问题的通用分类器 

 - 高度自动化的学习过程，可以处理数值型数据，名义特征和缺失数据 

 - 只使用最重要的特征 

 - 可以用于只有相对较少的训练案例的数据或者有相当多训练案例的数据 

 - 没有数学背景也可以解释一个模型的结果 

 - 比其他复杂模型更有效 
**缺点：**

 - 决策树模型在根据具有大量水平的特征进行划分时往往是有偏的 

 - 很容易过度拟合或者不能充分拟合模型 

 - 因为依赖于轴平行分割，所以在对一些关系建立模型时会有困难 

 - 训练数据中的小变化可能导致决策逻辑的较大变化 

 - 大的决策树可能很难理解，给出的决策可能看起来会违反直觉
### 5.2 C5.0决策树语法

在R语言中应用C50包中的C5.0()函数 
**创建分类器：**
`m <- C5.0(train, class, trails = 1, costs = NULL)`- train: 包含训练数据的数据框
- class: 包含训练数据每一行分类的一个椅子向量
- trials: 可选，用于控制自助法循环次数
- costs: 可选矩阵，用于给出与各种类型错误相对应的成本，在构建成本矩阵时，行表示预测值，列表示实际值。 

该函数返回一个C5.0模型的对象，能够用于预测。 
**进行预测：**`p <- predict(m, test, type = "class")`- m: 有函数C5.0()训练的一个模型
- test: 包含测试数据集的数据框，该数据框和用来创建分类器的训练数据有同样的特征
- type: 取值为“class”或“prob”，标识预测是最可能的类别值或原始的预测概率值。











