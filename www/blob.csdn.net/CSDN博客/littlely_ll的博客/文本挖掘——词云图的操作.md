# 文本挖掘——词云图的操作 - littlely_ll的博客 - CSDN博客





2017年02月04日 21:26:52[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：5765
个人分类：[网络数据抓取](https://blog.csdn.net/littlely_ll/article/category/6627741)










 弄了两天，今天看了一篇博文才有点搞的明白，第一天看了Rwordseg包，这个包可把我整惨了，安装它都使出吃奶的劲了，我怎么觉得还没安装到位。如果想安装的话请参考《Rwordseg使用说明》，李舰写的，也是个大牛。比较详细介绍了Rwordseg的使用。第二天看了第二篇文章《玩玩文本挖掘-wordcloud、主题模型与文本分类》，这个家伙也不好看，讲的啰里啰嗦的。用个简单的例子就行了呗，非得用个复杂的（我认为的），而且还用到python，我只能说，cao。把我搞得稀里糊涂的，最后搜了**挖掘机1990（**在此感谢**挖掘机1990**的博文）。看的明白一点。现在还有两块没看完。文本挖掘的另一个包tm少不了，现在还没详细看。现在的主要问题就在文本的分词和词频的统计，这两个做好，画出词云图就简单了。现在觉得，一个人学习R语言太艰苦了，没资料，没视频的，错了自己调试调试再调试，宝宝心里苦啊！好了，闲话少说，该干什么还得干什么，上程序。

library(Rwordseg)#载入分词包
library(tm) #载入文本挖掘包


# 分词
把要分析的文件，存为文本文件（txt后缀），放到某个目录

## 装载自定义词库

## 这里的自定义词库，是根据分析文件中的某些特殊用词，自己编写的一个词库文件，其实也是一个文本文件，每行一个词。为什么要装自定义词库勒，是为了准确进行分词。某些单词如果不设置为自定义词，那么分词的时候可能会分解成其他的词汇。比如"中国电信"，如果不设置为自定义词，那么就会被分解为"中国电信"；如果设置为自定义词，那么就会识别为一个词。
installDict(file.choose(),"mydict")#装载
listDict() #查看词典
loadDict(dictpath =getOption("dic.dir"))#在装在字典时就已经载入了


## 分词
segmentCN(file.choose(),returnType="tm")#这种模式分词后，会在分词文件的同一个目录生成一个"源文件名+.segment"的文本文件，就是分词的结果。Rwordseg的特点是分词很快。
#必须要注意，用Rwordseg分词后的文本文件，其编码格式是"UTF-8无BOM编码格式"，这种编码用TM包读入后，全是乱码。解决办法是用windows自带的记事本打开，然后另存，另存的时候选择编码格式为"ANSI"
##返回值选取tm是为了以后用corpus方便

# 建立语料库
这部分是读入分词后的文件，然后用TM包进行整理，清洗，变换成用于分析的"语料库"。

## **读入分词后的文本**
mydoc<-readLines(file.choose())

## **建立语料库**



（这里读取文本到变量，根据文本变量来建立语料库）
mydoc.vec<-VectorSource(mydoc)
mydoc.corpus<-Corpus(mydoc.vec)


## **删除停用词**



（就是删除一些介词、叹词之类的词语，这些词语本身没多大分析意义，但出现的频率却很高，比如"的、地、得、啊、嗯、呢、了、还、于是、那么、然后"等等。前提是必须要有一个停用词库，网上搜索即可下载，也是一个txt的文本文件，每行一个词。网上有两种版本，一种是500多个词的，一种是1000多个词的）
data_stw=read.table(file=file.choose(),colClasses="character")#读取停用词，挨个转换到一个列表中（1）
stopwords_CN=c(NULL)
for(iin 1:dim(data_stw)[1]){
  stopwords_CN=c(stopwords_CN,data_stw[i,1])
}
mydoc.corpus<-tm_map(mydoc.corpus,removeWords,stopwords_CN)#删除停用词（2）
我不知道这位博主是怎么去掉停止词的，但是我用这种方法没有去掉停止词，有两方面错误，第一是在（1）处，读取停止词的时候它用的是什么编码格式，而且在读取后有警告。
在（2）处删除停止词时，会出现错误，之后又出现很多看不懂的文字，但是这样不影响后续的分析，所以我索性把从（1）至（2）去掉，只是没有去掉停止词罢了。



# 进一步清洗数据
mydoc.corpus<-tm_map(mydoc.corpus,removeNumbers)#删除数字
mydoc.corpus<-tm_map(mydoc.corpus,stripWhitespace)#删除空白


## 进行内容分析



到这里要分析的数据已经准备好了，可以进行各种分析了，下面以聚类分析为例。


## 建立TDM矩阵



（TDM就是"词语×文档"的矩阵）
control=list(removePunctuation=T,minDocFreq=5,wordLengths= c(1, Inf),weighting = weightTfIdf)

设置一些建立矩阵的参数，用变量control来存储参数，控制如何抽取文档
removePunctuation表示去除标点
minDocFreq=5表示只有在文档中至少出现5次的词才会出现在TDM的行中
tm包默认TDM中只保留至少3个字的词（对英文来说比较合适，中文就不适用了吧……），wordLengths = c(1,Inf)表示字的长度至少从1开始。
默认的加权方式是TF，即词频，这里采用Tf-Idf，该方法用于评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度：
mydoc.tdm=TermDocumentMatrix(mydoc.corpus,control)#建立矩阵

在一份给定的文件里，词频 (term frequency, TF)指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）
#逆向文件频率 (inverse document frequency, IDF)是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。
某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于保留文档中较为特别的词语，过滤常用词。
library(wordcloud)
m<-as.matrix(mydoc.tdm)
wordfred<-sort(rowSums(m),decreasing=T)
set.seed(375)
op = par(bg = "lightgrey")#背景为亮黄色
wordcloud(words=names(wordfred),freq=wordfred,min.freq=2,random.order=F,col =rainbow(length(wordfred)))
par(op)#为画框添加背景

![](https://img-blog.csdn.net/20170207171146696?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


![](https://img-blog.csdn.net/20170207171200071?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl0dGxlbHlfbGw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


其中，我是用《斗破苍穹》的前四章进行文字挖掘的，结果当然并不理想，做了两次两次结果有些差别，但终归是做出来了。但是用整本书做的时候，错误就多了，难道是因为文件太大（斗破十几兆啊，光构建TDM花了大约七八分钟）。后续自己在慢慢解决吧。上图：

![文本挖掘——词云图的操作](http://simg.sinajs.cn/blog7style/images/common/sg_trans.gif)

![文本挖掘——词云图的操作](http://simg.sinajs.cn/blog7style/images/common/sg_trans.gif)
这是同样程序做了两次，有区别吧，即使设置了种子变量也是有差别？回头再看看。这里面竟然没有“萧炎”这个词，它应该挺多的啊，我只能说一个字，操蛋。ef搞的什么真不知道。在sogou上下载的词库也很艹，说有斗破的字库，打开一看，就一个成语“斗破苍穹”，连一个名都没有。后来自己添加词库，直接在字典中添加竟然没添加上，那只能用insertwords函数了。

||
|----|





