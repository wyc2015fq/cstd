# 决策树（二） - littlely_ll的博客 - CSDN博客





2017年01月05日 17:26:03[littlely_ll](https://me.csdn.net/littlely_ll)阅读数：622








## 1. 分类与回归树（CART）

CART树由特征选择、树的生成及树的剪枝组成，可用于分类，也可用于回归。CART假设决策树是二叉树，内部节点特征取值为“是”和“否”。CART算法由以下两步组成： 

（1）决策树生成： 基于训练数据集生成决策树，生成的决策树要尽量大； 

（2）决策树剪枝： 用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。
## 2. 最小二乘回归树生成算法

输入： 训练数据集D； 

输出： 回归树$f(x).$

在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树： 

（1）选择最优切分变量$j$与切分点s，求解 


$min_{j,s}[min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$

遍历变量$j$，对固定的切分变量$j$扫描切分点s，选择使上式达到最小的对$（j,s）$. 

（2）用选定的对$(j,s)$划分区域并决定相应的输出值： 


$R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}\gt s\}\\\hat{c}_m=\frac{1}{N}\sum_{x_i \in R_m(j,s)}y_i,x \in R_m,m=1,2$

（3）继续对两个子区域调用步骤（1）（2），直至满足停止条件。 

（4）将输入空间划分为M个区域$R_1,R_2,\cdots,R_M$，生成决策树： 


$f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)$
## 3. 基尼指数

基尼指数的定义： 

假设有K个类，样本点属于第k类的概率为$p_k$,则基尼指数为 


$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$

对于二类分类问题，样本点属于第一个类的概率为p，则概率分布的基尼指数为 


$Gini(p)=2p(1-p)$

对于给定样本集合D，其基尼指数为 


$Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$

其中，$C_k是D$中属于第k类的样本子集，K是类的个数。 

如果样本集合D根据特征A是否取某一可能值a被分割成$D_1和D_2$两部分，即 


$D_1=\{(x,y)\in D|A(x)=a\},D_2=D-D_1$

在特征A的条件下，集合D的基尼指数定义为 


$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$
## 4. CART生成算法

输入：训练数据集D，停止计算条件； 

输出：CART决策树。 

根据训练数据集，从根节点开始，递归对每个节点进行以下操作，构建二叉决策树： 

（1）设节点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每个特征A，对其可能的每个值a，根据样本点对A=a的测试为“是”或“否”将D分割成$D_1和D_2$两部分利用特征A条件下计算基尼指数的公式计算A=a时的基尼指数。 

（2）在所有可能的特征A以及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依照最优特征与最优切分点，从现结点生成两个子节点，将训练数据集依特征分配到两个子节点中去。 

（3）对两个子节点递归地调用（1），（2），直至满足停止条件。 

（4）生成CART决策树。 

算法停止计算的条件是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或没有更多特征。
## 5. CART剪枝

在剪枝中，需要计算子树的损失函数： 


$C_\alpha(T)=C(T)+\alpha|T|$

其中，T为任意子树，C(T)为训练数据的预测误差（如基尼指数），|T|为子树的叶结点个数，$\alpha\ge 0$为参数，$C_\alpha(T)为参数是\alpha$时的子树T的整体损失，参数$\alpha$权衡训练数据的拟合程度与模型的复杂度。 
**CART树剪枝算法**

输入： CART算法生成的决策树$T_0;$

输出： 最优决策树$T_\alpha.$

（1） 设$k=0，T=T_0.$

（2） 设$\alpha=+\infty.$

（3） 自下而上地对各内部结点t计算$C(T_t),|T_t|$以及 


$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}\\\alpha=min(\alpha,g(t))$

这里，g(t)为剪枝后整体损失函数减少的程度，$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据集的预测误差，$|T_t|是T_t$的叶结点个数。 

（4） 对$g(t)=\alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T。 

（5） 设$k=k+1,\alpha_k=\alpha,T_k=T.$

（6） 如果$T_k$不是由根节点及两个叶结点构成的树，则回到步骤（3）；否则，令$T_k=T_n.$

（7） 采用交叉验证法在子树序列$T_0,T_1,\cdots,T_N中$选取最优子树$T_\alpha$.
## 6. CART方法优缺点

**优点**

 - 将决策树的优点与对数值型数据建立模型的能力相结合 

 - 能自动选择特征，允许该方法与大量特征一起使用 

 - 不需要使用者事先指定模型 

 - 拟合某些类型的数据可能会比线性模型好得多 

 - 不要求用统计的知识来解释模型 
**缺点**

 - 不像线性回归那样常用 

 - 需要大量的训练数据 

 - 难以确定单个特征对于结果的总体净影响 

 - 可能比回归模型更难解释
## 7. CART的R实现

在R中使用rpart包中的rpart()函数实现CART算法 
**建立模型：**
`m <- rpart(dv ~ iv, data = mydata)`- dv: mydata数据框中需要建模的因变量
- iv: 一个R公式，用来指定mydata数据框中被用于建模的自变量
- data: 为包含变量dv和iv的数据框 

该函数返回一个回归树模型对象，该对象能够用于预测。 
**进行预测：**`p <- predict(m, test, type = "vector")`- m: 由函数rpart()训练的一个模型
- test: 测试数据集
- type: 给定返回预测值的类型，取值为“vector”（预测数值型数据），或者“class”（预测类别型数据），或者“prob”（预测类别的概率） 

该函数返回值取决于type参数，是一个汉语预测值的向量。 

对于生成的CART模型，可以用rpart.plot包中的rpart.plot()函数使决策树可视化。详细内容可参考rpart.plot文档。










