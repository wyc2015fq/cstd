# 【机器学习】交叉熵为什么能当损失函数 - zkq_1986的博客 - CSDN博客





2018年11月30日 11:55:05[zkq_1986](https://me.csdn.net/zkq_1986)阅读数：70








交叉熵为什么能当损失函数？因为最小化交叉熵与最小化KL散度等价。KL散度越小，说明两个分布越接近。

下面证明最小化交叉熵与最小化KL散度等价。

证明：

A的熵的公式：

![S(A) = -\sum_{i}P_A(x_{i})logP_A(x_{i})](https://www.zhihu.com/equation?tex=S%28A%29+%3D+-%5Csum_%7Bi%7DP_A%28x_%7Bi%7D%29logP_A%28x_%7Bi%7D%29)

A与B KL散度的公式：

![\begin{equation*} D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i)) \end{equation*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%2A%7D+D_%7BKL%7D%28A%7C%7CB%29+%3D+%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29+log%5Cbigg%28%5Cfrac%7BP_%7BA%7D%28x_i%29%7D%7BP_%7BB%7D%28x_i%29%7D+%5Cbigg%29+%3D+%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29log%28P_%7BA%7D%28x_i+%29%29-+P_%7BA%7D%28x_i%29log%28P_%7BB%7D%28x_i%29%29+%5Cend%7Bequation%2A%7D)

A与B交叉熵的公式：

![\begin{equation*} H(A,B)= -\sum_{i}P_{A}(x_i)log(P_{B}(x_i)) \end{equation*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%2A%7D+H%28A%2CB%29%3D+-%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29log%28P_%7BB%7D%28x_i%29%29+%5Cend%7Bequation%2A%7D)

因此，Ａ与Ｂ的交叉熵 ＝ Ａ与Ｂ的KL散度 - Ａ的熵

A的熵表示真实数据的熵，该值固定。所以最小化Ａ与Ｂ的交叉熵和最小化Ａ与Ｂ的KL散度等价。



p.s. 熵是一种衡量信息不确定性的指标，熵越小，不确定性越小。



