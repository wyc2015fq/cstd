# 【深度学习】BP神经网络 - zkq_1986的博客 - CSDN博客





2016年11月14日 20:22:29[zkq_1986](https://me.csdn.net/zkq_1986)阅读数：461











1、BP（Back Propagation）算法的出现

非循环多级网络的训练算法

UCSD PDP小组的Rumelhart、Hinton和Williams1986年独立地给出了BP算法清楚而简单的描述

1982年，Paker就完成了相似的工作

1974年，Werbos已提出了该方法

2、弱点：训练速度非常慢、局部极小点的逃离问题、算法不一定收敛。

3、优点：广泛的适应性和有效性。






BP网络拓扑结构













ß2.2.1 网络的构成 



神经元的网络输入：

  neti=x1w1i+x2w2i+…+xnwni



神经元的输出：







Þ应该将net的值尽量控制在收敛比较快的范围内

Þ可以用其它的函数作为激活函数，只要该函数是处处可导的









训练过程概述




样本：(输入向量，理想输出向量)

权初始化：“小随机数”。

1、向前传播阶段：

（1）从样本集中取一个样本(Xp，Yp)，将Xp输入网络；

（2）计算相应的实际输出Op：



Op=FL(…(F2(F1(XpW(1))W(2))…)W(L))



2、向后传播阶段——误差传播阶段：

（1）计算实际输出Op与相应的理想输出Yp的差；

（2）按极小化误差的方式调整权矩阵。

（3）网络关于第p个样本的误差测度：

**  （****4） 网络关于整个样本集的误差测度：**



**1、输出层权的调整**






wpq= wpq+∆wpq

∆wpq=αδqop

  =αfn′ (netq)(yq-oq)op

  =αoq(1-oq) (yq-oq)op  





**y****q****为****AN****q****实际输出，**




∆wpq=αδqop

  =αfn′ (netq)(yq-oq)op

  =αoq(1-oq) (yq-oq)op




**网络关于第****p****个样本的误差测度**

梯度下降法：

∆wpq= -α▽E




**梯度下降法**




X_new =
X_old–a.f'(X_old);







**梯度下降法**的执行步骤为：

  1.给定一个初始值，如
X_old=(x1,x2,...,xn);

 2.求函数f（x）在此点的梯度
f‘(X_old)（              
）；

3.
X_new =
X_old–a.f'(X_old);(a>0为固定步长，一般都比较小，相当于在f的负梯度方向走了较小的一步)

4.
将X_old设为X_new，循环执行步骤2,3,4，直到f的值不再变化或变化很小.



**坐标下降法**的执行步骤为：

1.首先给定一个初始点，如
X_0=(x1,x2,...,xn);

2.for
x_i=1:n

         固定除x_i以外的其他维度

         以x_i为自变量，求取使得f取得最小值的x_i；

   end

3. 循环执行步骤2，直到f的值不再变化或变化很小.














