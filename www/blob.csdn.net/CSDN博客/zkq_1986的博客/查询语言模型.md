# 查询语言模型 - zkq_1986的博客 - CSDN博客





2017年08月16日 20:12:38[zkq_1986](https://me.csdn.net/zkq_1986)阅读数：141
个人分类：[NLP](https://blog.csdn.net/zkq_1986/article/category/7016873)









# Query Language Model

# 1 TFIDF




 在一份给定的文件里，**词频**（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对**词数**(term count)的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 ![t_{i}](http://upload.wikimedia.org/wikipedia/zh/math/e/c/2/ec232cdf2784de6af619af9e5d2b4eed.png) 来说，它的重要性可表示为：

![\mathrm{tf_{i,j}} = \frac{n_{i,j}}{\sum_k n_{k,j}}](http://upload.wikimedia.org/wikipedia/zh/math/e/5/a/e5a7b43197068eddf42859f3995ebf15.png)


      以上式子中 ![n_{i,j}](http://upload.wikimedia.org/wikipedia/zh/math/6/c/b/6cb9ca4667e16389321534c9f714100a.png) 是该词![t_{i}](http://upload.wikimedia.org/wikipedia/zh/math/e/c/2/ec232cdf2784de6af619af9e5d2b4eed.png)在文件![d_{j}](http://upload.wikimedia.org/wikipedia/zh/math/c/4/a/c4aa4939af4846c2cc2c0af2919611ee.png)中的出现次数，而分母则是在文件![d_{j}](http://upload.wikimedia.org/wikipedia/zh/math/c/4/a/c4aa4939af4846c2cc2c0af2919611ee.png)中所有字词的出现次数之和。

**      逆向文件频率**（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到：

![\mathrm{idf_{i}} =  \log \frac{|D|}{|\{j: t_{i} \in d_{j}\}|}](http://upload.wikimedia.org/wikipedia/zh/math/0/2/5/0257ce95c505ab568d7898faa56a4f5c.png)


其中
- |D|：语料库中的文件总数
- ![|\{ j: t_{i} \in d_{j}\}|](http://upload.wikimedia.org/wikipedia/zh/math/a/2/2/a22c849cb4a0064eb5ebc806f690eebf.png)：包含词语![t_{i}](http://upload.wikimedia.org/wikipedia/zh/math/e/c/2/ec232cdf2784de6af619af9e5d2b4eed.png)的文件数目（即![n_{i,j} \neq 0](http://upload.wikimedia.org/wikipedia/zh/math/9/8/1/981e495461cfd42cf2a413a8ed175e7f.png)的文件数目）如果该词语不在语料库中，就会导致被除数为零，**因此一般情况下使用![1 + |\{j : t_{i} \in d_{j}\}|](http://upload.wikimedia.org/wikipedia/zh/math/1/b/e/1bea70e17e2167cc08346d650f10b29d.png)**


然后

![\mathrm{tf{}idf_{i,j}} = \mathrm{tf_{i,j}} \times  \mathrm{idf_{i}}](http://upload.wikimedia.org/wikipedia/zh/math/b/0/6/b06a060c28253c8dd2528811c447862e.png)


# 2 BM25

考虑的是tf, qtf，和文档长度



Given a query Q, containing keywords {\displaystyle
 q_{1},...,q_{n}}![q_1, ..., q_n](https://wikimedia.org/api/rest_v1/media/math/render/svg/4024517280628a21186b246939a2cc89c937f7e3),
 the BM25 score of a document D is:
{\displaystyle
 {\text{score}}(D,Q)=\sum _{i=1}^{n}{\text{IDF}}(q_{i})\cdot {\frac {f(q_{i},D)\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\cdot \left(1-b+b\cdot {\frac {|D|}{\text{avgdl}}}\right)}},}
![{\displaystyle {\text{score}}(D,Q)=\sum _{i=1}^{n}{\text{IDF}}(q_{i})\cdot {\frac {f(q_{i},D)\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\cdot \left(1-b+b\cdot {\frac {|D|}{\text{avgdl}}}\right)}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/43e5c609557364f7836b6b2f4cd8ea41deb86a96)

where {\displaystyle f(q_{i},D)}![f(q_i, D)](https://wikimedia.org/api/rest_v1/media/math/render/svg/7caf2f93a6527d4a76c07bd67980ec921167f990) is {\displaystyle
 q_{i}}![q_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6)'s [term
 frequency](https://en.wikipedia.org/wiki/Term_frequency) in the document D, {\displaystyle |D|}![|D|](https://wikimedia.org/api/rest_v1/media/math/render/svg/7c936c42ab206e72c1fbc1ea103cc9eca1802af1) is
 the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. {\displaystyle
 k_{1}}![k_{1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/376315fd4983f01dada5ec2f7bebc48455b14a66) and b are
 free parameters, usually chosen, in absence of an advanced optimization, as {\displaystyle k_{1}\in [1.2,2.0]}![k_1 \in [1.2,2.0]](https://wikimedia.org/api/rest_v1/media/math/render/svg/0af33de8946a560fed04ff522251656c8207f3db) and {\displaystyle
 b=0.75}![b = 0.75](https://wikimedia.org/api/rest_v1/media/math/render/svg/cc8113cfb95fd7d2a084b62f53ff1a54186bc11c).[[1]](https://en.wikipedia.org/wiki/Okapi_BM25#cite_note-1){\displaystyle
 {\text{IDF}}(q_{i})}![\text{IDF}(q_i)](https://wikimedia.org/api/rest_v1/media/math/render/svg/e34b65be68e2123b44a9106fed33a0253229e356) is
 the IDF ([inverse document frequency](https://en.wikipedia.org/wiki/Inverse_document_frequency))
 weight of the query term {\displaystyle q_{i}}![q_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6).
 It is usually computed as:
{\displaystyle
 {\text{IDF}}(q_{i})=\log {\frac {N-n(q_{i})+0.5}{n(q_{i})+0.5}},}
![\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5},](https://wikimedia.org/api/rest_v1/media/math/render/svg/c652b6871ce4872c8e924ff0f806bc8b06dc94ed)

where N is the total number of documents in the collection, and {\displaystyle
 n(q_{i})}![n(q_i)](https://wikimedia.org/api/rest_v1/media/math/render/svg/14dc99fdb5fe2428a1c46cd20a7255dbee944945) is
 the number of documents containing {\displaystyle q_{i}}![q_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6).






# 3 Query likelihood



Rank documents by the probability that the query could be generated by the document model (i.e. same topic)

Given query, start with P(D|Q)

Using Bayes’ Rule 

![](https://img-blog.csdn.net/20170819171113175)

Assuming prior is uniform, unigram model

![](https://img-blog.csdn.net/20170819171121920)





【Jelinek-Mercer Smoothing】 

![](https://img-blog.csdn.net/20170819171130267)


C_q_i：q_i在语料中出现的次数；|C|：语料中总词数（不是词汇数，相同的词可算多次）



【Dirichlet Smoothing】 

![](https://img-blog.csdn.net/20170819171135861)



# 4 K-L Divergence

描述两个分布的差异程度

![](https://img-blog.csdn.net/20170819182740265)







