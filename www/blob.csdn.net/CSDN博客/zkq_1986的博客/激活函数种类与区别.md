# 激活函数种类与区别 - zkq_1986的博客 - CSDN博客





2017年05月03日 15:32:21[zkq_1986](https://me.csdn.net/zkq_1986)阅读数：407
个人分类：[神经网络](https://blog.csdn.net/zkq_1986/article/category/6338867)









**1 激活函数类别**

（1）tanh（/ˈtænʃ/）

双曲正切 tanh=sinh/cosh=(ex-e-x)/(ex+e-x)

（2）sigmoid

f(x)=1/(1+e-(wx+b))

（3）Relu(读re路)

**rectified linear unit**(**ReLU**)

f(x)=max(0,x)

**2  sigmoid、tanh与Relu区别**

（1）sigmoid函数。这是传统神经网络中最常用的激活函数之一（另一个是tanh），对应的图像如图所示。

**![](https://img-blog.csdn.net/20170503153136514)**![](https://img-blog.csdnimg.cn/20181101162441656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3prcV8xOTg2,size_16,color_FFFFFF,t_70)

sigmoid函数的优点在于，它的输出映射在(0,1)内，单调连续，非常适合用作输出层，并且求导比较容易。但是，它也有缺点，因为软饱和性 [15] ，一旦输入落入饱和区，f ' (x )就会变得接近于0，很容易产生梯度消失。

（2）tanh函数。对应的图像如图所示。

![](https://img-blog.csdnimg.cn/20181101162620128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3prcV8xOTg2,size_16,color_FFFFFF,t_70)

tanh函数也具有软饱和性。因为它的输出以0为中心，收敛速度比sigmoid要快。但是仍无法解决梯度消失的问题。

（3）relu函数是目前最受欢迎的激活函数。softplus可以看作是ReLU的平滑版本。relu定义为f (x )=max(x ,0)。softplus定义为f (x )=log(1+exp(x ))。

![](https://img-blog.csdnimg.cn/20181101162752774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3prcV8xOTg2,size_16,color_FFFFFF,t_70)

由图可见，relu在x <0时硬饱和。由于x >0时导数为1，所以，relu能够在x >0时保持梯度不衰减，从而缓解梯度消失问题，还能够更快地收敛，并提供了神经网络的稀疏表达能力。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新，称为“神经元死亡”。





