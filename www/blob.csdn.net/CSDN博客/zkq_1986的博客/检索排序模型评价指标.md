# 检索排序模型评价指标 - zkq_1986的博客 - CSDN博客





2017年11月01日 21:44:27[zkq_1986](https://me.csdn.net/zkq_1986)阅读数：341








# 1 Normalized discounted cumulative gain (NDCG)

![](https://img-blog.csdn.net/20171101221254786)


![](https://img-blog.csdn.net/20171101221315534)

# 2 a-NDCG

![](https://img-blog.csdn.net/20180414224504587)


![](https://img-blog.csdn.net/20180414224409157)


其中,C_d 表示文档 d 所包含的子主题集合， s_i 表示子主题集合中的某一主题，N_i 表示位于文档 d 之前的文档集中包含 s_i 的文档数量.


通常情况，alpha取1/2.

例子：

sub topic="before,after,mid,no_of_time....."
doc_a的sub topic包含["before","mid""after"]
doc_b的sub topic包含["mid","after"]
doc_c的sub topic包含["before","after"]
doc_d的sub topic包含["no_of_time","mid"]
doc_e的sub topic包含["before","no_of_time"]
doc_f的sub topic包含['','',6,1,0.0,1.0,"before","after"]
doc_g的sub topic包含['','',7,1,1.0,1.0,"mid","after"]
以文档doc_f为例，
则上文公式C_d=“before”,"after"
依次把“before”,"after"当做s_i，对f前面的文档进行计数：
s1="before"的文档有a,c,e，此时N_i=3
s2="after"的文档有b,c，此时N_i=2


# 3 Precision and recall

Precision and recall both compute the fraction of relevant documents retrieved for a query q, but with respect to the total number of documents in the retrieved set Rq and the
total number of relevant documents in the collection D, respectively. Both metrics assume that the relevance labels are binary.


![](https://img-blog.csdn.net/20171101221238254)


# 3 Mean reciprocal rank (MRR) 

Mean reciprocal rank  is also computed over binary relevance judgments. It is given as the reciprocal rank of the first relevant document averaged over all queries.


![](https://img-blog.csdn.net/20171101221243374)




# 例子



比如，设测试集有4个query，他们的结果中，前三个query的第一个正确答案分别被排在第3，1，5位，而第四个query没有找到正确答案。则该系统的MRR得分就是： 


131115040.383


# 4 Mean average precision (MAP)

The average precision for a ranked list of documents R is given by


![](https://img-blog.csdn.net/20171101221248995)


where, Precisionq;i is the precision computed at rank i for the query q. The average precision metric is generally used when relevance judgments are binary, although variants using graded judgments have also been proposed [167]. The mean of the average precision over all queries gives the MAP score for the whole set.



This finite sum is equivalent to:
AveP=∑k=1n(P(k)×rel⁡(k))number of relevant documents{\displaystyle \operatorname {AveP} ={\frac {\sum _{k=1}^{n}(P(k)\times \operatorname {rel} (k))}{\mbox{number of relevant documents}}}\!}![\operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!](https://wikimedia.org/api/rest_v1/media/math/render/svg/64788ba190d585f3837f9d10fe24f3b9689dba4b)
where rel⁡(k){\displaystyle \operatorname {rel} (k)}![\operatorname{rel}(k)](https://wikimedia.org/api/rest_v1/media/math/render/svg/b44cbf8cf005a53f5c828094fd5583c351ae8642) is an indicator function equaling 1 if the item at rank k{\displaystyle k}![k](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40) is a relevant document, zero otherwise.[[10]](https://en.wikipedia.org/wiki/Information_retrieval#cite_note-Turpin2006-10) Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.
where ![k](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40) is the rank in the sequence of retrieved documents, ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) is the number of retrieved documents, ![P(k)](https://wikimedia.org/api/rest_v1/media/math/render/svg/b41614fb84549b21f2c7f2793bbd8a87a2105027) is the precision at cut-off ![k](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40) in the list, and ![\Delta r(k)](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e1d314ac3ed8c89ae2083c35ea89b65c0780e7a) is the change in recall from items ![k-1](https://wikimedia.org/api/rest_v1/media/math/render/svg/21363ebd7038c93aae93127e7d910fc1b2e2c745) to ![k](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40).[[9]](https://en.wikipedia.org/wiki/Information_retrieval#cite_note-zhu2004-9)







例子



假设有两个查询，查询1有4个相关文档，查询2有5个相关文档。某系统对查询1检索出4个相关文档，其rank分别为1,2,4,7；对于查询2检索出3个相关文档，其rank分别为1,3,5。

对于查询1，AP平均正确率为:(1/1+2/2+3/4+4/7)/4=0.83

对于查询2，AP平均正确率为:(1/1+2/3+3/5)/5=0.45


则平均正确率均值为:(0.83+0.45)/2=0.64









