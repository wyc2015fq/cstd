# 神经网络-损失函数 - xmdxcsj的专栏 - CSDN博客





2015年12月07日 20:17:17[xmucas](https://me.csdn.net/xmdxcsj)阅读数：14461标签：[神经网络](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)
个人分类：[神经网络](https://blog.csdn.net/xmdxcsj/article/category/5855803)









先上结论：在使用sigmoid作为激活函数的时候，cross entropy相比于quadratic cost function，具有收敛速度快，更容易获得全局最优的特点；使用softmax作为激活函数，log-likelihood作为损失函数，不存在收敛慢的缺点。 

对于损失函数的收敛特性，我们期望是当误差越大的时候，收敛（学习）速度应该越快。

## 一、quadratic + sigmoid

### （一）、定义

平方和损失函数定义 


$C=\frac{(y-a)^2}{2}$

其中$y$是期望输出，$a$是实际输出
### （二）、收敛特性

不幸的是，使用平方和作为损失函数的神经单元不具备这种性质（参考文献有一个非常直观的[例子](http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function)），具体分析如下： 

对于一个神经单元来讲，输入x和对应的输出a的关系满足 


$z=wx+b$


$a=\delta(z)$

根据链式法则，可以求得对应的偏导数 


$\frac{\partial C}{\partial w}=(\delta(z)-y)\delta'(z)x$


$\frac{\partial C}{\partial b}=(\delta(z)-y)\delta'(z)$

如果激活函数使用的是sigmoid函数的话，根据激活函数的形状和特性可知，当$\delta(z)$趋近于0或者趋近于1的时候，$\delta'(z)$会趋近于0，当$\delta(z)$趋近于0.5的时候，$\delta'(z)$会最大。 

比如说，取$y=0$，当$\delta(z)=1$的时候，期望值和实际值的误差$\delta(z)-y$达到最大，此时，$\delta'(z)$会趋近于0，所以就会发生收敛速度慢的问题。
## 二、cross entropy + sigmoid

### （一）、定义

为了解决上述收敛慢的问题，引入了交叉熵损失函数 


$C=-(y\ln a+(1-y)\ln (1-a))$

### （二）、两点特性

要想成为loss function，需要满足两点要求： 

1. 非负性 

2. 预测值和期望值接近时，函数值趋于0 

显然，quadratic cost function满足以上两点。cross entropy同样也满足以上两点，所以其可以成为一个合格的cost function。
### （三）、收敛特性

对于一个神经单元来讲，输入x和对应的输出a的关系满足 


$z=wx+b$


$a=\delta(z)$

根据链式法则，可以求得对应的偏导数$\frac{\partial C}{\partial w}$


$\frac{\partial C}{\partial w}=\frac{\delta(z)-y}{\delta(z)(1-\delta(z))}\delta'(z)x$

对于sigmoid函数来讲，满足 


$\delta'(z)=\delta(z)(1-\delta(z))$

所以，上式可化简为 


$\frac{\partial C}{\partial w}=(\delta(z)-y)x$

同理可得$\frac{\partial C}{\partial b}$


$\frac{\partial C}{\partial b}=\delta(z)-y$

由上面的推导可以看出，sigmoid函数的导数$\delta'(z)$被分子和分母约掉，最后的结果是正比于期望值和预测值的差，即为当期望值和预测值相差越大的时候，收敛速度会越快，很好地解决了平方和损失函数的收敛速度慢的问题。
### （四）、含义

交叉熵是用来衡量两个概率分布之间的差异。交叉熵越大，两个分布之间的差异越大，越对实验结果感到意外，反之，交叉熵越小，两个分布越相似，越符合预期。下面以**离散分布**为例讨论。 
$q(x)$表示估计x的概率分布，$p(x)$表示真实x的概率分布，交叉熵定义如下： 


$H(p(x),q(x))=H(p(x))+D(p(x)||q(x))$
$H(p(x))$表示p(x)的熵，定义如下： 


$H(p(x))=-\sum_{x\in X}p(x)logp(x)$
$D(p(x)||q(x))$表示$p(x)$和$q(x)$的KL距离（Kullback-Leibler divergence），也叫作相对熵，定义如下： 


$D(p(x)||q(x))=\sum_{x\in X}p(x)log\frac{p(x)}{q(x)}$

由此可得，交叉熵 


$H(p,q)=-\sum_{x\in X}p(x)logq(x)$

对于神经网络的二值输出（0或者1），假设神经网络输出$a$表示是输出1的概率（此时对应的$y=1$），那么$1-a$表示输出0的概率（此时对应的$1-y=0$），所以交叉熵可以定义成如下形式： 


$C=-(y\ln a+(1-y)\ln (1-a))$
## 三、softmax + log-likelihood

### （一）、softmax

softmax定义如下 


$z_j=\sum_kw_{jk}x_k+b_j$


$a_j=\frac{e^{z_j}}{\sum_ke^{z_k}}$
### （二）、log-likelihood



$C=-\ln{a_y}$
$a_y$表示类别y对应的预测概率，如果预测好的话，$a_y$会趋近于1，$C$会趋近于0，反之，$a_y$趋近于0，$C$趋近于极大。

### （三）、误差

根据链式法则 


$\frac{\partial C}{\partial w_{jk}}=\frac{\partial C}{\partial a_j}*\frac{\partial a_j}{\partial z_j}*\frac{\partial z_j}{\partial w_{jk}}=-\frac{1}{a_j}\frac{e^{z_j}*\sum_ke^{z_k}-e^{z_j}*e^{z_j}}{(\sum_ke^{z_k})^2}x_k=(a_j-1)*x_k$

同理 


$\frac{\partial C}{\partial b_{j}}=a_j-1$

跟上面的cross entropy类似，当$a_j$预测不好时，误差会很大，收敛会变快。
## 四、conclusion

### （一）、sigmoid

在激活函数使用sigmoid的前提之下，相比于quadratic cost function， cross entropy cost function具有收敛速度快和更容易获得全局最优（至于为什么更容易获得全局最优，个人感觉有点类似于动量的物理意义，增加收敛的步长，加快收敛的速度，更容易跳过局部最优）的特点。 

因为我们一般使用随机值来初始化权重，这就可能导致一部分期望值和预测值相差甚远。所以选择sigmoid作为激活函数的时候，推荐使用cross entropy。如果激活函数不是sigmoid，quadratic cost function就不会存在收敛速度慢的问题。

### （二）、softmax

对于分类问题，如果希望输出是类别的概率，那么激活函数选择使用softmax，同时使用log-likelihood作为损失函数。

## 五、reference

[http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function)
[https://en.wikipedia.org/wiki/Cross_entropy](https://en.wikipedia.org/wiki/Cross_entropy)










