# 神经网络-激活函数 - xmdxcsj的专栏 - CSDN博客





2015年12月13日 22:38:49[xmucas](https://me.csdn.net/xmdxcsj)阅读数：2086








## 一、种类

常见的神经网络激活函数包括sigmoid、softmax、relu、tanh等，具体函数形式和特点可以参考[1](#fn:footnote)

接下来简单说一下各类激活函数的特点。

### （一）、sigmoid
- 
输出值范围在0-1之间，非线性

- 
导数值简单，$y'=y(1-y)$

- 
当值很大或者很小的时候，导数值趋近于0，从而加重gradient vanish，收敛速度慢


### （二）、softmax
- 输出值在0-1之间，可方便表示概率，适合输出层使用

### （三）、relu

参考[2](#fn:footnote2)
- 
当输入x小于0的时候，输出为0，可以增加隐层节点的稀疏性

- 
计算快，导数为常数，不需要指数计算

- 
收敛速度快，因为导数值为常数，不会像sigmoid那样加重gradient vanish




### （四）、tanh
- 类似于sigmoid，存在导数值饱和的问题

## 二、选择

分类问题[3](#fn:footnote3)，最后一层使用softmax，损失函数使用cross entropy 

回归问题，最后一层使用sigmoid或者tanh，损失函数使用平方和 

层内部的非线性函数使用ReLU
- [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)[↩](#fnref:footnote)
- [https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning](https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning)[↩](#fnref:footnote2)
- [http://stackoverflow.com/questions/34229140/choosing-from-different-cost-function-and-activation-function-of-a-neural-networ?lq=1](http://stackoverflow.com/questions/34229140/choosing-from-different-cost-function-and-activation-function-of-a-neural-networ?lq=1)[↩](#fnref:footnote3)





