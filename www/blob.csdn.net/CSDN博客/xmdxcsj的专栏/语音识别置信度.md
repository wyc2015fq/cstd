# 语音识别置信度 - xmdxcsj的专栏 - CSDN博客





2015年10月10日 19:21:54[xmucas](https://me.csdn.net/xmdxcsj)阅读数：2490








## 发展

应用场景：smartly reject non-speech noises, detect/reject out-of-vocabularywords, detect/correct some potential recognition mistakes, clean up human transcriptionerrors in large training corpus, guide the system to perform un-supervisedlearning,
 provide side information to assist high-level speech understandingand dialogue management, and so on

首先，CM应用在对于非关键词的拒识，一开始采用grabage model对非关键词和噪音进行建模，后又使用rejection module用于区分非关键词和关键词以减少误警。

然后，CM在新词发现（比如out-of-vocabulary，OOV）上面的得到应用。

后来，CM在ASR的重要性凸现出来，有了基于MCE和MVE的区分度训练手段。

最后，由于对话系统的广泛应用，CM得到更多重视。

## Predictor features

能够很好地将正确识别结果和错误识别结果区分开来的特征都称为predictor，包括：

没帧的声学得分

n-best

词图中的候选路径

…

组合特征的这种方式效果不好

## Posterior probability

语音识别的算法可以用后验概率表示出来，如下图：

![](https://img-blog.csdn.net/20151010191845989?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

X表示声学特征

W表示识别结果

P(W)表示语言模型概率

P(X|W)表示似然概率，声学模型

P(X)表示声学特征的概率（人说这句话的概率），由于对于不同的W该值是常量，通常被忽略，所以最后算出的得分跟语音的置信度并不严格相等。

理论上，可以通过以下的全概率公式计算P(X):

![](https://img-blog.csdn.net/20151010191940174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


此处的H代表任何的假设，比如发音、单词、噪音和其他的事件，无法求解。

所以后需要有人对H进行了限制，比如all-phone方法、lattice方法等。其中lattice方法基于解码器的lattice计算后验概率，由于lattice构建的word graph是对所有候选的一种紧凑准确的表述形式，因此计算出的P(X)比较准确。

但是word graph相对复杂，一般使用nbest近似代替word graph。

### Word graph

Word graph上面的每一条边a代表一个word，C代表一条由START到END的路径，其概率如下：

![](https://img-blog.csdn.net/20151010191952246?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

第一项表示声学概率，第二项表示语言得分，h表示w之前的词历史。

计算每条边相对word graph的后验概率。

![](https://img-blog.csdn.net/20151010192000109?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

其中分子表示word graph中经过边a的所有途径；分母表示word graph的所有边的概率。可以使用前后向算法计算。

### 识别词的后验概率

可以用上边的后验概率作为识别词w的后验概率，但是缺陷是出现w的不只有这一条边a（w相同，但是w的起止时间不同），最好的方法是w选取起止时间对应边的最大值。

## Utterance verification

![](https://img-blog.csdn.net/20151010192027066?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

类似于置信度，可以使用LRT（likelihood ratio testing）对识别结果的准确性进行判断，LRT=P(X|H0)/P(X|H1)，其中P(X|H1)相对复杂，不容易计算，实际中可以采用background model或者anti-model的HMM来计算。H1对应的模型的训练中区分度训练是很重要的一部分。



## 参考文献



Confidence measures for speech recognition:A survey



