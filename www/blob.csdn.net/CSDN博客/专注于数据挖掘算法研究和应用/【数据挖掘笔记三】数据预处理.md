# 【数据挖掘笔记三】数据预处理 - 专注于数据挖掘算法研究和应用 - CSDN博客





2017年08月10日 10:05:10[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：6871
所属专栏：[算法导论专栏](https://blog.csdn.net/column/details/16283.html)











## 3.数据预处理

数据预处理技术：

数据清理用来清除数据中的噪声，纠正不一致；

数据集成将数据由多个数据源合并成一个一致的数据存储，如数据仓库；

数据归约通过如聚集、删除冗余特征或聚类来降低数据的规模；

数据变换（如规范化）把数据压缩到较小的区间，如[0,1]，可以提高涉及距离度量的挖掘算法的准确率和效率。

### 3.1数据预处理：概述

数据质量的三要素：准确性、完整性和一致性。然而，现实世界的大型数据库和数据仓库的数据都有不正确、不完整、不一致的特点，其原因或是由于采集错误、或是人为掩盖缺失、或由于格式不一致、或由于数据无法得到等。

影响数据质量的还有时效性（timeliness）、可信性（believability）、可解释性（interpretability）。

数据处理的主要步骤：

1）数据清零(datacleaning)，通过填写缺失的值，光滑噪声数据，识别或删除离群点，并解决不一致性来清理数据。

2）数据集成(dataintegration)，集成多个数据库、数据立方体或文本。

在为数据仓库准备数据时，数据清理和集成作为预处理步骤进行。

3）数据归约（datareduction），得到数据集的简化表示，小得多，但能够产生同样或接近的分析结果，数据归约策略包括维归约和数值归约。

在维归约中，使用数据编码方案，可得到原始数据的简化或压缩表示，包括数据压缩技术小波变换和主成分分析，以及属性子集选择和属性构造。

在数值归约中，使用参数模型，如归回和对数线性模型，或非参数模型，如直方图、聚类、抽样或数据聚集，用较小的表示取代数据。

4）数据变换（datatransformation），规划化、数据离散化和概念分层。

现实世界的数据一般是脏的、不完整的和不一致的。这句话中，去掉数据两个字，意思也是一样的吧。数据预处理就是可以改进数据质量，从而有助于其后挖掘的准确率和效率。高质量的决策必然依赖于高质量的数据，因此数据预处理是知识发现过程的重要步骤。检测数据异常，尽早地调整数据，并归约待分析的数据，将为决策带来高回报。

### 3.2数据清理

数据清理一般工作是填充缺失的值、光滑噪声并识别离群点、纠正数据中的不一致。

1）缺失值

填充属性值缺失的方法有：

a、忽略元组：当缺少类标号时可如此处理。忽略元组，等于该元组的剩余属性值也抛弃；

b、人工填充缺失值；

c、使用一个全局常量填充缺失值；

d、使用属性的中心度量（如均值或中位数）填充缺失值；

e、使用与给定元组属同一类的所有样本属性均值或中位数；

f、使用最可能的值填充缺失值：可用回归、贝叶斯、决策树等模型来推理归纳确定。

2）噪声数据

噪声(noise)是被测量的变量的随机误差或方差。数据光滑技术：

a、分箱（binning）：通过考察数据的近邻（即周围的值）来光滑有序数据值。分箱方法实现局部光滑，将有序的值分布到桶或箱中，可用箱均值光滑、箱中位数光滑、箱边界光滑；

b、回归（regression）：用一个函数拟合数据来光滑数据。线性回归和多元线性回归。

c、离群点分析（outlier analysis）：用聚类检测离群点。聚类将类似的值组织成群或簇，落在簇集合之外的值是离群点。

数据光滑方法用于数据离散化（一种数据变换形式）和数据归约。

3）数据清理作为一个过程

缺失值、噪声和不一致性导致不正确的数据，在处理缺失值和光滑噪声技术上，进一步将数据清理作为一个过程来看待，就是偏差检测（discrepancy detection）和数据变换（纠正偏差）两步迭代执行。

检测偏差还要根据唯一性原则、连续性原则和空值原则，也依赖功数据清洗工具（data scrubbing tool）和数据审计工具（data auditingtool）。

数据变换也用数据迁移工具（data migration tool）和ETL（Extraction/Transformation/Loading）。

### 3.3数据集成

数据集成是合并来自多个数据存储的数据。良好的集成有助于减少数据集的冗余和不一致，提供后面挖掘过程的准确性和速度。不过数据语义的多样性和结构对数据集成带来挑战。

1）实体识别问题

实体识别问题指的是在集成时，一个数据库的属性和另一个数据库的属性匹配问题。

2）冗余和相关分析

如果一个属性能有另一个或另一组属性所推导出，则该属性是冗余的。冗余可通过相关分析检测到。给定两个属性，相关分析可以度量一个属性和另一个属性的蕴含关系。

![](https://img-blog.csdn.net/20170810100214488?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170810100229754?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




3）元组重复

元组重复，要去掉重复的行。

4）数据值冲突的检测和处理

同一属性，单位不同可能导致数据值的冲突，需要检测和处理。

### 3.4数据归约

数据归约（datareduction）技术可以用来得到数据集的归约表示，小得多，但接近于保持原始数据的完整性。

1）数据归约策略概述

维归约（dimensionalityreduction）减少所考虑的随机变量或属性的个数，方法包括小波变换、主成分分析，把原数据变换或投影到较小的空间。属性子集选择也是一种维归约方法，其中不相关、弱相关或冗余的属性或维被检测或删除。

数量归约（numerosityreduction）用替代的、较小的数据表示形式替换原始数据，包括参数方法和非参数方法。参数方法，使用模型估计数据，使得一般只需要存放模型参数，而不是实际数据（离群点可能也要存放），如回归和对数-线性模型；非参数方法包括直方图、聚类、抽样和数据立方体聚集。

数据压缩（datacompression）使用变换，以便得到原数据的归约或压缩表示。如果原数据能够从压缩后的数据重构，而不损失信息，则该数据归约称为无损的；如果只能近似重构原数据，则该数据归约称为有损的。

2）小波变换

离散小波变换（DWT）是一种线性信号处理技术，用于数据向量X时，将它变换成不同的数值小波向量X*。两个向量具有相同的长度，当这种技术用于数据归约时，每个元组看做一个n维数据向量，即X=(x1,x2,…,xn)，描述n个数据库属性在元组上的n个测量值。

虽然小波变换后的向量维度不变，但小波变换后仅存放一部分最强的小波系数，就可保留近似的压缩数据。如保留大于用户设定阈值的小波系数，而不满足的置为0，这样可利用数据稀疏特点计算。小波变换技术可用于消除噪声，而不会光滑掉数据的主要特征，因此可以有效用于数据清理。给定一组系数，使用所用的DWT的逆，可构造原数据的近似。

DWT和离散傅里叶变换（DFT）有密切关系。DFT是一种涉及正弦和余弦的信号处理技术。一般来说，DWT也是一种有损压缩技术。对于给定的数据向量，如果DWT和DFT保留相同数目的系数，则DWT将提供原数据更准确的近似。因此，对于相同的近似，DWT所需空间小于DFT。与DFT相较，小波空间局部性相当好，有助于保留局部细节。只有一种DFT，但有若干族DWT。

DWT一般使用层次金字塔算法（pyramid algorithm），在每次迭代时将数据减半，导致计算速度很快，过程如下：

Ø  输入数据向量的长度L（2的整数幂），可在数据向量加0满足这一条件（L≥n），n是实际向量维度；

Ø  每个变换应用两个函数，一个是数据光滑函数，如求和或加权平均；一个是提取数据细节特征，如加权差分；

Ø  两个函数作用于X中的数据点对，即作用于所有的测量对（x2i,x2i+1），生成出两个L/2长度的数据集；

Ø  两个函数递归地作用于前面循环得到的数据集，直到得到的结果数据集的长度为2；

Ø  由以上迭代的数据集中选择的值被指定为数据变换的小波系数。

等价地，可以将矩阵乘法用于输入数据，以得到小波系数。所用的矩阵依赖于给定的DWT。矩阵必须使标准正交的，即它们的列是单位向量并相互正交，使得矩阵的逆是它的转置。通过将矩阵分解成几个稀疏矩阵的乘积，对于长度为n的输入向量，快速DWT算法的复杂度为O(n)。

小波变换可用于多维数据，如数据立方体，计算复杂性关于立方体中单元的个数是线性的。对于稀疏或倾斜数据和具有有序属性的数据，小波变换效果比较耗。小波变换的有损压缩优于JPEG压缩，也有很多实际应用，如指纹图像压缩、计算机视觉、时间序列分析和数据清理。

3）主成分分析

假设待归约的数据由用n个属性或维描述的元组或数据向量组成。主成分分析（principalcomponents analysis）PCA（也称Karhunen-Loeve，K-L方法）搜索k个最能代表数据的n维正交向量，其中k≤n。将原数据投影到一个小得多的空间上，实现维归约。

PCA基本过程如下：

Ø  规范化输入数据，使得每个属性都落入相同的区间，避免具有较大定义域的属性不会支配具有较小定义域的属性；

Ø  PCA计算k个标准正交向量，作为规范化输入数据的基。这些是单位向量，每一个都垂直于其他向量，这些向量称为主成分，输入数据是主成分的线性组合；

Ø  对主成分按重要性或强度降序排列，主成分本质上充当数据的新坐标系，提供关于方差的重要信息，就是说，多坐标轴进行排序，使得第一个坐标轴显示数据的最大方差，第二显示数据的次大方差，如此下去，可识别数据中的组群或模式；

Ø  主成分根据重要性降序排列，可以去掉较弱成分（即方差较小的）来归约数据，使用最强的主成分，可重构元数据。

PCA可用于有序或无序的属性，并且可以处理稀疏和倾斜数据。多于二维的多维数据可通过将问题归约为二维来处理。主成分用于多元回归和聚类分析的输入。与小波变换相比，PCA能够更好地处理稀疏数据，而小波变换更适合高维数据。

4）属性子集选择

属性子集选择通过删除不相关或冗余的属性（或维）来减少数据量。属性子集选择的目标是找出最小属性集，使得数据类的概率分布尽可能地接近使用所有属性得到的原分布。

找出最佳属性子集显然是最重要的。对于n个属性来说，有2n个可能子集，穷举搜索不现实。对于属性子集选择，通常使用压缩搜索空间的启发式算法。通过局部最优选择，获得全局最优解，或逼近最优解。

评估属性可通过统计显著性检验来确定，这种检验假定属性是相互独立的。也通过使用属性评估度量，如分类决策树所用的信息增益度量。属性子集选择的基本启发式方法包括：

Ø  逐步向前选择：由空属性集作为归约集开始，确定原属性集中最好的属性，并将它添加到归约集中，在其后的每一次迭代中，将剩下的原属性集中的最好的属性添加到该集合中；

Ø  逐步向后删除：由整个属性集开始，在每一步中，删除尚在属性集中最差的属性；

Ø  逐步向前选择和向后删除的组合：将逐步向前选择和逐步向后删除方法结合在一起，每一步选择一个最好的属性，并在剩余属性中删除一个最差的属性；

Ø  决策树归纳：决策树算法，如ID3、C4.5和CART，决策树归纳构造一个类似于流程图的结构，其中每个内部（非树叶）结点表示一个属性上的测试，每个分枝对应于测试的一个结果；每个外部（树叶）结点表示一个类预测，在每个结点上，算法选择最好的属性，将数据划分为类。

当决策树归纳用于属性子集选择时，由给定的数据构造决策树，不出现在树中的所有属性假定是不相关的，出现在树中的属性形成归约后的属性子集。上述方法的结束条件不同，可以使用一个度量阈值来决定何时停止属性选择过程。

在某些情况下，可基于其他属性创建一些新属性。属性构造可提高准确性和对高维数据结构的理解。通过组合属性，属性构造可以发现关于数据属性间联系的缺失信息，对知识发现是有用的。

5）回归和对数线性模型：参数化数据归约

回归和对数线性模型可以用来近似给定的数据。在线性回归中，对数据建模，使之拟合到一条直线，可用因变量y表示自变量x的线性函数y=wx+b，假定y的方差是常量。y和x是数值数据库属性，回归系数w和b分别为直线的斜率和y轴截距。回归系数可用最小二乘法求解，其最小化分离数据的实际直线与该直线的估计之间的误差。多元回归则是线性回归的扩展，允许用两个或多个自变量的线性函数对因变量y建模。

对数线性模型（log-linear model）近似离散的多维概率分布。给定n维元组的集合，可把每个元组看做n维空间的点。对于离散属性集，可用对数线性模型，基于维组合的一个较小子集，估计多维空间中每个点的概率，这使得高维数据空间可以由较低维数据空间构造。因此，对数线性模型也可用于维归约（由于较低维空间的点通常比原来的数据点占据的空间要少）和数据光滑（因为与较高维空间的估计相比，较低维空间的聚集估计受抽样变化的影响较小）。

回归和对数线性模型都可用于稀疏数据，对处理倾斜数据，回归更好，对高维数据，对数线性模型表现出更好的伸缩性。

6）直方图

直方图使用分箱来近似数据分布，是一种流行的数据归约形式。属性A的直方图（histogram）将A的数据分布划分为不相交的子集或桶。如果每个桶只代表单个属性值/频率对，则该桶称为单值桶。桶表示给定属性的一个连续区间。

等宽直方图中，每个桶的宽度区间是相等的。等频（等深）直方图中，每个桶大致包含相同个数的邻近数据样本。对于近似稀疏和稠密数据，以及高倾斜和均匀的数据，直方图是有效的。

7）聚类

聚类技术把数据元组看做对象，将对象划分为群或簇，使得在一个簇中的对象相互相似，而与其他簇中的对象相异。通常，相似性基于距离函数，用对象在空间中的接近程度定义。簇的质量用直径表示，直径是簇中两个对象的最大距离。形心距离是簇质量的另一种度量，定义为簇中每个对象到簇形中（表示平均对象，或簇空间中的平均点）的平均距离。

在数据归约中，用数据的簇代表替换实际数据，其有效性依赖数据的性质。对于被污染的数据，能够组织成不同的簇的数据，比较有效。

8）抽样

抽样可以作为一种数据归约技术使用，可用数据的小得多的随机样本（子集）表示大型数据集。假定大型数据集D包含N个元组，常见抽样方法：

Ø  s个样本无放回简单随机抽样（SRSWOR）：从D中抽取s个样本，而且每次抽取一个样本，不放回数据集D中；

Ø  s个样本有放回简单随机抽样（SRSWR）：从D中抽取一个元组后，记录它，然后放回原处，再参与下一次抽样；

Ø  簇抽样：如果D中的元组被分组，放入M个互不相交的簇，则可以得到s个簇的简单随机抽样（SRS），其中s＜M。

Ø  分层抽样：D被划分成不相交的部分，称为层，通过对每一层的SRS可以得到D的分层抽样，特别对于数据倾斜下有效。

采用抽样进行数据归约的优点是，得到样本的花费正比例于样本集的大小s，而不是数据集的大小N。抽样的复杂度可能亚线性（sublinear）于数据的大小。其他数据归约的技术至少需要完全扫描D。对于固定的样本大小，抽样的复杂度仅随数据的维数n线性地增加，而其他技术，如直方图，复杂度随n呈指数增长。

用于数据归约时，抽样最常用来估计聚集查询的问答。在指定的误差范围内，可以确定（使用中心极限定理）估计一个给定的函数所需的样本大小。样本的大小s相对于N可能非常小。对于归约数据的逐步求精，抽样是一种自然选择。通过简单地增加样本大小，这样的集合可以进一步求精。

9）数据立方体聚集

数据立方体存储多维聚集信息。每个属性都可能存在概念分层，允许在多个抽象层进行数据分析。数据立方体提供对预计算的汇总数据进行快速访问，适合联机数据分析和数据挖掘。

在最低抽象层创建的立方体称为基本方体（base cuboid）。最高层抽象的立方体称为顶点方体（apex cuboid）。

### 3.5数据变换与数据离散化

在数据预处理阶段，数据被变换或统一，使得挖掘过程可能更有效，挖掘的模式更易理解。数据变换策略包括：

Ø  光滑（smoothing）：去掉数据中的噪声，技术包括分箱、回归和聚类；

Ø  属性构造（或特征构造）：由给定的属性构造新的属性并添加到属性集中；

Ø  聚集：对数据进行汇总或聚集；

Ø  规范化：把属性数据按比例缩放，使之落入一个特定的小区间；

Ø  离散化：数值属性的原始值用区间标签或概念标签替换；

Ø  由标称属性产生概念分层：属性，如street，可以泛化到较高概念层，如city或country。

离散化技术可以根据如何进行离散化加以分类，如根据是否使用类信息，或根据离散的进行方向（自顶向下或自底向上）来分类。如果离散过程使用类信息，则为监督的离散化（supervised discretization），否则是非监督的（unsupervised）。如果离散化过程首先找出一个或几个点（称做分裂点或割点）来划分整个属性区间，然后在结果区间上递归地重复这一个过程，则成为自顶向下离散化或分裂。自底向上离散化和合并正好相反，将所有的连续值看做可能的分裂点，通过合并邻域的值形成区间，然后在结果区间递归地应用这一过程。

1）通过规范化变换数据

所用的度量单位可能影响数据分析。为避免对度量单位的依赖性，数据应该规划化或标准化。变换数据，使之在一个较小的共同区间，如[-1,1]或[0,1]。

规范化数据试图赋予所有属性相等的权重。对于涉及神经网络的分类算法或基于距离度量的分类（如最近邻分类）和聚类，规范化特别有用。如果使用神经网络后向传播算法进行分类挖掘，对训练元组中每个属性的输入值规范化将有助于加快学习阶段的速度。对于基于距离的方法，规范化可以帮助防止具有较大初始值域的属性与具有较小初始值域的属性相比权重过大。在没有数据的先验知识时，规范化也有用的。

规范化方法，有：最小-最大规范化、z分数规范化、按小数定标规范化。令A是数值属性，具有n个观测值v1，v2，…，vn。

![](https://img-blog.csdn.net/20170810100353675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




2）通过分箱离散化

分箱是一种基于指定的箱个数的自顶向下的分裂技术。通过使用等宽或等频分箱，然后用箱均值或中位数替换箱中的每个值，可以将属性值离散化，就想用箱的均值或箱的中位数光滑一样。

分箱并不使用类信息，因此是一种非监督的离散化技术。对用户指定的箱个数很敏感，也容易受到林群点的影响。

3）通过直方图分析离散化

像分箱一样，直方图分析也是一种非监督离散化技术，因为它不使用类信息。直方图把属性A的值划分成不相交的区间，称做桶或箱。

直方图分析算法可以递归地用于每个分区，自动地产生多级概念分层，直到达到一个预先设定的概念层数，过程终止。也可对每一层使用最小区间长度来控制递归过程。最小区间长度设定每层每个分区的最小宽度，或每层每个分区中值的最少数目。

4）通过聚类、决策树和相关分析离散化

a、聚类分析是一种流行的离散化方法，通过属性A的值划分成簇或组，聚类算法可以用来离散化数值属性A。聚类考虑A的分布以及数据点的邻近性，因此可以产生高质量的离散化结果。遵循自顶向下的划分策略或自底向上的合并策略，聚类可以用来产生A的概念分层，其中每个簇形成概念分层的一个结点。在前一种策略中，每一个初始簇或分区可以进一步分解成若干子簇，形成较低的概念层；在后一种策略中，通过反复地对邻近簇进行分组，形成较高的概念层。

b、决策树分类可用于离散化，使用自顶向下划分方法。离散化决策树使用类标号信息，是监督的离散化方法。决策树分类离散化技术主要思想是，选择划分点使得一个给定的结果分区包含尽可能多的同类元组。熵是最常用于确定划分点的度量。为了离散化数值属性A，该方法选择最小化熵的A的值作为划分点，并递归地划分结果区间，得到分层离散化，形成A的概念分层。

![](https://img-blog.csdn.net/20170810100444027?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




5）标称数据的概念分层产生

四种标称数据概念分层的方法：

a、由用户或专家在模式级显示地说明属性的部分序；

b、通过显示数据分组说明分层结构的一部分；

c、说明属性集但不说明它们的偏序；

d、只说明部分属性集；

模式和属性值计数信息都可以用来产生标称数据的概念分层。使用概念分层变换数据使得较高层的知识模式可以被发现。

总感觉翻译过来的特别怪。

### 3.6小结

1）数据质量用准确性、完整性、一致性、时效性、可信性和可解释性定义。质量基于数据的应用目的评估。

2）数据清理例程试图填补缺失的值，光滑噪声同时识别离群点，并纠正数据的不一致性。数据清理通常是一个两步的迭代过程，包括偏差检测和数据变换。

3）数据集成将来自多个数据源的数据整合成一致的数据存储。语义异种性的解决、元数据、相关分析、元组重复检测和数据冲突检测都有助于数据的顺利集成。

4）数据归约得到数据的归约表示，而使得信息内容的损失最小化。数据归约方法包括维归约、数量归约和数据压缩。维归约减少所考虑的随机变量或维的个数，方法包括小波变换、主成分分析、属性子集选择和属性构造。数量归约方法使用参数或非参数模型，得到原数据的较小表示，参数模型只存放模型参数，而非实际数据，如回归和对数线性模型；非参数方法包括直方图、聚类、抽样和数据立方体聚集。数据压缩方法使用变换，得到原数据的归约或压缩表示，如果原数据可以由压缩后的数据重构，而不损失任何信息，则数据压缩是无损的，否则，是有损的。

5）数据变换例程将数据变换成适于挖掘的形式。如规范化中，属性数据缩放，使其在较小区间，也包括数据离散化和概念分层技术。

6）数据离散化通过把值映射到区间或概念标号变换数值数据。这种方法可以用来自动地产生数据的概念分层，而概念分层允许在多个粒度层进行挖掘。离散化技术包括分箱、直方图分析、聚类分析、决策树分析和相关分析。对于标称数据，概念分层可以基于模式定义以及每个属性的不同值个数产生。

尽管已经有很多数据预处理的方法，由于不一致或脏数据的数量巨大，以及问题本身的复杂性，数据预处理仍然是一个活跃的研究领域。







