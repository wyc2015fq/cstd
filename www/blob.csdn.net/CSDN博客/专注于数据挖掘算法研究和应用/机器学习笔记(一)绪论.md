# 机器学习笔记(一)绪论 - 专注于数据挖掘算法研究和应用 - CSDN博客





2017年01月03日 12:45:13[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：2886
所属专栏：[机器学习专栏](https://blog.csdn.net/column/details/16315.html)









## 1.绪论

### 1.1引言

机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能；经验，则以数据的形式存在，故而，机器学习所研究的，正是在计算机上从数据中产生模型的算法，即学习算法。基于学习算法和经验数据所产生的模型，可以应用到新情况的分析和判断。

机器学习是研究关于学习算法的学科，比较形式化的定义是：假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则就说关于T和P，该程序对E进行了学习。

模型用以泛指从数据中学得的结果。模型指全局性结果，如一颗决策树；模型指局部性结果，如一条规则。机器学习是关于学习算法，通过学习算法，从经验中（样本数据中）训练出一个模型，并应用模型。机器学习的关心点就是学习算法，能够从经验中提取模型。

### 1.2基本术语

1）关于数据的术语

数据是经验的表示，是机器学习的基础。一个数据集是一组记录的集合，其中每条记录是关于一个事件或对象的描述，称为示例或样本。

反映事件或对象在某方面的表现或性质的事项，称为属性或特征，如姓名；属性或特征上的值，称为属性值，如张三；属性张成的空间成为属性空间或样本空间或输入空间或特征向量。如姓名、年龄、职业作为三个维度刻画一个人这样对象的特征，在三维空间中可以找到对应的点。

总结上这个关系，横向上，属性和属性值构成一个对象，一个对象就是一条记录，一组记录构成一个集合就是数据集，关系就是属性->记录->数据集；纵向上，属性和属性值构成一个对象，一个对象就是一个特征向量，有多少个属性就构成多少维的属性空间，关系就是属性->向量->空间。如下表：

![](https://img-blog.csdn.net/20170103124806607?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


数据集有m条记录，每条记录有n个维度刻画。

现在形式化定义下特征向量及其维度：

![](https://img-blog.csdn.net/20170103124014581?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


2）关于学习的术语

从数据中学得模型的过程称为学习或训练，该过程通过执行某个学习算法来完成。训练过程中使用的数据称为训练数据，其中每个样本称为一个训练样本，训练样本组成的集合称为训练集。学得模型体现了数据的某种潜在规律，是一种假设，而规律自身是真相或真实，学习过程就是不断找出或逼近真相。将模型称为学习器，是给定学习算法在给定数据和参数空间上的实例化。

![](https://img-blog.csdn.net/20170103124303834?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




学习到的模型，是要用来分析和预测新样本，衡量这种能力的称为泛化能力。泛化能力是指学得的模型适用于新样本的能力。具有强泛化能力的模型能很好适用于整个样本空间。

训练集一般是整个样本空间一个小的采样，如何采样可以很好地反映出样本空间的特性？通过假设样本空间中全体样本服从一个未知分布（distribution）D，所获得的样本都是独立且从该分布上采样获得，即独立同分布（independent and identically distributed,简称i.i.d）。从一个服从分布D的样本空间中采样独立的样本作为训练集，D分布和训练集是正向互动，越多训练集对D分布的信息掌握越多，就越有可能通过学习获得具有强泛化能力。

总结下，需要掌握的概念：

第一：输入空间，属性和属性值、记录或特征向量、数据集（m条，d维）；

第二：模型学习，监督学习（带标记）和无监督学习（无标记）；

第三：输出空间，离散的分类和连续的回归；聚类的簇；

第四：模型评估，泛化能力，独立同分布D。

### 1.3假设空间

归纳（induction）和演绎（deduction）是科学推理的两大基本手段。归纳，是从特殊到一般的泛化过程，从个体升华到规律；而演绎，则是从一般到特殊的特化过程，从规律（基础原理）推演出具体情况。在数学证明中，这两个是常见手法，如在数学公里系统中，基于一组公里和推理规则推导出与之相洽的定理，是为演绎。显然，机器学习是从样例中学习，从一个个具体样例中总结出一个规则一个模型，这是一个归纳过程，称之为归纳学习（inductive learning）。

归纳学习，广义上指从样例中学习，狭义是指从训练数据中学得概念，即概念学习。概念学习过程是一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配（fit）的假设。假设空间，就是样本训练集中所有可能的特征所有可能取值的规模。

训练集中每条记录的特征取值都是假设空间中的一个，我们定义假设空间中的一个假设（明确定义了特征及其取值），然后从训练集中找出匹配的记录，用以证明这个假设就是我们所要学习的概念。可以理解，假设空间就是记录的类别集合，归纳学习就是判断每个记录属于这个集合中的那个类别，这个类别就是一个概念，对属于这个类别的训练集进行学习就能获得这个概念。

现实中，假设空间规模很大，而学习过程是基于有限样本训练集进行的，所以样本集可能存在多个假设。比如对于二分类学习算法，我们希望从训练集中学习到1或0的假设空间。可以这样理解，假设空间是特征及其值得组合，从训练集中学习到的假设就是概念。

### 1.4归纳偏好

通过对训练集的学习得到的模型或概念是假设空间中的一个假设，但有时可能存在多个假设与其一致。就是说，特征的不同取值（不同的假设）可能是同样的概念，这样新的样本就不知道分类到那个类别了，于是乎，对模型或假设的选取就很重要。选择怎样的模型更好呢？归纳偏好，机器学习算法在学习过程中对某种类型假设的偏好。归纳偏好，终归也是去到特征及其值得选择上。

归纳偏好，可以看做是学习算法的一种启发式进化，在庞大的假设空间中进行偏好选择。文中提到的存在多条曲线与有限样本训练集一致的案例，可以很清晰阐述。偏好选择，采用什么策略呢？自然是具体算法和场景具体来定。奥卡姆剃刀的选择原则是最基本的：若有多个假设与观察一致，则选最简单的那个。

选择一个好的模型，就是模型评估中的泛化能力，归纳偏好则从假设空间中集合中找出一个最佳假设。文中通过对二分类问题的所有真实目标模型进行误差求和，得出学习算法和误差的无关性。而这个数学证明是基于真实目标函数是均分分布的。从这个过程中得出：一个学习算法的归纳偏好与问题本身相关。

至此，我们可以梳理出几个重要概念：

1）假设空间，所有特征及其取值的组合，每种组合有一个目标（类别）；

2）样本空间，训练集中的所有记录；

3）归纳学习，建立样本空间和假设空间的映射关系；

4）归纳偏好，当样本空间对应多个假设时，选择一个假设作为最佳模型；

### 1.5发展历程

大致三个过程：基于逻辑推理的推理期（符号表示）、基于知识工程的知识学习（规则匹配）、基于统计数理的机器学习。

机器学习的发展历程，也有赖于相关技术的突破和齐头并进，如今的大数据实际是重要的助力。这里引入信息熵的定义。

信息熵：信息的量化度量，离散随机事件的出现概率。信息的基本作用就是消除人们对事物的不确定性。信息的不确定性越大，熵也就越大，要掌握确切信息所需要的信息量也就越大。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高；信息熵是系统有序化程度的一个度量。

通俗地理解，度量信息的熵，是用来指示信息的不确定性程度。熵越大，需要越多的信息来定义事件。那么怎么计算熵呢？

![](https://img-blog.csdn.net/20170103124446579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


信息熵的一个重要应用领域就是自然语言处理。例如，一本50万字的中文书平均有多少信息量。我们知道，常用的汉字约7000字。假如每个汉字等概率，那么大约需要约13比特（即13位二进制数，213=8192）表示一个汉字。

应用信息熵就是，一个汉字有7000种可能性，每个可能性等概率，所以一个汉字的信息熵是： 

H=-((1/7000)·log(1/7000)+(1/7000)·log(1/7000)+…(1/7000)·log(1/7000))=12.77(bit)

实际上由于前10%汉字占常用文本的95%以上，再考虑词语等上下文，每个汉字的信息熵大约是5比特左右。所以一本50万字的中文书，信息量大约是250万比特。需要注意这里的250万比特是个平均数。

### 1.6应用现状

大数据时代的三大关键技术：机器学习、云计算、众包。收集、存储、传输、管理大数据的最终目的还是利用数据，而机器学习显然是重要的数据挖掘技术。

以大数据为应用标签，旗下囊括了众多技术，核心还是数据挖掘的机器学习。

人类如何学习？机器如何学习？我相信随着大数据的相关技术和理论的逐步前进，机器学习也将不断演化。有一天图灵测试的结果，人类和机器应该是50%的，人机一体。

### 1.7阅读材料

免费机器学习算法库Weka：基于Java开发，http://www.cs.waikato.ac.nz/ml/weka/



