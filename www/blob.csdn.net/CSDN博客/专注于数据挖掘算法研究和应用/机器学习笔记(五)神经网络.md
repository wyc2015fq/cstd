# 机器学习笔记(五)神经网络 - 专注于数据挖掘算法研究和应用 - CSDN博客





2017年02月08日 11:18:49[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：5569
所属专栏：[机器学习专栏](https://blog.csdn.net/column/details/16315.html)









## 5.神经网络

### 5.1神经元模型

神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。对这句话的理解，简单提要下，主角是简单单元（输入单元、功能单元），特性是适应性和并行互连，功能是模拟生物神经反应。

神经网络是一个数学模型，其最基本的成分是神经元（neuron），即简单单元。在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个阈值（threshold），那么它就会被激活，即兴奋起来，向其他神经元发送化学物质。这个过程，神经网络模型加以数学简化并模拟。实际上，很多数学模型都源于对自然和人的观察所得，如飞机模型是模拟鸟。

在这个模型中，很重要就是神经元的互连以及输入和输出（阈值触发）。从最简单的M-P神经元模型来看，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的链接（connection）进行传递，神经元收到的总输入值与神经元的阈值进行比较，然后通过激活函数（activation function）处理以产生神经元的输出。

![](https://img-blog.csdn.net/20170208110612157?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




理想中的激活函数是阶跃函数，将输入值映射为输出值0和1,1对应于神经元兴奋，0对应于神经元抑制。不过阶跃函数不具有连续和光滑性质，因此常用sigmoid函数。

如上定义，一个神经网络是包含许多参数的数学模型，模型中包含若干函数，所以不管是否真的模拟了生物神经网络，归根到底还是要数学来支撑神经网络学习。下文深入看模型中的函数和参数是如何通过机器学习获得，从而构建具有一定层次结构的神经网络。

### 5.2感知机与多层网络

感知机（Perceptron）由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，也称阈值逻辑单元（threshold logic unit）。感知机可实现逻辑与、或、非运算，通过给定训练集，权重wi和阈值 可通过学习得到。如果将阈值 看做是第m+1个输入元，那么输入值xm+1是-1.0（称为哑结点，dummy node），权重值就是wm+1，如此可统一为对权重的学习。感知机的权重学习规则相对简单，给定训练样例（x,y），若当前感知机的输出为y-，则感知机权重将调整为：![](https://img-blog.csdn.net/20170208110828081?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
就是根据实际样例y值和感知机模型输出值y-的差距进行权重调整，如果二者相当（预测正确），那就不用调整。

感知机只有输出层神经元进行激活函数处理，就是只拥有一层功能神经元（functional neuron），其学习能力非常有限。感知机可容易实现的逻辑与、或、非操作，是线性可分（linearly separable）问题，若两类模式是线性可分的，则存在一个线性超平面将它们分开，这样的感知机在学习权重值过程中会收敛（converge），否则会发生振荡（fluctuation），权值难以稳定，如异或操作。这个例子，可知，对神经网络模型的权值（阈值已统一为第m+1个权值）学习过程中，具备单层功能神经元是不够的（可解决线性可分问题），在解决非线性可分为问题时，需要引入多层功能神经元。多层功能神经元，是在输入层和输出层之间加上隐含层（hidden
 layer），隐含层和输出层都是具有激活函数的功能神经元。

多层前馈神经网络（multi-layer feedforward neural networks）：每层神经元与下层神经元全互连，同层神经元之间不存在互连，也不存在跨层互连的神经元，从拓扑结构上看，不存在回或环路。输入层神经元接收外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。输入层神经元仅接收输入，不进行函数处理，隐含层和输出层包含功能神经元，处理数据。

神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权（connection weight）以及每个功能神经元的阈值，简言之，神经网络模型要学得就是连接权和阈值。下面就要看用什么算法来学习出神经网络模型的权值和阈值。

### 5.3误差逆传播算法

误差逆传播算法（error BackPropagation，BP算法）是训练多层前馈神经网络模型的常见算法。

1）定义一个多层前馈神经网络

给定训练集D={(x1,y1),(x2,y2),…,(xm,ym)}，其中xi∈Rd，yi∈Rl，输入示例有d个属性描述，输出l维实值向量。若输入示例中d个属性含离散属性，则需进行处理，若属性值之间存在序关系则进行连续化，否则转化为k维向量（k是属性值数），就是变成k个属性。

![](https://img-blog.csdn.net/20170208111021825?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




即真实输出和预测输出的误差，目标是训练出均方误差最小，该函数关系到上述定义的阈值和权值，共有(d+l+1)q+l个参数。其中，输入层到隐层有d*q个权值、隐层到输出层有q*l个权值，隐层有q个阈值、输出层有l个阈值。

4）BP算法训练

BP是一个迭代算法，在迭代的每一轮中采用上文定义的感知机学习规则来持续优化参数，直到目标函数 最小值。

![](https://img-blog.csdn.net/20170208111123491?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170208111317936?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




一个训练样例更新连接权和阈值。通过推导基于累积误差最小化的更新规则，可得到累积误差逆传播（accumulated error backpropagation）算法。标准BP算法每次更新都只针对单个样例，参数更新非常频繁，而且对不用样例进行更新的效果可能出现抵消现象，因此，为了达到同样的累积误差极小点，标准BP算法往往需要进行更多次数的迭代。累积BP算法，直接针对累积误差最小化，在读取整个训练集D一遍后才对参数进行更新，其参数更新的频率低，但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得更好的解，尤其是在训练集D非常大时更明显。标准BP和累积BP类似于标准梯度下降法和随机梯度下降法。

经过证明，只需一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数。然后，如何设置隐层神经元的个数是一个未决问题，在实际任务中，需要通过试错法不断调整。其实，到这里，可以理解的就是，多层神经网络的参数个数和参数值都是训练的目标。

![](https://img-blog.csdn.net/20170208111452780?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)






### 5.4全局最小和局部极小

BP算法中，用E表示神经网路在训练集上的误差，它是关于连接权和阈值的函数。实际上，神经网络的训练过程就是一个参数寻优过程，在参数空间中，寻找一组最优参数使得E最小。BP算法是基于梯度下降法来求解最小。

最小，有全局最小（global minimum）和局部最小（local minimum）。局部最小就是参数空间中的某个点，其邻域点的误差函数值均不小于该点的函数值；全局最小就是参数空间所有的点的误差函数值均不小于该点的误差函数值。

在一个参数空间内，梯度为零的点，只要其误差函数值小于邻点的误差函数值，就是局部最小点，可能存在多个局部最小值，但却只有一个全局最小值。在参数寻优过程中，期望找到的是全局最小值。

采用梯度下降寻优，迭代寻找最优参数值，每次迭代中，先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，则已达到局部最小，更新量为零，意味着参数的迭代更新停止。那么，此时的局部最小是否就是全局最小呢？如果误差函数存在多个局部最小，就无法保证一定能找到全局最小。如果参数寻优陷入局部最小，那自然不是想要的结果。

在现实任务中，常采用以下策略来试图跳出局部最小，从而进一步接近全局最小：

1）以多组不同参数值初始化多个神经网络，按标准BP方法训练后，取其中误差最小的解作为最终参数；

2）使用模拟退火（simulated annealing）技术，模拟退火在每一步都以一定的概率接收比当前解更差的结果，从而有助于跳出局部最小；在每步迭代过程中，接受次优解的概率回逐步降低，从而保证算法的稳定；

3）使用随机梯度下降，与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算时加入了随机因素，于是即便陷入局部最小点，其计算出的梯度可能不为零，也就有机会跳出局部最小点继续搜索。

值的一说的是，这些策略都是启发式，并无实际保证可以找打全局最优。对此，我们应该明白，机器学习算法本身多数就是基于启发式的，或可见效，但需要在实际任务中逼近。遗传算法也常用来训练神经网络以更好地逼近全局最小。

### 5.5其他常见神经网络

1）RBF网络

RBF（RadialBasis Function，径向基函数）网络是一种单隐层前馈神经网络，使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。假定输入为d维向量x，输出为实值，则RBF网络可表示为：

![](https://img-blog.csdn.net/20170208111607265?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




2）ART网络

ART（AdaptiveResonance Theory，自适应谐振理论）网络是竞争型学习的重要代表，由比较层、识别层、识别阈值和重置模块构成。其中比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类。

这个网络和前文不同的是，识别层神经元是动态的，网络结构不是固定的了。竞争型学习（competitive learning）是无监督学习策略，网络中的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制，这种机制也称为胜者通吃（winner-take-all）原则。

ART网络在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元。竞争规则是：计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜。获胜神经元将向其他识别层神经元发送信号，抑制其激活。若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属的类别，同时，网络连接权将会更新，使得以后再接收到相似输入样本时该模式类会计算出更大的相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值，则重置模块将在识别层增加一个新的神经元，其代表向量就设置为当前输入向量。

可以看出，识别阈值是很关键的参数，如较高，则输入样本会被划分的比较精细，识别层神经元数目过多；若较低，则划分的比价粗略。ART较好地缓解了竞争型学习中的可塑性-稳定性窘境（stability-plasticity dilemma），可塑性是指神经网络要有学习新知识的能力，而稳定性是指神经网络在学习新知识时要保持对旧知识的记忆；这就使得ART网络具有增量学习（incremental learning）或在线学习（online learning）。增量学习是指在学得模型后，再接收到训练样例时，仅需根据新样例对模型进行更新，不必重新训练整个模型，并且先前学得的有效信息不会被冲掉；在线学习是指每获得一个新样例就进行一次模型更新。在线学习是增量学习的特例，而增量学习可视为批模式（batch-mode）的在线学习。早期的ART网络只能处理布尔型输入数据，后面发展成一个算法族，包括能处理实值输入的ART2网络、结合模糊处理的FuzzyART网络以及可进行监督学习的ARTMAP网络等。

3）SOM网络

SOM（Self-OraganizingMap，自组织映射）网络是一种竞争学习型的无监督神经网络，能将高维输入数据映射到低维空间（通常为二维），同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。

SOM网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量（低维空间坐标点），网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置。SOM的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的。

SOM的训练过程是：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元（best matching unit），然后最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。

4）级联相关网络

一般的神经网路模型是假定网络结构是固定的，训练的目的是利用训练个样本来确定最优的连接权、阈值等参数；而结构自适应网络则将网络结构当作学习目标之一，在训练过程中找到最符合数据特点的网络结构，也叫构造性（constructive）神经网络。ART网络的隐层神经元数目在训练过程中不断增长，所以也属于结构自适应网络。级联相关网络也是结构自适应网络的代表。

级联相关网络有两个主要成分：级联和相关。级联是指建立层次连接的层级结构，在开始训练时，网络只有输入层和输出层，处于最小拓扑结构，随着训练进行，新的隐层神经元加入，从而创建起层级结构，当新的隐层神经元加入时，其输入端连接权值是冻结固定的。相关是指通过最大化新神经元的输出与网络误差之间的相关性（correlation）来训练相关参数。与一般的前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经元数目，且训练速度较快，但在数据较小时易陷入过拟合。

结构自适应网络的动态性一般是在训练过程中隐层数目及其神经元数目的变化体现。上文的神经网路类型，从固定网络到动态网络，都是无环，下文的网络则允许出现环形结构。

5）Elman网络

与前馈神经网络不同，递归神经网络（recurrent neural networks）允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化。

Elman网络是递归神经网络的代表，结构与多层前馈网络相似，不同的是隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入。隐层神经元通常采用sigmoid激活函数，而网络的训练则通过推广的BP算法进行。

6）Boltzmann机

神经网络中有一类模型是为网络状态定义一个能量（energy），能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。

Boltzmann机就是一种基于能量的模型（energy-based model），其神经元分显层和隐层，显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。Boltzmann机中的神经元都是布尔型，只取0、1两种状态，1表示激活，0表示抑制。



![](https://img-blog.csdn.net/20170208111839547?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

### 5.6深度学习

参数越多的模型复杂度越高，容量越大，能胜任更复杂任务的学习；但一般情形下，复杂模型的训练效果低，易陷入过拟合，随着云计算、大数据时代的来临，计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加可降低过拟合风险，如之，代表复杂模型的深度学习得以发展。一句话，因为计算能力和数据容量的提升，导致深度学习可以建构复杂模型来解决现实任务中较为复杂的问题，存有上百亿个参数的深度学习模型。

深层的神经网络模型就是典型的深度学习模型。对于神经网络模型，提高复杂度，可以增加隐层数目，从而增加神经元连接权、阈值等参数；也可通过增加隐层神经元数目来实现。不过增加隐层的数目比增加隐层神经元数目更有效，因为增加隐层数不仅增加了拥有激活函数的神经元数目，还增加了激活函数的嵌套层数。然而，多隐层（指三个以上，深度学习模型通常有八九层甚至更多）神经网络难以直接用经典算法（如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会发散而不能收敛到稳定状态。

无监督逐层训练（unsupervised layer-wise training）是多隐层网路训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，而本层隐结点的输出作为下一层隐结点的输入，称为预训练，在预训练全部完成后，再对整个网络进行微调（fine-tuning）训练。预训练+微调的做法可视为将大量参数分组，对每组先找到局部最优，然后基于局部最优结果联合起来进行全局寻优，利用了模型大量参数所提供的自由度的同时，节省了训练开销。

权共享（weightsharing），让一组神经元使用相同的连接权，也是一种节省训练开销的策略。这个策略在卷积神经网络中发挥了重要作用。文中提到的卷积神经网络用于手写数字识别案例，有兴趣可进一步深入研究学习。

深度学习的训练过程，多隐层堆叠、每层对上一层的输出进行处理的机制，可以看作是在对输入信号进行逐层加工，从而把初始的、与输出目标之间联系不太密切的输入表示，转化为成与输出目标联系更为密切的表示，使得原来仅基于最后一层的输出映射难以完成的任务称为可能。这大体也是分治策略的思想，换言之，逐渐将初始的低层特征表示转化为高层特征表示后，用简单模型即可完成复杂的分类等学习任务。由此，可将深度学习理解为进行特征学习或表示学习。

以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家来设计，这称为特征工程（feature engineering）。特征的好坏对泛化性能有至关重要的影响，人类专家设计出好特征也并非易事；特征学习则通过机器学习技术自身来产生好特征，这使机器学习向全自动数据分析又前进了一步。

这段话，我的理解是，以前输入的特征是人来提供，现在连输入的特征都是机器学习而来，尤其是好的特征通过机器自动学习而来更难能可贵。换句话说，人不需要任何介入，机器自己学习数据，自己训练。未来人工智能是这个方向的，一切都是自学习而来，而要支持这种自学习的模型，其复杂度可想而知，神经网络是否能满足，尚未可知。




