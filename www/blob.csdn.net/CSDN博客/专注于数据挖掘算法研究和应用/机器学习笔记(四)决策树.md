# 机器学习笔记(四)决策树 - 专注于数据挖掘算法研究和应用 - CSDN博客





2017年02月04日 10:34:09[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：4329
所属专栏：[机器学习专栏](https://blog.csdn.net/column/details/16315.html)









## 4.决策树

### 4.1基本流程

决策树（decisiontree）基于树结构进行决策，符合人的思维机制，是一类常见的机器学习方法。一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点。叶结点就对应于决策结果；内部结点则对应属性值分类，每个内部结点包含的样本集合根据属性测试的结果（值）划分到子结点中；根结点包含样本全集，从根结点到每个叶结点的路径对应一个判定测试序列。决策树学习的目的是为了产生一棵泛化能力强，可判定未见示例分类结果的决策树，其基本流程遵循简单而直观的分而治之策略。

决策树学习的基本算法描述如下：

输入：训练集D={(x1,y1),(x2,y2),…(xm,ym)}

      属性集A={a1,a2,…,ad}

过程：函数TreeGenerate(D,A)

      1）生成结点node；

      2）if D中的样本全属于同一类别C then 

           将node标记为C类叶节点； return

         end if

      3）if A=∅ or D中样本在A上取值相同 then

           将node标记为叶结点，其类别标记为D中样本数最多的类；return

         end if

4）从A中选择最优划分属性a*；

      5）for a*中的每一个值 a* v do

             为node生成一个分支；令Dv表示D中在a*上取值为a* v 的样本子集；

             if Dv为空then

               将分支结点标记为叶结点，其类别标记为D中样本数最多的类；return

             else

               以TreeGenerate(Dv,A-{ a*})为分支结点

             end if

          end for

    输出：以node为根结点的一棵决策树。

决策树生成是一个递归过程，有三种情形会导致递归终止：1）当前结点包含的样本全属于同一类别，无需划分；2）当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；3）当前结点包含的样本集为空，不能划分。

第2种情形下，把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在第3种情形下，也是把当前结点标记为叶结点，不过将其类别设定为父结点所含样本最多的类别。二者的区别在于，第2中情形利用当前结点的后验分布；而第3种情形则把父结点的样本分布作为当前结点的先验分布。

### 4.2划分选择

前文的决策树生成算法中第4步从A中选择最优划分属性a*是重点，要如何选择才能做到最优呢？随着划分过程的深入，希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度（purity）越来越高。用结点的纯度来衡量属性划分的最优选择，用信息熵（informationentropy）这一指标来度量样本集合纯度。

![](https://img-blog.csdn.net/20170204102839535?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




一般来说，信息增益越大，使用属性a进行划分所获得的纯度提升越大，如此，用信息增益大小作为决策树划分属性的选择。在决策树生成算法上，选择属性a*=arg max Gain(D,a)作为划分依据。

配合文中的西瓜集例子可以很好理解这个信息增益计算。考虑一种特殊情况，就是属性a的值v恰好有m个值（和D样本集数量一样），就是说每个样本在属性a上都有不同的值，共有m个值，这个分支将产生m个，每个分支结点只包含一个样本，其纯度是最大，选择a作为属性划分依据显然是理所当然。但是这样的划分所生成的决策树，不具备泛化能力，无法对新样本进行有效预测。从这个特殊的属性例子上来说，信息增益准则对属性取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，不直接使用信息增益，而使用增益率来选择最优划分属性。

![](https://img-blog.csdn.net/20170204102950053?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




在候选属性集合a中，选择划分后基尼指数最小的属性作为最优划分属性，即

a*=arg minGain_index(D,a)。

从划分选择的指标选择上看，算法在考量特殊情况下，为避免各种偏好带来的不利影响，需优化，优化过程就需要数学定义。

### 4.3剪枝处理

为解决决策树学习算法带来的过拟合，需剪枝（pruning）。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因为训练样本学得太好了，以至于把训练集自身的一些特点当作所有数据集都具有的一般性质而导致过拟合，如此，可通过主动去掉一些分支来降低过拟合的风险。回顾下，过拟合就是把训练集的特点（个性）当作所有数据集的特点（共性）。

决策树剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。预剪枝是指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。预剪枝就是在生成过程中就判断泛化性能来剪枝，后剪枝则是在生成树后来判断泛化性能来剪枝。

要看泛化性能是否提升，重心就落在了泛化性能的评估上。泛化性能评估上，采用留出法，即预留一部分数据作为验证集以进行性能评估。

1）预剪枝

预剪枝的基本过程是：1）划分训练集和验证集；2）基于信息增益准则，应用训练集选择最优划分属性来生成分支；3）对分支的划分前后用验证集分别计算精度；4）如果有提升，则划分，无提升则禁止划分。

通过预剪枝剪去决策树不少分支，避免过拟合，减少决策树训练时间开销和测试时间开销。但是，被剪去的分支，虽然当前分支不能提升泛化性能，但有可能其后续划分可以显著提高。预剪枝这种基于贪心策略的算法，给预剪枝决策树带来了欠拟合的风险。回顾欠拟合概念，就是数据集的特点（共性）没有从训练集的特点（个性）中训练出来。

2）后剪枝

后剪枝先从训练集上生成一棵完整的决策树，然后用验证集对内部结点计算精度来剪枝。后剪枝决策树通常比预剪枝决策树保留更多分支。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树要大得多。

### 4.4连续与缺失值

1）连续值

对于基于离散属性生成的决策树，信息增益准则作为划分属性的最优选择，对于可取值数目可分支；但如果属性值是连续性的，属性可取值数目不再有限，分支怎么划分呢？这个时候，连续属性离散化技术可以用上，最简单策略就是采用二分法（bi-partition）对连续属性进行处理。

![](https://img-blog.csdn.net/20170204103107762?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




2）缺失值

现实任务中常会遇到不完整样本，即样本的某些属性值缺失，尤其在属性数目较多的情况下，往往会有大量样本出现缺失值。如果简单放弃不完整样本，仅使用无缺失值的样本进行学习，显然对数据信息是极大的浪费，因此需要考虑利用有缺失属性值的训练样例来进行学习。

存在缺失值情况生成决策树，需要解决两个问题：1）如何在属性值缺失的情况下进行划分属性选择？2）给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

![](https://img-blog.csdn.net/20170204103234122?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)






### 4.5多变量决策树

若把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。决策树所形成的分类边界有一个明显的特点：轴平行（axis-parallel），即它的分类边界由若干个与坐标轴平行的分段组成。

分类边界的每一段都是与坐标轴平行的，这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，此时决策树会相当复杂，由于要进行大量的属性测试，预测时间开销会很大。若使用斜的划分边界，则决策树模型将大为简化。
![](https://img-blog.csdn.net/20170204103346341?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



与传统的单变量决策树（univariate decision tree）不同，在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。

文中的图示比较清晰的展示上文描述。本来很难理解多变量决策树在本章中的意义，后来想到最优划分属性选择上的贪心策略时，才意识到，泛化性能好的学习算法应该是分类边界定义很好的。从这点来看，多变量决策树，就是在最优划分属性选择之上的进一步优化，通过对非叶结点的属性线性组合找到更好的边界。抽象之于数学，如之。






