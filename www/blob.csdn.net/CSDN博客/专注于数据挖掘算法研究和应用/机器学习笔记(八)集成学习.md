# 机器学习笔记(八)集成学习 - 专注于数据挖掘算法研究和应用 - CSDN博客





2017年03月13日 09:56:28[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：5882
所属专栏：[机器学习专栏](https://blog.csdn.net/column/details/16315.html)









## 8.集成学习

### 8.1个体与集成

集成学习（ansemblelearning）通过构建并结合多个学习器来完成学习任务，也称为多分类器系统（multi-classifiersystem）、基于委员会的学习（committee-based learning）。

集成学习的一般结构：先产生一组个体学习器（individual learner），再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生，如决策树算法、BP神经网络等。如果集成中只包含同种类型的个体学习器，则集成是同质的（homogeneous）；同质集成中的个体学习器也称为基学习器（base learner），相应的学习算法称为基学习算法（base learningalgorithm）。如果集成中包含不同类型的个体学习器，则集成是异质的（heterogenous）；异质集成中的个体学习器由不同的学习算法生成，个体学习器称为组件学习器（component
 learner）。

说了定义，那么集成学习器是否比个体学习器泛化性能更显著呢？先引入弱学习器（weak learner）的定义，弱学习器常指泛化性能略优于随机猜测的学习器，例如在二分类问题上精度略高于50%的分类器。换句话说，弱学习器的结果接近于靠猜。集成学习通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能，这对弱学习器尤其明显，因此集成学习的很多理论研究是针对弱学习器进行的，而基学习器有时也被直接成为弱学习器。

现在问题是：集成学习把多个学习器结合起来，怎样能获得比单一学习器更好的性能呢？要获得好的集成，个体学习器应好而不同，即个体学习器要有一定的准确性，即学习器不太坏，并且要有多样性（diversity），即学习器间具有差异。

![](https://img-blog.csdn.net/20170313094143263?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




如上，集成学习就是把所有个体学习器的结果做简单的投票，这样就能获得比个体学习器更好的泛化性能。等等，没有这么好的事了，要这样说，需要满足一个关键假设：基学习器的误差相互独立。然后在现实任务中，个体学习器时为解决同一问题训练出来的，它们显然不可能相互独立。事实上，个体学习器的准确性和多样性本身就存在冲突。一般的，准确性很高之后，要增加多样性就要牺牲准确性。

因此，如何产生并结合好而不同的个体学习器，就是集成学习研究的核心，就是怎么集成？集成怎么样的个体学习器？根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类：1）个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表是Boosting；2）个体学习器间不存在强依赖关系、可同时生成的并行化方法，代表是Bagging和随机森林（Random Forest）。

### 8.2Boosting

Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数达到事先指定的值T，最终将这T个基学习器进行加权结合。简单来说，就是基于同一训练集和同一个算法训练出T个基学习器，每一轮训练都针对上一轮的训练结果调整样本分布。

Boosting族算法最著名的代表是AdaBoost，其中yi∈{-1,+1}，f是真实函数，算法描述如下：

![](https://img-blog.csdn.net/20170313094326452?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170313094417431?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170313094508876?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




这个就是算法中的样本分布更新公式。

上面这三点的公式，从基于加性模型迭代式优化指数损失函数的角度推导出了AdaBoost算法。

Boosting算法要求基学习器对特定的数据分布进行学习，可通过重赋权法（re-weighting）实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过重采样法（re-sampling）来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。这两种方法无优劣差别。

Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（如AdaBoost中检查当前基分类器是否是比随机猜测好），一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止。在此种情形下，初始设置的学习轮数T也许还远未达到，可能导致最终集成中值包含很少的基学习器而性能不佳。若采用重采样法，则可获得重启动机会以避免训练过程过早停止，即在不抛弃不满足条件的当前基学习器之后，可根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程可以持续到预设的T轮完成。

从偏差-方差分解的角度看，Boosting主要关注降低偏差。偏差描述的是预测值（估计值）的期望与真实值之间的差距；偏差越大，越偏离真实数据。方差描述的是预测值的变化范围，离散程度，也就是离其期望值的距离；方差越大，数据的分布越分散。因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。可以参照文中的西瓜数据集并以决策树桩为基学习器运行AdaBoost算法，比较不同规模的集成及其基学习器所对应的分类边界。





### 8.3Bagging与随机森林

本文说明采用并行化的个体学习器生成方式，和上文的Boosting串行化要求个体学习器存在强依赖关系不同的是，该生成方式是基于个体学习器应尽可能相互独立。独立的个体学习器可以得到泛化性能强的集成；当然现实中不存在绝对的独立，不过可以设法使基学习器尽可能具有较大差异。一种方法就是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集子集中训练出一个基学习器。不过如果采样出的每个子集完全不同，那么每个基学习器只用到了部分训练数据，可能都无法进行有效学习。因此，考虑使用相互有交叠的采样子集。

![](https://img-blog.csdn.net/20170313094708183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




假定基学习器的计算复杂度为O(m)，则Bagging的复杂度大致为T(O(m)+O(s))，因采样与投票/平均过程的复杂度O(s)很小，且T是一个不太大的常数（训练轮数），因此，训练一个Bagging集成与直接使用基学习算法训练一个学习器的复杂度同阶，可见Bagging是一个高效的集成学习算法。与标准的AdaBoost算法只适用于二分类任务不同，Bagging能不经修改地用于多分类、回归等任务。

自助采样过程还给Bagging带来一个优点：由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下的约36.8%的样本可用作验证集来对泛化性能进行包外估计（out-of-bag estimate），为此需记录每个基学习器所使用的训练样本。令Dt表示ht实际使用的训练样本集，令Hoob(x)表示对样本x的包外预测，即仅考虑哪些未使用x训练的基学习器在x上的预测，有：

![](https://img-blog.csdn.net/20170313094809833?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




事实上，包外样本还有其他用途，如当基学习器是决策树时，可使用包外样本来辅助剪枝，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本来辅助早起停止以减小过拟合风险。

从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。

随机森林（RandomForest，简称RF）是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。参数k控制了随机性的引入程度：若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分；一般情况下，推荐值k=log2d。

随机森林简单、容易实现、计算开销小，在很多现实任务中展现出强大的性能，被誉为”代表集成学习技术水平的方法”。随机森林对Bagging做了小改动，但与Bagging中基学习器的多样性仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，使得最终集成的泛化性能可通过个体学习器之间差异度的增加来进一步提升。

随机森林的收敛性与Bagging相似。随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低，然后随着个体学习器的数目增加，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率常优于Bagging，因为在个体决策树的构建过程中，Bagging使用的是确定型决策树，在选择划分属性属性时对结点的所有属性进行考察，而随机森林使用的随机型决策树只需考察一个属性子集。

其实对于集成学习，首先要证明的就是为什么集成比个体好？第一节已证明，随着集成中个体分类器数目T的增大，集成的错误率将指数级下降。既然集成比个体好，那么如何生成基学习器并结合呢？第二节和第三节分别从个体学习器生成的不同方式介绍了串行Boosting方法和并行Bagging方法，这里主要说明过了个体生成方法，而对结合方法则采用简单投票法或平均法，那么下一节就会重点说明基学习器如何结合更好得到集成学习器。

### 8.4结合策略

对于个体学习器生成方式，上面介绍了串行的Boosting和并行的Bagging，那么这里就要说到个体学习器生成之后如何结合，从而形成集成学习的效应。

集成学习器由三方面的好处：

1）统计上，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险；

2）计算上，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很差，而通过多次运行之后进行结合，可降低陷入较差的局部极小点风险；

3）表示上，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器是无效的，而通过结合多个学习器，由于相应假设空间有所扩大，有可能学得更好的近似。

一言以蔽之，不断迭代和结合总能克服单个的缺陷。假定集成包含T个基学习器{h1,h2,…,hT}，其中hi在示例x上的输出为hi(x)。下面三种是常见的对hi进行结合的策略。

![](https://img-blog.csdn.net/20170313094947492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170313095037509?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




信度可转化为类概率使用。若此类值未进行规范化，例如支持向量机的分类间隔值，则必须使用一些技术如Platt缩放（Platt scaling）、等分回归（isotonic regression）等进行校准（calibration）后才能作为类概率使用。虽然分类器估计出的类概率值一般都不太准确，但基于类概率进行结合却往往比直接基于类标记进行结合性能更好。若基学习器的类型不同，则其类概率值不能直接进行比较，这种情形下，通常可将类概率值转化为类标记输出然后再投票。

3）学习法

当训练数据很多时，用学习法的结合策略集成，通过另一个学习器来进行结合。Stacking是学习法的典型代表，个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器（meta-learner）。

Stacking先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记仍被当做样例标记。假定初级学习器使用不同学习算法产生，即初级集成是异质的，算法描述如下：

![](https://img-blog.csdn.net/20170313095205855?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能有很大影响。研究表明，将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归（multi-response linear regression，MLR）作为次级学习算法效果较好，在MLR中使用不同的属性集更佳。

贝叶斯模型平均（bayes model averaging,BMA）基于后验概率来为不同模型赋予权重，可视为加权平均法的一种实现。对Stacking和BMA进行比较，理论上来说，若数据生成模型恰在当前考虑的模型中，且数据噪声较少，则BMA不差于Stacking；然后在现实应用中无法确保数据生成模型一定在当前考虑的模型中，甚至可能难以用当前考虑的模型来进行近似。因此，Stacking通常优于BMA，因为其鲁棒性比BMA好，而且BMA对买模型近似误差非常敏感。

个人感觉，集成的结合策略是一个因地制宜的解决方案，前人摸索的也只能作为参考，在实际应用中，还要考虑训练集的独特性。

### 8.5多样性

从基学习器生成方式到结合策略都做了介绍，这一节主要是说如何构架多样性的基学习器。要构建泛化能力强的集成，个体学习学习器应好而不同，其理论分析如下。

![](https://img-blog.csdn.net/20170313095404608?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




求解，就可以得到最优集成了。实则不然，因为 是在集成构造之后才能进行估计，所以不是一个可直接操作的多样性度量，换句话说，用于事后评估但无法事前求解。另外值得注意的是，这个理论的推导过程只适用于回归学习，不能直接推广到分类任务上去。从误差-分歧分解的过程就可以看出，无论是预测输出的结果还是概率密度都体现了连续性。

证明了好而不同的个体学习器可构建强大泛化性能的集成学习器，那么怎么定义个体学习器是不同的呢？是多样性的呢？用多样性度量指标。顾名思义，多样性度量（diversity measure）是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。要比较差异化，典型做法就是对个体分类器进行两两相似或不相似性分析。

给定数据集D={(x1,y1),(x2,y2),…,(xm,ym)}，对二分类任务，yi∈{-1,+1}，分类器hi和hj的预测结果列联表（contingency table）为：

||**hi=+1**|**hi=-1**|
|----|----|----|
|**hj=+1****hj=-1   **|ab|cd|


其中，a表示hi和hj均预测为正类的样本数目，b、c、d含义类推；m=a+b+c+d。

基于这个列联表，常见的多样性度量：

![](https://img-blog.csdn.net/20170313095511249?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




我们肯定了好而不同的个体学习器集成之后泛化性能更佳，也给出如何评估多样性的个体学习器，那么问题是怎么让个体学习器呈现多样性呢？尤其是同质个体学习器。这里就要说到如何增强个体学习器多样性的方法。一般思路是在学习过程中引入随机性，常见做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。

1）数据样本扰动

给定初始数据集，从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器。数据样本扰动通常是基于采样法，如在Bagging中采用自助采样、Adaboost中采用序列采样等。很多常见的基学习器，如决策树、神经网络等，训练样本稍加变化就会导致学习器显著变动，数据样本扰动对这类不稳定基学习器很有效。但有些基学习器对数据样本的扰动不敏感，如线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等，这类基学习器称为稳定学习器（stable base learner），对该类基学习器进行集成往往需使用输入属性扰动等其他机制。

2）输入属性扰动

训练样本通常由一组属性描述，不同的子空间（subspace，即属性子集）提供了观察数据的不同视角。显然，从不同子空间训练出的个体学习器必然有所不同。随机子空间算法（random subspace）就依赖于输入属性扰动，算法从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器。算法如下：

![](https://img-blog.csdn.net/20170313095607766?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




对包含大量冗余属性的数据，在子空间中训练个体学习器不仅能产生多样性大的扰动，还会因属性数的减少而大幅节省时间开销，同时，由于冗余属性多，减少一些属性后训练出的个体学习器也不至于太差。若数据只包含少量属性，或者冗余属性较少，则不宜使用输入属性扰动法。

3）输出表示扰动

思路是对输出表示进行操作以增强多样性。可对训练个样本的类标记稍作变动，如翻转法（flipping output）随机改变一些训练样本的标记，也可对输出表示进行转化，如输出调制法（output smearing）将分类输出转化为回归输出后构建个体学习器；还可将原任务拆解为多个可同时求解的子任务，如ECOC法利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。

4）算法参数扰动

基学习算法一般都有参数设置，如神经网络的隐层神经元数、初始连接权值等，通过随机设置不同的参数，往往可产生差别较大的个体学习器。如负相关法（Negative Correlation）显示地通过正则化项来强制个体神经网络使用不同的参数。对参数较少的算法，可通过将其学习过程中某些环节用其他类似方式代替，从而达到扰动的目的，如可将决策树使用的属性选择机制替换成其他属性选择机制。使用单一学习器时通常需使用交叉验证等方法来确定参数值，这事实上已使用了不同参数训练出多个学习器，只不过最终仅选择其中一个学习器进行使用，而集成学习则相当于把这些学习器都利用起来；由此可见，集成学习技术的实际计算开销并不比使用单一学习器大很多。

不同的多样性增强机制可同时使用，如随机森林中同时使用了数据样本扰动和输入属性扰动。




