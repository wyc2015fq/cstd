# 机器学习笔记(三)线性模型 - 专注于数据挖掘算法研究和应用 - CSDN博客





2017年01月17日 10:32:30[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：5969
所属专栏：[机器学习专栏](https://blog.csdn.net/column/details/16315.html)









## 3.线性模型

### 3.1基本形式

线性模型（linearmodel）形式简单、易于建模，如果能把问题都用线性模型来刻画，那现今的世界就单调多了，好在我们的宇宙是如此的丰富，以至于需要通过更强大的非线性模型（nonlinear model）来描述。然而，线性模型作为最基本的，再复杂的也都归于最简单，非线性模型在很多情况下都是在线性模型基础上通过引入层级结构或高维映射而来，分治策略思想。下面定义线性模型的基本形式：

给定由d个属性描述的示例x=(x1;x2;...;xd)，其中xi是x在第i个属性上的取值，线性模型将会学习到一个通过属性的线性组合来进行预测函数，即：

f(x)=w1x1+w2x2+…+ wdxd+b，用向量形式写成：f(x)=wTx+b

其中w=(w1;w2;...;wd)，w和b得到之后，模型就确定。

### 3.2线性回归

线性回归（linearregression）通过学习到一个线性模型来尽可能准确地预测实值输出标记。这句话的意思就是说，训练出一个线性模型的学习器，然后用来预测实值输出。给定数据集D={(x1,y1), (x2,y2),…, (xm,ym)}，其中xi=( xi1;xi2;...;xid)，yi∈R。

先考虑只有一个属性的线性回归，即yi ≈f(xi)=wxi+b。若是离散属性，其值之间存在序（order）关系，要将其连续化为连续值，假定属性有k个离散属性值，则转化为k维向量；若是离散属性，但值之间是无序的，在引入不恰当关系时会导致距离计算出现误差。

有模型了，有两点要研究了，一是求解w和b，二是衡量f(x)和y的差别。用均方误差来度量回归任务中的学习器性能。最小化均方误差：

![](https://img-blog.csdn.net/20170117101245905?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170117101337697?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170117101519385?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




（regularization）。

线性模型简单，但也有诸多变化。对于样例(x,y)，y∈R，定义线性回归模型y=wTx+b，使模型的预测值逼近真实的标记y。变化在于，能否令模型的预测值逼近y的衍生变化呢？就是说，wTx+b=g(y)，如将示例对应的输出标记定义为指数尺度上的变化，即g(y)=lny，也就是lny= wTx+b，折就是对数线性回归，也就是让e
wTx+b=y。这种关系的演变，虽然还是线性回归，但实际上已是求取输入空间到输出空间的非线性函数映射。一般化定义这种衍生，设g是单调可微函数（连续且充分光滑），令y=g-1(wTx+b)或者表达为g(y)= wTx+b，这样的模型称之为广义线性模型（generalizedlinear model），其中函数g称为联系函数（linkfunction）。对数线性回归g(y)=lny是一个特例。广义线性模型的参数估计通过加权最小二乘法或极大似然法进行。极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。



### 3.3对数几率回归



上文定义的广义模型解决了分类任务的线性模型回归学习，只需找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。

考虑二分类任务，其输出标记y∈{0,1}，而线性回归模型产生的预测值z= wTx+b是实值（连续的），需将z值转化为0/1值，建立z和y的联系。一般采用单位阶跃函数（unit-step function）：

![](https://img-blog.csdn.net/20170117101707334?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




回归任务是针对连续型的，为支持分类任务（离散型），采用定义广义线性模型来实现，其中对数几率回归模型就支持分类学习方法。用线性回归模型来构建分类学习器，可以直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题；不仅能预测出类别，也可以得到近似概率预测，在利用概率辅助决策上很有用。另外，几率函数是任意阶可导的凸函数，在数值优化算法上可直接求取最优解。总结来说，三个优点：可对分类直接建模无需事先假设数据分布、可做近似概率预测、可直接求取最优解。机器学习算法，和统计概率的密切关系从这可看出一般，而统计概率本身所用的函数性质也很重要。或者说，机器学习算法的核心在于概率性，也就是说，机器学习的稳定性是基于概率的。有点难直白说，只能意会了。

下面就看如何估计对数几率模型中的w和b值。

![](https://img-blog.csdn.net/20170117101807870?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170117101858274?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




梯度下降法(gradientdescent)是一个最优化算法，通常也称为最速下降法。常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。

极大似然估计，只是一种概率论在统计学的应用，是参数估计的方法之一。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。极大似然估计的思想是：已知某个参数能使这个样本出现的概率最大，即把这个参数作为估计的真实值。极大似然估计是一种粗略的数学期望，误差大小还要通过区间估计度量。

### 3.4线性判别分析

对于线性模型，上文介绍了线性回归及其衍生的对数几率回归，下面介绍一种经典的线性模型。线性判别分析（Linear Discriminant Analysis,LDA），经典的线性学习方法，其思想是：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本分类时，将其投影到同样的直线上，并根据投影点位置来确定新样本的类别。点和线的朴素关系彰显，这数学怎么定义呢？更关心，多分类多属性上，如何降维投射到点上。

![](https://img-blog.csdn.net/20170117102123575?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170117102226278?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170117102323935?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20170117102407312?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




于是，通过这个投影减小样本点的维数，且投影过程中使用了类别信息，因此LDA通常也被看做是一种经典的监督降维技术。

忍不住吐槽，我讨厌符号数学啊，但是数学的美也正在于符号的应用，这么多符号要记住，还要记住符号内表达加减乘除。

### 3.5多分类学习

面对多分类学习任务时，更多时候基于二分类学习方法延伸来解决。这里顺便说下多分类任务和多标记任务的区别，多分类任务是一个样本只属于一个分类但有多个选择；多标记任务是一个样本可以属于多个分类可以一个选择可以多个选择。多标记任务不是多分类任务，如一幅图像可以标注为“蓝天”、“白云”、“自然”等。现在来说说多分类学习如何通过二分类学习方法实现。

不是一般性，考虑N个类别，C1，C2，…，CN，多分类学习的基本思路是拆解法，将多分类任务拆成若干个二分类任务求解。总的来说，是问题拆解和结果集成，1）先对问题进行拆解，对拆分出的每个二分类任务训练一个分类器；2）测试时，对这些二分类任务的分类器预测结果进程集成以获得最终的多分类结果。

拆分策略有三类：一对一，OVO；一对多，OVR，这里多就是其余的分类；多对多，MVM。显然OVO和OVR是MVM的特例。



1）OVO策略

给定数据集 ![](https://img-blog.csdn.net/20170117102635608?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) ，OVO策略将这N个类别两两配对，产生NX(N-1)/2个二分类任务。如将类别Ci和Cj训练一个分类器，该分类器把数据集D中的Ci类样例作为正例，Cj类样例作为反例。测试阶段时，新样本提交给所有分类器去预测，得到NX(N-1)/2个分类结果，一般选择结果被预测最多的类别作为最终结果。

2）OVR策略

每次把一个类别作为正例，其他类别作为反例，训练N个分类器。测试时，若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果；若有多个分类器预测为正类，则考虑个分类器的预测置信度，选择置信度最大的类别作为最终标记结果。

OVR策略只需训练N个分类器，而OVO策略需训练NX(N-1)/2分类器，后者存储开销和时间开销都比较大；不过在训练时，前者每个分类器均使用全部样例，而后者每个分类器仅用到两个类别的样例，类别过多时，后者的开销又比较小。二者的预测性能，取决于具体的数据分布，多数情形下差不多。

3）MVM策略

该策略每次将若干类作为正类，若干类作为反类，正反类的构造需严格设计，常用的技术是纠错输出码（Error Correcting Output Codes,ECOC）。ECOC引入编码思想对类别进行拆分，支持在解码过程中的容错性，主要工作过程分两步：

第一步编码：将N个类别做M次划分，每次划分将一部分类别作为正类，一部分类别作为反类，从而形成一个二分类训练街，这样一共产生M个训练集，可训练出M个分类器。

第二步解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码；将这个预测编码和每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。

类别划分通过编码矩阵（coding matrix）指定。编码矩阵有二元码和三元码两类，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可以指定停用类（用0表示）。在解码阶段，各分类器的预测结果联合起来形成了测试示例的编码，将该预测编码和各类所对应的编码进行比较，将距离最小的编码所对应的类别作为预测结果。比较有海明距离和欧式距离两种计算方法。

二元码如下表，C3无论是海明距离还是欧式距离都是最小，所以作为最终标记结果。

纠错输出码的意义在于，ECOC编码对分类器的错误有一定的容忍和修正能力。比如在某个分类器上预测错误，但整体编码的距离计算仍能产生正确结果（即还是最短距离）。自然，对同一个学习任务来说，ECOC编码越长，纠错能力越强，当然是牺牲性能了，因为编码越长，要训练的分类器就越多。对同等长度的编码来说，理论上，任意两个类别之间的编码距离越远，则纠错能力越强。在码长较小时，可根据这个原则计算出理论最优编码，然而码长稍大一些就难以有效地确定最优编码，这是NP难问题。

||f1|f2|f3|f4|f5|海明距离|欧式距离|
|----|----|----|----|----|----|----|----|
|C1|-1|1|-1|1|1|3|2sqrt(3)|
|C2|1|-1|-1|1|-1|4|4|
|C3|-1|1|1|-1|1|1|2|
|C4|-1|-1|1|1|-1|2|2sqrt(2)|
|示例预测结果|-1|-1|1|-1|1|||


在理论和实践之间，一个理论纠错性质很好，但导致二分类问题较难的编码，与另一个理论纠错性质差一些，但导致的二分类问题较简单的编码，最终产生的模型性能强弱不定。我自己更倾向于实践中摸索耗能低的方法。

### 3.6类别不平衡问题

所谓类别不平衡问题是指分类任务中不同类别的训练样例数差别很大，导致训练出的学习器失真。怎么说呢？一般情况，假设分类学习任务中不同类别的样例数时相对平衡，数目相当的，如果有所倾斜，而且比例很大，那就失去意义了。如果假定样例中正例很少、反例很多，在通过拆分法解决多分类问题时，即使原始问题中不同类别的训练样例数目相当，在使用OVR、MVM策略后产生的二分类任务仍可能出现类别不平衡现象。

对线性分类器y=wTx+b来说，对新样本x进行分类时，将y值和一个阈值进行比较，分类器的决策规则是：若y/(1-y)>1，则预测为正例。当训练集中正、反样例数目不等时，令m+代表正例数目、m-代表反例数目，则观测几率是m+/m-，通常假设训练集是真实样本总体的无偏采样，因此观测几率就代表了真实几率，于是只要分类器的预测几率高于观测几率就可判定为正例，即：y/(1-y)> m+/m-，则预测为正例。如果从根源上可以做到无偏采样，那就没有类别不平衡问题，可惜一般情况下都无法实现，于是只好找到类别不平衡的解决方法。

再缩放，类别不平衡问题解决方法的一个策略。如上文线性分类器，调整预测值：![](https://img-blog.csdn.net/20170117102823990?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmpzc2hhcnBzd29yZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)  正如上文说的，无法做到无偏采样，观测几率也会失真，未必能有效地基于训练集观测几率来推断出真实几率。有三类做法来改正：1）欠采样，去除一些反例，使得正反例数目相当；2）过采样，增加一些正例，使得正反例数相当；3）基于原始训练集学习，但用训练好的学习器进行预测时，对输出结果进行阈值移动（threshold-moving）。

欠采样少了训练样例，过采样多了训练样例，二者开销自然是后者大了。另外过采样，如果是对原始训练集重复采集正例，则会出现严重的过拟合，可通过对训练集里的正例进行插值来产生额外的正例，代表性算法是SMOTE。欠采样法同样面临问题，如果随机丢失反例，可能失去信息，可利用集成学习机制，将反例划分为若干个集合供不同学习器使用，对单个学习来说是欠采样，但整体来说并没有缺失也就不会丢失信息。









