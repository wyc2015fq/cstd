# 最大似然估计MLE和最大后验估计MAP理解 - 专注于数据挖掘算法研究和应用 - CSDN博客





2018年02月09日 13:11:08[fjssharpsword](https://me.csdn.net/fjssharpsword)阅读数：639








1、频率学派和贝叶斯派

频率学派认为参数是固定而未知的，关心似然函数。

贝叶斯派认为参数是随机的有分布的，关心后验分布。

2、MLE、MAP公式

![](https://img-blog.csdn.net/20180209125405995)


3、参数估计-MLE

![](https://img-blog.csdn.net/20180209125817341)


4、参数估计-MAP

![](https://img-blog.csdn.net/20180209125850896)

MAP与MLE最大的不同在于p(参数)项，MAP将先验知识加入，优化损失函数。

5、MLE、MAP、Bayesian统一理解

![](https://img-blog.csdn.net/20180209130226494)


ML（最大似然估计）：给定一个模型的参数，然后试着最大化p(D|参数)。即给定参数的情况下，看到样本集的概率。目标是找到使前面概率最大的参数。逻辑回归都是基于ML做的。MLE不把先验知识加入模型中。


MAP（最大后验估计）：最大化p(参数|D)。


Bayesian：考虑了所有可能的参数，即所有的参数空间（参数的分布）。


MLE和MAP的目标都是一样的：找到一个最优解，然后用最优解做预测。贝叶斯模型会给出对参数的一个分布，比如对模型的参数, 假定参数空间里有参数1、参数2、 参数3、...、参数N，贝叶斯模型学出来的就是这些参数的重要性（也就是分布），然后当我们对新的样本预测的时候，就会让**所有的模型**一起去预测，但每个模型会有自己的权重（权重就是学出来的分布）。最终的决策由所有的估计根据其权重做出决策。




5、从统计学角度理解机器学习，不无三要素：特征工程、目标函数、模型学习，机器学习的本质是用计算机统计地估计复杂函数。


![](https://img-blog.csdn.net/20180209130908852)


定义**假设空间（Model assumption）**：如线性分类，线性回归，逻辑回归，SVM，深度学习网络等。


定义**损失函数**（目标函数）并优化求解（如：梯度下降，牛顿法等）。


不同的模型使用不同的算法，如逻辑回归通常用梯度下降法解决，神经网络用反向推导解决，贝叶斯模型则用MCMC来解决。

**机器学习 = 模型 + 优化**（不同算法）




参考:

https://www.cnblogs.com/shixisheng/p/7136890.html

http://www.cnblogs.com/little-YTMM/p/5399532.html




