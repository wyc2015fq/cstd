# SVM理论系统学习 - 家家的专栏 - CSDN博客





2011年02月21日 21:21:00[依海之燕](https://me.csdn.net/yihaizhiyan)阅读数：1875











### [http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html](http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html)





支持向量机方法的基本思想:

  (1)  在对给定的数据逼近的精度与逼近函数的复杂性之间寻求折衷，以期获得最好的推广能力； 

（2）它最终解决的是一个凸二次规划问题，从理论上说，得到的将是全局最优解，解决了在神经网络方法中无法避免的局部极值问题.???

（3）它将实际问题通过非线性变换转换到高维的特征空间，在高维空间中构造线性决策函数来实现原空间中的非线性决策函数，巧妙地解决了维数问题，并保证了有较好的推广能力，而且算法复杂度与样本维数无关. 

为适应训练样本集的非线性，传统的拟合方法通常是在线性方程后面加高阶项。此法诚然有效，但由此增加的可调参数未免增加了过拟合的风险。支持向量回归算法采用核函数解决这一矛盾。用核函数代替线性方程中的线性项可以使原来的线性算法“非线性化”，即能做非线性回归。与此同时，引进核函数达到了“升维”的目的，而增加的可调参数是过拟合依然能控制。? 

引自：[http://blog.chinaunix.net/u/28514/showart_327701.html](http://blog.chinaunix.net/u/28514/showart_327701.html)

结构风险最小化：

VC 维在有限的训练样本情况下，当样本数 n 固定时，此时学习机器的 VC 维越高学习机器的复杂性越高。VC 维反映了函数集的学习能力，VC 维越大则学习机器越复杂(容量越大)。    
    所谓的结构风险最小化就是在保证分类精度（经验风险）的同时，降低学习机器的 VC 维，可以使学习机器在整个样本集上的期望风险得到控制。   
    推广的界（经验风险和实际风险之间的关系，注意引入这个原因是什么？因为训练误差再小也就是在这个训练集合上，实际的推广能力不行就会引起过拟合问题还。所以说要引入置信范围也就是经验误差和实际期望误差之间的关系）：
    期望误差R(ω) ≤ Remp (ω)+ Φ（n/h）注意Remp (ω)是经验误差也就是训练误差（线性中使得所有的都训练正确），Φ（n/h）是置信范围，它是和样本数和VC维有关的。上式中置信范围Φ 随n/h增加，单调下降。即当n/h较小时，置信范围Φ 较大，用经验风险近似实际风险就存在较大的误差，因此，用采用经验风险最小化准则，取得的最优解可能具有较差的推广性；如果样本数较多，n/h较大，则置信范围就会很小，采用经验风险最小化准则，求得的最优解就接近实际的最优解。可知：影响期望风险上界的因子有两个方面：首先是训练集的规模 n，其次是 VC 维 h。可见，在保证分类精度（经验风险）的同时，降低学习机器的 VC 维，可以使学习机器在整个样本集上的期望风险得到控制，这就是结构风险最小化（Structure Risk Minimization，简称 SRM）的由来。
     在有限的训练样本情况下，当样本数 n 固定时，此时学习机器的 VC 维越高(学习机器的复杂性越高)，则置信范围就越大，此时，真实风险与经验风险之间的差别就越大，这就是为什么会出现过学习现象的原因。机器学习过程不但要使经验风险最小，还要使其 VC 维尽量小，以缩小置信范围，才能取得较小的实际风险，即对未来样本有较好的推广性，它与学习机器的 VC 维及训练样本数有关。   
     线性可分的问题就是满足最优分类面的面要求分类面不但能将两类样本正确分开（训练错误率为 0），而且要使两类的分类间隔最大（这个是怎么回事呢？原来是有根据的，这个让俺郁闷了好久呢。在 @ 间隔下，超平面集合的 VC 维 h 满足下面关系：    h = f （1/@*@） 其中， f().是单调增函数，即 h 与@的平方成反比关系。因此，当训练样本给定时，分类间隔越大，则对应的分类超平面集合的 VC 维就越小。）。根据结构风险最小化原则，前者是保证经验风险（经验风险和期望风险依赖于学习机器函数族的选择）最小，而后者使分类间隔最大，导致 VC 维最小，实际上就是使推广性的界中的置信范围最小，从而达到使真实风险最小。注意：置信范围大说明真实风险和经验风险的差别较大。    
       解释到这里了，终于有点眉目了，哦原来就是这么回事啊，真是的。总结一下就是训练样本在线性可分的情况下，全部样本能被正确地分类（咦这个不就是传说中的 yi*(w*xi+b)）>=1的条件吗），即经验风险Remp 为 0 的前提下，通过对分类间隔最大化（咦，这个就是Φ（w）＝(1/2)*w*w嘛），使分类器获得最好的推广性能。    
      那么解释完线性可分了，我们知道其实很多时候是线性不可分的啊，那么有什么区别没有啊？废话区别当然会有啦，嘿嘿那么什么是本质的区别啊？本质的区别就是不知道是否线性可分但是允许有错分的样本存在（这个咋回事还是没明白hoho）但是正是由于允许存在错分样本，此时的软间隔分类超平面表示在剔除那些错分样本后最大分类间隔的超平面。这里就出现了新词松驰因子，干吗用滴？就是用来控制错分样本的啊。这样的话经验风险就要跟松驰因子联系在一起了。而C就是松驰因子前面的系数，C>0 是一个自定义的惩罚因子，它控制对错分样本惩罚的程度，用来控制样本偏差与机器推广能力之间的折衷。c越小，惩罚越小，那么训练误差就越大，使得结构风险也变大，而C 越大呢，惩罚就越大，对错分样本的约束程度就越大，但是这样会使得第二项置信范围的权重变大那么分类间隔的权重就相对变小了，系统的泛化能力就变差了。所以选择合适的C还是很有必要的。选择核函数。核函数有很多种，如线性核、多项式核、Sigmoid 核和 RBF（Radial Basis function）核。本文选定 RBF 核为 SVM 的核函数（RBF 核K(x, y) = exp(－γ || x －y ||的平方),γ > 0）。因为RBF 核可以将样本映射到一个更高维的空间，可以处理当类标签（Class Labels）和特征之间的关系是非线性时的样例。Keerthi 等[25]证明了一个有惩罚参数C 的线性核同有参数(C,γ )（其中C 为惩罚因子，γ 为核参数）的 RBF 核具有相同的性能。对某些参数，Sigmoid核同 RBF 核具有相似的性能[26]。另外，RBF 核与多项式核相比具有参数少的优点。因为参数的个数直接影响到模型选择的复杂性。非常重要的一点是0< Kij ≤1与多项式核相反，核值可能趋向无限(γxi xj + r >1)或者0 < γxi xj + r <1，跨度非常大。而且，必须注意的是Sigmoid 核在某些参数下是不正确的（例如，没有两个向量的内积）。 (4)用交叉验证找到最好的参数 C 和γ 。使用 RBF 核时，要考虑两个参数 C 和γ 。因为参数的选择并没有一定的先验知识，必须做某种类型的模型选择（参数搜索）。目的是确定好的(C,γ)使得分类器能正确的预测未知数据（即测试集数据），有较高的分类精确率。值得注意的是得到高的训练正确率即是分类器预测类标签已知的训练数据的正确率）不能保证在测试集上具有高的预测精度。因此，通常采用交叉验证方法提高预测精度。k 折交叉验证（k-fold cross validation） 是将训练集合分成 k 个大小相同的子集。其中一个子集用于测试，其它 k-1 个子集用于对分类器进行训练。这样，整个训练集中的每一个子集被预测一次，交叉验证的正确率是 k次正确分类数据百分比的平均值。它可以防止过拟合的问题。 

Svm的训练预测过程：


- 收集数据，相关性分析（p卡方检验），特征选择（主成份分析）。 
- 归一化数据。就是根据实际要求，将数据的取值范围转化为统一的区间如[a,b],a,b为整数。方法参考：[http://slt-ml.blogspot.com/2008/06/spss.html](http://slt-ml.blogspot.com/2008/06/spss.html)
- 利用抽样技术将数据集分为训练集和测试集。抽样技术有分层抽样，简单抽样（等概率抽样） 
- 将数据转化为软件（接口）所支持的格式。就libsvm（c＋＋，java）来说，我们可以使用FormatDataLibsvm.xls将数据转化为libsvmm所要求 的格式。参考：[http://slt-ml.blogspot.com/2008/06/formatdatalibsvmxlslibsvm.html](http://slt-ml.blogspot.com/2008/06/formatdatalibsvmxlslibsvm.html)
- 选择核函数，可以优先考虑rbf。 
- 对训练集利用交叉验证法选择最好的参数C和r（rbf核函数中的参数gama）。可以通过网格法寻找出最优的参数，注意一次交叉验证得到一个参数对所对应的模型精度，网格法目的就是找到使得模型精度达到对高的参数对（这里的参数对可能不止两个，有可能也有其他的），可以使用一些启发式的搜索来降低复杂度，虽然这个方法笨了点，但是它能得到很稳定的搜索结果。需要提到的这里在对训练集进行分割的时候涉及到抽样，一个较好的方法就是分层抽样。从这步可以看出其实 Cross－Validation是一种评估算法的方法。 
- 用6中得到的参数对在整个训练集合上进行训练，从而得出模型。 
- 利用测试集测试模型，得到精度。这个精度可以认为是模型最终的精度。当然有人会担心3步中抽样会有一定的误差，导致8得到的精度不一定是最好的，因此可以重复3－8得到多个模型的精度，然后选择最好的一个精度最为模型的精度（或者求所有精度的均值做为模型精度）。

引自：[http://www.mipang.com/blog/68773.ed4.htm](http://www.mipang.com/blog/68773.ed4.htm)

置信风险与两个量有关，一是样本数量，显然给定的样本数量越大，我们的学习结果越有可能正确，此时置信风险越小；二是分类函数的VC维，显然VC维越大，推广能力越差，置信风险会变大




