# 《吴恩达机器学习》第七章笔记 - zhuzhuxia183的博客 - CSDN博客





2019年02月24日 23:39:40[喵小姐的邻居小小朱](https://me.csdn.net/zhuzhuxia183)阅读数：18
所属专栏：[机器学习](https://blog.csdn.net/column/details/33594.html)












### 这里第七章：Logistic 回归
- [引言](#_1)
- [算法的定义](#_18)
- [决策边界](#_25)
- [代价函数](#_35)
- [优化算法](#_56)
- [多类别分类](#_64)




# 引言

分类问题，如

1、邮件是否是垃圾邮件

2、诊断是否患有某病

3、商务网站检测用户使用的信用卡是否异常
这些问题，都有一个特征，就是最终的结果$y\in\{0,1\}$
- 我们可以考虑使用线性回归来进行拟合，但是最终注意，然后设置一个阀值，如$h_\theta(x)>=0.5$时我们认为为结果1，$h_\theta(x)<0.5$时认为结果为0

但是这样会有一些问题：

1、单个的点会导致阀值点映射的特征变量阀值影响比较大

2、线性回归的范围太大了，>>1 或者<<0- 所以引入logistic回归来解决这个分类问题
所以logistic回归是一种分类算法

# 算法的定义

$$h_\theta(x) = \frac{1}{1+e^{\theta^TX}}$$

函数图像：![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224230757249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
我们可以认为假设函数时y=1的概率估计函数
$$h_\theta(x)=P(y=1|x,\theta)$$

# 决策边界

对于$h_\theta(x)$函数观察可知，
$$\theta^TX>=0=>y=1$$
$$\theta^TX<0=>y=0$$
几个例子
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224231612316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
其实对于单个变量，是可以考虑多项式情况的
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224231812221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
# 代价函数

如何仿照之前的线性回归：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224232043255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)

但是会出现非凸函数的情况，例如为波浪状，有多个极小值点，不好进行梯度下降
所以logistic 回归的损失函数为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224232230197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019022423224195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)

以 y = 1时进行分析

1、$h_\theta(x)->0$时，损失函数趋紧与无穷大，使用非常大的代价来进行惩罚

2、$h_\theta(x)->1$时，损失函数为0，即损失为0- 利用简化后的代价函数+梯度下降完成对整个训练集的学习
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224232727239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)

=>
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224232835225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
梯度下降(额，关于为什么对单个$\theta$求导结果是这个，mark一下，有空推导下)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224233309224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
同时注意，特征缩放也是有意义的～

# 优化算法

会有一些写好的比梯度下降更加好的算法，优点是

1、不需要设置学习速率，会自动适应

2、收敛的比梯度下降更快
缺点：

更复杂，一般是数值计算的专家写的

# 多类别分类

如，邮件有多个标签，如何分类，例如如下图
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190224233646215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)

分解成三个分类问题，所以我们可以求出三个概率值，然后找最大的对应的标签～














