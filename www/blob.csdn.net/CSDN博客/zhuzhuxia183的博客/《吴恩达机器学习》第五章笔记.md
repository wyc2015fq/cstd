# 《吴恩达机器学习》第五章笔记 - zhuzhuxia183的博客 - CSDN博客





2019年02月21日 13:25:47[喵小姐的邻居小小朱](https://me.csdn.net/zhuzhuxia183)阅读数：29
个人分类：[机器学习](https://blog.csdn.net/zhuzhuxia183/article/category/8658837)

所属专栏：[机器学习](https://blog.csdn.net/column/details/33594.html)












### 这里第五章：多变量线性回归
- [多变量线性回归的定义](#_2)
- [对应的多元梯度下降](#_7)
- [特征缩放](#_10)
- [学习率](#_15)
- [多项式的扩展](#_28)
- [正规方程求解](#_33)
- [正规方程中逆不存在时](#_41)




# 多变量线性回归的定义
- 指每一样样本中，不只是有一个特征量，而是多个
- 我们为了保持通过另常数项对应的特质量设置为1，直接构成一n+1纬度的向量
- $h_\theta(x) = \theta ^{T}X=X^{T}\theta$

# 对应的多元梯度下降

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190221130804907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)

# 特征缩放

含义：尽量使得每个特征都位于-1～1之间，使得梯度下降的过程更加的快速直接
- $$x_1 = \frac{x_1-u_1}{S_1}$$其中，$u_1$指平均值，$s_1$指特征值的范围，$max-min$

# 学习率

学习率$\alpha$是梯度下降中很重要的一个参数
- 
1、如何确保梯度下降正常运行

分析通过损失函数，判断是否限于某一个阀值进行判断

通过观察在某个学习率下，损失函数随着迭代次数的变量曲线来判断- 
2、这么选择学习率

从0.01以3倍的方式进行增加，不断尝试

- 
常见的问题

1、学习如果太小了，收敛太慢了

2、学习太大了，一般会有损失函数不断变大或者波动变化
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190221131644410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
# 多项式的扩展

其实我们可以根据曲线的形式，对每个原始特征量进行选择，即组合成新的特征量，如n次方的形式，即可以通过自由的选择特征量，来拟合复杂曲线
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190221132157296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190221131843890.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXpodXhpYTE4Mw==,size_16,color_FFFFFF,t_70)
# 正规方程求解

通过利用线性代数中的投影方式，来直接求解对应的参数结果
- 优点：1、不要考虑很多的参数，如学习率，直接求解 2、无迭代  3、不需要特征缩放
- 缺点：当特征量很大的时候，不好使用，而且
注意，即使在特征数目很大的情况下，梯度下降还是可以正常运行的

# 正规方程中逆不存在时

对应着：

1、包含了多余的特征

2、如样本数目<<特征量，我们需要删除某些特征，同时需要进行正则化

octave 中prinv(A)在逆不存在时，会输出伪逆。








