# 机器学习（六）：最大似然估计、参数估计 - ChihkAnchor的博客 - CSDN博客





2019年04月14日 09:03:15[Chihk-Anchor](https://me.csdn.net/weixin_40871455)阅读数：27标签：[似然																[极大似然](https://so.csdn.net/so/search/s.do?q=极大似然&t=blog)](https://so.csdn.net/so/search/s.do?q=似然&t=blog)
个人分类：[机器学习](https://blog.csdn.net/weixin_40871455/article/category/8809411)








最大似然估计，通俗说，**利用已知的样本结果**，**反推最有可能（最大概率）导致这样结果的参数值**

似然也是用于表征概率的，只不过这个概率是我们已经知道事件的结果，而去反推事件发生环境的参数的概率，我们认为事件是在最可能发生该事件的环境参数下发生的，有点绕

举个例子：抛硬币，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上，不管怎样，我们现在就是有着这么一个perfect结果，我们现在就很好奇，到底是什么导致了这么完美的结果，于是我们就要反推做这个实验时的涉及到的参数，我们认为所有的一切都不可能是巧合，只有各种条件都是满足的，才有可能发生这样完美的1/2的概率，此时1/2的概率是作为结果的，因为我们做实验得出了这样的结果啊。经过我们计算发现，一枚标准的硬币，使得发生了【两面朝上的概率均为50%】这个事件的可能性最大；但是，如果是不标准的硬币，随便抛，能不能得出正反面各50%的频率这个结果呢，想想这也是有可能的嘛；所以我们只能说，如果这个硬币是标准硬币，发生正反面各50%的概率是最大的，因为这样子搞可以最大可能的出现我们期待的结果。我们运用出现的结果来判断这个事情本身的性质（参数），就是似然。 似然描述的是结果已知的情况下，该事件在不同条件下发生的可能性，似然函数的值越大说明该事件在对应的条件下发生的可能性越大。

再来个例子：有两个妈妈带着一个小孩到了你的面前，妈妈A和小孩长得很像，妈妈B和小孩一点都不像，问你谁是孩子的妈妈，你说是妈妈A。好的，那这种时候你所采取的方式就是极大似然估计：妈妈A和小孩长得像，所以妈妈A是小孩的妈妈的概率大，这样妈妈A看来就是小孩的妈妈，妈妈A就是小孩的妈妈。

极大似然估计就是在只有概率的情况下，忽略低概率事件直接将高概率事件认为是真实事件的思想。

结果和参数相互对应的时候，似然和概率在数值上是相等的，如果用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为：

![](https://img-blog.csdnimg.cn/20190410110326395.png)

![](https://img-blog.csdnimg.cn/20190410110417708.png)是条件概率的表示方法，θ 是前置条件，在 θ 的前提下，事件 x 发生的概率，相对应的似然可以表示为：

![](https://img-blog.csdnimg.cn/20190410110458324.png)

可以理解为已知结果为 x ，参数为 θ 对应的概率，即：

![](https://img-blog.csdnimg.cn/20190410110712882.png)

需要说明的是两者在数值上相等，但是意义并不相同，L 是关于 θ 的函数，而 PP 则是关于 x 的函数，两者从不同的角度描述一件事情。

### 似然函数的最大值

似然函数的最大值意味着什么？

概率描述的是在一定条件下某个事件发生的可能性，概率越大说明这件事情越可能会发生；

似然描述的是结果已知的情况下，该事件在不同条件下发生的可能性，似然函数的值越大说明该事件在对应的条件下发生的可能性越大。

我们关注似然函数的最大值，因为我们需要根据已知事件来找出产生这种结果最有可能的条件，目的当然是根据这个最有可能的条件去推测未知事件的概率。在抛硬币的事件中，环境参数 p 可以取 [0, 1] 内的所有值，这是由硬币的性质所决定的，如果硬币均匀p=0.5，如果硬币不均匀则p就为其他值，显而易见，当 p=0.5 这种均匀硬币最有可能产生我们观测到的正反各为50%朝上。

![](https://img-blog.csdnimg.cn/20190410111237900.png)

似然函数的最大值意味着某个环境参数下最有可能发生我们观测到的事件，那我们就认为这个环境参数就是真实的环境参数，因为只有在这个环境参数下才能最大概率的发生我们观测到的结果，这样我们就可以根据似然函数通过求导算出极大值，计算出该极大值下的环境参数，然后我们就可以用这个环境参数正向计算其他事件发生的概率P(x|θ)

### 对数似然函数

实际问题往往会涉及到多个独立事件，在似然函数的表达式中通常都会出现连乘：

![](https://img-blog.csdnimg.cn/20190410112107298.png)

对多项乘积的求导往往非常复杂，但是对于多项求和的求导却要简单的多，对数函数不改变原函数的单调性和极值位置，而且根据对数函数的性质可以将乘积转换为加减式，这可以大大简化求导的过程：

![](https://img-blog.csdnimg.cn/20190410112136674.png)

**例子：**

假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？很多人马上就有答案了：70%。而其后的理论支撑是什么呢？

我们假设罐中白球的比例是p，那么黑球的比例就是1-p。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜色服从同一独立分布。这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的概率是P(Data | M)，这里Data是所有的数据，M是所给出的模型，表示每次抽出来的球是白色的概率为p。如果第一抽样的结果记为x1，第二抽样的结果记为x2... 那么Data = (x1,x2,…,x100)。

P(Data | M)= P(x1,x2,…,x100|M)

　　　　　= P(x1|M)P(x2|M)…P(x100|M)

　　　　　= p^70(1-p)^30.

那么p在取什么值的时候，P(Data |M)的值最大呢？将p^70(1-p)^30对p求导，并其等于零。

　　　　70p^69(1-p)^30-p^70*30(1-p)^29=0。

　　　　解方程可以得到p=0.7。

所以当p=0.7时，P(Data|M)的值最大。这和我们常识中按抽样中的比例来计算的结果是一样的。

**连续变量：**

给定一堆数据，假如我们知道它是从某一种分布中随机取出来的，可是我们并不知道这个分布具体的参数，即“模型已定，参数未知”。例如，我们知道这个分布是正态分布，但是不知道均值和方差；或者是二项分布，但是不知道均值。 最大似然估计就可以用来估计模型的参数。极大似然估计的目标是找出一组参数，使得模型产生出观测数据的概率最大

即：

![](https://img-blog.csdnimg.cn/20190410121017868.png)

我们假设每个观测数据是独立的，那么有

![](https://img-blog.csdnimg.cn/20190410121129281.png)

为了求导方便，对目标取log，最优化似然函数等同于最优化对数似然函数：

![](https://img-blog.csdnimg.cn/20190410121328368.png)



### 最大似然估计小结

**最大似然估计总是能精确地得到解吗？**

简单来说，不能。在真实的场景中，更有可能的是，对数似然函数的导数仍然是难以解析的（也就是说，很难甚至不可能人工对函数求微分）。因此，一般采用期望最大化（EM）算法等迭代方法为参数估计找到数值解，但总体思路还是一样的。

**为什么叫「最大似然（最大可能性）」，而不是「最大概率」呢？**

大多数人倾向于混用「概率」和「似然度」这两个名词，但统计学家和概率理论家都会区分这两个概念。通过观察这个等式，我们可以更好地明确这种混淆的原因

![](https://img-blog.csdnimg.cn/20190410121927734.png)

这两个表达式是相等的！所以这是什么意思？我们先来定义 P(data; μ, σ) 它的意思是「在模型参数μ、σ条件下，观察到数据 data 的概率」。值得注意的是，我们可以将其推广到任意数量的参数和任何分布。

另一方面，L(μ, σ; data) 的意思是「我们在观察到一组数据 data 之后，参数μ、σ取特定的值的似然度。」

这个等式的含义就是：给定参数后数据的概率等于给定数据后参数的似然度。但是，尽管这两个值是相等的，但是似然度和概率从根本上是提出了两个不同的问题：一个是关于数据的，另一个是关于参数值的。这就是为什么这种方法被称为最大似然法（极大可能性），而不是最大概率。






