# 神经网络之感知器与词向量 - 刘炫320的博客 - CSDN博客
2017年04月18日 20:16:52[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1061
# 1. 写在前面
本文并不是写词向量如何使用神经网络感知器训练出来的，而是对于词向量，如何使用神经网络模型。本文是入门级，高手可以跳过。
# 2. 词向量
自从word2vec火了以后，词向量就变得越来越重要，google的引文统计中，Word2vec的引文数量上万了（一篇3000多，一篇6000多）。那么word2vec到底有什么用？
# 2.1 词的向量表示
这个最重要的一点来源于计算语言学，因为我们自然语言处理对于语言处理要讲究可以计算性，那么文字是不能够参与计算的，于是，就要把文字进行数字编码。
在word2vec以前，文字编码方式也是有很多形式，例如我们现在的UTF-8的编码，也是一种编码。但是这种编码并没有什么含义，只是单纯的编码。后来，出现了one-hot编码，也就是把所有词汇映射到一个大词典中，然后如果这个词出现了，就在这个词典对应位置上置为1，其他位为0，这样一行组成的编码称为one-hot编码。但是这种编码虽然提高了维护性（新来的词汇可以多添加一位就行，而不需要对原来的编码做任何改动），但是单词和单词之间仍然没有联系。
然后2013年word2vec出现了，随之而来的就是word embedding，这其实是把单词映射到了一个VSM向量空间里，但是并不是随机的映射，而是带有语义关系的映射，在这个VSM空间里不同的坐标代表的语义不同，相同的语义的词汇的坐标应当是相邻的，而且相同的距离也表示了相同的语义差异。
这样一来，就把语义信息融合进入自然语言处理中了。在这之前，更多的使用的是结构特征和词汇、语法特征。但是这个怎么用呢？
目前流行的深度学习目前还并不太懂，倒是神经网络接触了一点，现在才对如何使用向量模型通过神经网络来进行实验。
# 3. 假想的神经网络实验
## 3.1 简单的神经网络实验
这个实验并没有真正做过，我只是假想了这样一个实验，来进行讲解这个。
假设我们是一个词汇的分类问题。输入的词是一个向量X，是一个$1 \times 50$维的向量。最终的分类别有10类。我们的隐藏层有8个隐藏层，那么最终的神经网络结构应该是如下图： 
![感知器](https://img-blog.csdn.net/20170418165323523?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
这样子的话应该明确。这才是一个完整的神经网络，它包括一个输入层，一个隐藏层，一个输出层。像我们刚才说的那样，输入层有50维，那么这个是51维，其中$X_0$是偏置。输入层主要是真正的输入向量。隐藏层则是8个隐藏层，而输出层则是分类的类别，这里是10个，每个里的数值是这个类别的可能性，这个值位于0-1之间，我们要做的，就是让真实类别的可能性尽可能为1。
而这时候，就有误差了，然后我们依据损失函数，来对误差进行反向传播，对每个权值进行相应的调整，调整方法常见的有SGD随机梯度下降法、Adam（Adaptive Moment Estimation）算法等，具体的可以参见：[深度学习若干优化算法](http://blog.csdn.net/u012759136/article/details/52302426)。
这样经过一系列计算后，一个简单的神经网络模型就搭成了（这可是30年前就已经有的理论了）。
## 3.2 RNN
RNN，我们这里指的是循环神经网络（Recurrent Neural Networks）。 
这个图想必大家应当十分熟悉了： 
![RNN](http://static.open-open.com/lib/uploadImg/20150829/20150829181722_413.png)
这个图讲的是一个RNN展开后的结果，看起来特别像感知机，但是很多人只解释了从下到上这一过程，也就是我们之前讲的神经网络这层，而另外一个维度，从左到右的过程，却很少有人讲到，这里其实是这样的： 
$h_t=g(h_{t-1},x_t)$
这个g（）就有点抽象了，其实很简单。如果我们之前普通的神经网络描述成h=f(W·X+b)的形式，那么g也应该没什么问题，其实转换过来就是 
$h_t=g(f(W·X_{t-1}+b),X_t)$
这种形式的意思就是说，$h_t$仅和前一个输入及当前输入有关。但是g()的形式是十分多样的，最简单的： 
$h_t=W·X_{t-1}+U·X_t+b$
这是一种，还有很多其他形式的组成形式，这个以后才能碰到。如果还要深入理解的话，可以移步：[RNN简单入门](http://blog.csdn.net/prom1201/article/details/52221822)。
# 4 .激活函数
激活函数是感知器一个非常重要的组织部分，它可以把最终最终的结果归一化正则化等等，能够更加体现出结果。
常见的激活函数有我们最熟悉的Sigmoid函数，也有常用的Relu和现在可以归一化的Softmax函数，也是为了可以处理非线性分类。
具体的介绍我就不多说了，详见：[为什么要有激活函数](https://www.zhihu.com/question/22334626/answer/21036590)、[常见的激活函数与总结](http://www.jianshu.com/p/6df4ab7c235c)、 
[SoftMax VS Sigmoid](https://baijiahao.baidu.com/po/feed/share?wfr=spider&for=pc&context=%7B%22sourceFrom%22:%22bjh%22,%22nid%22:%22news_3393646120346460921%22%7D)。
