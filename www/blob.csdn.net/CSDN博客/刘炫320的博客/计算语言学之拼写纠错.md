# 计算语言学之拼写纠错 - 刘炫320的博客 - CSDN博客
2017年06月10日 16:27:22[刘炫320](https://me.csdn.net/qq_35082030)阅读数：3326
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 1. 引言
拼写检查是一个非常底层的自然语言处理方面的任务。多用在信息检索、输入法等，其实也可以扩展到寻找同义词等相关领域。这里我们主要针对英文、中文中的拼写检查的方法，进行一个简要的概述，因为这方面是一个很热门的研究方向，所以材料很多，我们只是进行入门介绍。
# 2. 英文拼写纠错
无论是英文拼写纠错还是中文拼写纠错，都需要两部分，一个是发现错误，一个是纠正错误。这里我们提供2种匹配方法。
## 2.1 基于字典的字符匹配
### 2.1.1 发现拼写错误
假设说，我们有一个非常完备的包含了所有正确单词的字典，那么我们只需要对输入的单词与字典中的单词一一比较，即可发现有无错误了。这是非常简单的想法。
但是如果这个词典数量非常大，在我入手的语料中，曾经入手过包含2000万单词的语料，那么这个数量如果进行一一比较的话，那速度是可以想象的。不过如果使用Trie树（我们之前讲过），那么速度可以显著性提升。也就是一个类似有限状态自动机的东西。每一个结点为27个子树（分别是26个英文字母和一个空格作为结束符），其用户在输入的时候，就同时进行路径行走，这样输入完毕就可以获得最终的单词了。如果输入的单词所走的路径不存在了，那么就提示出现了拼写错误。
### 2.1.2 纠正拼写错误
上面的情况我们已经可以发现错误了，那么如何进行纠错是我们第二步要做的，如果根据实际情况，我们可能要列举出它可能是哪个词，这时候，可以使用按键在键盘的位置而去找它最有可能的转移概率，这样可以给出几个候选的。
例如我们输入：cross,结果输成ceoss,呢么有可能是r和e离的太近而导致的误按，这样在输入e的时候就有可能根据转移概率列出r、f、d、s、w这五个相邻按键的转移概率并给出提示，这里也可以继续增加一种先验概率，那就是n-gram模型。为了简化，我们只考虑2-gram,这就类似马尔科夫模型了。继续上例讲解，那么除了计算p(r|e)、p(f|e)…等，还需要计算一个值那就是p(cr),p(cf)的值等等。
## 2.2 基于统计的单词匹配
上面的那个基于字典的字符匹配主要针对的是以字符为基础单位的拼写检查，下面我们介绍以单词为基础的拼写检查。同样的，还是需要两个步骤，第一个步骤，探测出错误，第二个，纠错。
### 2.2.1探测拼写错误
这里我们不再纠结于单词中的某一个字母上的不同，而是从单词整体上考虑，这个单词是否是合适的。除了在2.1中讲到的基于字典的拼写错误发现外，还可以使用基于统计的发现不合适的单词。
### 2.2.2纠正拼写错误
在这里，使用[剪辑距离](http://blog.csdn.net/qq_35082030/article/details/62237442)以及使用噪声信道模型也是非常高效的算法，如果考虑上下文的话，再组合n-gram模型即可。
剪辑距离我们在链接中已经讲过了，而且比较详细，下面我们讲解一下噪声信道模型。噪声信道模型被广泛应用于语音识别、拼写纠错、机器翻译、中文分词、词性标注、音字转换等。
它是一个比较普适的模型，其形式化定义为： 
$I=arg\max_IP(I|O)=arg\max_I\frac{P(O|I)P(I)}{P(O)}=arg\max_IP(O|I)P(I)$
不要被一串公式吓到了，如果用到了我们这个模型上，你就会认为，对，应该就是这种模型。下面给出应用到我们拼写纠错上的公式： 
现在已知的错误单词用x表示，最大可能的正确单词用w表示，那么： 
$\hat{w}=arg\max_{w \in V}P(w|x)$
$\hat{w}=arg\max_{w \in V}\frac{P(x|w)P(w)}{P(x)}贝叶斯公式$
$\hat{w}=arg\max_{w \in V}P(x|w)P(w) 对于每一个x来讲，P(x)不变$
P(w)为先验概率，P(x|w)为转移概率，二者可以基于训练语料库建立语言模型和转移矩阵即可得到。
最终，在[拼写检查示例](http://blog.csdn.net/fkyyly/article/details/42933281)中，给出了一个详细的例子，融合了上述所讲的几种方法，最终纠正的过程。它是采用了先确定错误类型，再通过计算最小编辑距离获取候选纠正词，最后结合Unigram模型建立了语言模型，从而得出了最终的纠正词。
## 2.3 不是真的“错”
事实上，除了确实写错了词之外，还有一部分错误是单词使用不当，或者说，这个词本身是正确的，但是用错了，例如，i want to eating，这时候可能是eating的时态不对，这时候，就没有办法通过上面的基于no-word的错误了，因为eating本身就是合法单词，这种错误属于real-word，这样我们就需要使用n-gram模型来解决了，以[“I want to eat Chinese food”](https://my.oschina.net/keyven/blog/516808)为例，介绍了如何使用n-gram模型进行纠错和语法填空。具体的语言模型可以参考[语言模型简介](http://blog.csdn.net/qq_35082030/article/details/70143557)中的介绍。
## 2.4 几种英文单词纠错方法小结
(1)误拼字典法。
收集大规模真实文本中拼写出错的英文单词并给出相应的正确拼写，建造一个无歧义的误拼字典。在进行英文单词拼写检查时，查找误拼字典，如命中，则说明该单词拼写有误，该词的正确拼写字段为纠错建议。该方法的特点是侦错和纠错一体化，效率高。但英文拼写错误具有随机性，很难保证误拼字典的无歧义性和全面性，因此查准率低、校对效果差。
(2)词形距离法。
这是一种基于最大相似度和最小串间距离的英文校对法。其核心思想是构造单词的似然性函数，如该单词在词典中，则单词拼写正确；否则，按照似然性函数，在词典中找到一个与误拼单词最相似的词作为纠错候选词。该方法的特点是节省存储空间，能反映一定的常见拼写错误统计规律，是一种模糊校对法。
(3)最小编辑距离法。
通过计算误拼字符串与词典中某个词间的最小编辑距离来确定纠错候选词。所谓最小编辑距离是指将一个词串转换为另一个词串所需的最少的编辑操作次数(编辑操作是指插入、删除、易位和替换等)。还有人提出了反向最小编辑距离法，这种方法首先对每个可能的单个错误进行交换排列，生成一个候选集，然后，通过查词典看哪些是有效的单词，并将这些有效的单词作为误拼串的纠错建议。
(4)相似键法。
相似键技术是将每个字符串与一个键相对应。使那些拼写相似的字符串具有相同或相似的键。当计算出某个误拼字符串的键值之后，它将给出一个指针。指向所有与该误拼字符串相似的单词，并将它们作为给误拼字符串的纠错建议。
(5)骨架键法。
通过构建骨架键词典，在英文单词出现错误时，先抽取出该错误单词的骨架键，然后再去查骨架键词典，将词典中与该单词具有相同骨架键的正确单词作为该单词的纠错建议。
(6)N-gram法。
基于n元文法，通过对大规模英文文本的统计得到单词与单词问的转移概率矩阵。当检测到某英文单词不在词典中时。查转移概率矩阵，取转移概率大于某给定阈值的单词为纠错建议。
(7)基于规则的技术。
利用规则的形式将通常的拼写错误模式进行表示，这些规则可用来将拼写错误蛮换为有效的单词。对于一个误拼字符串，应用所有合适的规则从词典中找到一些与之对应的单词作为结果，并对每个结果根据事先赋予生成它的规则的概率估计计算一个数值，根据这个数值对所有候选结果排序。
# 3. 中文拼写纠错
中文拼写错误和英文不同，英文不需要二次转换，而中文是由拼音/笔画转化成所需要的汉字，然后再由汉字组成词。目前大多使用的是基于拼音的拼写。下面我们讲解基于拼音的中文拼写纠错。
## 3.1基于拼音的中文拼写纠错
英文的基于字符的拼写错误检查对于基于拼音的中文拼写错误仍然可以适用。例如前后鼻音，或者按错字母等。这样，基本上就可以在拼音层面进行错误发现和纠正。
|序号|输入关键词|纠错提示|
|----|----|----|
|1|氯华钠|氯化钠|
|2|氯哈钠|-|
|3|摆渡|-|
|4|摆度|百度|
另一个可以纠正的部分就是同音字纠错。如同谷歌的搜索纠错一样。它只能纠正同音字的错误。其原理也是如此：
1）首先根据字转换成拼音，这点没什么问题。 
2）然后对于同一个音的所有候选字计算出修正概率： 
$P（w|x）=P(x|w)P(w)$
其中每个音匹配的不同字的状态转移概率为P（w）（基于统计获得），而P(x|w)则是对于不同字对应的正确字的转移概率（基于统计获得），最终可以得出最终的纠正后的结果。
3）如果再搭配n-gram模型，那么效果会更好一些。
## 3.2基于字的中文拼写纠错
上述方法只能适用于同音字，对于不同音字的错误就只能依靠n-gram模型了，但是n-gram模型是短距离中文纠错，对于长距离中文纠错，也就是中文语法错误诊断评测CGED（ACL-IJCNLP2015 workshop）上官方已经给出了四种语法错误（Redundant、Missing、Selection、Disorder）： 
![这里写图片描述](http://static.oschina.net/uploads/space/2015/1013/201147_rQ3I_1451225.png)
[基于依存树的同义词查找的长距离中文纠错](https://my.oschina.net/keyven/blog/516808)中给出了一个解决办法，相应的，如果想详细了解中文纠错的，可以先看一看《基于n-gram及依存分析的中文自动差错方法》（马金山，刘挺，李生）
## 3.3 几种基于上下文的文本错误校对方法小结。
现有的基于上下文的文本错误校对方法有三类：①利用文本的特征，如字形特征、词性特征或上下文特征；②利用概率统计特性进行上下文接续关系的分析；③利用规则或语言学知识，如语法规则、词搭配规则等。
基于词语同现与搭配特征的校对方法有很多种，较好的有Bayesian方法和基于Winnow方法。各种N-gram模型，如长距离N-gram、触发对N-gram等模型，都可以利用目标词上下文中的词同现特征或搭配特征，采用最大似然估计法、互信息、相关度等方法检测文本中的错误，并通过相邻词间的转移概率确定纠错候选词，实现对目标词的校正。
# 4. 小结
本文中，我们对于中英文的纠错都给出了相应的办法，而且不止一种，其实总而言之，一方面是根据词汇信息、一方面是根据语法信息、一方面是根据上下文信息来进行判别。当然总结了所有的方法组合在一起，也许可以获得比较好的效果。
