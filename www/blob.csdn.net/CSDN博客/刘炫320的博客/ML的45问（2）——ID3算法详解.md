# ML的45问（2）——ID3算法详解 - 刘炫320的博客 - CSDN博客
2017年04月09日 11:31:55[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1372
所属专栏：[《Machine Learning》重点和难点](https://blog.csdn.net/column/details/16516.html)
# 1. 写在前面
这次我们主要介绍关于决策树的相关问题，尤其是针对ID3算法的一些问题进行相应的解答。
# 2. ID3算法过程
ID3的能处理的数据都是离散值的。接下来我们看算法：
```
创建Root结点
    如果Example都为正，
        那么返回label=正的单节点数Root
    如果Example都为副，
        那么返回label=副的单节点数Root
    如果Attribute为空，
        那么返回Root,label=Example中最普遍的Target_attribute的值。
    否则
        A←Atrribute中分类Eaxmples能力最好的属性（*）
        Root的决策属性←A
        对于A的每个可能值Vi
            在Root下加一个新的分支对应测试A=Vi
            令Examplevi为Examples满足A属性值为Vi的子集
            如果Example为空
                在新分支下，加一个叶子节点，
                结点label=example中最普遍的Target_attribute值。
            否则
                在新分支下，加一个ID3的子树（包含所有满足的样例，所有标签，但是特征数-1）。
        结束
    返回Root
```
在ID3算法中，（*）所采用的度量标准是信息增益最大的特征。具体方法如下： 
1. 首先计算当前集合熵 
$Entropy(S)=Entropy（X+，Y-）$
$=-\frac{X}{X+Y}log_2 \frac{X}{X+Y}- \frac{Y}{X+Y}log_2 \frac{Y}{X+Y}$
其中X为正例数，Y为负例数。 
2. 其次，选取一个候选属性，按照其取值进行分类，得出新集合$S_1,S_2,...,S_n$
统计新分类后的熵和Entropy($S_1,S_2,...,S_n$) 
3. 计算Entropy(S)-Entropy($S_1,S_2,...,S_n$)的值（0~1之间） 
4. 对所有候选属性均做2，3步 
5. 找出最大的Value的属性作为其分支结点。
若样本全为正或者全为负，则熵为0，若正负相等，则熵为1。
# 3. ID3算法路径终止条件
决策树的终止条件有两种： 
1. 所有的属性都已经被这条路径所包括。 
2. 与这个结点关联的所有训练样例都具有同样的目标属性值。
# 4. ID3算法避免过拟合的方法
避免过拟合的方法和避免所有事情发生的方法一样： 
1. 及早停止树的增长，比如设置层数或者迭代次数。 
2. 后修剪，当生成完毕后，再进行二次编辑，根据修建方法又可以分为2种：
- 错误率降低修剪。
- 规则后修剪。
总的来说，及早停止树增长，就如同预言家一样，是很难做到的，因此目前常用的都是错误率降低修剪和规则后修剪。
# 5. ID3算法假设空间大小及特征
ID3的算法假设空间是所有从简单到复杂的树的集合，其评估函数是信息增益度量。
# 6. ID3算法优缺点
优点： 
1. 假设空间包含所有的决策树，搜索空间完整。 
2. 健壮性好，不受噪声影响。 
3. 可以训练缺少属性值的实例。 
缺点： 
1. 仅维护单一的当前假设。 
2. 不进行回溯，容易陷入局部最优。
# 7. ID3算法归纳偏置
ID3有2条归纳偏置： 
1. 较短的树比较长的树优先（奥坎姆剃刀原理） 
2. 信息增益高的属性更靠近根节点的树优先。
# 8. 把候选消除法应用到决策树假设空间的四个困难
- 如果目标函数不在假设空间时，候选消除法的变形空间是空的。
- 如果一个属性的值比较多，一棵决策树会很宽。
- 求出S.G及其边界十分困难，不同形状的决策树可能等价。
- 候选决策树的重复信息很多，构建时计算量会很大。
# 9. 小结
本章，我们只讨论关于决策树ID3算法的相关问题，比较基础，更多关于决策树的内容，我们会放到《统计学习方法》中来讲。好了下期见。
