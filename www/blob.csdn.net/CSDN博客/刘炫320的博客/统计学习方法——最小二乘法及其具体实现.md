# 统计学习方法——最小二乘法及其具体实现 - 刘炫320的博客 - CSDN博客
2019年03月21日 11:11:34[刘炫320](https://me.csdn.net/qq_35082030)阅读数：101
所属专栏：[统计学习方法笔记](https://blog.csdn.net/column/details/16460.html)
# 1. 引言
最小二乘法作为线性拟合常用的一种方法，勒让德( A. M. Legendre)于1805年在其著作《计算慧星轨道的新方法》中提出的，被广泛应用于各种数据拟合的方法中。曾经在某软时，也遇到这题，今有幸弄清最小二乘法的原理和计算方法，特地分享出来，供大家查阅和指点。
本文主要内容如下：
（1）介绍最小二乘法原理和相关知识
（2）介绍最小二乘法的计算方法
（3）使用Matlab进行最小二乘法的实现
（4）最小二乘法的深入思考
# 2. 最小二乘法原理和相关知识
最小二乘法是线性拟合的一种常用方法，最早接触于高中时简单的统计分析中。它的目的是给定一组数据，求得与此组数据最相近的公式。其中，最相近公式的原型（Prototype）需要根据先验知识给出，如对常见的 $y=k\times x+b$的拟合。则最小二乘法求得k和b的方法为使得理论值和观测值之差的平方和达到最小。
$E=\sum_{k=1}^N (y_i- \hat y)^2$
最小二乘法不仅可以对简单的式子进行拟合，还可以对多项式线性拟合以及非线性拟合，在第三章中我们逐步去探索。
# 3. 最小二乘法的计算方法
## 3.1. 一元线性拟合和非线性拟合
最简单也是高中就接触到的二维线性问题，已知有N个数据${(x_1,y_1),(x_2,y_2)...(x_n,y_n)}$，假设线性函数的形式为$y=k\times x+b$，则被转换为一下最优化问题:
$\min_{a,b}\sum_{n=1}^{N}(y_n-(k\times x+b))^2$
这是一个无约束的最优化问题，分别对a和b求导即可获得。
$k=\frac{N \sum_{n=1}^N x_n \times y_n-(\sum_{n=1}^{N} x_n)(\sum_{n=1}^{N} y_n)}{(N\sum_{n=1}^{N}(x_n)^2-(\sum_{n=1}^{N}x_n)^2)}$
$b=\frac{(\sum_{n=1}^Ny_n)}{N}-k \times \frac{(\sum_{n=1}^{N}x_n)}{N}$
这是在高中课本上的式子，计算十分繁琐。
那么如何从一元线性拟合转换为一元非线性拟合？
例如：$y=k\times ln(x)+b$
只需要令$t=ln(x)$，则原式可转换为$y=k\times t+b$即可。
## 3.2. 多项式和多元函数拟合
从第一部分我们知道如何进行一元线性和非线性的拟合。
那么如何进行多项式拟合呢？我们使用矩阵来表示，一般的，所有的多项式和多元函数拟合均可转换为以下公式：$Y=W\times X$
其中Y,W,X均为矩阵，此时目标函数就简化为：
$\min_{W}(Y-W^TX)^T(Y-W^TX)$
其中W即为所有未知数的集合：
$W=\begin{bmatrix}1      & \cdots & 1      \\x_1      & \cdots & x_n      \\\vdots & \ddots & \vdots \\z_1      & \cdots & z_n\end{bmatrix}$
令$Q=(Y-W^TX)^T(Y-W^TX)$，对w进行求导，并令为0，则有(这里省略过程，一般使用矩阵的迹计算)
$\frac{\partial Q}{\partial W}=2(-X)(Y-W^TX)^T=0$
于是有$XY_T-XX_TW=0$，假设$XX^T$可逆，那么有$W=(XX^T)^{-1}XY^{T}$
至此完成了最优值的求解。
其实说了这么多，最重要的是最后一行，只需要知道X和Y即可一步求解出W了。
## 3.3 Matlab代码
这里我们给出Matlab的详细实现代码，事实上，使用任何一个语言都可以实现，尤其是对于矩阵的运算方便的话。
这里以求解$y=kx^2+blog(z)$为例。
```matlab
%%首先给出待拟合数据
x=[]
z=[]
y=[]
%%转换为最终状态y=wx
xf=x^2
zf=log(z)
yf=y
%%进行求解
xfa=[xf,zf]
w=inv(xfa*xfa')*xfa*yf'
```
w即为获得的求解结果，其中第一个数代表xf的参数，第二个数代表zf的参数，即与xfa的顺序一致。
现在留个思考题，如果要拟合一下公式，该计算步骤是怎么样呢？其中给出自变量x,z和因变量y，求解a,b,c，欢迎在评论区留言！（如今这题在评论区中已经有解，即拆分所有括号重新形成多项式再进行变量代还即可）
$y=k(ax^2+bx)(ax-cz)$
# 4. 最小二乘法的深入思考
接下来是对于最小二乘法的更进一步思考，主要包括：
## 4.1 最小二乘法简单形式为$WX=Y$,为什么不直接使用$W=YX^{-1}$求解W？
首先，其公式是对的，第二，若X为方阵，且有逆矩阵（即行列式不为0），则可以使用此方法，否则无法应用。
## 4.2 什么情况下可以使用最小二乘法？什么情况下不能够使用最小二乘法？
一般对于简单的计算，我们一般可以使用最小二乘法。当目标函数是广义线性模型时（即都可以通过某种变换转换为$Y=WX$的形式），可以使用最小二乘法。广义线性模型一般式为：
$Y=g^{-1}WX$
其中$g^{-1}$为反函数。
但是，在这期间有很多操作是假定的。首先，我们假定$XX^{T}$是可逆的，这个条件是当$XX^{T}$为满秩矩阵或者正定矩阵时，才成立。因此若$XX^{T}$不满足此条件时该如何处理。
一种情况是参数过多而样例过少，导致X的行数多于列数，$XX^{T}$显然是不满秩，此时可解出多个W，根据正则化即可获得所需解。
另一种情况是可以使用矩阵的伪逆来代替矩阵的逆，既可以避免没有逆矩阵的问题，还可以简化计算，更快的获得解，而只需要降低略微的精度。
## 4.3最小二乘法和随机梯度下降法的区别是什么？
首先，线性回归的模型假设。这是最小二乘方法的优越性前提，否则不能推出最小二乘是最佳（即方差最小）的无偏估计，具体请参考高斯-马尔科夫定理。特别地，当随机噪声服从正态分布时，最小二乘与最大似然等价。
其次，最小二乘法一步获得全局最优解（如果这个精确解存在的话），而随机梯度下降法是迭代法求解。
