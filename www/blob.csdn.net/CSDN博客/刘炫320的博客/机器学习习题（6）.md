# 机器学习习题（6） - 刘炫320的博客 - CSDN博客
2017年07月17日 21:30:10[刘炫320](https://me.csdn.net/qq_35082030)阅读数：871
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
# 1.前言
这一节关于正则化的题目比较多。
# 2. 习题
## 2.1 习题1（信息增益）
> 
如下表是用户是否使用某产品的调查结果（）
|UID|年龄|地区|学历|收入|用户是否使用调查产品|
|----|----|----|----|----|----|
|1|低|北方|博士|低|是|
|2|高|北方|本科|中|否|
|3|低|南方|本科|高|否|
|4|高|北方|研究生|中|是|
正确答案:C
解析： 
【年龄】[]中数字代表记录ID  
  低：[1,是]，[3,否]  
  高：[2,否]，[4,是]  
【地区】 
  南方：[3,否] 
  北方：[1,是]，[2,否]，[4,是] 
【学历】 
 本科： 
[2,否]，[3,否] 
研究生：[4,是] 
博士：[1,是] 
【收入】 
低：[1,是] 
中：[2,否]，[4,是] 
高：[3,否]
对于每一维特征，分别使用其候选值使用决策树的方法观察信息增益，可以看到使用学历特征可以使得分类后的熵为0，也就是信息增益最大。因此选C。
## 2.2 习题2（L1与L2范数）
> 
在Logistic Regression 中,如果同时加入L1和L2范数,会产生什么效果()
A.可以做特征选择,并在一定程度上防止过拟合
B.能解决维度灾难问题
C.能加快计算速度
D.可以获得更准确的结果
正确答案:A
解析： 
Ｌ１范数具有系数解的特性，但是要注意的是，Ｌ１没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。
在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。因此选择A。
对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅《[范数规则化](http://blog.csdn.net/zouxy09/article/details/24971995/)》。
## 2.3 习题3（正则化）
> 
机器学习中L1正则化和L2正则化的区别是？
A.使用L1可以得到稀疏的权值
B.使用L1可以得到平滑的权值
C.使用L2可以得到稀疏的权值
D.使用L2可以得到平滑的权值
正确答案:AD
解析：
L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0. 
L2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。
L1正则化/Lasso 
L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。
L2正则化/Ridge regression 
L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。
可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。
因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。
具体的，可以参阅《[机器学习之特征选择](http://www.cnblogs.com/nolonely/p/6435083.html)》与《[机器学习范数正则化](http://blog.csdn.net/zouxy09/article/details/24971995/)》。
## 2.4 习题4（势函数法）
> 
位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()
A.后验概率
B.先验概率
C.类概率密度
D.类概率密度与先验概率的乘积
正确答案:AD
解析：
事实上，AD说的是一回事。 
具体的，势函数详解请看——《[势函数法](http://www.cnblogs.com/huadongw/p/4106290.html)》。
## 2.5 习题5（隐马尔可夫）
> 
隐马尔可夫模型三个基本问题以及相应的算法说法正确的是（ ）
A.评估—前向后向算法
B.解码—维特比算法
C.学习—Baum-Welch算法
D.学习—前向后向算法
正确答案:ABC
解析：评估问题，可以使用前向算法、后向算法、前向后向算法。
# 3. 小结
本章主要描述了5个问题，分别是信息增益、范数、正则化、势函数法和隐马尔可夫模型，有熟悉的，也有陌生的，要好好学习，弥补不足。
