# 神经网络与深度学习（5） - 刘炫320的博客 - CSDN博客
2017年09月02日 11:15:07[刘炫320](https://me.csdn.net/qq_35082030)阅读数：747标签：[深度学习																[神经网络																[计算图																[反向传播																[吴恩达](https://so.csdn.net/so/search/s.do?q=吴恩达&t=blog)](https://so.csdn.net/so/search/s.do?q=反向传播&t=blog)](https://so.csdn.net/so/search/s.do?q=计算图&t=blog)](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)
个人分类：[深度学习笔记](https://blog.csdn.net/qq_35082030/article/category/7137295)
所属专栏：[深度学习与神经网络课程笔记](https://blog.csdn.net/column/details/17134.html)
# 1. 计算图模型
计算图模型是深度学习与神经网络的基础。如果你对神经网络了解不是那么透彻的话，你可能是第一次听说计算图模型，计算图不仅仅可以用在深度学习中，它在其他领域也有相关的应用。
在深度学习中，我们把一次训练过程看作是一个前向传播和后向传播相结合的过程。下面我们分别看一下前向传播和后向传播的过程。
# 2. 前向传播
前向传播就是传统的计算图，它可以把任意的计算式转换为计算图来进行理解和运行。下面我们就举一个例子来说明这一点。
假设我们现在有一个式子$J(a,b,c)=3(a+b*c)$这是一个非常简单的式子。那么它最终转换为计算图的话，就是如下图所示： 
![这里写图片描述](https://img-blog.csdn.net/20170902101804693?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
事实上，你可能会问，为什么会解释成这样的一张图，吴恩达没有告诉你。这其实是计算机实现运算的最基本的过程。在CPU中有ALU逻辑运算单元，它只能够实现两个数的逻辑运算，而在这里，我们把它可以扩充为若干输入和复杂的一次性运算的组合。只需要不断的重复这样一个过程。就能够实现无限复杂的式子（如果你学过计算机组成原理的话）。
这样如果我们把abc分别赋值，通过一次又一次的运算，数据流就从左流到右边了，最终得出33这个数值，这就是前向传播。相对的，如果从右向左的话，就是反向传播了，如下图的红线所示： 
![这里写图片描述](https://img-blog.csdn.net/20170902102200789?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
# 3.反向传播
正如吴恩达所讲，反向传播其实是传播导数，也就是想去衡量当中间变量发生改变的时候，会对最终输出有多大的影响，从而根据这个影响的大小来改变我们的输入的变量。总的来说，就是一句话，使用链式法则对于每一个非最终输出变量进行求导即可，具体的如下图所示： 
![这里写图片描述](https://img-blog.csdn.net/20170902103630312?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
也就是说，例如dv=3，du=3,da=3,db=6(实际上是3*c，只不过此例c=2)等等，这样就可以计算出所有的导数了，而最快速的计算方法当然是从右向左逐步计算，也就是红色的箭头，这就是反向传播的过程。
另外，在代码实现中，吴恩达建议使用da来表示$\frac{dj}{da}$，因为所有的导数都是对dj的，因此可以省略不写。
当然这只是一种一个小部分，如果你对于微积分以及链式法则不太明白的话，建议你还是要看一下视频，视频里有非常详尽的讲解。
# 4. 梯度下降法的实现
尽管吴恩达觉得使用计算图来实现logistic回归的梯度下降有点大材小用（事实上确实如此），但是为了能够更好的理解，他还是使用了此例。
先来回顾一下logistic回归所需要的3个式子： 
![这里写图片描述](https://img-blog.csdn.net/20170902104916079?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
假设w只含有2个变量，那么这些式子最终组成的计算图如下： 
![这里写图片描述](https://img-blog.csdn.net/20170902105012834?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
我们所要关注的就是L与$w_1,w_2和b$之间的相互影响力，也就是他们的导数。
根据上面一节的介绍，我们会从右到左计算出所有路径上的导数，然后根据链式法则求得最后的导数。 
![这里写图片描述](https://img-blog.csdn.net/20170902105233150?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
可以看到$\frac{dl}{da}=-\frac{y}{a}+\frac{1-y}{1-a}，而\frac{dl}{dz}=a-y这样，我们的dw_i=x_i*dz,db=dz$。这样，就已经可以获得单个样本的后向传播过程，接下来我们看一下如何在所有样本中使用这个。
# 5. 应用到m个样例中
在上面一节中，我们已经能够完成一个样例的所需要的所有计算过程，那么事实上，我们知道最终是要使得成本函数优化为最小，也就是如下图所示： 
![这里写图片描述](https://img-blog.csdn.net/20170902110736690?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
那么具体的算法如下(为了使得更加容易理解，我特地在吴大大的笔记上附上了注释)： 
![这里写图片描述](https://img-blog.csdn.net/20170902110929408?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
经过这样几步，我们就完成了所有导数的计算，接下来使用一步梯度下降，就可以完成最终的参数调整了： 
![这里写图片描述](https://img-blog.csdn.net/20170902111049522?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
这只是其中一次调整，实际上我们可能需要上千步的调整才能使得我们的模型逐渐稳定下来。但是这确实也暴露了这种循环的缺点，需要非常大量的迭代循环才能够完成。这就要用到深度学习的黑科技（向量化），从而使得我们的计算能够不需要使用一个循环，并且大大加快计算速度（通过GPU）。下一节我们将会介绍这类黑科技（在其他的应用中，我们同样可以使用向量化来加速我们的程序运算速度）。
