# 机器学习习题（15） - 刘炫320的博客 - CSDN博客
2018年02月21日 19:45:31[刘炫320](https://me.csdn.net/qq_35082030)阅读数：856
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
给大家拜个晚年了！在新的9题机器学习习题中，我们主要讲解了4个方面： 
集成学习里随机森林与GBDT等相关知识，PCA降维的相关知识，聚类算法的相关知识，KNN的相关知识。
> 
1.对于随机森林和GradientBoosting Trees, 下面说法正确的是: 
  1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的  
  2.这两个模型都使用随机特征子集, 来生成许多单个的树  
  3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好 
  A. 2  
  B. 1 and 2  
  C. 1, 3 and 4  
  D. 2 and 4
参考答案: （A） 
解析：
1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所以说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系。 
2.这两个模型都使用随机特征子集, 来生成许多单个的树。
所以A是正确的。
这里随机森林（RF）和GBDT更详细的可以参考《[RF与GBDT](http://www.52ml.net/15084.html)》，而这两个的基础算法区别可以参见《[Bagging与Boosting](https://www.cnblogs.com/liuwu265/p/4690486.html)》。更多的重采样方法可以看《[重采样的若干方法](http://blog.csdn.net/whu_paprika/article/details/53362865)》。
> 
2.对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 : 
  A. 正确的  
  B. 错误的
参考答案: （B） 
解析：
这个说法是错误的。首先，“不依赖”和“不相关”是两回事；其次, 转化过的特征, 也可能是相关的。PCA与LDA的相关知识可以参见《[PCA与LDA介绍](http://blog.csdn.net/qq_35082030/article/details/75578084)》。
> 
3.对于PCA说法正确的是 : 
  1.我们必须在使用PCA前规范化数据  
  2.我们应该选择使得模型有最大variance的主成分  
  3.我们应该选择使得模型有最小variance的主成分  
  4.我们可以使用PCA在低维度上做数据可视化 
  A. 1, 2 and 4  
  B. 2 and 4  
  C. 3 and 4  
  D. 1 and 3  
  E. 1, 3 and 4
参考答案: （A） 
解析：
- 
PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响
- 
我们总是应该选择使得模型有最大variance的主成分 
- 
有时在低维度上作图是需要PCA降维帮助的
对于LDA和PCA的介绍可参阅《[PCA与LDA介绍](http://blog.csdn.net/qq_35082030/article/details/75578084)》。
> 
4.对于下图, 最好的主成分选择是多少 ? 
![这里写图片描述](https://img-blog.csdn.net/20180119094510717?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/1/fill/I0JBQkFCMA==)
  A. 7  
  B. 30  
  C. 35  
  D. 不确定
参考答案: （B） 
解析：主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。
> 
5.数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是 : 
  A. 单个模型之间有高相关性  
  B. 单个模型之间有低相关性  
  C. 在集成学习中使用“平均权重”而不是“投票”会比较好  
  D. 单个模型都是用的一个算法
参考答案: （B）
解析：
A.与B相悖。
B.具有低相关性才能够三个臭皮匠赛过诸葛亮。
C.平均权重和投票各有千秋，具体的可以详见《[集成学习的若干方法](http://blog.csdn.net/whu_paprika/article/details/53362865)》
D.说了使用多个模型或者多个算法
> 
6.在有监督学习中， 我们如何使用聚类方法？（B）  
  1.我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习  
  2.我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习  
  3.在进行监督学习之前， 我们不能新建聚类类别  
  4.我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习 
  A. 2 和 4  
  B. 1 和 2  
  C. 3 和 4  
  D. 1 和 3
参考答案: （B） 
解析：
1.我们可以为每个聚类构建不同的模型， 提高预测准确率；
2.“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。所以B是正确的。
其实这道题是有问题的，聚类方法是非监督学习的一种，是不能在有监督学习中的。如果非要理解此题，应当是数据是有监督的数据（即带类别标签的数据），但是我还想额外使用聚类算法加入到传统的有监督学习过程中。那么这时候1，2就说得通了。
> 
7.以下说法正确的是 :  
  1.一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的  
  2.如果增加模型复杂度， 那么模型的测试错误率总是会降低  
  3.如果增加模型复杂度， 那么模型的训练错误率总是会降低  
  A. 1  
  B. 2  
  C. 3  
  D. 1 and 3
参考答案: （C） 
解析：
A在之前的题目中就已经讲过，在对于不平衡的数据集进行预测时，正确率不能反映模型的性能。
BC模型越复杂，在训练集上越容易表现好，在测试集上越容易表现的不好。
> 
8.对应GradientBoosting tree算法， 以下说法正确的是 :  
  1.当增加最小样本分裂个数，我们可以抵制过拟合  
  2.当增加最小样本分裂个数，会导致过拟合  
  3.当我们减少训练单个学习器的样本个数，我们可以降低variance  
  4.当我们减少训练单个学习器的样本个数，我们可以降低bias 
  A. 2 和 4  
  B. 2 和 3  
  C. 1 和 3  
  D. 1 和 4
参考答案: （C） 
解析：
第一点最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节，其实就是挑选决策参数个数，详情可见《[决策树大全](http://blog.csdn.net/webzjuyujun/article/details/49495545)》。
第二点是考bias和variance概念的，这个我们之前已经讲过，参与决策的个体越多，偏差越小，方差越大，详情可以参见《[偏差、误差与方差](https://www.zhihu.com/question/27068705)》和《[偏差与方差的权衡](https://www.cnblogs.com/ooon/p/5711516.html)》。
> 
9.以下哪个图是KNN算法的训练边界 ?  
![这里写图片描述](https://img-blog.csdn.net/20180119095221157?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/1/fill/I0JBQkFCMA==)
  A) B  
  B) A  
  C) D  
  D) C  
  E) 都不是
参考答案：（B） 
解析：
KNN算法肯定不是线性的边界，所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。
其实这么说也不一定对，KNN算法其实使用的是距离作为衡量标准，但是距离的衡量方法也有很多种，详见《[KNN算法详解](http://blog.csdn.net/qq_35082030/article/details/60965320)》。
