# 机器学习习题（16） - 刘炫320的博客 - CSDN博客
2018年02月22日 13:05:15[刘炫320](https://me.csdn.net/qq_35082030)阅读数：827
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
在最新的一期中，我们主要介绍了交叉验证的相关知识、t-SNE的相关知识、线性回归的相关知识、可决系数的相关知识、相关系数的相关知识。
> 
1.下面的交叉验证方法 :  
  i. 有放回的Bootstrap方法  
  ii. 留一个测试样本的交叉验证  
  iii. 5折交叉验证  
  iv. 重复两次的5折交叉验证  
  当样本是1000时，下面执行时间的顺序，正确的是： 
  A. i > ii > iii > iv  
  B. ii > iv > iii > i  
  C. iv > i > ii > iii  
  D. ii > iii > iv > i
参考答案：（B）
解析：
Bootstrap方法是传统的随机抽样，验证一次的验证方法，只需要训练1个模型，所以时间最少。
留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，需要训练1000个模型。
5折交叉验证需要训练5个模型。
重复两次的5折交叉验证，需要训练10个模型。
> 
2.变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？ :  
  1.多个变量其实有相同的用处  
  2.变量对于模型的解释有多大作用  
  3.特征携带的信息  
  4.交叉验证 
  A. 1 和 4  
  B. 1, 2 和 3  
  C. 1,3 和 4  
  D. 以上所有
参考答案：（C） 
解析：注意，这题的题眼是考虑模型效率，所以不要考虑选项B。
> 
3.对于线性回归模型，包括附加变量在内，以下的可能正确的是 :  
  1.R-Squared 和 Adjusted R-squared都是递增的  
  2.R-Squared 是常量的，Adjusted R-squared是递增的  
  3.R-Squared 是递减的， Adjusted R-squared 也是递减的  
  4.R-Squared 是递减的， Adjusted R-squared是递增的 
  A. 1 和 2  
  B. 1 和 3  
  C. 2 和 4  
  D. 以上都不是
参考答案：（D） 
解析：R-Squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-Squared有R-Squared和predicted R-Squared所没有的问题。每次为模型加入预测器，R-Squared递增或者不变。
这里R-Squared成为可决系数，也被称为R2系数，也被称为拟合优度。说到拟合优度一般理解为回归直线与观测值的一个拟合程度。
如果样本回归线对样本观测值拟合程度越好，各样本观测点与回归线靠得越近，由样本回归做出解释的离差平方和与总离差平方和越相近；反之，拟合程度越差，相差越大。可决系数的取值范围在0到1之间，它是一个非负统计量。
但是，一般说来，较高的R2数值比较低的R2数值要好。R2也不能反映误差。就比如你的努力程度和历次考试成绩，虽然越努力成绩越好，但是你不能保证自己没有失误啊。这个失误就是残差，但是失误肯定不是主要部分，所以R2还是很大的。
更多关于可决系数和校正可决系数，可参见《[可决系数百度百科](https://baike.baidu.com/item/%E5%8F%AF%E5%86%B3%E7%B3%BB%E6%95%B0/8020809?fr=aladdin)》与《[可决系数与校正可决系数](http://bbs.pinggu.org/thread-3034786-1-1.html)》和《[线性回归中的若干问题](https://www.zhihu.com/question/22935472)》。
> 
4.对于下面三个模型的训练情况， 下面说法正确的是 : 
![这里写图片描述](https://img-blog.csdn.net/20180119100538843?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/1/fill/I0JBQkFCMA==)
  1.第一张图的训练错误与其余两张图相比，是最大的  
  2.最后一张图的训练效果最好，因为训练错误最小  
  3.第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型  
  4.第三张图相对前两张图过拟合了  
  5.三个图表现一样，因为我们还没有测试数据集 
  A. 1 和 3  
  B. 1 和 3  
  C. 1, 3 和 4  
  D. 5
参考答案：C
解析：其实这个图是非常经典的图，也就是说第一张图为欠拟合，第二张图是正常的，第三张图是过拟合。从这方面讲C是对的。
但是其实如果从严格意义上讲，如果没有测试集是没有办法说明是否是过拟合还是欠拟合的。但是我们有一个假设，认为我们的采样已经足够，训练样本大致等同实际分布，也就是说即使是测试样本，也应当大致与训练样本同分布。那么这样这题就是C了。
> 
5.对于线性回归，我们应该有以下哪些假设？ 
  1.找到利群点很重要, 因为线性回归对利群点很敏感  
  2.线性回归要求所有变量必须符合正态分布  
  3.线性回归假设数据没有多重线性相关性 
  A. 1 和 2  
  B. 2 和 3  
  C. 1,2 和 3  
  D. 以上都不是
参考答案：（D）
解析：
离群点要着重考虑，第一点是对的。
不是必须的，当然如果是正态分布，训练效果会更好。
有少量的多重线性相关性是可以的，但是我们要尽量避免。
所以真要选，应当是1。
> 
6.我们注意变量间的相关性。在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论？ 
  1.Var1和Var2是非常相关的  
  2.因为Var和Var2是非常相关的, 我们可以去除其中一个  
  3.Var3和Var1的1.23相关系数是不可能的 
  A. 1 and 3  
  B. 1 and 2  
  C. 1,2 and 3  
  D. 1
参考答案：（C）
解析：
Var1和Var2的相关系数是负的，所以这是多重线性相关，我们可以考虑去除其中一个。
一般的，如果相关系数大于0.7或者小于-0.7，是高相关的。
相关系数的范围应该是[-1,1]。
关于相关系数的详细介绍参见《[相关系数百度百科](https://baike.baidu.com/item/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/3109424?fr=aladdin)》与《[协方差与相关系数](https://www.zhihu.com/question/20852004)》。
> 
7.如果在一个高度非线性并且复杂的一些变量中“一个树模型可比一般的回归模型效果更好”是： 
  A. 对的  
  B. 错的
参考答案：（A）
解析：树模型可以处理非线性模型，并且树模型的区分度更好一些。想象一下决策树和LR。
> 
8.下面对集成学习模型中的弱学习者描述错误的是？ 
  A. 他们经常不会过拟合  
  B. 他们通常带有高偏差，所以其并不能解决复杂学习问题  
  C. 他们通常会过拟合
参考答案：（C） 
解析：弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。
> 
9.下面哪个/些选项对 K 折交叉验证的描述是正确的？ 
  1.增大 K 将导致交叉验证结果时需要更多的时间  
  2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心  
  3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量 
  A. 1 和 2  
  B. 2 和 3  
  C. 1 和 3  
  D. 1、2 和 3
参考答案：（D) 
解析：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。
> 
10.最出名的降维算法是 PAC 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？（B） 
  A. X_projected_PCA 在最近邻空间能得到解释  
  B. X_projected_tSNE 在最近邻空间能得到解释  
  C. 两个都在最近邻空间能得到解释  
  D. 两个都不能在最近邻空间得到解释
参考答案：（B） 
解析：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。
我们之前降到的LDA或者PCA都是线性降维，而t-SNE则是非线性降维，它是在SNE的基础上，在低维度使用t分布替代高斯分布来解决长尾问题，于2014年提出，其可视化效果远胜于其他算法（在识别手写体数字的基础上）。是一个非常新的算法。
关于t-SNE算法的更多解释参见《[从SNE到t-SNE再到LargeVis](http://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/)》与《[比PCA更高级的算法](https://yq.aliyun.com/articles/70733)》。
