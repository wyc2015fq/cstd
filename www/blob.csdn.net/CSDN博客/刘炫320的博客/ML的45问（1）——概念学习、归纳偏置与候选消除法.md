# ML的45问（1）——概念学习、归纳偏置与候选消除法 - 刘炫320的博客 - CSDN博客
2017年03月20日 21:08:21[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1262
所属专栏：[《Machine Learning》重点和难点](https://blog.csdn.net/column/details/16516.html)
# 0. 写在前面
从这章开始，我们针对机器学习的45问进行一个个的解答，这45问来自于Tom M.Mitchell的机器学习一书。大家可以参考一下。希望这45个问题能够解决一些关于机器学习相关知识的疑惑。
# 1. 机器学习的定义
机器学习的定义是： 
对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E中学习。
# 2. 学习问题的三个特征与选择训练经验的三个要素
## 2.1学习问题的三个特征
- 任务的种类
- 衡量任务提高的标准
- 经验来源
## 2.2选择训练经验的三个要素
- 训练经验能否为系统的决策提供直接或间接的反馈
- 学习器能在多大的程度上控制样例序列
- 训练样例的分布能多好的表示实例分布
# 3. 设计学习系统的流程
- 选择训练经验：与自己博弈/与专家博弈
- 选择目标函数：棋盘走子/分值计算
- 选择目标函数的表示形式:多项式/人工神经网络
- 选择目标函数的逼近算法:梯度下降/线性规划
- 最终的系统设计 
1） 执行系统，用来产生目标结果 
2）鉴定器，用来生成训练样例 
3）泛化器，用来生成评估函数 
4）实验生成器，用来产生实验数据
# 4. 概念学习的定义
概念学习是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数。
# 5.假设的表示形式
概念学习有3个级别： 
最特殊的：∅ ，表示不接收任何值 
普通的：$V_1,V_2,V_3...$，表示的是明确指定的属性值 
最一般的:?,表示接受任何值
# 6. 实例空间与假设空间的大小
$|V_i|表示第i个属性的可能取值的数目$
实例空间大小为:$|V_1|\times|V_2|\times|V_3|\times ...$
假设空间大小为:$1+(|V_1|+1)\times(|V_2|+1)\times(|V_3|+1)\times ...$
# 7. 概念学习的归纳偏置
断言假设的形式为属性的合取是一种归纳偏置 
另外也有一个假设： 
1. 是目标概念C包含在给定的假设空间H中 
2. 一般的假设比特殊假设包含的正例数更多
# 8.Find-S算法
Find-S算法找的是最大特殊假设，算法步骤如下： 
1. 初始化H为<∅,∅,∅,∅,…> 
2. 对每一个正例，使用它去修改H与其不一致的值 
    1） 如果为∅，则修改为具体值 
    2）如果为具体值又不相同，则改为? 
3. 输出最终的H
# 9.候选消除法
候选消除法比Find-S算法更加高级，因为它同时维护一个最特殊假设，也维护一个最一般假设。 
$S_0$表示的是最特殊的假设，$G_0$表示的是最一般的假设。 
对于一个正例d来讲，要使S进行一般化，也就是要让S满足d，这样S就不断向?靠拢。 
对于一个负例d来讲，要使G进行特殊化，把？变成d的反例，而且每次只需修改一个属性值即可。也就是说，如果出现了一个负例的多个属性值不同，那么每一个都只需要修改一个属性值，这样就形成了一个层次的多个不同的G。 
但是，这里要遵循一个原则：S要比G特殊，也就是说
|∅|sunny|?|
|----|----|----|
|1|2|3|
如果S到了i的位置(1≤i≤3)，则G必须是(i≤j≤3) 
否则就删掉或者不用写出来。
最终，全部的变形空间，每一次只能特殊化一个参数。
# 10. 变形空间的定义与表示
变形空间就是与训练样例D一致的所有假设$V_{S_{H·D}}$
可使用最特殊边界与最一般边界所夹的空间表示。
# 11.小结
我们这次的前十个问题，主要是对于第一章、第二章的内容进行了相关的讲解。这些都是平时学习中的一些问题和解答。
