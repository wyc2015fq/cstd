# 机器学习之近邻算法（1） - 刘炫320的博客 - CSDN博客
2016年05月22日 16:26:13[刘炫320](https://me.csdn.net/qq_35082030)阅读数：434
今天，先介绍一个关于机器学习的简单的内容。之所以简单，是因为这种算法是一种懒惰学习算法，他没有显式的训练方法，它在训练时只需要把样本保存起来即可，等到测试时再拿出来进行处理。相对应的，如果在训练阶段就对样本进行学习就称之为急切学习。
近邻算法通常称之为K近邻算法。K是一个参数，决定了你判断的深度。这里先介绍一下它在我们日常生活中的应用形式，以便于大家的理解。稍后，我们再介绍具体的算法过程。
夏天到了，我们就要买水果，或者我们需要判断一个女生是否漂亮。那么，我们如何进行判断呢？
拿西瓜来说（如果拿女生来说，我可能要遭受攻击啦！！！），我们可能要看他的颜色，看他的大小，看他的形状，听他的回声等等，这些就是所谓的特征。那么这些特征如何进行使用，只需要把这个西瓜的所有特征摆成一列。我们在日常中，只会对特征进行定性判断，比如颜色是绿色的，还是黄色的，比如大小，是大还是小，或者圆不圆，这都是二元分类。但是在K近邻算法里，这些都不重要，你是二元分类的话，只需定义成0和1，但是通常采用量化标准，比如大小用半径表示，是20厘米的，还是15厘米的等等。一旦所有的特征都进行量化后，也许就可能出现这样的情况：
|名称|颜色|大小|形状|回声|
|----|----|----|----|----|
|待吃西瓜|1.5|20|1.03|1|
那么这样的话，对于学过线性代数的人，就一定很熟悉，这就是一个1*4的向量[1.5,20,1.03,1],可能对于不熟悉线性代数的同学来讲，这样就比较抽象，但是如果让你在平面上画一个点，那么你肯定知道（X,Y）坐标，我们这里是4个特征，也就是要4个维度，这样，就是一个4维空间了。怎么样，高维度的空间离你其实并不遥远。
好了，我们扯远了。那么根据这几个特征，我们如何进行判断这个西瓜的好坏呢，当然要经验。那么这个经验在哪里？就是我们曾经吃过的西瓜，那么，就有可能有以下这样的数据：
|名称|颜色|大小|形状|回声|品质|
|----|----|----|----|----|----|
|已吃西瓜1|1.2|22|1.02|1|1|
|已吃西瓜2|0.7|13|1.3|0|0|
其中我们把标准绿色称为1，大小则是半径，形状则是根据圆率，回声简化一下，根据有无回声确定1还是0，那么品质就是好吃和不好吃了，我们定义成1和0，当然也可以定义成很多种类，很好吃，还行，一般，凑合，不好吃，不能吃等等，但是我们简化它。
那么现在有“经验”了，那么如何判断现在待吃的西瓜是不是好吃呢。我们平时总是听到人们这样说：“嗯，你摸摸它有没有回声，有回声的好吃。”、“你看它歪不歪，歪的甜”等等，这些也就是我们的规则了。那么既然是机器学习，就不可能这样草率的判断了，那么就需要综合判断这些所有的特征。也就是说，要判断所有特征之间的差异才行，比如，颜色上有多大差异，大小上有多少差异，形状上有多大差异等等，加在一起才行。那么这个行为在数学上怎么表示呢。
既然都把一个西瓜看成是一个四维空间里的点了，那么两个西瓜之间的差异如何表示呢？对了，就是两个点之间的距离，距离不用我说，应该知道怎么算吧？什么？高维度的不会？两维的是坐标差之和开根号，四维的也是坐标差之和开根号，对，就是这样简单。那么距离越远的，差异越大，反之越小。这样不就能知道它和哪个西瓜相似，那就和那个西瓜的品质上应该相同的可能性大一些。
有些人会说了，你看这明显大小和回声这种不是一个等级的啊，大小都是几十的，回声只有1和0，这不公平。没错，如果所有的特征的比重都相同的话，还需要均一化，也就是要把所有特征的范围都变成同一个维度，通常取【0，1】这个区间。如果比重不同的话，再根据比重进行加权即可。
嗯，做的差不多了吧？只需要选离他最近的那个西瓜，应该就能判断出这个西瓜的品质了吧。那K呢？你的K还没用到呢，为了进一步提高准确率，计算机会选取离他最近的一部分西瓜作为对比，通常K取值小于20，这是为了效率，也可以随意。那么只需要看离他最近的西瓜里那种类别的西瓜最多，就判断它是哪种类别的了。
至此，我们的西瓜判断已经完成了，可以开吃了！
（有这时间，我西瓜早吃完了！！！）
下面，我们进行计算机的专业描述：
1、先对数据集各维度特征均一化
2、  计算已知类别数据集中的点与当前点之间的距离：
3、  按照举例递增次序排序；
4、  选取与当前点距离最小的k个点；
5、  确定前k个点所在类别的出现频率；
6、返回前k个点出现频率最高的类别作为当前点的预测分类。
怎么样，是不是很简单？虽然近邻算法如此简单，但是泛化错误率不会超过贝叶斯最有分类器的错误率的2倍。如果要求不是很严格的话，比如我们这次的吃西瓜，是完全可以胜任的！
这样的话，我们的K近邻算法就已经介绍完毕了，这只是个入门介绍，供大家参考。
            
