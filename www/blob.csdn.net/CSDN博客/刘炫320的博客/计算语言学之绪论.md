# 计算语言学之绪论 - 刘炫320的博客 - CSDN博客
2017年03月03日 22:35:53[刘炫320](https://me.csdn.net/qq_35082030)阅读数：605
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 0. 写在前面
从这一讲开始，我们开始进行计算语言学的正式讲解。
# 1. 自然语言处理？计算语言学？
关于自然语言处理，有多种说法，可以说**自然语言生成**，**自然语言理解**，**自然语言处理**，**中文信息处理**以及**计算语言学**。但这些说法都是有区别的，例如，**自然语言理解**是偏向于人工智能方向说的，但是其实自然语言理解是自然语言处理的一部分，自然语言要处理，首先要能够理解，其次才是生成，这是一个输入，一个输出的完整过程。因此我个人认为，**自然语言处理**包含了**自然语言理解**与**自然语言生成**。**自然语言处理**则是偏向应用方面的说法。**中文信息处理**则是特指中文的语言处理，而**计算语言学**则是从语言学的角度来说的。
# 2. 自然语言处理的历史
## 1. 创世纪
既然搞清楚了我们在说什么东西后，我们就来了解一下自然语言处理的历史。自然语言处理的发展和人工智能的发展密切相关，自然语言处理的起家之作，就是尝试进行机器翻译。早在二战时期，机器翻译的前身，谍战中的密码加密解密就已经广泛流行在各国的暗战之中。大名鼎鼎的，图灵之前在英国就是创造了这么一台广义上的计算机，用于破解德军的密码，从而大大减少了英军的损失。
## 2. 第一个高潮
后来二战结束后，1946年的2月14日（没错，就是情人节），世界上第一台计算机ENIAC诞生，标志着机器翻译正式与计算机接轨。1954年美国IBM公司和乔治敦大学合作开发的机器翻译原型系统第一次向人们展示了一个现实的机器翻译系统，这是自然语言处理的第一场胜利。在1956年，达茅斯会议提出了“人工智能”，这是人工智能的第一个高潮，也是自然语言处理的上升期。
## 3. 第一个低谷
1964年，美国科学院成立语言自动处理咨询委员会（ALPAC）对机器翻译等一系列自然语言处理相关项目进行了一次评估，并与1966年11月给出了评估结果，其评估结果十分悲观，认为这项技术没有发展前景，这让很多业界人士非常失望。但真正让人工智能跌入低谷的，不仅仅是这次评估报告，而是确实在实践中，由于物理硬件和指导思想上的偏差，没有任何实质性的进步，从此，自然语言处理伴随着人工智能一起跌入了冰封期。
## 4. 第二个高潮
自然语言处理的高潮是随着人工智能的高潮一并出现的。在1970年以后，人工智能领域，机器学习开始逐渐展现出应有的实力，大多数人开始从规则转向统计，这使得人们在人工智能领域看到了一丝曙光。而自然语言处理不仅由于这次人工智能领域上的进步，自身在计算语言学理论方面也有了一定的进步，因此，各种我们现在常用的理论基础（宏观篇章理论、浅层语义分析等），甚至是机器翻译（1972年，美国科学家Winograd提出了SCHRDLU模型），都有了实质性的发展。
## 5.第一个平稳发展期
自从基于统计的机器学习方法占据主流以后，一直到2000年前后，人工智能的发展都处于一种平稳发展阶段，虽然算不上冰封期，但是绝对已经没有了七八十年代那种井喷式的研究成果出现。在九十年代到新世纪初，互联网才是IT界的主流，如今的巨无霸们（1998年的谷歌，1999年的百度，1998年的腾讯），以及现在不属于超一流企业的老牌三剑客（1998新浪，1997年网易，1998年搜狐），都是乘着互联网的发展起家的。在这阶段，更多的是小进步，直到深度学习的产生，才有了质的飞越。
## 6.第三个高潮期
深度学习其原理也不是新东西，但是在2006年，Geoffrey Hinton先分层进行预训练，再把预训练的结果当成模型参数的初始值，再从头进行正常的训练过程的方法，让深层神经网络的实现成为了可能。从此，深度学习开始崭露头角。2009年，在语音识别上，使用深层神经网络的语音识别模型一下子使得错误率降低了20%多，是过去很多年的总和。2012年，在图像识别上，深层神经网络第一次参加比赛，就把识别记录提升了10%，为84.7%，目前已经达到了95%以上的准确度。
当然，我们这篇文章的主题自然语言处理上面，也不会丢下这种利器，最近最出名的莫过于谷歌的Word2vec工具，这是谷歌2013年开源的获取词向量的工具，在此基础上进行的研究更是数不胜数，也都取得了不错的效果。
## 7. 未来
未来的方向又在哪里呢？我不知道。也许就会出现革命性的事件，也许，还会进入平稳发展阶段。
# 3. 自然语言处理的应用
自然语言处理方向很多，常见的有机器翻译、信息抽取、问答系统、自动校对、自动文摘、文档分类、分析检索、舆情分析、信息过滤、文字识别、语音识别等。
但是实际上，自然语言处理还是存在一些发展问题，这里包括但不仅限于形态学、语法学、语义学、语用学方面的问题。尤其是针对于中文的多异性，目前消歧也是一个十分热门的研究方向。
那么就具体工程而言，基于理性主义的实现方法简单、快捷，可以很迅速的搭建起一个满意度为百分之七八十的系统，而基于经验主义的实现方法就比较复杂和缓慢，但其满意度可以无限接近于100%，因此适当的结合两套体系，是让工程既好又快的搭建出的关键。
# 4. 补充
关于中文难以处理的问题，这并不是使用的技术不好或者理论不足，这时源于中文的语言体系。常见的语言体系里，都把语言分为三种，它们分别是：孤立语、屈折语、黏着语（也有分为四种的，其中把复综语看做是特殊的黏着语）。
孤立语的代表就是汉语，它的主要特点就是，缺乏词形变化，词序严格，虚词十分重要，复合词多，派生词少。
屈折语的代表则是英语，他的特点就是有比较丰富的词形变化，通过词形变化来表示词与词之前的关系，一种词性变化的语素可以表示几种不同的语法意义，词尾和词干或词根结合十分紧密，脱离开词尾，句子中词根不能独立存在。
而黏着语的代表则是日语，它的特点则是词的前面和中间不发生变化，只是词的尾部发生变化来表示语法意义、变词语素的每一种变化只表示一种语法意义，多种语法意义需要用多个变词语素表示、词根与变词语素不紧密，两者有很大独立性。
第四种复综语，则是词和句子分辨不出，其代表语言是美洲印第安语。 
事实上，网上说汉语是最难学的语言，与汉语是世界上最难学的文字，还是稍有点不同的。论文字的难易程度上，很多榜单给出的中文（Mandarin）并不是最难的，因为中文的变化确实比较少，从发音上讲处于中等难度，从语法上讲也是中等难度，中文客观上难主要是因为他是孤立语系，复合词比较多，而且很容易生成一个新的词。词与词之间比较孤立，并没有太大的联系。也就是说没有太多的固定搭配，只是语序比较重要。
中文主要是由于其二维认知性，它同英语这种的差异就是它的字是由一个二维结构构成，而英语则是一个一维的拼写。这才是国外为什么觉得中国汉字难学的原因，因为很难认。
但是以上这些难点，对于计算机处理来讲，恰恰都不是难点，无论你语法再多，计算机都可以一遍记住，无论你字形多难，有多少词汇量，对于计算机来讲，都是瞬间记忆，永久不忘。中文难处理，很大一部分是在语义学和语用学上面，也就是具有很多没有任何标志的歧义。甚至连上下文都没给出，例如，他的意思。这句话有可能是说他这个字的意思，有可能说是第三人他的想法。这就算再给你一个短期的上下文，“我是说他的意思是”，仍然你看不出来，但是较长距离的相关性，在自然语言处理上都是想避免的。但不可否认的是，中文的词与词的相关性的长度要远远大于英语。
# 5. 总结
通过以上的介绍，我们对于自然语言处理有了初步的了解，但这仅限于背景知识。不过俗话说，工欲善其事必先利其器，先打好基础，再往下面学真正的学术知识的时候，才不会丈二和尚摸不着头脑。好了，这期我们就到这了，下期见。
