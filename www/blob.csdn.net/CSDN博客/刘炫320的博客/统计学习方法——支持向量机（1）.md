# 统计学习方法——支持向量机（1） - 刘炫320的博客 - CSDN博客
2018年04月08日 21:10:57[刘炫320](https://me.csdn.net/qq_35082030)阅读数：240
所属专栏：[统计学习方法笔记](https://blog.csdn.net/column/details/16460.html)
# 0.写在前面
支持向量机是如此的大名鼎鼎，以至于我迟迟不敢动手，今天终于要解开它的神秘面纱了，而它真的是体量太大，以至于我得分3个章节来讲。今天我们讲解最简单的第一部分，线性可分的支持向量机，即保证所有样本都可以被划分正确。
# 1.支持向量机基础
## 1.1实际意义
支持向量机的实际意义基本上和感知机一样，使用一个超平面来区分正负样例。因此它最简单的形式是只能进行二分类的。但是它的特殊之处是，它既不是找到一个超平面，使得误分类点样本到超平面的总距离最小（感知机），也不是找到一个超平面使得类别间的距离最大，类别内的距离最小（LDA），更不是找到一个超平面使得误分类点样本数目最少（离散的，不可导）。它是找到距离超平面距离最近的两个划分正确点距离超平面距离最远，也就是说，它并不是尽可能减少错分的，而是尽可能保证分对的一定对。有时候我们考虑问题也应该如此，把自己能做好的做到最好，然后再尽可能的做对那些我们不确定的事情。
## 1.2目标函数
如上所说，支持向量机的目标是找到一个划分的超平面$\boldsymbol{w}\times \boldsymbol{x}+b=0$。那么任一点$x_i$到该平面的距离为$r=\frac{|w\times x_i+b|}{||w||}$。
另外规定如下： 
$\begin{cases} w\times x_i+b> 0,  & y_i=1 \\w\times x_i+b< 0,  & y_i=-1 \\\end{cases}$
为了保证尺度统一也为了方便计算，对上式进行一个放缩： 
$\begin{cases} w\times x_i+b\ge 1,  & y_i=1 \\w\times x_i+b\le -1,  & y_i=-1 \\\end{cases}$
这样一方面可以保证$y_i(w\times x_i+b)\ge1$
另一方面可以使得最近的两个异类（正样例、负样例）到达超平面的距离之和为$r=\frac{2}{||w||}$,这里隐含$w\times x_i+b=1$。 
这样就能顺理成章的得到支持向量机最原始的目标函数： 
$\begin{cases}\max_{w,b}\frac{2}{||w||} \\S.t. &y_i(w\times x_i+b)\ge1\end{cases}$
这个公式是支持向量机最原始的目标函数，下面请看变戏法。
## 1.3目标函数变换
### 1.3.1第一次变换形态——正负号变换
原始目标函数等价于： 
$\begin{cases}\min_{w,b}\frac{1}{2}||w||^2 \\S.t. &y_i(w\times x_i+b)\ge1\end{cases}$
这一波操作都能看得懂，对吧。接下来就要用到拉格朗日乘子法了。
### 1.3.2第二次变换形态——拉格朗日乘子法变换
拉格朗日乘子法是解决约束问题最值的，在数学建模中比较常见的。但是那时候看拉格朗日乘子法都比较简单，因为通常都是只有那么1，2，3个约束条件，然后只需要多加几个可见的拉格朗日乘子即可。如果不记得拉格朗日乘子法了，请参见《[拉格朗日乘子法](https://baike.baidu.com/item/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/1946079?fr=aladdin)》。 
下面进行拉格朗日乘子法变换： 
$\max_{\alpha}L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^m\alpha_i(1-y_i(wx_i+b))$
这才是真正的拉格朗日乘子法变换，也是为对偶算法进行铺垫。那么问题就变成了 
$\min_{w,b}\max_{\alpha}L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^m\alpha_i(1-y_i(wx_i+b))$
### 1.3.3第三次变换形态——对偶变换
接下来进行对偶变换： 
$\max_{\alpha}\min_{w,b}L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^m\alpha_i(1-y_i(wx_i+b))$
没错这样的话，就是第二形态，这是对偶后的形态，此时只需要对w,b进行求偏导即可获得第一层最值。 
$w=\sum_{i=1}^m\alpha_iy_ix_i$
$0=\sum_{i=1}^m\alpha_iy_i$
带入原公式即可获得对偶形态的最终形式： 
$\max_\alpha-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j(x_ix_j)+\sum_{i=1}^N\alpha_i$
$S.t. \alpha_iy_i=0,\alpha_i\ge0$
但是不是什么时候都能够让你对偶的，需要满足[KTT条件](https://blog.csdn.net/johnnyconstantine/article/details/46335763)，比如上公式满足的条件为： 
$\begin{cases}\alpha_i\ge0\\1-y_i(wx_i+b)\le0 \\\alpha_i(1-y_i(wx_i+b))=0\end{cases}$
## 1.4线性可分支持向量机解法
好了，这样的话，我们就把它最终的形式确定下来了。这样求解线性可分的支持向量机的算法就可以表示如下： 
1. 构造并求解约束最优化问题 
$\min_\alpha\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j(x_ix_j)-\sum_{i=1}^N\alpha_i$
$S.t.  \alpha_iy_i=0,\alpha_i\ge0$
2. 计算
$w=\sum_{i=1}^N\alpha_iy_ix_i$,并选择$\alpha 的一个正分量\alpha_j>0（\alpha 的实际意义在于每个样本点是否是支持向量）$，计算
$b=y_j-\sum_{i=1}^N\alpha_iy_i(x_ix_j)$
3. 求得分离超平面 
$wx+b=0$
4. 确定分类决策函数 
$f(x)=sign(wx+b)$
具体的一个例子，可以参见《统计学习方法》P107页。
## 1.5SMO优化算法
SMO优化算法目的是为了加速凸优化问题的求解过程。它采用的是启发式方法，固定其他参数，只调整两个变量，然后不断的迭代，直到最终收敛为止。它的b使用的是支持向量的平均值，即： 
$b=\frac{1}{|S|}\sum_{s\in S}(y_s-\sum_{i\in S}\alpha_iy_ix_ix_s)$
这里我们只是略微提及一下这个算法，后面还会经常碰到。
# 2.小结
在本章中，我们主要讲解了支持向量机的基础部分，尤其是对于其最简单的形式线性可分的支持向量机进行相关的讲解。特别是对于其目标函数的来源以及目标函数的变换做了细致的讲解。另外稍微提及了一下SMO算法。在接下来的过程中，我们将会讲解线性不可分的支持向量机与核函数等。
