# 统计学习方法——决策树 - 刘炫320的博客 - CSDN博客
2017年04月16日 16:54:58[刘炫320](https://me.csdn.net/qq_35082030)阅读数：780标签：[id3决策树																[c4-5算法																[剪枝算法																[cart算法																[C5-0算法](https://so.csdn.net/so/search/s.do?q=C5-0算法&t=blog)](https://so.csdn.net/so/search/s.do?q=cart算法&t=blog)](https://so.csdn.net/so/search/s.do?q=剪枝算法&t=blog)](https://so.csdn.net/so/search/s.do?q=c4-5算法&t=blog)](https://so.csdn.net/so/search/s.do?q=id3决策树&t=blog)
个人分类：[统计学习方法](https://blog.csdn.net/qq_35082030/article/category/6775564)
所属专栏：[统计学习方法笔记](https://blog.csdn.net/column/details/16460.html)
# 0. 写在前面
决策树模型我们之前已经简要介绍过了，[简要介绍决策树](http://blog.csdn.net/qq_35082030/article/details/51488003)以及[ID3决策树若干问题解答](http://blog.csdn.net/qq_35082030/article/details/69803278)。这次我们将会详细的介绍决策树的整个知识内容，其中包括决策树的3个学习过程：特征选择、决策树的生成和决策树的修剪。以及常见的集中决策树模型。
# 1. ID3与C4.5决策树
决策树作为一种基本的分类与回归方法，经常被我们所用。尤其是因为它具有高可读性、分类速度快等优点，深受一些人的喜爱。其实它也是属于符号学派的一种。多半应用于分类问题中。
## 1.1 决策树的分类
往大了说，决策树分类是分类与回归2种，但是我们 通常考虑的是分类问题。其实是因为Quinlan的1986年的ID3算法、1993年的C4.5算法以及目前商用的5.0算法是一类，他们都是用于分类问题。而CART算法，则被用来回归。
## 1.2 决策树的学习
决策树是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。
用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，讲实例分配到其子结点，这时，每个子节点对应着该特征的一个取值。如此递归的对实例进行测试并分配，直至到达叶结点。最后将实例分到叶结点的类中。
一般决策树如图所示，一般由if-else规则组成： 
![决策树图](http://leijun00.github.io/images/datamining/decision-tree/decision-tree-example.gif)
## 1.3 优化函数和损失函数
决策树的生成过程，就是一个建树的过程，那么如果没有任何优化措施，那么整个搜索空间将会是星辰大海般的可能，因为从所有可能的决策树中选取最优的决策树是NP完全问题，因此决策树算法通常是采用启发式算法，能找到一个次最优的树就行。
那么一般都是有一个优化函数，而这个优化函数是用来指导每一个内部结点的取值的。另外有一个损失函数，则是用来评估整棵树构建的质量的。接下来的ID3和C4.5的介绍中，都是对这两个函数来进行介绍。
## 1.4 特征选择
正如上面所说，一个优化函数是用来进行特征选择的，那么在若干的候选的特征中，该如何选择哪一个特征作为当前节点的属性呢？
### 1.4.1 ID3算法
ID3算法是使用的信息增益来进行特征选择的。所谓的信息增益，其实就是为了使得每次分类后，所有样本的不确定性进一步下降，因为到了最后对于每一个样本来说，应当有确定的标签才对。
这也就是说，每次选择那个分类能力最好的特征，使得分类后比分类前，所有的样本更加确定。而这个确定性，是使用的熵来决定的。
学术点讲，特征A对训练数据集D的信息增益g(D,A)，就是集合D的经验熵H（D）与特征A给定条件下D的经验条件熵H（D|A）之差，即： 
$g(D,A)=H(D)-H(D|A)$
这信息增益也等价于训练数据集中类与特征的互信息。
说到这，还是不能理解怎么算的，其实我们在[概率论与信息论基础知识](http://blog.csdn.net/qq_35082030/article/details/60882746)这里已经讲过了关于熵的计算，我们这里只讲述其思想：
- 计算数据集D的经验熵H(D): 
$H(D)=-\displaystyle \sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2 \frac{C_k}{D}$
用人话来说，就是根据所分类C别在当前集合D内计算熵。
- 计算特征A对数据集D的经验条件熵H（D|A）： 
$H(D|A)=\displaystyle \sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i) =-\displaystyle \sum_{i=1}^{n}\frac{|D_i|}{|D|}\displaystyle \sum_{n=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2 \frac{D_{ik}}{D}$
用人话来说就是在分类后的每个小类别$D_i$中再根据C来计算一下熵，然后把所有小类的熵根据比重都加起来。
- 计算信息增益： 
$g(D,A)=H(D)-H(D|A)$
这个就不用说了吧。
这就是ID3算法的核心思想。
### 1.4.2 C4.5算法
C4.5和ID3没有什么本质的不同，唯一的不同之处，就是在于特征选择上，它不再用信息增益来算了，而是使用信息增益比来算。信息增益会更加偏爱与某一特征有多个属性值的特征。
特征A对训练集D的信息增益比$g_R(D|A)$定义为其信息增益$g(D，A)$与训练数据及D关于特征A的值的熵$H_A(D)$之比： 
$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$
其中: 
$H_A(D)=-\displaystyle \sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2 \frac{D_i}{D}$
n是特征A取值的个数。
## 1.5 决策树的生成
决策树的生成在[ID决策树的生成](http://blog.csdn.net/qq_35082030/article/details/69803278)中已经讲解了，我们在这就不再赘述了，对于C4.5来讲，与ID3的唯一区别就是选择特征时，不使用信息增益而使用信息增益比来选择特征。
## 1.6 决策树的剪枝
正像我们刚才提到的那样，我们解决完了优化函数，现在需要考虑损失函数了。损失函数是用来评估一个模型的好坏的，而对应的操作则是剪枝。这个在[ID3防止过拟合的方法](http://blog.csdn.net/qq_35082030/article/details/69803278)中已经讲过了，这里我们详细讲一下。这次，我们使用的是结构损失函数，来进行评估： 
$C_\alpha(T)=\displaystyle \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$
其中经验熵$H_t(T)=-\displaystyle \sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$
这里，|T|为树T的叶结点个数，t是树T的叶结点，改叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$\alpha$是参数，参数越大，选择的树越简单，参数越小，选择的树也就比较复杂。
具体的算法如下：
输入：生成算法产生的整个树T，参数$\alpha$
输出：修剪后的子树$T_\alpha$
（1）计算每个结点的经验熵 
（2）递归的从树的叶结点向上回缩。 
设回缩前后的整体树为$T_B$与$T_A$，如果 
$C_\alpha(T_A)  \leq C_\alpha(T_B) $
则进行剪枝，父节点变为新的叶结点。 
（3）返回（2），直至不能继续为止，得到损失函数最小的子树。
这里我们使用的是结构损失函数，在修剪时，我们之前也说过可以使用错误率降低后修剪等其他损失函数。
# 2. CART算法
CART全名是分类与回归树，它可以用于回归是比ID3系列的优势所在，但是它其实是一个二叉树，内部的结点只有是或者否两种选择。
而CART算法同上面的树一样由两步组成： 
（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 
（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树。
这两步骤的核心就在于设置优化函数和损失函数。
## 2.1 CART生成
在这一步，我们需要考虑的就是特征的选取，也就是优化函数的选取。 
它有两种模式，既是回归树也是分类树。
### 2.1.1回归树生成
回归树生成也好，分类树生成也好，都是要对特征进行选取，那么一个特征有N个属性，而CART树又是一个二叉树，那么该从哪里进行切分是一个问题，因此这时候，一种优化函数使用的是平方差最小准则。
一个回归树对应着输入空间的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元$R_1,R_2,...,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可以表示为： 
$f(x)=\displaystyle \sum_{m=1}^M\hat{c}_mI(x \in R_m)$
这其实就是意思是如果x在某一个划分$R_m$内，则其输出值为$c_m$。 
其中$c_m$就是在$R_m$上所有实例$x_i$对应的输出$y_i$的均值。 
这也是最小二乘回归树生成算法：
输入：训练数据集D 
输出：回归树f(x)
在训练数据集所在的输入空间中，递归的将每个区域划分为2个子区域并决定每个子区域上的输出值，构建二叉决策树：
- 选择最优切分变量j与切分点s，求解： 
$\min_{j,s}[\min_{c1}\displaystyle \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2+\displaystyle \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2]$
遍历变量j，对固定的切分变量j扫描切分点s,通过上式达到最小值对(j,s)
- 用选定的对（j,s）划分区域并决定相应的输出值： 
$R_1=\{x|x \leq s\},R_2=\{x|x > s\}$
$\hat{c}_m=ave(y_i|x_i \in R_m),m=1,2$
- 继续对两个子区域调用步骤（1），（2），直至满足停止条件。
- 将输入空间划分为M个区域$R_1,R_2,...,R_M$,生成决策树： 
$f(x)=\displaystyle \sum_{m=1}^M\hat{c}_mI(x \in R_m)$
### 2.1.2 分类树生成
而对于分类树来讲，则有很多评判标注，我们这里只使用基尼指数来进行评判，具体的，我们只描述基尼指数，实际上基尼指数和熵的含义是差不多的，表现能力上也与熵类似。
分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$,则概率分布的基尼指数定义为： 
$Gini(p)=\displaystyle \sum_{k=1}^Kp_k(1-p_k)=1-\displaystyle \sum_{k=1}^Kp_k^2$
对于给定集合D，其基尼指数为：
$Gini(D)=1-\displaystyle \sum_{k=1}^K（\frac{|C_k|}{|D|}）^2$
其中$C_k$表示D中属于k类的样本子集，K是类的个数。
如果是在特征A的条件下，集合D的基尼指数定义为： 
$Gini(D,A)=\displaystyle \sum_{i=1}^M\frac{|D_i|}{|D|}Gini(D_i)$
一般的我们都选择基尼最小的那个分类点作为内部结点。回归树的算法就不在赘述了。
## 2.2 CART剪枝
CART剪枝算法也是比较复杂的，不过真正的思想和前面的ID3系列算法没有本质区别，核心都在于结构损失函数： 
$C_\alpha(T)=C(T)+\alpha|T|$
其中C（T）可以用很多度量方法，例如熵、基尼指数、误差平方和等。 
下面给出CART剪枝算法：
输入：CART算法生成的决策树$T_0$; 
输出：最优决策树$T_\alpha$
(1) 设k=0,$T=T_0$
(2)设$\alpha=+\infty$
(3)自下而上地对个内部结点t计算$C(T_t),|T_t|$以及 
$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$
$\alpha=min(\alpha,g(t))$
这里$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。 
(4) 对g(t)=$\alpha$的内部结点t进行剪枝，并对叶结点t以多数表决发决定其类，得到树T. 
(5)设$k=k+1,\alpha_k=\alpha,T_k=T$
(6)如果$T_k$不是由根节点及两个叶结点构成的树，则回到步骤（3），否则令$T_k=T_n$。 
(7)采用交叉验证法在子树序列$T_0,T_1,T_2,...T_n$中选取最优子树$T_\alpha$。
# 3. C5.0决策树
C5.0决策树比C4.5有诸多改进，但是它是商业版本，因此没有公开具体源码，不过还是可以知道C4.5与C5.0之间的差异： 
C5.0和C4.5算法的对比：
- C5.0算法通过构造多个C4.5算法，是一种boosting算法。准确率更高
- C5.0算法运行速度快，可以出来例如,C4.5需要9个小时找到森林的规则集,但C5.0在73秒完成了任务。
- C5.0运行内存小。C4.5需要超过3 GB.(工作不会对早些时候完成32位系统),但C5.0需要少于200 mb。
- C5.0算法，可以人为的加入客观规则
- C5.0可以处理较大的数据集，特征可以是：数字，时间，日期，名义字段
- C5.0可以加入惩罚项，（也就是第1条中boosting过程）
具体的还可以参考一下：[C5.0PPT](https://wenku.baidu.com/view/8dbc492e647d27284b735138.html)或者[各种决策树对比](http://www.cnblogs.com/tippoint/archive/2012/03/02/2376671.html)。
# 4. 小结
本次讨论班上，我们主要讨论了ID3、C4.5和CART等常见决策树的生成与剪枝的方法。另外也对C5.0做了一点补充介绍。其实决策森林也应该算在决策树内，这里没有提及，在相应的章节中，我们会对此做出介绍。
