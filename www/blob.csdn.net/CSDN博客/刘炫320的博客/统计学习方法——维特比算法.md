# 统计学习方法——维特比算法 - 刘炫320的博客 - CSDN博客
2017年12月15日 20:11:46[刘炫320](https://me.csdn.net/qq_35082030)阅读数：368
所属专栏：[统计学习方法笔记](https://blog.csdn.net/column/details/16460.html)
# 0.写在前面
本来这章是要讲解条件随机场的，后来发现自己对于条件随机场中的维特比算法了解就不够，因此我们把条件随机场拆开来看。这里先看维特比算法。
维特比算法主要是求解一个最优序列的算法。但是它也是有条件的，那就是要知道所有应该知道的参数。
维特比算法实际是用动态规划解隐马尔科夫模型预测问题。其实就是求概率最大路径，这是一条路径对应着一个状态序列。
# 1.前向算法
但是维特比算法直接讲是比较复杂的，我们可以以“前向算法”作为开胃菜。前向算法和维特比算法不同，但是又有联系，准确来说：
前向算法是求某一观察序列现象的概率大小。
维特比算法则是给出某一观察序列，并求发生这种观察序列的最可能状态并且回溯出这一序列路径发生的过程。
我们假设已经对于隐马尔科夫模型有一定的了解了，这里给出前向概率的定义：
> 
给定隐马尔科夫模型$\lambda$，定义到时刻t部分观测序列为$o_1,o_2,...o_t$且状态为$q_i$的概率为前向概率，记作 
$\alpha_t(i)=P(o_1,o_2,...o_t,i_t=q_i|\lambda)$
**观测序列概率的前向算法**
输入：隐马尔科夫模型$\lambda$，及观测序列概率$P（O|\lambda）$
输出：观测序列概率$P（O|\lambda）$
（1）初值 
$\alpha_1(i)=\pi_ib_i(o_i),i=1,2,...,N$
*注释：$\alpha_1(i)$就是第1层第i种情况的概率值，$\pi$是第0层挑选第i种情况的概率值，$b_1(o_i)$则是在第1层是出现第i种观测情况的概率值。这里虽然不能够解释清楚，但是在接下来的例题中我们就能够很直观的了解到它是什么含义了。*
（2）递推 对t=1,2,…,T-1 
$\alpha_{i+1}(i)=[\sum_{j=1}^{N}\alpha_t(j)a_{ji}]b_i(o_{t+1}) ,i=1,2,...,N$
*对于下一层的情况就是把上一层所有可能会出现目标情况全部加起来后再乘以发生目标观测状态的概率*。
（3）终止 
$P（O|\alpha）=\sum_{i=1}^{N}\alpha_T(i)$
*注释：这里就是把所有出现这种可能的情况都加起来*
这里有很多注释看的似懂非懂，不过不用担心，我们会在例题中详细讲解。总而言之，就是说对所有出现目标观测值的可能情况都计算一遍即可。
# 2.前向算法示例
以李航的统计学习方法177页的例题为例，我们讲解一下前向算法。 
题目略，但是需要对题目进行一些解读：
首先矩阵A是状态转移矩阵，他为什么是3*3的矩阵呢，那是由于状态集合Q为3个元素，也就是说有3种状态，对应盒子和球的模型就是有3个盒子。那么这个A矩阵该怎么解读呢？以第一行为例，则是表示第一个盒子从t时刻到t-1时刻转移到第一个盒子、第二个盒子、第三个盒子的概率是0.5,0.2,0.3，其他的类推即可。
其次矩阵B是发射矩阵，或者是混淆矩阵。它为什么是3*2的矩阵呢，是因为它是3个状态，2个观察结果（红，白）的展示。也就是说，第一行表示的是第一个盒子里摸出红色球的概率是0.5，白色球的概率是0.5。
剩下的就是$\pi$了，它表示的是初始选择概率分布，比如，刚开始的时候，选第一个盒子，第二个盒子、第三个盒子的概率为0.2,0.4,0.4。
那么对于最终的问题就是，在这种状态转移和混淆矩阵的情况下，出现观察序列{红，白，红}的概率是多少。
第一步，计算初值（只列出1个来解释）： 
$\alpha_1(1)=\pi_1b_i(o_1)=0.2\times0.5=0.1$
这个式子的解释是，如果选择的是第一个盒子，而且从第一个盒子中拿到红球的概率为0.1。
第二步，递推计算。 
$\alpha_{2}(1)=[\sum_{i=1}^{3}\alpha_1(i)a_{i1}]b_1(o_{2})=0.154\times0.5=0.077$
这个式子的解释是，中括号里表示在第一次抓到各个盒子出现红球的概率转移到第二次选到第一个盒子的概率和。而$b_1(o_{2})$则表示的是第二个盒子抓到白球($o_2$)的概率。
第三步，终止。 
把最后一步所有情况求和后，就是出现目标观察序列的概率值了。
这样一个过程下来，其结果的意义应该很清楚，那就是把所有出现这种观察序列的各种情况的概率求和。
# 3.维特比算法
那么维特比算法是什么呢？它是在给出观察序列情况下，求出概率最大的状态序列。也就是说它和前向算法还不一样，但是思想上是大致相同的，只不过它要的是整个过程中的一个可能路径，而并非最终的结果。它在计算出每一时刻时的最优值时，还要记住之前的路径中出现的情况，这样才能够回溯出整个状态序列。
它需要记住2个目标值$\delta$和$\psi$,其中前者表示的最大概率，后者则是表示路径。
**维特比算法：**
输入：模型$\lambda=（A,B,\pi）$和观测$O（o_1,o_2,...,o_T）$
输出：最优路径$I^{*}=(i_1^{*},i_2^{*},...,i_T^{*})$
(1)初始化 
$\delta_1(i)=\pi_ib_i(o_1),i=1,2,...,N$
*注释：是不是似曾相识*
$\psi_1(i)=0,i=1,2,...,N$
(2)递推，对t=2,3,…,T 
$\delta_t(i)=\max_{1<=j<=N}[\delta_{t-1}(j)a_{ji} ]b_i(o_t),i=1,2,...,N$
$\psi_t(i)=arg\max_{1<=j<=N}[\delta_{t-1}(j)a_{ji}]$
(3)终止 
$P^{*}=\max_{1<=j<=N}\delta_{T}(i)$
$i^{*}_T=arg\max_{1<=j<=N}[\delta_{T}(i)]$
(4)最优路径回溯，对t=T-1,T-2,…,1 
$i^{*}_T=\psi_{t+1}(i^{*}_{t+1})$
# 4.维特比算法示例
还是以上面那个盒子和球模型，维特比算法就是要求出现这种观察球类后的盒子的顺序。 
（1）初始化如前向算法一样。
（2）t=2时，注意到： 
$\delta_2(1)=\max_{1<=j<=3}[\delta_1(j)a_{j1}]b_1(o_2)=\max_j\{0.1\times0.5,0.16\times0.3,0.28\times0.2\}\times0.5=0.028$
*注释：前向算法时，是把所有这种可能都计算出来，而维特比算法只找那个最大的可能，并且记录下来。*
$\psi_2(1)=3$
其他的以此类推即可。
（3）这里不用多说
（4）然后逆向寻找即可。
好了，到这里为止，我们就给出维特比算法的手算的方法了，至于它的代码实现，可以参考《[维特比算法之红白球示例](http://blog.csdn.net/crystal555/article/details/7537007)》。这里只解决了隐马尔科夫模型的3个基本问题之一。剩下的两个我们会单独来讲解。
