# 机器学习之朴素贝叶斯算法（1） - 刘炫320的博客 - CSDN博客
2016年05月27日 10:59:58[刘炫320](https://me.csdn.net/qq_35082030)阅读数：637
这已经是机器学习的第三种算法了。说起朴素贝叶斯，可能大家都不太清楚 是什么。但是如果学过概率论与数理统计，你可能对贝叶斯定理有所了解，但又记不清是在哪里。没错，这么重要的一个定理，在概率论与数理统计上面，只用了很小的一个篇幅来介绍它。这也不怪书，因为它的表达形式实在是太简单了。
先来看一下它的数学表达形式：
P(AB)=P(B|A)P(A)=P(A|B)P(B)
没错，这就是贝叶斯定理。其实只是个概率计算的公式而已。那么如何来应用它呢。我们一般都知道，非洲的黑种人比较多，也就是说非洲的人是黑种人的概率大一些。那么如果我们碰到的是一个黑人，那么你会猜他是哪里来的？非洲，对不对？这种逻辑就是根据贝叶斯定理而得来的。
那么朴素贝叶斯，又为什么叫做朴素呢。因为它把事物具备的特征都看成是相互独立的，比如我们还是在看人，人可能有皮肤的颜色，身高，体质还有……嘿嘿，我邪恶了。等等特征，那么这些特征是相互独立的吗？当然不是，比如黑种人平均身高没有白种人高，还有黑种人跑步能力较强等等，特征和特征之间是有关系的。但是朴素贝叶斯都把他们看作是相互独立的。
原则上讲，朴素贝叶斯具有客观上最小的误差率，因为它需要的参数最少。但是正是因为有了这个架设，才导致朴素贝叶斯的误差率不尽如人意，这就需要进一步的优化才行，这都是后话了。我们现在讲朴素贝叶斯如何运用到机器学习中。
上面那个公式我们都看过了。下面我们换种表达形式：
p(Ci|W)=P(W|Ci)P(Ci)/P(W)
这里我们把W看作是一个特征向量，包含很多特征Wi，那么Ci则表示是分类的类别。那么我们知道P（W）对于每个类别来讲都是常数，因此可以忽略不计。那么只需要计算分子上面的大小就可以比较等号左边值得大小，那么P(W|Ci)可以用P（Wi|Ci）全部相乘得到，这是归功于朴素贝叶斯把所有特征都相互独立，才会有这公式，不然的话，没办法做出这种情况的。那么P（Ci）是什么呢，也就是所有样本中这个类别的分类情况，比如样本有2类，第一类呢有6个，第二类呢有4个，那么P(C1)=0.6,P(C2)=0.4，这一个是十分好算的。那么只需要计算P（Wi|Ci)即可。只需要看这个特征在第一个种类中有多大概率即可。那么朴素贝叶斯的训练是为了什么呢，就是为了找出这个P（Wi|Ci)一旦根据训练样本集可以得出这些概率，那么当这些特征出现的时候，就可以依据这些特征的多少和概率来判断该样本的类别了。
当然训练样本越多，其特征的概率分布就越准确。这就是朴素贝叶斯的奥秘了！
