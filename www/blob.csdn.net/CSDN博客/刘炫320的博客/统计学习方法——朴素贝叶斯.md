# 统计学习方法——朴素贝叶斯 - 刘炫320的博客 - CSDN博客
2017年04月13日 20:13:02[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1303
所属专栏：[统计学习方法笔记](https://blog.csdn.net/column/details/16460.html)
# 0.写在前面
朴素贝叶斯实际上是非常简单的一种机器学习方法，我们在之前的很多地方都讲过了，所以这里我们不再阐述具体的原理，可以移步：[朴素贝叶斯](http://blog.csdn.net/qq_35082030/article/details/51513617)。 
但是，对于讨论班里，争论最多的就是课后的2个习题，因此，我们重点放在这两个习题上。他们分别是：
> 
4.1 用极大似然估计法推出朴素贝叶斯法中的概率估计公式（4.8）及公式（4.9）。 
  4.2 用贝叶斯估计法推出朴素贝叶斯法中的概率估计公式（4.10）及公式（4.11）。
# 1. 极大似然估计法
极大似然估计方法是非常常见的估计方法（MLE）。而这里的公式4.8和公式4.9分别是： 
$P(Y=c_k)=\frac{\displaystyle \sum_{i=1}^{N}I(y_i=c_k)}{N}          (4.8)$
$P(X=a_l|Y=c_k)=\frac{\displaystyle \sum_{i=1}^{N}I(x_i=a_l,y_i=c_k)}{N}          (4.9)$
这里的4.9我简化了一下，其实道理都是一样的，那个j只不过是表示第几个特征。 
那么我们怎么推呢,首先我们来推公式4.8。 
目的式：$P(Y=c_k)=\frac{\displaystyle \sum_{i=1}^{N}I(y_i=c_k)}{N} $
为了方便计算，我们令$P（Y=c_k）=\theta_k,N_k=\displaystyle \sum_{i=1}^{N}I(y_i=c_k)$,N为样本总数。
我们想求的是极大似然函数值，那么这就是求极值的问题，而这个又有一个约束条件：$\displaystyle \sum_{i=1}^{N}\theta_i=1$
于是，我们就可以使用拉格朗日乘子法来解决带有约束条件的极值问题。
不知道大家还记得拉格朗日乘子法怎么做么。首先就是把约束条件作为其中一项，然后乘以$\lambda$后与原函数相加，求导等于0，解方程即可。
对于这题，我们的似然函数是： 
$L(\theta_k,y_1,y_2,...,yn)=\prod_{i=1}^np(y_i)=\prod_{i=1}^n\theta_k^{N_k}$
取对数，称为对数似然函数，结果为：$l(\theta_k)=Ln(L(\theta_k))=\displaystyle \sum_{k=1}^{N}N_kIn\theta_k$
最终的拉格朗日乘子法得到的函数为$l(\theta_k,\lambda)=\displaystyle \sum_{k=1}^{N}N_kIn\theta_k+\lambda(\sum_{i=1}^{N}\theta_i-1)$
那么求导： 
$\begin{cases}\frac{\partial l}{\partial \theta_1}=\frac{N_1}{\theta_1}+\lambda=0\\\frac{\partial l}{\partial \theta_2}=\frac{N_2}{\theta_2}+\lambda=0\\\frac{\partial l}{\partial \theta_3}=\frac{N_3}{\theta_3}+\lambda=0\\···\\\frac{\partial l}{\partial \theta_n}=\frac{N_n}{\theta_n}+\lambda=0\\\displaystyle \sum_{k=1}^{n}\theta_k=1\end{cases}$
我们先对每一个$\theta_i$求值，然后全部加起来后，得到$\lambda=-N$,带入约束条件就求得我们需要的值了： 
$\theta_k=\frac{N_k}{N} $
$P(Y=c_k)=\frac{\displaystyle \sum_{i=1}^{N}I(y_i=c_k)}{N} $
同样的，具有条件概率的写法也是如此。 
我们假设：$\mu_{lk}=p(x=a_l|y=c_k)$,其他的假设如上述所说。 
则似然函数: 
$L(\mu;(x_1,y_1),(x_2,y_2),...,(x_n,y_n))=\prod_{i=1}^Np(x_i,y_i)=\prod_{l=1}^L\prod_{k=1}^K(\mu_{lk}·\theta_k)^{N_{lk}}$
同样的，取似然估计以后，得出的结论和之前的一样，因此我们也会得出相同的结论：$\mu_{lk}=\frac{N_{lk}}{N_k}$。
# 2. 贝叶斯估计法
对于第二题，我想，大家应该知道贝叶斯公式： 
$p(X=a_l|Y=c_k)=\frac{p(X=a_l,Y=c_k)}{p(Y=c_k)}$
其实如果上下都约掉N就可得： 
$p(X=a_l|Y=c_k)=\frac{\displaystyle \sum_{i=1}^{N}I(x_i=a_l,y_i=c_k)}{\displaystyle \sum_{i=1}^{N}I(y_i=c_k)}$
看过这篇[语言模型](http://blog.csdn.net/qq_35082030/article/details/70143557)的同学都会知道，这里是为了防止0概率的出现而设置的数据平滑处理： 
$p(X=a_l|Y=c_k)=\frac{\displaystyle \sum_{i=1}^{N}I(x_i=a_l,y_i=c_k)+\lambda}{\displaystyle \sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda}$
同样的4.11也是一样可以得到，这里就不赘述了。
# 3. 极大似然估计与贝叶斯估计的不同。
贝叶斯估计与极大似然估计的一个主要不同就在于，贝叶斯估计有一个先验概率，而极大似然估计没有，准确说是每种可能的先验估计都相等，也就忽略了。
但是贝叶斯的先验估计也并非是一直准确的，如果对于先验估计有一个很透彻的了解，那么贝叶斯估计应当是比较好用的。但是实际上，很多情况下，我们并不知道先验概率分布，这就导致很多时候，先验概率分布也是一种猜测，猜得对了，效果好，猜的不对，效果就不好。
有的说，极大似然估计是对点估计，而贝叶斯估计是对分布估计。我觉得其实是差不多的。
