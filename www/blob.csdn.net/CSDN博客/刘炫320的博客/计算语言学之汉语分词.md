# 计算语言学之汉语分词 - 刘炫320的博客 - CSDN博客
2017年05月01日 10:51:31[刘炫320](https://me.csdn.net/qq_35082030)阅读数：628
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 1. 汉语分词定义
世界上语言种类我们之前提过，像英语一样的屈折语在词与词之间是使用空格隔开的，像日语这种黏着语和汉语这种孤立语，则并没有使用空格隔开，这也意味着，汉语和日语这种更需要在语义上理解其句子的含义，而结构也是依赖于意义而产生的。
简单来讲，汉语分词就是把汉语像英语一样，词与词之间用空格间隔。而这工作需要使用计算机来自动完成。
这虽然看似简单的一个任务，却耗费了中国近20年的时间，至今没有达到理想水平。在开始时，通常采用基于规则的方法来做这项任务，但是由于汉语实在过于复杂，因此没有取得一定的成果。后来转向基于统计的方法，取得了一定的成果，但仍未达到理想水平。其主要原因主要在于第二部分所讲的。
# 2. 汉语分词的难点
汉语分词的难点有以下几个，首先汉语分词的规范就不统一，不同的学者有不同的看法。因此对于汉语文本中出现的词语认同率只能达到70%。因此标准没有统一，接下来就很难做出一定的规范。
其次，由于汉语结构依赖于语义而存在，因此不同的理解会造成不同的结构。这就导致了歧义的发生。而一旦发生歧义，那么不仅影响的是当前词的分词结果，更会影响整体的分词结果。
第三，未登录词的问题。未登录词一是指训练集中未出现的词，二是指已有词表中未收录的词。事实上，未登录词就是训练集中没有的词。没有出现的词既可能是由于训练集不够大，没有收录当前已存在的词，也有可能是由于新词的不断产生。而实验证明，未登录词对于分词精度的影响远远超出了歧义切分。
# 3. 汉语歧义切分问题
汉语的歧义切分问题是普遍存在的。总的来讲分为交集型切分歧义以及多义组合型切分歧义。
## 3.1 交集型切分歧义
交集型切分歧义定义如下：字符串AJB称作交集型切分歧义，如果满足AJ、JB同时为词，此时汉字串J称作交集串。
例如：“组合成”，其中“组合”是一词，“合成”是一词。这样就形成了交集型切分歧义。
## 3.2多义组合型切分歧义
对于这点，应当采用其新定义：汉字串AB称作多义组合型切分歧义，如果A、B、AB同时为词，且至少存在一个上下文语境C，使得A、B在语法、语义上均成立。
例如，“平淡”一词，其中“平”和“淡”在任何时候都不能单独成立。因此“平淡一词”不能称作多义组合型切分歧义。
事实上，如果使用了新的定义，其数量是十分稀少的。因此歧义切分问题，虽然存在，但并不致命，尚且仍有办法去解决。因此还是取得了不错的成果。
# 4. 未登录词切分问题
未登录词切分主要分为以下几种情况：
- 出现的新词汇，包括但不仅限于网络用语。
- 专有名词，包括人名、地名、组织机构名。
- 专业名词和研究领域名称。
这三类未登录词对于自动分词的精度影响十分巨大，据统计，在识别错误的情况中，未登录词导致的错误的占比竟高达98.33%。因此如何消除未登录词对于自动分词所造成的影响，是当前自动分词的重点和难点。
更确切来讲，未登录词的两个主要问题，一个是在新词的发现和识别上。另一个是在命名实体识别上。命名实体识别包括但不限于人名、地名、组织机构名。因此这两个问题，是自动分词提升正确率的2条途径。
# 5. 自动分词方法
汉语自动分词方法在1994年以前，被归结为16种方法。这些方法大多数是基于词表+规则+算法组成的。因此我们成为其传统分词方法。而基于统计的分词方法则是在2000年以后。但是即使是传统分词方法，其算法的巧妙性，仍然值得我们学习。
## 5.1 N-最短路径方法
N最短路径算法其实就是一个词典+单源点最短路径问题。这个问题可以通过贪心算法或者动态规划算法解决。 
例如： 
![N最短路径](https://img-blog.csdn.net/20170501101448087?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
这是“他说的确实在理”这么一句话的有向无环图。只需要从0到7找到对端路径即可。我们可以知道大概有这么几种可能： 
1. 他 说 的确 是在 理。5 
2. 他 说 的 确实 在理。5 
3. 他 说 的确 实 在理。5 
4. 他 说 的 确 实在 理。6
而我们知道，这句话应当被分成，他 说 的 确实 在理。虽然它也是5但是依靠机器来做，不一定能够从3个最短路径中，挑选出这个最佳合适的路径。因此，对于不同的路径来讲，其代价不应是相同的，应当根据其概率的不同而设置不同的权值。甚至其路径成本不应是相加，而是相乘。
最后提一下，其N的含义指的是一条连线最多可以跨几个基本单元，上述例子中，是N=3。
## 5.2 基于词的N元句法模型的分词方法
好吧，其实就是上面提到的改进形式，对于不同的路径来讲，其代价不应是相同的，应当根据其概率的不同而设置不同的权值。甚至其路径成本不应是相加，而是相乘。
对于词表中已经出现的词，我们记为1，其他的则使用N元语言模型来做。还记得N-gram模型吗？
这种划分出一个词又一个词的模型都统称为生成式模型，其特点是一次性生成最终结果，而无需再进行其他形式处理。微软亚洲研究院（MASR）使用基于统计语言模型的分析系统方法，在经过对命名实体识别处理后，其自动分词的正确率和召回率达到了96.3%和97.4%。注意的是，这是处理命名实体以后的结果，也就是仅针对歧义切分的影响进行了统计。
接下来的介绍的方法，就是基于判别式模型了。
## 5.3 基于字构词的汉语分词方法
在2002年时，一种全新的方法被提出了，那就是在分词任务上，使用分类模型来进行实验。这种方法把分词变成了对每个字进行词位标记，这样，就把原本生成式任务，变成了分类式任务。
例如，规定每个字只有4个词位：词首（B）、词中（M）、词尾（E）和单独成词（S），那么任务就变成对每个字进行一个词位分类。但不能仅仅是分类，还应考虑上下文之间的关系，这样就变成了序列化标注问题。通常来讲，序列化标注问题有词袋模型、隐马尔可夫模型、条件随机场模型等处理效果较好。
当然， 后面还有基于词的感知机算法与生成式与判别式相结合的算法。这里不一一介绍，自然是组合型的更胜一筹。当然，目前的分词则大多是与词性标注同时进行，这样能同时提升双方的效果。
