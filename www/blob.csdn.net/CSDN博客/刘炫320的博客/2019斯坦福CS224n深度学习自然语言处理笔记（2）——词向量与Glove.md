# 2019斯坦福CS224n深度学习自然语言处理笔记（2）——词向量与Glove - 刘炫320的博客 - CSDN博客
2019年04月08日 10:43:06[刘炫320](https://me.csdn.net/qq_35082030)阅读数：71
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)

### 文章目录
- [1. 为什么不直接使用词共现矩阵获得词向量？](#1__4)
- [1.1 词共现矩阵方法（窗口统计和全局统计）](#11__6)
- [1.2 解决上述问题方法——SVD](#12_SVD_15)
- [1.3 基于统计和直接预测方法比较](#13__31)
- [2. Glove](#2_Glove_40)
- [3. 词向量评估](#3__49)
- [4. 一词多义](#4__65)
视频课程链接：《[深度学习与自然语言处理（2）](https://www.bilibili.com/video/av46216519/?p=2)》
继续上一节的内容。还是沿着之前的思路，首先想到为什么不直接使用词共现矩阵，然后提出SVD的解决方法。在比较了基于统计和直接预测两种方法后，提出Glove模型。接着对于词向量的评估方法和一词多义问题提出相应的解决方法。
# 1. 为什么不直接使用词共现矩阵获得词向量？
在上一节中，最后提出一个问题，为什么不直接使用词共现矩阵获得词向量？
## 1.1 词共现矩阵方法（窗口统计和全局统计）
其方法有2种，第一种是局部窗口，只统计在它附近窗口内的词，第二种方法是全文词共现，这就是所说的LSA方法，从而能够获得主题信息。
根据例子我们可以发现，直接使用词共现是由以下4个缺陷:
- 词表大小会不断增长
- 高维空间需要大量的空间存储
- 数据稀疏
- 不够健壮（鲁棒性差，鲁棒=robust）
## 1.2 解决上述问题方法——SVD
所以，其解决办法思路是，能不能找到一个固定的，低维的矩阵来把词共现的意思蕴含其中呢？于是就是用了降维方法。常用的降维方法就是奇异值分解（Singular Value Decomposition, SVD），之所以叫做奇异值，就是因为它来源于积分方程（设A为m*n阶矩阵，q=min(m,n)，A*A的q个非负特征值的算术平方根叫作A的奇异值。）。最初不是这个名字，而是为标准乘子（canonical multipliers），也就是标准型上的对角元素。这里我们扯远了。
奇异值分解方法如下：
$A=U\sum V^T$
其中$\sum$为半正定m×n阶对角矩阵，对角线上的值即为奇异值。一般的，我们规定对角线的值按照从大到小排列。其目的一是为了保证对角矩阵的唯一性，目的二为尽可能保留原有信息量。
U和V是正交的。
当然，这仍然会有之前出现的一些问题，例如高频词（the,has,have）等，统计方法是否科学等。其解决方法如下：
- 最高频率设定阈值，如100
- 使用皮尔逊系数取代频率，并将负数置为0
- 使用倾斜窗口采样更多的词
- 等等
其结果也大致可看。
## 1.3 基于统计和直接预测方法比较
那么基于统计和直接预测两种方法比较如下：
|优缺点|LSA，PCA|NNLM, RNN|
|----|----|----|
|优点|更快的训练|泛化能力强|
|优点|充分利用统计信息|可以捕捉到复杂的匹配信息|
|缺点|只是初步捕捉到词的相似度|适配语料库大小|
|缺点|高频词没有赋予合适权重|没有有效利用统计信息|
# 2. Glove
因此，后面的改动大家也都知道了，就是Glove的出现，由于Pennington本人另一个发现，说的是使用向量差来编码含义，即：
$w_i·w_j=logP(i|j)$
$w_x·（w_a-w_b）=log\frac{P(x|a)}{P(x|b)}$
因此Glove的目标就是
$J=\sum_{i,j=1}^Vf(X_{ij})(w_i^T\tilde w_j +b_i+\tilde b_j-logX_{ij})^2$
它具有训练更快，能够适应不同大小的语料库和词向量等优势。
# 3. 词向量评估
接下来的问题就是如何进行词向量的评估，一般的NLP的评估分为内在的（Intrinsic）和外在的(extrinsic)两种方法，区别如下。
|内在的|外在的|
|----|----|
|在某一具体子任务中评估|在一个真实任务中评估|
|能够快速计算|需要很长的时间计算准确率|
|能够更好的理解系统|无法判断它的子系统是有问题的|
|无法判断是否有用除非和某一个真实任务有联系|如果能够比另一个子系统取得更优，则表明有效|
具体的，内在性评估方法有常见的词距离是否表示相同的含义（anology）和信息检索等。
后面的实验（On the Dimensionality of Word Embedding）也证明以下问题：
- glove是比sg模型要好，
- 1000维以上的词向量没有性能提升。
- 如果使用类似维基百科的语料比使用新闻语料要更好，这是因为维基百科更能显示词与词之间的联系。
# 4. 一词多义
剩下要解决的问题就是接下来的方向，可能存在一词多义现象。
其中一个解决方法是增加标号，同一个词使用不同的标号表示不同的含义。
另一个方法是，根据不同含义进行加权求和，例如;
$v_{pike}=\alpha_1v_{pike_1}+\alpha_2v_{pike_2}+\alpha_3v_{pike_3}$
其中$\alpha_1=\frac{f_1}{f_1+f_2+f_3}$
然而随着技术的发展，现有的解决方法是使用不同层次的编码层，从而获得不同的含义，这就使得一个词不仅基于该词本身的含义（最后一层输出），还基于其上下文（前n层的输出）。
