# 机器学习习题（12） - 刘炫320的博客 - CSDN博客
2018年01月21日 15:10:09[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1315
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
最近才知道七月在线收录了一些我的机器学习的题目及解析，我也去刷了一番，有时候看到我自己的答案解析，还是挺亲切的。
但是有些不是我解析的题目翻遍了网络也没有找到题目的答案，或者答案不太正确，只好自己来做一下解析了。
当然有些题目也是有争议的，我只是给出参考答案和解析。
> 
1.以下哪些方法不可以直接来对文本分类？ 
  A. Kmeans  
  B. 决策树  
  C. 支持向量机  
  D. KNN
参考答案: A
解析：分类不同于聚类。尽管这题选择A，但是实际上描述是不严谨的。在学术上，我们把分类与聚类分开来。但是在工业界上，越来越多的需求是要求在无监督的情况下对文本进行分类，而分类的标签则取决于需求，这点要做到很好是很难的。
> 
2.假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是： 
  A.这个被重复的特征在模型中的决定作用会被加强  
  B.模型效果相比无重复特征的情况下精确度会降低  
  C.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。  
  D.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题  
  E.NB可以用来做最小二乘回归  
  F.以上说法都不正确
参考答案：BD 
解析：朴素贝叶斯的最大假设就是：各个因素相互独立。事实上，并不是如此，这也是为什么称为朴素的原因。
A，看D就知道，如果两个维度比较相似，那么这两个维度可能高度相关，那么就需要转到D。
B，当维度重复时，习得的联合概率分布有误，所以精确度会降低。
C，多出一个维度的特征，其训练出的模型就会有所不同。这与把两个维度的值重复的情况是不同的。
D，两列特征高度相关时（也就是有可能是因为重复了），那就无法用这种疑似重复的特征来分析问题了。
E，最小二乘回归是用在判别式的机器学习方法中，比如SVM,logtic等。它是寻找几何误差最小的预测模型。而贝叶斯是生成式模型。
> 
3.域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？ 
  A. 伪逆法-径向基（RBF）神经网络的训练算法，就是解决线性不可分的情况  
  B. 基于二次准则的H-K算法：最小均方差准则下求得权矢量，二次准则解决非线性问题  
  C. 势函数法－非线性  
  D. 感知器算法－线性分类算法
参考答案：D
解析：这道题很明显是D，D是线性的，单层感知器甚至连异或都无法表示。
> 
5.Fisher线性判别函数的求解过程是将M维特征矢量投影在（）中求解。 
  A. M-1维空间 
  B. 一维空间 
  C. 二维空间 
  D. 三维空间
参考答案：B
解析：记得在之前的有道题目里我对于Fisher线性判别准则有过介绍：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
其实就是LDA，说起LDA那更简单了，请移步《[LDA与PCA](http://blog.csdn.net/qq_35082030/article/details/75578084)》。
> 
6.如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（） 
  A. 无偏的，有效的 
  B. 无偏的，非有效的 
  C. 有偏的，有效的 
  D. 有偏的，非有效的
参考答案：B
解析：OLS即普通最小二乘法。由高斯—马尔可夫定理，在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量。根据证明过程可知，随机误差中存在异方差性不会影响其无偏性，而有效性证明中涉及同方差性，即异方差会影响参数 OLS估计量的有效性。
> 
7.在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）（假设precision=TP/( TP+ FP),recall=TP/( TP+ FN)。） 
  A. Accuracy:$\frac{TP+ TN}{all}$
  B. F=$\frac{2\times recall \times precision}{recall+precision}$
  C. G-mean:$\sqrt{precision \times recall}$
  D. AUC: ROC曲线下面积
参考答案：A
解析：对于分类器，主要的评价指标有 precision， recall，F-score，以及 ROC曲线等。在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么( TP+ TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。当样本不均衡时，建议采用BCD方法来评价。
> - 影响聚类算法结果的主要因素有() 
  A. 特征选取 
  B. 已知类别的样本质量 
  C. 模式相似性测度 
  D. 分类准则
参考答案：ACD
解析： 
聚类的目标是使同一类对象的相似度尽可能地大；不同类对象之间的相似度尽可能地小。
聚类分析的算法可以分为
- 划分法（Partitioning Methods）
- 层次法（Hierarchical Methods）
- 基于密度的方法（Density-Based Methods）
- 基于网格的方法（Grid-Based Methods）
- 基于模型的方法（Model-Based Methods）
- 谱聚类（Spectral Clustering）等，
不同的方法对聚类效果存在差异（D正确）；
特征选取的差异会影响聚类效果（A正确）。
聚类的目标是使同一类对象的相似度尽可能地大，因此不同的相似度测度方法对聚类结果有着重要影响（C正确）。
由于聚类算法是无监督方法，不存在带类别标签的样本，因此，B选项不是聚类算法的输入数据。
> - 模式识别中，不属于马氏距离较之于欧氏距离的优点是（） 
  A. 平移不变性 
  B. 尺度不变性 
  C. 考虑了模式的分布
参考答案：A
解析：这里是各种距离的特性的考察。 
欧氏距离（Euclidean distance）也称欧几里得度量、欧几里得度量，是一个通常采用的距离定义，它是在m维空间中两个点之间的真实距离。在二维和三维空间中的欧氏距离的就是两点之间的距离。
特性： 
平移不变性 
旋转不变性
马氏距离(Mahalanobis distance)是由印度统计学家马哈拉诺比斯提出的，表示数据的协方差距离。为两个服从同一分布并且其协方差矩阵为Σ的随机变量与的差异程度:
如果协方差矩阵为单位矩阵,那么马氏距离就简化为欧氏距离,
如果协方差矩阵为对角阵,则其也可称为正规化的欧氏距离。
它是一种有效的计算两个未知样本集的相似度的方法。对于一个均值为μ，协方差矩阵为Σ的多变量向量，样本与总体的马氏距离为(dm)^2=(x-μ)’Σ^(-1)(x-μ)。 
在绝大多数情况下，马氏距离是可以顺利计算的，但是马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧式距离的最大差异之处。
特性： 
不考虑量纲影响（尺度不变性） 
排除变量之间的相关性影响。（考虑了模式的分布）
> 
9.影响基本K-均值算法的主要因素有(）  
  A. 样本输入顺序；  
  B. 模式相似性测度；  
  C. 聚类准则；  
  D. 初始类中心的选取
参考答案：BCD
解析： 
K-均值算法隐含地假设输入数据的顺序不影响结果。（A错）
K-均值的三大要素：
- 选定某种距离作为数据样本间的相似性度量
- 选择评价聚类性能的准则函数
- 相似度的计算根据一个簇中对象的平均值来进行。
所以B对、C对。
K-均值算法通常使用的初始化方法有Forgy和随机划分(Random Partition)方法：  
（1）Forgy方法随机地从数据集中选择 个观测点作为初始的均值点；  
（2）随机划分方法则随机地为每一观测指定所属聚类，然后运行“更新(Update)”步骤，计算随机分配的各聚类的图心，作为初始的均值点。
特点：Forgy方法易于使得初始均值点散开，随机划分方法则把均值点都放到靠近数据集中心的地方。 
适用性：随机划分方法一般更适用于K-调和均值和模糊K-均值算法；Forgy方法更适用于期望-最大化(EM)算法和标准K-均值算法
因此D对。
