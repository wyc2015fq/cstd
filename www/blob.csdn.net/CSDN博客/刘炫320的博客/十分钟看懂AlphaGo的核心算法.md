# 十分钟看懂AlphaGo的核心算法 - 刘炫320的博客 - CSDN博客
2017年06月16日 16:39:42[刘炫320](https://me.csdn.net/qq_35082030)阅读数：5304
围棋是一个完全信息博弈问题。而完全信息博弈，通常能被简化为寻找最优值的树搜索问题。它含有 b 的 d 次方个可能分支，在国际象棋中 b≈35，d≈80；而在围棋中 b≈250，d≈150。很显然，对于围棋，用穷举法或简单的寻路算法（heuristics）是行不通的。但有效的方法是存在的：
从策略（policy） P(a|s) 中取样 action，降低搜索广度
通过位置评估降低搜索深度
把策略和值用蒙特卡洛树搜索（MCTS）结合起来。
通常的步骤是：
用一个 13 层的 CNN，直接从人类棋步中训练一个监督学习策略网络 Pσ。输入为 48 x 19 x 19 的图像（比方说，它的组成棋子颜色 是 3 x 19 x 19），输出是使用 softmax 层预测的全部落子的概率。精确度是 55.7%。
训练一个能在运行时快速取样 action 的快速策略 Pπ。这会用一个基于小型模式特征的线性 softmax。精确度是 24.2%，但它计算一次落子只用 2 微秒，而不像 Pσ 需要 3 毫秒。
训练一个增强学习策略网络 Pρ ，通过优化博弈结果来进一步提升监督策略网络。这把策略网络向赢棋优化，而不是优化预测精确度。本质上，Pρ 与 Pσ 的结构是一样的。它们的权重使用相同值 ρ=σ 初始化。对弈的两个选手，是当前策略网络 Pρ 和随机（防止过拟合）选择的此前的策略网络迭代。
训练一个价值网络（value network）Vθ，来预测强化学习策略网络自己和自己下棋的赢家。该网络的架构和策略网络类似，但多出一个特征平面（当前玩家的颜色），并且输出变成了单一预测（回归，均方差损失）。根据完整棋局来预测对弈结果，很容易导致过拟合。这是由于连续落子位置之间高度相关，只有一子之差。因此，这里使用了强化学习策略网络自己与自己对弈新生成的数据。该数据从包含 3000 万个不同位置的独立棋局中抽取。
把策略网络、价值网络、快速策略和蒙特卡洛树搜索结合起来。一个标准的蒙特卡洛树搜索过程包含四步：选择、扩展、评估、备份。为了让大家更容易理解，我们只粗略讲了讲它如何在模拟中选择状态的部分（如对数学感兴趣，请到原始论文中找公式）。
状态分数=价值网络输出+快速运行（fast rollout）的策略结果+监督学习策略网络输出
高状态得分（或者说落子）会被选择。价值网络输出和快速运行策略结果是评估函数，在叶子节点进行评估（注意，为了评估快速运行，需要一直到最后一步）。监督学习策略网络输出是一个当前阶段的 action 概率，充作选取分数的奖励分。该分数会随访问次数而退化，以鼓励探索。注意强化学习策略网络只被用于辅助，来生成价值网络，并没有直接在蒙特卡洛树搜索中使用。
到这就结束了，以上就是战胜了人类的 AlphaGo 算法！
原文：[One-PageAlphaGo 十分钟看懂AlphaGo的核心算法](http://www.ccf.org.cn/c/2017-06-14/597535.shtml)
英文论文：[Mastering the game of Go with deep neural networks and tree search](http://web.iitd.ac.in/~sumeet/Silver16.pdf)
