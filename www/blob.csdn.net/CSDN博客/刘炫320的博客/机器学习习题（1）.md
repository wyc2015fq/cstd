# 机器学习习题（1） - 刘炫320的博客 - CSDN博客
2017年07月08日 15:11:33[刘炫320](https://me.csdn.net/qq_35082030)阅读数：3128
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
# 1. 前言
从这章开始，我们将进入机器学习实战题目训练，今天的成果是全军覆没！
# 2. 习题1（过拟合问题）
> 
在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）
A. 增加训练集量
B. 减少神经网络隐藏层节点数
C. 删除稀疏的特征
D. SVM算法中使用高斯核/RBF核代替线性核
正确答案：D
解析：
一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。
B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合
D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。
# 3. 习题2（时序模型）
> 
下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测
A.AR模型
B.MA模型
C.ARMA模型
D.GARCH模型
正确答案：D
解析：
AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。 
MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。
GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。
# 4. 习题3（线性分类器）
> 
以下()属于线性分类器最佳准则?
A.感知准则函数
B.贝叶斯分类
C.支持向量机
D.Fisher准则
正确答案：ACD
解析： 
线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。
感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。 
支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）
Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。
# 5. 习题4（HK算法）
> 
基于二次准则函数的H-K算法较之于感知器算法的优点是()?
A.计算量小
B.可以判别问题是否线性可分
C.其解完全适用于非线性可分的情况
D.其解的适应性更好
正确答案：BD
解析：
HK算法思想很朴实,就是在最小均方误差准则下求得权矢量.
他相对于感知器算法的优点在于,他适用于线性可分和非线性可分得情况,对于线性可分的情况,给出最优权矢量,对于非线性可分得情况,能够判别出来,以退出迭代过程.
# 6.习题5（各种分类器特点）
> 
以下说法中正确的是()
A.SVM对噪声(如来自其他分布的噪声样本)鲁棒
B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同
C.Boosting和Bagging都是组合多个分类器投票的方法,二者都是根据单个分类器的正确率决定其权重
D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少
正确答案：BD
解析：
A、SVM对噪声（如来自其他分布的噪声样本）鲁棒
SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。
B、在AdaBoost算法中所有被分错的样本的权重更新比例相同 
AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。
C、Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重。
Bagging与Boosting的区别：
取样方式不同。
- Bagging采用均匀取样，而Boosting根据错误率取样。
- Bagging的各个预测函数没有权重，而Boosting是有权重的。
- Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。
# 7. 小结
通过这5道题目的练习，我们对于机器学习又有了更加深刻的认识，包括过拟合问题、时序模型、线性分类器、HK算法、各种分类器特点等有了一定的认识。我们将会在接下来的日子里再接再厉！
