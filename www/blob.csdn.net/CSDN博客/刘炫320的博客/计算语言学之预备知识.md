# 计算语言学之预备知识 - 刘炫320的博客 - CSDN博客
2017年03月08日 21:37:21[刘炫320](https://me.csdn.net/qq_35082030)阅读数：646
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 0.写在前面
这一讲，我们主要是来复习一下我们将要用到的一些数学概念，这里主要包含3大部分，第一部分是概率论的相关知识，第二部分是信息论的相关知识，第三部分则是支持向量机的相关知识。我们这一讲，主要围绕这三个问题进行讲解。
# 1.概率论相关知识
## 1.1. 概率的定义
概率论的第一个问题，就是什么是概率：概率是从随机试验中的事件到实数域的映射函数，用以表示事件发生的可能性。
准确来讲，我们个人是没有办法对一个事物得到其概率的真实值，尤其是在教科书里，更是把极大似然估计看作是概率的估计，约等于概率。这种想法，对于初学者来讲，会使得概率变得有血有肉，但对于极大似然估计的理解，就增加了难度，很多人搞不清楚什么是极大似然估计的原因，就是因为极大似然估计的位置让概率所占有了，所以你很难再去找一个合适的定义来定义极大似然估计。
事实上，两者之间的的关系是这样的： 
如果${s_1,s_2,...,s_n}$是一个试验的样本空间，在相同的情况下重复试验N次，观察到样本$s_k$的次数为$n_N(s_k)$,那么，$s_k$在这N次试验中的相对频率为： 
$q_N(s_k)=\frac{n_N(s_k)}{N}$
当N越来越大是对频率$q_N(s_k)$就越来越接近$s_k$的概率$P（s_k）$。而这种方法称为最大似然估计。
## 1.2.条件概率与联合概率
这两个可能有点像，不过不应该搞混掉。
### 1.2.1.条件概率与联合概率
条件概率如果A和B是样本空间Ω上的两个时间，P（B）>0,那么，在给定B时A的条件概率P（A|B）为 
$P(A|B)=\frac{P(A∩B)}{P(B)}$
这种是条件概率，这种值得是在事件B的条件下，事件A的概率。 
而联合概率就是分子上的那个P（A∩B）。
### 1.2.2. 条件概率分布与联合概率分布
既然提到了分布，那和概率还是有一点区别，也是有一点联系的。 
假设$（X_1,X_2）$为一个二维的离散型随机变量，$X_1$全部的可能取值为$a_1,a_2,...$，$X_2$全部的可能取值为$b_1,b_2,...$。那么 
$（X_1,X_2）$的联合概率分布为： 
$p_{ij}=P(X_1=a_i,X_2=b_j)$
而条件概率分布是长这样的： 
$P(X_1=a_i|X_2=b_j)=\frac{P(X_1=a_i,X_2=b_j)}{P(X_2=b_j)}=\frac{p_{ij}}{P(X_2=b_j)}=\frac{p_{ij}}{\displaystyle\sum_kp_{kj}}$
而其分母，其实是边缘概率分布，大家看一下表格就知道了。 
![条件概率分布](https://img-blog.csdn.net/20170308202137037?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
其中蓝色的部分为P（X1）的边缘概率分布，黄色部分为P（X2）的边缘概率分布。
## 1.3.贝叶斯法则与贝叶斯决策理论
### 1.3.1. 贝叶斯法则
贝叶斯法则被我们多次提及，这是一项非常重要的法则，在概率论中被广为使用。主要用来对于问题的两个不同方面可以进行概率的相互转化的时候用到。其基本公式为： 
$P（B|A）=\frac{P(B∩A)}{P(A)}=\frac{P(A|B)P(B)}{P(A)}$
推广来讲，假设A为样本空间Ω的事件，$B_1,B_2,...B_n$为Ω的一个划分，如果A是Ω的子集，P（A）>0,并且i≠j，$B_i∩B_j=∅,P(B_i)>0$，则： 
$P（B_j|A）=\frac{P(A|B_j)P(B_j)}{P(A)}=\frac{P(A|B_j)P(B_j)}{\displaystyle\sum_{i=1}^{n}P(A|B_i)P(B_i)}$
这才是最重要的式子，一定要牢记。
### 1.3.2. 贝叶斯决策理论
一旦知道了贝叶斯法则，那么贝叶斯决策理论就应运而生了。贝叶斯决策理论是统计方法处理模式分类问题的基本理论之一。只需要把公式里的变量赋予相应的物理意义： 
$P（B_j|A）=\frac{P(A|B_j)P(B_j)}{P(A)}=\frac{P(A|B_j)P(B_j)}{\displaystyle\sum_{i=1}^{n}P(A|B_i)P(B_i)}$
其中$B_j$表示各类别的状态。
简单来讲，非洲多为黑种人，而欧洲多为白种人，如果你遇到一个人，这个人是黑种人，那么你肯定猜测他来自非洲的可能性比较大，如果你遇到的是白种人，那么你肯定猜测他来自欧洲的可能性比较大。这就是贝叶斯决策理论在我们日常生活中的应用，你可能已经不知不觉就学会了这个理论。
## 1.4.随机变量和分布
对于我们一个问题的所有不确定性因素都可以成为随机变量，而这些随机变量都会遵循的某些分布，这些分布可能是简单的，也可能是复杂的。在正常处理问题的过程中，我们多半会用高斯分布来模拟噪声分布，而且收到了比较好的效果，另外二项式分布也经常被使用。当然，像LDA模型里的Dirichlet分布，也是一个对于主题模型很好效果的模拟，尽管我们并不知道真实的主题分布是什么样子。
## 1.5.显著性检验
另外对于显著性检验，也是我们对于模型假设的一个有效性的评估。显著性检验就是事先对总体（随机变量）的参数或总体分布形式做出一个假设，然后利用样本信息来判断这个假设（备择假设）是否合理，即判断总体的真实情况与原假设是否有显著性差异。
或者说，显著性检验要判断样本与我们对总体所做的假设之间的差异是纯属机会变异，还是由我们所做的假设与总体真实情况之间不一致所引起的。 显著性检验是针对我们对总体所做的假设做检验，其原理就是“小概率事件实际不可能性原理”来接受或否定假设。
其中“小概率事件实际不可能性原理”指的是小概率事件在一次试验中是几乎不可能发生的，假若在一次试验中小概率事件事实上发生了。那只能认为该事件不是来自我们假设的总体，也就是认为我们对总体所做的假设不正确。
这个部分，我们在概率论与数理统计中学到过，就不在深入解读，大家知道大概即可，通常显著性测验的数值P的阈值是1%、5%、10%、20%等，其检验方式也有很多包括T检验、U检验、方差分析等。
# 2.信息论相关知识
提到信息论，不得不提到香农和熵，作为信息论的核心，香农使用熵这个来自于物理理论的术语很好的刻画了信息世界的内在混乱程度。而且正是由于信息世界的2进制基础单位，使得我们对于熵有了一个更为直观的认识，这也就是香农提出熵的背景——信息编码长度问题。
## 2.1. 熵的定义
熵的本质为系统的内在混乱程度，也就是说，如果事物内部越混乱，则其熵越大，其内部越统一，则熵越小。用在信息论中，这是指信息可编码程度越复杂，也就是信息内部各部分之间区分度非常大，则其熵越大，而如果信息可编码程度越简单，则熵越小。其定义式为： 
$H(P)=-\displaystyle\sum_{x∈R}p(x)log_2p(x)$
其中X是一个离散型随机变量，其概率分布为p(x)=P(X=x)。
熵又称为是**自信息**，这时和后面的互信息相区别的。所谓的自信息，就是指描述一个随机变量自身的不确定性，如果熵越大，它的不确定性越大，正确估计其值的可能性就越小。
就上一讲讨论的汉语是否是最难的学的语言，在熵的衡量标准中就可以确切体现。一般来讲，英语的熵为4.02，而汉语高达9.71。这也就是为什么外国人总是觉得汉语难学了，因为所需要的编码程度是英语的2倍还多。
## 2.2.联合熵和条件熵
### 2.2.1.联合熵
如果X，Y是一对离散型随机变量，X，Y~p(x,y)，X,Y的联合熵H（X,Y）定义为： 
$H(X,Y)=-\displaystyle\sum_{x∈X}\displaystyle\sum_{y∈Y}p(x,y)log_2p(x,y)$
联合熵实际上描述的是一对随机变量平均所需要的信息量。
### 2.2.2. 条件熵
在给定X的情况下，Y的条件熵定义为： 
$H(Y|X)=-\displaystyle\sum_{x∈X}p(x)H(Y|X=x)=\displaystyle\sum_{x∈X}\displaystyle\sum_{y∈Y}p(x,y)log_2p(y|x)$
可能看起来不是太清楚，没关系，经过变换后，我们可以把联合熵和条件上进行相互转换，其中间人就是自信息： 
$H(X,Y)=H(X)+H(Y|X)$
讲到这，是不是想起条件概率和联合概率了？
### 2.2.3.自信息与互信息
自信息在我们上面讲过了，就是熵。那么互信息的定义为： 
$I(X;Y)=H(Y)-H(Y|X)=H(X)-H(X|Y)$
这个I(X;Y)反应的是再知道了Y的值以后X的不确定性减少量，可以理解为Y的值透露了多少关于X的信息量。
互信息体现的是两个变量之间的依赖程度，如果I(X;Y)>>0，则表明X和Y是高度相关的。如果I(X;Y)=0，则表明X和Y是相互独立的。当然如果互信息是负的，通常不出现。
互信息在词汇聚类、汉语自动分词、词义消歧等问题的研究中具有重要用途，不过在汉语自动分词与歧义消解中，通常使用双字耦合度来替代互信息，以便于更精确的表示两字之间的依赖程度。
### 2.2.4.相对熵和交叉熵
#### 2.2.4.1. 相对熵
相对熵又称为KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。对两个分布p(x)和q(x)，相对熵的定义为： 
$D(p||q)=\displaystyle\sum_{x∈X}p(x)log\frac{p(x)}{q(x)}$
当两个随机分布完全相同时，其相对熵为0，而当两个随机分布的差别增加时，其相对熵的期望也越大。
#### 2.2.4.2. 交叉熵
交叉熵主要用来衡量的是估计模型与真实概率分布之间差异情况的。 
随机变量X和模型q之间的交叉熵为： 
$H(X,q)=H(X)+D(p||q)=-\displaystyle\sum_{x∈X}p(x)logq(x)$
模型的交叉熵越小，则表示模型的表现越好。而在自然语言中，我们通常使用困惑度来替代交叉熵。
# 3.支持向量机相关知识
支持向量机理应不该在这里描述，但是考虑到在将来有可能用到，因此在这简要说明，等真正用到的时候，我们会做详细讲解。
首先，支持向量机与我们之前所讲的感知器模型十分的相似，但是更为复杂。
## 3.1. 线性可分
针对于线性可分的样本集，基本上和感知器模型大致相同，通常是二分类问题。但是与感知器不相同的是它的决策函数为： 
以最大间隔分开数据的超平面称为最优超平面。
而对于多分类问题，则是给每个类关联一个超平面，然后把新点赋予超平面距离最远的一类空间。
## 3.2. 线性不可分
通常来讲，线性可分的数据集少之又少，通常都是线性不可分的。即使是线性可分的数据集，通常我们也没有办法去证明它是线性可分的，只好假定为线性不可分的数据集。
针对线性不可分的数据集，支持向量机有自己的绝招，那就是核函数。所谓的核函数，就是可以把样本映射到某个高位特征空间，这样的话，我们就能在高维空间里线性可分了。
但是高维的空间也就意味着更大的时间复杂度，但是支持向量机可以使用一种映射变化，使得其高维的点与点的内积可以转换为低维的内积，这样就可以大大减少计算量。
而如何构造核函数，则是研究支持向量机的一个重要方向，目前主要有多项式核函数，径向基核函数，多层感知机，动态核函数等。我们将会在后面的支持向量机的专题中，进行相关解答。
# 4.小结
我们这次讲解了3个部分，分别是概率论、信息论以及支持向量机的一些初级知识，为了给下面的学习做铺垫，欢迎随时回来复习。
