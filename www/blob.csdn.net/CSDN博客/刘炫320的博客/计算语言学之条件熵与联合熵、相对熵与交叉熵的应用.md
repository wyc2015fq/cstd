# 计算语言学之条件熵与联合熵、相对熵与交叉熵的应用 - 刘炫320的博客 - CSDN博客
2017年06月06日 21:36:28[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1379标签：[相对熵																[交叉熵																[损失函数																[KL距离																[条件熵](https://so.csdn.net/so/search/s.do?q=条件熵&t=blog)](https://so.csdn.net/so/search/s.do?q=KL距离&t=blog)](https://so.csdn.net/so/search/s.do?q=损失函数&t=blog)](https://so.csdn.net/so/search/s.do?q=交叉熵&t=blog)](https://so.csdn.net/so/search/s.do?q=相对熵&t=blog)
个人分类：[计算语言学](https://blog.csdn.net/qq_35082030/article/category/6775565)
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 1. 引言
条件熵与联合熵我们在之前已经讲过，大家可以类比一下条件概率和联合概率，就大概知道是什么意思了。不过这里我们还是复习一下条件熵与联合熵的公式。
# 2. 条件熵与联合熵的应用
条件熵：在给定X的情况下，Y的条件熵定义为： 
$H(Y|X)=-\displaystyle\sum_{x∈X}p(x)H(Y|X=x)=\displaystyle\sum_{x∈X}\displaystyle\sum_{y∈Y}p(x,y)log_2p(y|x)$
联合熵：如果X，Y是一对离散型随机变量，X，Y~p(x,y)，X,Y的联合熵H（X,Y）定义为： 
$H(X,Y)=-\displaystyle\sum_{x∈X}\displaystyle\sum_{y∈Y}p(x,y)log_2p(x,y)$
联合熵实际上描述的是一对随机变量平均所需要的信息量。
这么讲实在是过于抽象，那么我们接下来以一个通信传输为例，来介绍一下条件熵与联合熵的计算。
> 
例：一个二进信源X发出符号集{0,1},经过离散无记忆信道传输,信道输出用Y表示.由于信道中存在噪声,接收端除收到0和1的符号外,还有不确定符号“2”。 
![这里写图片描述](https://img-blog.csdn.net/20170606153947218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
如图所示，X是输入的，Y是输出的，中间的分数表示其符号转移概率。此时：$H(x)=H(x_0,x_1)=-\frac{2}{3}log\frac{2}{3}-\frac{1}{3}log\frac{1}{3}=0.92bit$
条件熵如何计算呢，根据公式我们可知： 
$H(Y|X)=-\sum_{ij}p(x_i,y_j)logp(y_j|x_i)$
$=-p(x_0,y_0)logp(y_0,x_0)-p(x_0,y_1)logp(y_1,x_0)-p(x_0,y_2)logp(y_2,x_0)-p(x_1,y_0)logp(y_0,x_1)-p(x_1,y_1)logp(y_1,x_1)-p(x_1,y_2)logp(y_2,x_1)$
$=-\frac{1}{2}log\frac{3}{4}-\frac{1}{6}log\frac{1}{4}-\frac{1}{6}log\frac{1}{2}-\frac{1}{6}log\frac{1}{2}=0.88bit$
联合熵H（XY）的计算就水到渠成了。根据公式可得： 
$H(XY)=H(X)+H(Y|X)=1.8bit$
# 3. 相对熵的应用
相对熵也被称为是KL距离，其主要度量两个分布的相似度。准确来讲，指的是在相同事件空间里，概率分布P(x)的事件空间，若用概率分布 Q（x）编码时，平均每个基本事件（符号）编码长度增加了多少比特。
另一种理解就是，已知Q的分布，用Q分布近似估计P，P的不确定度减少了多少。下面我们复习一下相对熵的定义： 
$D(p||q)=\displaystyle\sum_{x∈X}p(x)log\frac{p(x)}{q(x)}$
当两个随机分布完全相同时，其相对熵为0，而当两个随机分布的差别增加时，其相对熵的期望也越大。
## 3.1 简单例子说明KL距离的含义
假如一个字符发射器，随机发出0和1两种字符，真实发出概率分布为A，但实际不知道A的具体分布。现在通过观察，得到概率分布B与C。各个分布的具体情况如下： 
A(0)=1/2，A(1)=1/2
B(0)=1/4，B(1)=3/4
C(0)=1/8，C(1)=7/8 
 那么，我们可以计算出得到如下： 
$D(A||B)=\frac{1}{2}log(\frac{\frac{1}{2}}{\frac{1}{4}})+\frac{1}{2}log(\frac{\frac{1}{2}}{\frac{3}{4}})=\frac{1}{2}log(\frac{4}{3})$
$D(A||C)=\frac{1}{2}log(\frac{\frac{1}{2}}{\frac{1}{8}})+\frac{1}{2}log(\frac{\frac{1}{2}}{\frac{7}{8}})=\frac{1}{2}log(\frac{16}{7})$
也即，这两种方式来进行编码，其结果都使得平均编码长度增加了。我们也可以看出，按照概率分布B进行编码，要比按照C进行编码，平均每个符号增加的比特数目少。从分布上也可以看出，实际上B要比C更接近实际分布（因为其与A分布的KL距离更近）。
虽然KL被称为距离，但是其不满足距离定义的三个条件：1）非负性（满足）；2）对称性（不满足）；3）三角不等式 （不满足）。
这里只是给出了一个简单的例子，引用的是[KL距离（相对熵）](http://www.cnblogs.com/nlpowen/p/3620470.html)，下面我们介绍一下它的具体应用。在对于2篇文章的相似度上有用处。
## 3.2 计算两个文章的相似度
在[KL距离一](http://m.blog.csdn.net/article/details?id=48929833)、[KL距离二](http://m.blog.csdn.net/article/details?id=48929859)、[KL距离三](http://m.blog.csdn.net/article/details?id=48929869)中设计了一个3篇文章之间的相似度的实验。
试验中，使用的3篇文章，分别是《《小团圆》究竟泄了张爱玲什么“秘密”？》、《《小团圆》：张爱玲的一个梦》、《1945年毛zedong和蒋介石在重庆谈判前的秘密情报战》，共使用了2种方法计算概率，一：以字符为单位计算概率；二：以汉语词为单位计算概率。然后计算三篇文章的KL距离，为了简化表达，我们把两篇文章分别记为《小团圆的秘密》、《小团圆的梦》和《秘密情报战》。
a.以字符为单位的计算结果如下： 
《小团圆的秘密》与《小团圆的梦》的KL距离： 2.269998592E9 
《小团圆的梦》与《小团圆的秘密》的KL距离4.099975168E9 
《小团圆的秘密》与《秘密情报战》的KL距离 3.029988864E9 
《秘密情报战》与《小团圆的秘密》的KL距离 4.289972736E9 
《小团圆的梦》与《秘密情报战》的KL距离4.10997504E9 
《秘密情报战》与《小团圆的梦》的KL距离3.539982336E9
b.以词为单位计算结果如下： 
《小团圆的秘密》与《小团圆的梦》的KL距离： 5.629955584E9 
《小团圆的梦》与《小团圆的秘密》的KL距离8.62991872E9 
《小团圆的秘密》与《秘密情报战》的KL距离 6.50994432E9 
《秘密情报战》与《小团圆的秘密》的KL距离 8.029924864E9 
《小团圆的梦》与《秘密情报战》的KL距离9.219941376E9 
《秘密情报战》与《小团圆的梦》的KL距离7.739928576E9
从上面结果可以看出：《小团圆的秘密》与《小团圆的梦》之间距离最近，《秘密情报战》与《小团圆的梦》直接的概率分布距离近于《秘密情报战》与《小团圆的秘密》之间的概率分布。因此就可以判定两个小团圆说的更相近。 
还有一些交叉熵的细节请参阅[交叉熵](http://www.cnblogs.com/awishfullyway/p/6068565.html)。
# 4. 交叉熵
通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。交叉熵的公式为： 
$H(X,q)=H(X)+D(p||q)=-\displaystyle\sum_{x∈X}p(x)logq(x)$
一般的交叉熵主要用作损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。
## 4.1 交叉熵能够作为代价函数的原因
交叉熵能够作为代价函数的原因如下：
第一，  他是非负的，即C>0..交叉熵函数中的求和的所有独立项都是负数，因为其中的对数函数的定义域是（0,1）；在求和前面有一个负号。
第二，  对于所有的训练输入x，如果神经元实际的输出接近目标值，那么交叉熵将会接近0 。假设在这个例子中y = 0 而a ≈ 0。这正是我们想得到的结果。实际输出和目标输出之间的差距越来越小，最终的交叉熵的值就越低了。
综上所述，交叉熵是非负的，在神经元达到很好的正确率的时候会接近0。这些实际就是我们想要的代价函数的特性。这些特性也是二次代价函数具备的，所以交叉熵可以作为代价函数使用。
## 4.2 交叉熵能够替代二次代价函数的原因
交叉熵作为损失函数的最大好处在于使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。
通常来讲，我们在训练神经网络模型的时候通常采用的是二次代价函数，也就是方差代价函数作为代价函数，就是长这样的： 
$C=\frac{(y-a)^2}{2}$
其中y是预期输出，a是实际输出。（$a=\sigma(z) ,z=w\times x+b$） 
然后是这样计算代价函数对w和b的导数的： 
$\frac{\partial C}{\partial \omega}=(a-y)\sigma'(z)\times x$
$\frac{\partial C}{\partial b}=(a-y)\sigma'(z)$
其中，z表示神经元的输入，表示激活函数。从以上公式可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数，如下图所示： 
![这里写图片描述](https://img-blog.csdn.net/20160730214519794?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
这就导致了在误差很大和误差很小的时候，梯度下降非常慢，这也是为什么这种学习速度会非常慢的原因，而且，这种情况也不符合我们的期望：错误越大，改正的幅度越大，从而学习得越快。
如果换用交叉熵作为代价函数，即：$C=-\frac{1}{n}\displaystyle \sum_x[ylna+(1+y)ln(1-a)]$
这样，就可以对梯度的下降速度就可以达到我们想要的效果： 
$\frac{\partial C}{\partial \omega_j}=-\frac{1}{n}\sum_x(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)})\frac{\partial \sigma}{\partial \omega_j}$
$=-\frac{1}{n}\sum_x(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)})\sigma'(z)x_j$
$=\frac{1}{n}\sum_x(\frac{\sigma'(z)x_j}{\sigma(z)(1-\sigma(z))}(\sigma(z)-y))$
$=\frac{1}{n}\sum_xx_j(\sigma(z)-y)$
同理，b的梯度为： 
$\frac{\partial C}{\partial b}=\frac{1}{n}\sum_x(\sigma(z)-y)$
由上式可以知道，误差越大，其迭代的幅度越大，因此，一般来讲，使用交叉熵会比二次代价函数的训练效果要好很多。
## 4.3 交叉熵的代价函数来源
交叉熵的代价函数是如何产生的？假设在使用二次代价函数实践时发现学习速度下降，并且解释了其原因是由于σ′(z)项，在观察公式之后，可能会想到选择一个不包含σ′(z)的代价函数，那么对于训练样本x，其代价函数能够满足： 
$\frac{\partial C}{\partial \omega_j}=x_j(a-y)$
$\frac{\partial C}{\partial b}=(a-y)$
于是乎，由链式法则可以得到： 
$\frac{\partial C}{\partial b}=\frac{\partial C}{\partial a}a(1-a)$
因此就可以有： 
$\frac{\partial C}{\partial a}=\frac{a-y}{a(1-a)}$
对方程积分可得： 
$C=-[y\times lna+(1-y)\times ln(1-a)]+constant$
然后整个代价函数就为： 
$C=-\frac{1}{n}\sum_x[y\times lna+(1-y)\times ln(1-a)]+constant$
交叉熵函数的形式是$-[y\times lna+(1-y)\times ln(1-a)]$而不是$-[a\times lny+(1-a)\times ln(1-y)]$，为什么？因为当期望输出的y=0时，lny没有意义；当期望y=1时，ln(1-y)没有意义。而因为a是sigmoid函数的实际输出，永远不会等于0或1，只会无限接近于0或者1，因此不存在这个问题。
对数似然函数也常用来作为softmax回归的代价函数，在上面的讨论中，我们最后一层（也就是输出）是通过sigmoid函数，因此采用了交叉熵代价函数。而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的是代价函数是log-likelihood cost。
其实这两者是一致的，logistic回归用的就是sigmoid函数，softmax回归是logistic回归的多类别推广。log-likelihood代价函数在二类别时就可以化简为交叉熵代价函数的形式。
