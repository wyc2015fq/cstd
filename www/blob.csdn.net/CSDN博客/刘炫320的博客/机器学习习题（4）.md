# 机器学习习题（4） - 刘炫320的博客 - CSDN博客
2017年07月15日 09:27:38[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1362
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
# 1. 前言
今天状态还不错。
# 2. 习题1（SPSS基础）
> 
SPSS的界面中，以下是主窗口是（ ）
A.语法编辑窗口
B.数据编辑窗口
C.结果输出窗口
D.脚本编辑窗口
正确答案：B
解析：SPSS是属于数据分析软件，当然主窗口是在数据编辑上。
# 3. 习题2（分类与聚类）
> 
以下哪些方法不可以直接来对文本分类？
A.Kmeans
B.决策树
C.支持向量机
D.KNN
正确答案： A
解析：
聚类是无监督的，它之所以不能称为分类是因为它之前并没有类别标签，因此只能聚类。
复习一下K-means算法，主要分为赋值阶段和更新阶段。算法步骤：
（1）随机选择K个点作为初始的质心
（2）将每个点指配到最近的质心
（3）重新计算簇的质心，直到质心不再发生变化   。
K均值容易陷入局部最小值，无法表示类的形状，大小和宽度，是一种硬分类算法，针对它的这些缺点，提出了二分K均值和软K均值。
其他3个都是常见的分类方法。
# 4. 习题3（特征选择）
> 
机器学习中做特征选择时，可能用到的方法有？
A.卡方
B.信息增益
C.平均互信息
D.期望交叉熵
正确答案：ABCD
解析：
卡方是传统的常见的数理统计学上的特征，信息增益在决策树中运用的特别多，互信息在新词发现上有用到，而交叉熵最常见的就是损失函数。
具体的，可以参考《[卡方详解](http://blog.csdn.net/fighting_one_piece/article/details/37908235)》、《[信息增益详解](http://blog.csdn.net/fighting_one_piece/article/details/37908535)》、《[期望交叉熵详解](http://blog.csdn.net/fighting_one_piece/article/details/38562183)》、《[互信息详解](http://blog.csdn.net/fighting_one_piece/article/details/38304817)》。另外，还有其他特征可以选择，详见《[特征选择汇总](http://blog.csdn.net/fighting_one_piece/article/details/38441865)》。
# 5. 习题4（分类与聚类）
> 
以下描述错误的是：
A.SVM是这样一个分类器，他寻找具有最小边缘的超平面，因此它也经常被称为最小边缘分类器（minimal margin classifier）
B.在聚类分析当中，簇内的相似性越大，簇间的差别越大，聚类的效果就越差。
C.在决策树中，随着树中结点数变得太大，即使模型的训练误差还在继续减低，但是检验误差开始增大，这是出现了模型拟合不足的问题。
D.聚类分析可以看作是一种非监督的分类。
正确答案：ABC
解析：
A. SVM的策略是最大间隔分类器。
B. 簇内的相似性越大，簇间的差别越大，聚类的效果就越好。你想啊，分类或者聚类效果的好坏其实就看同一类中的样本相似度，当然是越高越好，说明你分类越准确。
C. 训练误差减少与测试误差逐渐增大，是明显的过拟合的特征。
# 6. 习题5（先验概率）
> 
统计模式分类问题中，当先验概率未知时，可以使用()
A.最小最大损失准则
B.最小误判概率准则
C.最小损失准则
D.N-P判决
正确答案：AD
解析：
A. 考虑$p(w_i)$变化的条件下，是风险最小
B. 最小误判概率准则， 就是判断$p(w_1|x)和p(w_2|x)$哪个大，x为特征向量，$w_1和w_2$为两分类，根据贝叶斯公式，需要用到先验知识。
C. 最小损失准则，在B的基础之上，还要求出$p(w_1|x)和p(w_2|x)$的期望损失，因为B需要先验概率，所以C也需要先验概率。
D. N-P判决，即限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算$p(x|w_1)和p(x|w_2)$的比值，不需要用到贝叶斯公式。
# 7. 小结
本章主要介绍了SPSS基础、分类与聚类问题、特征选择与概率论中先验概率的问题。
