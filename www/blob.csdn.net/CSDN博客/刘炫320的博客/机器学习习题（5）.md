# 机器学习习题（5） - 刘炫320的博客 - CSDN博客
2017年07月16日 12:13:42[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1506
所属专栏：[机器学习习题集](https://blog.csdn.net/column/details/16442.html)
# 1. 前言
书到用时方恨少！
# 2. 习题1（矩阵相乘）
> 
深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为$m*n，n*p，p*q，且m<n<p<q$，以下计算顺序效率最高的是（） 
  A.(AB)C
B.AC(B)
C.A(BC)
D.所以效率都相同
正确答案：A
解析： 
首先，根据简单的矩阵知识，因为 A*B ， A 的列数必须和 B 的行数相等。因此，可以排除 B 选项， 
然后，再看 A 、 C 选项。在 A 选项中，$ m*n$ 的矩阵 A 和$ n*p $的矩阵 B 的乘积，得到 $m*p $的矩阵 A*B ，而 $A*B $的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 $m*n*p $次乘法运算。同样情况分析 A*B 之后再乘以 C 时的情况，共需要 $m*p*q $次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是 $m*n*p+m*p*q$ 。同理分析， C 选项 A (BC) 需要的乘法次数是 $n*p*q+m*n*q $。 
由于$ m*n*p< m*n*q ， m*p*q<n*p*q $，显然 A 运算次数更少，故选 A 。
# 3. 习题2（贝叶斯）
> 
Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:() 
  A.各类别的先验概率P(C)是相等的
B.以0为均值，sqr(2)/2为标准差的正态分布
C.特征变量X的各个维度是类别条件独立随机变量
D.P(X|C)是高斯分布
正确答案：C
解析：
朴素贝叶斯的条件就是每个变量相互独立。
# 4. 习题3（支持向量机）
> 
关于支持向量机SVM,下列说法错误的是（） 
  A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力
B.Hinge 损失函数，作用是最小化经验分类错误
C.分类间隔为1/||w||，||w||代表向量的模
D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习
正确答案：C
解析：
A正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 
B正确。 
C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。 
D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和$a_i*y_i*x_i$，a变小使得w变小，因此间隔2/||w||变大
# 5. 习题4（HMM）
> 
在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计() 
  A.EM算法
B.维特比算法
C.前向后向算法
D.极大似然估计
正确答案：D
解析： 
EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法
维特比算法： 用动态规划解决HMM的预测问题，不是参数估计
前向后向算法：用来算概率
极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数
注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。
# 6. 习题5（贝叶斯）
> 
假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是： 
  A.这个被重复的特征在模型中的决定作用会被加强
B.模型效果相比无重复特征的情况下精确度会降低
C.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。
D.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题
E.NB可以用来做最小二乘回归
F.以上说法都不正确
正确答案：BD
解析：暂时没有很好的完整解析。
# 7. 小结
我们这一章中，主要讲解了矩阵相乘、贝叶斯、支持向量机和隐马尔可夫模型的相关习题。
