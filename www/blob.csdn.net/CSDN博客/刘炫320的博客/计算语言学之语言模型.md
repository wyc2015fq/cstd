# 计算语言学之语言模型 - 刘炫320的博客 - CSDN博客
2017年04月12日 15:55:09[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1216
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 0. 写在前面
这一章我们介绍语言模型。不过要说的是，这里的语言模型基本上是基于字词的，但是其思想也是要掌握的，如果以后到句子、段落、篇章的时候，这些思想都是十分有用的。
# 1. 语言模型
语言模型（LM）在自然语言处理中占有重要地位，而且像n元语法模型是一个简单但是比较有效的模型。只能说比较有效，但是想要提高到非常高的地步，还需要继续改进才行。
## 1.1 n元语法
我们正常人的思维，肯定是这样想的，一句话的每个单词，都会与之前所有出现的词相关，甚至是与后面出现的词也相关（双向RNN）。
而一个语言模型通常构建为字符串s的概率分布p(s)，这里p(s)试图反应的是字符串s作为一个句子出现的频率。对于一个由l个基元（基元就是基本单元，这里一般指字、词、短语，没有再大的了）构成的句子$s=w_1w_2···w_l$其概率计算公式可以表示为： 
$p(s)=p(w_1)p(w_2|w_1)p(w_3|w_1w_2)···p(w_l|w_1···w_{l-1})$
$=	\prod_{i=1}^l p(w_i|w_1···w_{i-1})$
上面就是n元语法，它只考虑前n-1个词与当前词的关系，而且n的取值一般是1,2,3,…,7等这种比较小的数。之所以这样做，当然是为了简化计算，因为如果我们考虑的前n个词过多的话，那么我们的自由参数都是几何式增长，计算机一是训练不来，二是根本没有这么多语料可供我们使用。
### 1.1.1 一元文法
一元文法就是n=1,也就是只考虑当前词，这样的话，就相当于是统计词频了。没有什么太大价值。
### 1.1.2 二元文法
二元文法则是n=2，这就有价值了，我们称为是一阶马尔科夫链。因为有一个概率是我们能够看到的，但是会影响最终结果的： 
$p(s)\approx \prod_{i=1}^l p(w_i|w_{i-1})$
而$p(w_i|w_{i-1})=\frac{c(w_{i-1}w_i)}{ \displaystyle \sum_{w_i}c(w_{i-1}w_i)}$这时最大似然估计。
如果是这样子，那么它就是这样一个样子，需要在头和尾分别添加一个开头标记和结尾标记$w_0和w_l$的内容。
### 1.1.3 三元文法
三元文法则是n=3，这时平时用的比较多的，我们称为二阶马尔科夫链。同样的，它的样子我们也可以写出来： 
$p(s)\approx \prod_{i=1}^l p(w_i|w_{i-1}w_{i-2})$
但有时候，如果数据太过稀疏的话，我们可能要考虑数据平滑了。 
或者说可以使用下面式子来近似： 
$ p(w_i|w_{i-1}w_{i-2})=q_1f(w_i|w_{i-1}w_{i-2})+q_2f(w_i|w_{i-1})+q_3f(w_i)$
其中$q_1+q_2+q_3=1$，而且$f(w_i|w_{i-1}w_{i-2})、f(w_i|w_{i-1})、f(w_i)$是通过训练语料得到的相对频率。
## 1.2 语言模型评价
评价一个语言模型的性能通常就是使用交叉熵或者困惑度来进行， 
一个n元文法，总结来讲，可以使用如下公式来统一： 
$p(s)\approx \prod_{i=1}^{l+1} p(w_i|w_{i-n+1}^{i-1})$
那么$p(w_i|w_{i-n+1}^{i-1})=p(w_i|w_{i-1})=\frac{c(w_{i-n+1}^{i-1})}{ \displaystyle \sum_{w_i}c(w^{i-1}_{i-n+1})}$
其中$\displaystyle \sum_{w_i}c(w^{i-1}_{i-n+1})$表示的是出现次数总和。
那么对于句子（$t_1,t_2,···,t_{l_T}$）构成的测试集T，可以通过T中所有句子的概率乘积来计算测试集的概率P（T）。 
那么交叉熵$H_p(T)的定义为$
$H_p(T)=-\frac{1}{W_T}log_2p(T)$
这里的$W_T$是以词为单位度量的文本T的长度。这一个式子可以作如下解释： 
利用与模型有关的压缩算法对数据集合中的W_T个词进行编码，每一个编码所需要的平均比特位数。
而困惑度只需要是这样的： 
$PP_T(T)=2^{H_p(T)}$
交叉熵和困惑度都是越小越好，一般的困惑度为50~1000之间。
# 2. 数据平滑
上面我们说过了N元文法最大的缺陷就是容易引起数据稀疏，也就是说，有可能一些情况的概率是0，那么解决办法就是数据平滑。我们这里简要介绍一下几种数据平滑方法。
## 2.1 加法平滑
加法平滑是非常常用而又特别简单的平滑方法，其重要含义，就是给所有的可能都增加一个基础概率，这个基础概率就是让每种可能均分那么一点概率。数学表达式如下： 
$p_{add}(w_i|w^{i-1}_{i-n+1})=\frac{\delta+c(w^{i-1}_{i-n+1})}{\delta |V| +\displaystyle \sum_{w_i}c(w^{i-1}_{i-n+1})}$
## 2.2 古德-图灵估计法
上面的加法平滑，实际上是改变了已出现的概率情况，它把每个概率都减少了不同的值，但并没有按比例减少。下面这种古德——图灵估计法则是按比例减少已出现的概率，然后再均分给那些没有出现的概率的情况。
## 2.3 Katz平滑方法（属于后备平滑）
这种方法是当某一事件在样本中出现的概率大于k时，运用最大似然估计经过减值来估计其概率。当某一事件的频率小于k时，使用低阶的语法模型作为代替高阶语法模型的后备，然后再使用归一化来进行处理。
## 2.4Jelinek-Mercer平滑方法
这种方法，我们在上面也提过了，例如对于三元语法：
$ p(w_i|w_{i-1}w_{i-2})=q_1f(w_i|w_{i-1}w_{i-2})+q_2f(w_i|w_{i-1})+q_3f(w_i)$
这样处理就是Jelinek-Mercer平滑方法。
## 2.5 绝对减值法
绝对减值法，就是对于已有概率的事件，减去相同的量，然后再把这匀出来的概率均分给未见的情况。
## 2.6 Kneser-Ney平滑方法
这种平滑方法是对于已有概率的事件，按比例减去不同的量，然后再把这匀出来的概率均分给未见的情况。
# 3. 语言模型的自适应
N元模型的缺点我们都十分明确，也就是它只考虑了与前n-1个词有关，但这显然和实际情况不符。
为此，需要进行语言模型自适应，这里提供3种模型。
## 3.1. 基于缓存的语言模型
基于换粗你的语言模型的原理就是，之前出现过的词，再出现的概率会更大，这个编程过程中的“局部性原理”差不多。因此我们需要建立一个缓存区，记录下之前出现过的词语。因此模型就变成了这样： 
$ p(w_i|w^{i-1}_1)=\lambda p_{cache}(w_i|w^{i-1}_1)+\lambda p_{n-gram}(w_i|w^{i-1}_1)$
$\lambda $可以通过EM算法求得。 
$p_{cache}(w_i|w^{i-1}_1)=\frac{1}{k}\displaystyle \sum_{j=i-k}^{i-1}I(w_j=w_i)$
但同样的，未考虑缓存区内的顺序，因此有改进： 
$p_{cache}(w_i|w^{i-1}_1)=\frac{1}{k}\beta \displaystyle \sum_{j=i-k}^{i-1}I(w_j=w_i)e^{-\alpha (i-1)}$
其中$\alpha$为衰减率，$\beta$为归一化常数 
当然，还有其他改进模型，我就不一一举例。
## 3.2. 基于混合的语言模型
基于混合的语言模型是为了解决可能训练语料不是同源的，但是为了保证测试效果比较好而做出的这种办法。 
它的想法是，对于每一个来源相同的子语料$S_i$，训练处不同的子模型$M_i$，然后通过求和得出最终模型： 
$ p(w_i|w^{i-1}_1)=\displaystyle \sum_{j-1}^n\lambda_j p_{M_j}(w_i|w^{i-1}_1)$
其中：$\displaystyle \sum_{j-1}^n\lambda_j=1$这个可以通过EM算法，使得困惑度最低。
## 3.3. 基于最大熵的语言模型
基于最大熵的语言模型则是通过一组不同约束的模型，从中选择熵最大的模型。 
例如可以是距离为1的标准二元模型，或者是距离为2的二元模型。然后再通过线性插值，使用后备方法进行数据平滑。
# 4. 小结
事实上，在网上关于这方面的描述还是很少的。大多集中于科研论文中。因此难度还是挺大的。可能只有专攻计算语言学的，才会细致的研究这方面内容吧。
