# 2019斯坦福CS224n深度学习自然语言处理笔记（1）——绪论与Word2Vec - 刘炫320的博客 - CSDN博客
2019年04月04日 13:15:57[刘炫320](https://me.csdn.net/qq_35082030)阅读数：91
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)

### 文章目录
- [1.绪论](#1_4)
- [2. 语言的来源](#2__6)
- [3. 符号理论的表示及运算方法](#3__21)
- [4. Word2Vec推导过程](#4_Word2Vec_42)
课程视频链接《[深度学习与自然语言处理（1）](https://www.bilibili.com/video/av46216519/?p=1)》
——————————————————————————
本文内容整理自2019年斯坦福CS224n深度学习自然语言处理课程，其笔记为本人听课心得，重点在于对于知识内容的思考，并非课程原文笔记，应称为课后笔记。
# 1.绪论
在本堂课中，其基础技能需要懂得并应用：Ipython，numpy和Pytorch。其他的关于自然语言处理和深度学习，上了这堂课，你就会了解。
# 2. 语言的来源
语言，语言是传递信息的声音和文字，是人类沟通的主要方式（其他方式包括图像、触感等）。它作为人类文明的载体，距今已有5000多年的历史。如此古老的本领，在历史的长河之中，没有出现质的改变（功能没有改变，方式没有改变）。
在信息和信息传播活动，人类历史中共有五次巨大变革：
第一次：语言的诞生，是历史上最伟大的信息技术革命，语言使人类描述信息、交流信息的方式得到了大大的改进．
第二次：文字的诞生，为了便于交流和长期存储信息，就要创造一些符号代表语言，这些符号经过相当长的一段时间后逐渐演变成文字，并固定下来．
第三次：印刷术的诞生，使得知识可以大量生产、存储和流通，进一步扩大了信息交流的范围．
第四次：电磁波的应用，电磁波的应用使得信息传递的速度大大提高．
第五次：计算机技术的应用，使人们对信息的处理能力、处理速度产生了飞跃．
然而，当今自然语言（人类使用的语言，包含以上第一次、第二次革命的结果），相较于当今5G的网络传播速度（第五次革命），是相当的缓慢，但是仍然被我们人类广泛使用，那这是为什么？这是因为语言虽然说的东西少，但是听的内容多。这其中的信息增益，来源于对于世界的认知。
所谓的意思（Meaning）指两个方面，一方面是单词本身的意思（Representation for words），另一方面是使用单词想要表达的意思（Express by using words）。例如，我渴了。句子本身是说我缺少水分，而另一个含义是，你需要给我倒杯水。这是使用这句话想要表达的意思。
从语言学角度来说，文字等价于符号(signifier)，也就是符号化所要表示的意思(signified the idea thing)。从这个角度讲，符号学派的理论就自然站得住脚。
# 3. 符号理论的表示及运算方法
基于符号理论，一个比较著名的应用是WordNet,NLTK工具包中包含这个应用，它将单词之间的关系描述为同义词和上位词（synonym set and hypernyms）， 也就是什么是什么的关系。这样就能够很好的解决不同符号所表示的不同含义及其之间的关系。在中文中，也存在同样的工具（知网，即HowNet），不是大家所熟知的论文检索网站。
这样，WordNet就可以获得层次化的单词间的关系，但是同样存在以下几个问题：
- 缺少细微差别，一词多义
- 无法添加新的含义
- 构建过程过于主观
在文本表示上，如上所讲，已经有一定的方法，接下来需要解决的是如何进行语言间的运算（计算机要做的事）。
传统的自然语言处理（NLP）方式中，将单词看作为离散的符号（discrete symbols），就像一个词典一样，一个词对应一个编号，更一般的，使用独热（One-Hot）编码的形式。这样，符号就可以转换为数值进行运算。
同样的，该种方法同样存在一些问题，例如，词汇表太大，英文词汇超过50万个。不能够计算相似度（一种方法是使用wordnet，另一种方法就想去学习一种基于向量的表示方式）
那么，如何去基于单词向量本身学习呢？在1957年，一种理论提出：词汇的含义来源于其上下文（word’s means is given by it’s context）。跟随着这个思路，从神经语言模型（2003）到Word2Vec（2013）的道路就都说的通了。
神经网络模型的目的是，将单词进行分布式表示，即把词映射到一个向量空间中，使得相似的词拥有相似的位置。Word2Vec模型具有以下特点：
- 拥有大规模语料库
- 词使用分布式向量表示
- 对于每一个文本，均有一个中心词和一个上下文。
- 使用相似度计算中心词和上下文的概率
下面就是“硬核”Word2Vec的推导过程。
# 4. Word2Vec推导过程
我们使用Skip-Gram模型举例说明，
首先，Word2Vec同样使用极大似然估计，就像我们上面所说，它需要使得给定中心词，使得上下文词出现概率最大，即：
$L(\theta)=\prod P(w_{t+i}|w_t;\theta)$
为了更容易计算，我们设计的目标函数如下：
$J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}\sum^{T}_{t=1}\sum_{-m\le j\le m }logP(w_{t+i}|w_t;\theta)$
最小化目标函数就是最大化似然函数，自然就是最大化准确率。
这就自然而然的就引出计算问题，如何计算$P(w_{t+i}|w_t;\theta)$?
我们曾经遇到的极大似然估计，大部分都是用频率来近似概率获得其P，这里并不能这样。
我们定义$v_w$是中心词的词向量，$u_w$是上下文的词向量，也就是说，其实同一个词，即有作为中心词的时候的词向量，也有作为上下文时的词向量。这样做只是为了后面更好的计算和操作，使用单个向量也是能够解决这个问题。
于是，我们的概率计算公式就为:
$P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w \in V}exp(u_w^Tv_c)}$
其中o表示目标词，V表示词表。其含义为，上面为两个词的矩阵的乘积最大，下面为正则项。这就有点像softmax函数一样。此处，我们先穿插一点，可以看到，其实这种计算方法就是使得越共现的词，向量乘积越大,乘积越大，则概率越大。最终会实现和某一个词的相关词和其都相似，就使得这些相关词向量更加相似。
另一个问题是，我们这里所谓的$\theta$是谁？答案是$v_c$，即我们是对其求$v_c$的偏导，首先我们先写出我们目标函数：
$\frac{\partial }{\partial v_c}logP(o|c)=\frac{\partial }{\partial v_c}log\frac{exp(u_o^Tv_c)}{\sum_{w \in V}exp(u_w^Tv_c)}$
$=\frac{\partial }{\partial v_c}logexp(u_o^Tv_c)-\frac{\partial }{\partial v_c}log{\sum_{w \in V}exp(u_w^Tv_c)}$
此时我们可以分开计算被减数和减数的偏导
$\frac{\partial }{\partial v_c}logexp(u_o^Tv_c)=u_o$
$\frac{\partial }{\partial v_c}log{\sum_{w \in V}exp(u_w^Tv_c)}=\frac{\sum_{x \in V}exp(u_x^Tv_c) u_x}{\sum_{w \in V}exp(u_w^Tv_c)} $
因此最终的目标函数为：
$\frac{\partial }{\partial v_c}logP(o|c)=u_o-\frac{\sum_{x \in V}exp(u_x^Tv_c) u_x}{\sum_{w \in V}exp(u_w^Tv_c)}$
$=u_o-\sum_{x \in V}\frac{exp(u_x^Tv_c) u_x}{\sum_{w \in V}exp(u_w^Tv_c)}$
$=u_o-\sum_{x \in V}P(x|c)u_x$
这样就是可以计算的了，更新$\theta$时的方法如下，但是所有的参数很多，想直接更新会计算非常复杂。
$\theta^{new}=\theta^{old}-\alpha \bigtriangledown_{\theta}J(\theta)$
解决方法是，我们采样一个窗口内的单词来更新，并不更新所有的单词。
在SG模型中的负采样方法则如下：
$J_{neg-sample}=-log(\sigma(u_o^{T}v_c))-\sum_{k-1}^{K}log(\sigma(-u_k^Tv_c))$
其中K为我们采样的单词，根据单词采样概率获得，其单词采样概率计算方法如下，这样可以使得概率更小的单词更会被采样：
$P(w)=\frac{U(w)^{\frac{3}{4}}}{Z}$
至此我们的SG模型就已经讲述完毕了。
从以上我们可以看出，为什么不直接使用词共现矩阵来做这个事情呢？这个下节课会讲到。
