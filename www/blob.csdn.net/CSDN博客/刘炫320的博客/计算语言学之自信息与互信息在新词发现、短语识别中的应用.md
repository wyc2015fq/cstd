# 计算语言学之自信息与互信息在新词发现、短语识别中的应用 - 刘炫320的博客 - CSDN博客
2017年06月05日 22:23:31[刘炫320](https://me.csdn.net/qq_35082030)阅读数：1809
所属专栏：[自然语言处理（计算语言学）概论](https://blog.csdn.net/column/details/16369.html)
# 1. 引言
在[计算语言学预备知识](http://blog.csdn.net/qq_35082030/article/details/60882746)中，我们介绍了熵（自信息）与互信息；条件熵与联合熵；相对熵与交叉熵。这里我们要说明的是关于其在自然语言中的具体应用。
# 2. 熵（自信息）与互信息
现在我们先来复习以下熵及互信息的公式定义：
熵（自信息）：$H(P)=-\displaystyle\sum_{x∈R}p(x)log_2p(x)$
所谓的自信息，就是指描述一个随机变量自身的不确定性，如果熵越大，它的不确定性越大，正确估计其值的可能性就越小。
互信息：$I(X;Y)=H(Y)-H(Y|X)=H(X)-H(X|Y)$
这个I(X;Y)反应的是再知道了Y的值以后X的不确定性减少量，可以理解为Y的值透露了多少关于X的信息量。
这两个我们在之前的预备知识中，都已经讨论过了。现在拿出来，主要是为了理解如何在自然语言应用中使用他们。一般来讲，互信息的使用范围广泛，包括词汇聚类、汉语分词、新词短语发现、词义消歧中都有应用。
# 3 新词发现中的应用
这里，我们使用汉语短语识别的例子来介绍。首先用的例子是[基于互信息和左右信息上的短语提取识别](http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html#%E7%9F%AD%E8%AF%AD%E6%8F%90%E5%8F%96)，在这里，可以很清楚的看到互信息是如何被使用并用在短语提取识别中的。
## 3.1word2vec中的新词发现方法
在正式讲之前，我们先使用一个简单的公式来介绍一下如何进行新词发现。这里，我们使用[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)中的方法。 
下面看公式： 
$score(w_i,w_j)=\frac{count(w_iw_j)-\delta}{count(w_i)\times count(w_j)}$
这里的$w_i和w_j$都是某个词，而$\delta$则是被用来作为减少出现偶见词的可能。例如$\delta=3$，则出现3次以下的二阶共现词就会被过滤掉。
当然，如果只是使用一次的话，那么这个可能没有太大的用处，因为它只能组合二阶共现词，也就是说，一个短语只能由2个词共现，那么在论文中，作者是使用这个公式进行2-4次循环实验，也就是说，把合成后的结果再进行一次实验，并且降低$\delta$的值保证长短语会被发现，这是一个非常巧妙的方法。
## 3.2 hanks的互信息+左右熵的方法
好了，言归正传，我们介绍如何使用互信息来进行短语识别的。
1）首先，要对文本进行基本分词。然后统计一阶共现、二阶共现和三阶共现。如果需要进行大规模统计，则需要和作者一样，使用前缀树来存储词和词频，这样会快很多。（前缀树也成为Trie树，是自然语言处理的基本数据结构）这里注意一点，作者的一阶共现使用的是字典的频度，而并非实验样本中一阶共现。
2）统计完成后只需要计算每两个词语的互信息、左熵和右熵即可。 
对于二元互信息的计算，和我们上面的互信息的公式有点差别： 
$MI(x,y)=log_2{\frac{P(x,y)}{P(x)P(y)}}$
公式中的x和y指的是两个相邻的单词，P值是它的出现概率。也就是说，如果这个二元互信息越大，则这两个词的相互影响越大，也就是说明这有可能是个新词。
而另一个特征就是左右熵，也就是嫌疑新词的左右边界的显著性。例如“屌丝”这个词的左右熵就都很大，因此这个词有可能是个新词。左右熵的计算公式如下： 
$E_l(W)=-\sum_{\forall a \in A}P(aW|W)\times log_2P(aW|W)$
$E_R(W)=-\sum_{\forall b \in B}P(Wb|W)\times log_2P(Wb|W)$
这里的aW和Wb分别表示的就是嫌疑新词W和左边的一个词与右边的一个词的共现。
3）计算完成互信息与左右熵后，作者是简单的把这3者相加，然后取TopN即可。
这样，就完成了新词发现功能。
## 3.3 基于双字耦合度+t-测试差的方法
然而事实上，新词发现的功能不仅仅可以使用互信息与左右熵，早在2007年时，贺敏,龚才春,张华平等的《一种基于大规模语料的新词识别方法》中就提出了使用双字耦合度来进行新词发现。几乎同时，王思力，王斌的《基于双字耦合度的中文分词交叉歧义处理方法》也在中文信息学报刊登，其中除了介绍双字耦合度外，还介绍了t-测试差。最终的实验结果表明：双字耦合度+t-测试差要好于互信息+t-测试差。
双字耦合度(Coupling Degree of Double Character,CDDC)是为了描述一个词中连续两个字的结合紧密程度，其定义如下：设$c_i,c_{i+1}$是两个连续出现的汉字,统计语料库中$c_i,c_{i+1}$连续出现在一个词中的次数和连续出现的总次数,二者之比就是$c_i,c_{i+1}$的双字耦合度。其计算公式如下： 
$Couple(c_i,c_{i+1})=\frac{N(...c_ic_{i+1}...)}{N(...c_ic_{i+1}...)+N(...c_i|c_{i+1}...)}$
t-测试差是Church借鉴统计学中t检验的思想,在《 Using statistics in lexical analysis》中提出了用于计算词之间结合紧密度的t公式，而孙茂松,黄昌宁,邹嘉彦等在《利用汉字二元语法关系解决汉语自动分词中的交集型歧义》中对t公式进行了改进，是的其适用于中文分词研究。修改后的t公式为： 
$t_{x,z}(y)=\frac{p(z|y)-p(y|x)}{\sqrt{\sigma^2(p(z|y))+\sigma^2(p(y|x))}}$
其中，$t_{x,z}(y)$表示汉字字符串xyz的t-测试值，其中$P(z|y)$和$p(y|x)$分别是字符串yz和xy的Bigram概率，$\sigma^2(p(z|y))$和$\sigma^2(p(y|x))$分别是二者的方差。其结果表示如下，如果值大于0，则y与z结合的可能性更大一些，如果小于0，则y与x的结合可能性更大，如果等于0，则两者差不多。
最终的计算公式如下： 
$t_{x,z}(y)=\frac{\frac{r(y,z)}{r(y)}-\frac{r(x,y)}{r(x)}}{\sqrt{\frac{r(y,z)}{r^2(y)}+\frac{r(x,y)}{r^2(x)}}}$
其中，$r(x,y)$和$r(y,z)$分别表示语料库中xy和yz连续出现的次数，r(x)、r(y)分别表示x,y在语料库中出现的次数。
因为t公式求的是汉字结合强度的比较值,涉及到两个字间位置,这个值是基于字的,并非基于字间位置,因此不便于直接用来计算某个字间位置连与断的概率。因此孙茂松提出了t-测试差的概念。具体计算如下： 
定义字符串xyzw中的yz之间的t-测试差为： 
$\Delta t=t_{x,z}(y)-t_{y,w}(z)$
如果结果大于0，则表示yz倾向于判断连接，反之为断开。
# 4 小结
到此为止，我们就介绍完成了新词发现的相关工作。在下一章中，我们会介绍关于条件熵与联合熵、相对熵与交叉熵的部分应用。
