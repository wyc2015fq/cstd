# ML的45问（3）——神经网络与感知器法则、反向传播算法 - 刘炫320的博客 - CSDN博客
2017年04月15日 15:47:14[刘炫320](https://me.csdn.net/qq_35082030)阅读数：470标签：[神经网络																[梯度下降																[反向传播																[归纳偏置																[神经元](https://so.csdn.net/so/search/s.do?q=神经元&t=blog)](https://so.csdn.net/so/search/s.do?q=归纳偏置&t=blog)](https://so.csdn.net/so/search/s.do?q=反向传播&t=blog)](https://so.csdn.net/so/search/s.do?q=梯度下降&t=blog)](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)
个人分类：[ML](https://blog.csdn.net/qq_35082030/article/category/6800875)
所属专栏：[《Machine Learning》重点和难点](https://blog.csdn.net/column/details/16516.html)
# 0. 写在前面
今天我们主要介绍关于人工神经网络的相关问题。
# 1. 三种神经网络单元及其形式
人工神经网络有3种基本的神经元，分别是感知器模型、线性单元和Sigmoid单元。
## 1.1 感知器模型
感知器模型是神经网络模型提出来的最早的神经单元之一。它比较简单，如果使用公式来表示的话： 
$O（\overrightarrow{X}）=sgn(\overrightarrow{W}·\overrightarrow{X})$
其中 
$sgn(y)=\begin{cases} 1,  & y>0 \\-1, & other\end{cases}$
这种描述是不是太过于抽象了，但是如果我们换一种描述方式,感知器模型是这样的： 
![感知机模型](https://img-blog.csdn.net/20170415103133842?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
这是感知机模型的图形化表示，而通常的$x_0=1,w_0是阈值$其他的$x_i$才是真正的输入。
这种描述可能还是太高大上了，咱再换一种，但是这种描述并不是太准确。其实就是一个线性拟合。 
$F(X)=w_0+w_1·x_1+w_2·x_2+w_3·x_3+···+w_n·x_n$
如果F（X）>0就输出1，否则输出-1。
很显然，这种感知机的表现能力有限，例如异或这种逻辑，它就没办法表示。这也是当时感知机模型跌入低谷的一个很重要的原因。（当时还没找到多个神经元之间的关系如何传递，例如BP算法。）
## 1.2 线性单元
线性感知单元和感知器模型之间有2个不同，一是取消了结果二值化，二是取消了$x_0=1$这个假设，所有的输入都是真正的输入。于是，其公式就变成了这样： 
$O（\overrightarrow{X}）=\overrightarrow{W}·\overrightarrow{X}$
这样输出的结果也是连续的，线性的，因此成为线性单元。结构图如下： 
![线性单元](https://img-blog.csdn.net/20170415104805802?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 1.3 Sigmoid单元
但是这两种都是有缺陷的，第一种感知器单元是因为结果不连续，没办法进行微分积分，就没法做成多层感知器模型。
而第二种线性单元输入结果虽然连续，但是没有办法进行分类，这就有可能导致最终结果太过离散了。因此，Sigmoid单元就融合了这两个单元的优点。它是这样的： 
$O（\overrightarrow{X}）=sgn(\overrightarrow{W}·\overrightarrow{X})$
其中 
$sgn(y)=\frac{1}{1+e^{-y}}$
这是一个非常著名的S模型的简化版，而且它的求导非常方便。这也是它流行的一个原因。
其结构图如下： 
![Sigmoid单元](https://img-blog.csdn.net/20170415105434728?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
# 2. 训练法则
上面提到了3种神经单元，那么每种神经单元应当对应相应的训练法则，这样才能够让神经单元有用。
## 2.1 感知器法则
感知器法则是特地为了感知器模型而设计的，它是这样修改所有的参数的： 
$w_i \gets w_i+\Delta w_i$
其中$\Delta w_i=\eta(t-o)x_i$
这里，$w_i$是参数，而t是训练样例的目标输出值，o是模型的输出值，而$\eta$是学习速率，也就是每步的步长，通常是一个比较小的数。
而整个训练过程就是把样例一个一个输入进去，不断的调整$w_i$的值，最终停止的时候，所有的训练样例都会满足这个函数。
事实证明，在有限次的使用感知器训练法则后，上面的训练过程会收敛到一个能正确分类所有训练样例的权向量，但是前提是训练样例线性可分，并且使用了充分小的$\eta$。
## 2.2 delta法则
delta法则是为了线性模型而产生的训练法则，它可以发展成多层网络的训练法则（反向传播算法）。
delta法则就是梯度下降法。我们常用的训练误差的表示方式使用的是一个非常容易求导的公式： 
$E(\overrightarrow w)=\frac{1}{2} \displaystyle \sum_{d \in D}(t_d-o_d)^2$
这其实是最小误差平方和的表现形式。而我们最终的评价目标就是使得这个E最小，于是我们就需要求偏导。
$\nabla E(\overrightarrow w)=[\frac{\partial E}{\partial w_0}, \frac{\partial E}{\partial w_1},···，\frac{\partial E}{\partial w_n}]$
这就是梯度，梯度的概念我们在高数中讲过了，就不在这讲原理，只是说，梯度就是权空间的一个向量，确定了使E最陡峭上升的方向。因此梯度下降法就被表述成： 
$w_i \gets w_i+\Delta w_i$
其中
$\Delta w_i=-\eta \frac{\partial E}{\partial w_i}$
但真正计算的时候，不会太麻烦，因为我们刚才的最小误差平方和的式子非常容易求导： 
$\frac{\partial E}{\partial w_i}=\frac{\partial}{\partial w_i}\frac{1}{2}\displaystyle \sum_{d \in D}(t_d-o_d)^2$
$=\frac{1}{2}\displaystyle \sum_{d \in D}\frac{\partial}{\partial w_i}(t_d-o_d)^2$
$=\frac{1}{2}\displaystyle \sum_{d \in D}2(t_d-o_d)\frac{\partial}{\partial w_i}(t_d-o_d)$
$=\displaystyle \sum_{d \in D}(t_d-o_d)\frac{\partial}{\partial w_i}(t_d-\overrightarrow w·\overrightarrow x_d)$
$=\displaystyle \sum_{d \in D}(t_d-o_d)(-x_{id})$
这样我们就得到了最终的结果： 
$\Delta w_i=\eta \displaystyle \sum_{d \in D}(t_d-o_d)(x_{id})$
也就是说，它仅仅是跟输入值，输出值，目标值和学习速率有关，而且运算十分简单。于是，就产生出了下面这个线性单元的梯度下降算法：
GRANDIENT-DESCENT(training_examples,$\eta$) 
    training_examples 中每一个训练样例形式为序偶<$\overrightarrow x,t$>,其中$\overrightarrow x$是输入值向量，t是目标输出值，$\eta$是学习速率。
- 初始化每个$w_i$为某个小的随机值
- 遇到终止条件之前，做以下操作： 
- 初始化每个$\Delta w_i$为0
- 对于训练样例training_examples中的每个<$\overrightarrow x,t$>，做： 
- 把实例$\overrightarrow x$输入到此单元，计算输出o
- 对于线性单元的每个权$w_i$，做
- $\Delta w_i=\Delta w_i+\eta(t-o)(x_{i})$   (*)
- 对于线性单元的每个权$w_i$，做: 
- $w_i=w_i+\Delta w_i $ (**)                              
这是批量梯度下降夏泽，如果是随机梯度下降，只需要在每次更新权$w_i$时候，直接修改权值即可。也就是把公式（**）去掉，然后把公式（*）改为 $w_i=w_i+\eta(t-o)(x_{i})$
对于梯度下降的方式和方法，目前有三种方法，一种是批量梯度下降，也就是所有样例的误差总和全部加起来后，再去修改权值。第二种是随机梯度下降，也就是对每一个样例算出误差后，都会及时的把权值修改掉。第三种则是小批量随机梯度下降，它介于第一种和第三种之间，一次只把若干个（一般是5个）样例的误差总和加起来，修改权值。
当然还有其他的优化误差的方法，也不仅仅是随机梯度下降了。这个大家可以自行去查阅。下面是给出三种方法的对比表格：
|随机梯度下降|小批量态度下降|批量梯度下降|
|----|----|----|
|每个样例的误差来更新权值|几个样例的误差总和来更新权值|所有样例的误差总和来更新权值|
|梯度下降幅度小|梯度下降幅度大|梯度下降幅度最大|
|能以较大概率避免陷入局部最小值|能增加陷入局部最小值的概率|有可能陷入局部最小值|
## 2.3多层神经网络训练法则
前面2种都是单个神经单元的训练法则，下面这个才是真正的多层的神经网络训练法则，正式因为有了它，才让整个人工神经网络又活跃了起来，这就是反向传播算法。 
下面这是2层的，不过多层的也是一样的。 
BACKPROPAGATION（training_examples,$\eta,n_{in},n_{out},n_{hidden}$） 
raining_examples 中每一个训练样例形式为序偶<$\overrightarrow x,t$>,其中$\overrightarrow x$是输入值向量，t是目标输出值，$\eta$是学习速率。$n_{in}$是网络输入的数量，$n_{out}$是网络输出的数量，$n_{hidden}$是隐藏层单元数。
- 创建具有$n_{in}$个输入，$n_{hidden}$个隐藏单元,$n_{out}$个输出单元的网络
- 在遇到终止条件前： 
- 对于训练样例training_examples中的每个<$\overrightarrow x,t$>：
- 把输入沿网络向前传播 
- 把实例$\overrightarrow x$输入网络，并计算网络中每个单元u的输出$o_u$
- 使误差沿网络反向传播 
- 对于网络的每个输出单元k，计算它的误差项$\delta_k$
- $\delta_k \gets o_k(1-o_k)(t_k-o_k)$
- 对于网络的每个隐藏单元h，计算它的误差项$\delta_h$
- $\delta_h \gets o_h(1-o_h)\displaystyle \sum_{k \in outputs}w_{kh}\delta_k$
- 更新每个网络权值$w_{ji}$
- $w_{ji} \gets w_{ji}+\Delta w_{ij}$
- 其中$\Delta w_{ij}=\eta\delta_jx_{ji}$
这样一个随机梯度下降的包含两层sigmoid单元的前馈网络的反向传播算法就完成了。
# 3. 人工神经网络的归纳偏置
其归纳偏置是：在数据点间平滑插值，如果两边都为正例，则中间的值也为正例。
# 4. 反向传播算法的终止条件
反向传播算法的终止条件有3种。
- 迭代次数达到了一个固定值。
- 在训练样例的误差降到某个阈值以下时。
- 咋分离的验证样例集合的误差符合某个标准时。
# 5. 小结
这一章，我们主要针对了人工神经网络的4个问题进行了相关的总结，重点在于梯度下降法的推导和应用。下一节，我们将对评估假设的相关问题进行解答。
