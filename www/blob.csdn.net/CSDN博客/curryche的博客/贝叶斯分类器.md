# 贝叶斯分类器 - curryche的博客 - CSDN博客





2018年07月19日 11:11:21[curryche](https://me.csdn.net/whwan11)阅读数：82








## 1.贝叶斯决策论

在分类任务中，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑的是如何基于这些概率和判别损失来选择最优的类别标记。

### 判别损失

$\lambda_{ij}$表示将真实标记为$c_j$的样本分类为$c_i$所产生的损失。

### 条件风险



$R(c_i|x)=\sum\limits_{j=1}^{N}\lambda_{ij}P(c_j|x)$

表示将样本分类为类$c_i$所产生的期望损失，$P(c_j|x)$是后验概率。 

总体风险 


$R(h)=E_x\big[R(h(x)|x)\big]$
$h:X\rightarrow Y$是判别准则。

### 贝叶斯判定准则

为了最小化总体风险，只需在每个样本上选择那个使条件风险最小的类别标记，即 


$h^*(x)=\arg\min_{c\in Y} R(c|x)$

判别损失可写成 


$\lambda_{ij}=\begin{cases}0,& \text{i=j}\\1,& \text{otherwise}\end{cases}$
此时条件风险为 


$R(c|x)=1-P(c|x)$

最小化分类错误的贝叶斯分类器为 


$h^*(x)=\arg\max_{c\in Y} P(c|x)$

即对每个样本选择使后验概率最大的类别标记。

### 判别模型与生成模型

**判别模型**

给定$x$，直接建模$P(c|x)$来预测$c$。 

例如决策树、BP神经网络、支持向量机等。
**生成模型**

先对联合概率分布$P(x,c)$建模,然后再获得$P(c|x)$。

### 贝叶斯定理



$P(c|x)=\frac{P(c)P(x|c)}{P(x)}$

根据贝叶斯定理，可以把估计条件概率$P(c|x)$的问题转化成估计类先验概率$P(c)$和似然$P(x|c)$.

## 2.极大似然估计

首先假定$P(x|c)$具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计，估计的方法是最大似然估计（MLE）。

参数估计的两种方法：频率主义学派和贝叶斯学派 
**频率主义学派**：参数是客观存在的固定值，需要通过优化似然函数值来确定参数值。 
**贝叶斯学派**：参数本身也存在分布，先假定参数服从某个分布，再计算参数的后验分布。
最大似然估计属于频率主义。

$D_c$表示训练集中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是 


$P(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c)$

数据集的似然是数据集中所有样本同时出现的概率，对参数$\theta_c$进行极大似然估计就是去寻找使样本集的数据出现的可能性最大的参数值。

一般采用对数似然 


$\begin{align*}LL(\theta_c)&=logP(D_c|\theta_c)\\&=\sum_{x\in D_c}logP(x|\theta_c)\\\end{align*}$

则参数的极大似然估计为 


$\hat{\theta_c}=\arg\max_{\theta_c} LL(\theta_c)$

将假设的参数化的样本概率分布模型代入，求解最大化问题即可得到最优参数。

## 3.朴素贝叶斯分类器

### 属性条件独立性假设

用于分类的属性（特征）在类确定的条件下是条件独立的。

则条件概率可以重写为 


$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{d}^{i=1}P(x_i|c)$

d为属性数目，$x_i$为x在第一个属性值上的取值。 

由于对所有类别来说$P(x)$都相同，朴素贝叶斯分类器 


$h_{nb}(x)=\arg\max_{c\in y}P(c)\prod^{d}_{i=1}P(x_i|c)$
类先验概率为 


$P(c)=\frac{|D_c|}{|D|}$

对某属性的条件概率为 


$P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}$

### 拉普拉斯修正

为了避免因训练集样本不充分而导致的概率值估计为0的问题，在估计概率值时引入修正项，令$N$为类别数，$N_i$为第i个属性可能的取值数。拉普拉斯修正后的概率估计为 


$P(c)=\frac{|D_c|+1}{|D|+N}$



$P(x_i|c)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}$

### 懒惰学习、增量学习

在实际任务中，若任务对预测速度要求较高，则可以将分类器所涉及的所有概率估值事先计算并存储起来，在进行预测时只需查表即可判断。

懒惰学习 

事先不进行任何训练，收到预测请求时再根据当前数据集进行概率估值。

增量学习 

若数据不断增加，在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正。








