# 深度学习 - 深度前馈网络 - curryche的博客 - CSDN博客





2018年07月11日 22:26:06[curryche](https://me.csdn.net/whwan11)阅读数：377








**深度前馈网络**（deep feedforward network），又称**前馈神经网络**或者**多层感知机**（multilayer perceptron，MLP），是典型的深度学习模型。 

之所以称之为前馈网络，是因为模型输出和输入之间没有反馈连接。

### 1.代价函数

深度神经网络设计中一个重要的方面是代价函数的选择，前馈神经网络的代价函数主要分成两类。

#### 1.1使用最大似然学习条件分布

学习条件分布指神经网络学习的是给定x条件下y的概率分布，这类型代价函数的形式是条件概率的负的对数似然，等价于训练数据和模型分布间的交叉熵。 


$J(\theta)=-E_{x,y~p_{data}}logp_{model}(y|x)$

对输出分布的最大似然估计等价于对线性模型均方误差的最小化。 

优点： 

很多输出单元的激活函数会包含指数函数，容易饱和（函数变平，即梯度趋于0），负对数似然函数能够抵消输出单元中的指数效果，使代价函数的梯度足够大，具备较好地学习能力。
#### 1.2学习条件统计量

不学习条件概率分布，只是学习给定x时y的某个条件统计量，例如均值或者方差。 

学习y的均值，等价于最小化均方误差。 

学习y的中位数，此时代价函数通常被称为**平均绝对误差**。 

不足： 

均方误差和平均绝对误差在使用基于梯度的优化方法时效果不佳，一些饱和的输出单元结合这些代价函数时会产生非常小的梯度，从而使代价函数的学习能力变弱。
### 2.输出单元

#### 2.1线性单元

用于高斯输出分布 

线性输出单元会产生一个向量： 


$y=w^Th+b$
线性输出单元通常用来产生条件高斯分布的均值。此时最大化其对数似然等价于最小化均方误差。 

由于线性单元不易饱和，所以易于采用基于梯度的优化算法。

#### 2.2sigmoid单元

用于**伯努利bernoulli**输出分布。 

sigmoid单元的输出为 


$\hat{y}=\sigma(w^Th+b)$
sigmoid单元通常结合最大似然来使用。

#### 2.3softmax单元

用于**multinoulli**输出分布。 


$z=w^Th+b$



$softmax(z)_i=\frac{exp(z_i)}{\sum_{j}exp(z_j)}$

softmax激活函数可能会饱和，当其中一个输入最大且远远大于其他的输入时，响应的softmax输出会饱和到1，当$z_i$不是最大且最大值非常大时，响应的输出会饱和到0.

### 3.隐藏单元

#### 3.1整流线性单元ReLU

#### 3.2logistic sigmoid单元和双曲正切函数tanh（z）







