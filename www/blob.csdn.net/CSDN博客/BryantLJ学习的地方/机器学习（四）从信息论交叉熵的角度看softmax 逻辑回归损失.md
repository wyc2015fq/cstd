# 机器学习（四）从信息论交叉熵的角度看softmax/逻辑回归损失 - BryantLJ学习的地方 - CSDN博客





2017年03月10日 19:53:57[遍地流金](https://me.csdn.net/u012177034)阅读数：4516








机器学习中会常见到softmaxLoss，逻辑回归损失（或者叫交叉熵损失），这两种损失的来源可以由两方面考虑，一方面可以看做是来源于**概率论中的极大似然估计**，此部分可参见[机器学习（二）](http://blog.csdn.net/u012177034/article/details/52154161)，另一方面可以看做是来源于**信息论中的交叉熵损失**。 

本文主要从信息论中交叉熵的角度来解读这两种损失的由来。

# softmax损失与逻辑回归损失的来源

## 交叉熵的定义

**信息量**：事件X=x0的信息量为

$I(X_0)=-log(p(x0))$
**熵**：信息量的度量/期望，对于二值事件来说，

$Entropy=-[p(X=x_0)log(p(X=x_0))+(1-p(X=x_0))log(1-p(X=x_0))]$对于多值事件来说

$Entropy=-\sum_{x \in X}p(x)log(p(x))$由熵的定义可知，其表示该事件的不确定性大小，熵值越大，表明事件X的随机性越大 
**相对熵**：也叫作KL散度，信息增益，其主要是2个随机分布之间的距离度量。对于两个分布p,q，其KL距离为

$D_{KL}(p||q)=\sum_{x \in X}[p(x)log(\frac{p(x)}{q(x)})]$其表示的含义是，当样本的真实分布为p，而估计的样本分布q的有效性。其值越小，表示估计的分布q越接近真实分布p。 
**交叉熵**

同样是衡量两个分布之间的相似性。**其中x代表该样本的某一个类别，X代表该样本的所有可能类别集合，p(x)表示样本取值为x的真实概率，q(x)表示样本取值为x的预测概率**。

$对于一个确定的样本\zeta而言，p(\zeta=x)的取值要么为0，要么为1$，对于这个样本的交叉熵损失如下 


$CrossEntropy(p,q)=-\sum_{x \in X}p(\zeta =x)log(q(\zeta =x))$当p(x)为固定的分布时，交叉熵与相对熵是等价的。当p(x)表示为样本的真实分布，q(x)表示预测的分布时，这两者同样表示了预测与真实之间的差距。
特别的，当事件X为二值事件(取值为0或1)时，假设

$真实样本X分布P得出的prob(x=1)=p$

$由假设模型Q预测出来的样本概率prob(x=1)=q$则此时用来预测的分布Q和真实的分布P之间的交叉熵为

$CrossEntropy(P,Q)=-[p*log(q)+(1-p)*log(1-q)]=-[label_{X}*log(h_{\theta}(X))+(1-label_{X})*log(1-h_{\theta}(X))]$




