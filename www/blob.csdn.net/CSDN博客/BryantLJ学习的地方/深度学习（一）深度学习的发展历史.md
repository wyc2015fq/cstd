# 深度学习（一）深度学习的发展历史 - BryantLJ学习的地方 - CSDN博客





2016年08月19日 17:19:06[遍地流金](https://me.csdn.net/u012177034)阅读数：29605








本次深度学习系列主要从以下几个方面记录，主要为CNN相关 

另外最后会专留一章讲述**CNN与计算机视觉中的目标检测**的发展。

$\bullet$**发展历史**
$\bullet$**基础结构**
$\bullet$**损失函数**
$\bullet$**优化方法**
$\bullet$**训练trick**
**学习任一门知识都应该先从其历史开始，把握了历史，也就抓住了现在与未来**

———by BryantLJ

首先盗一张图（来自于“深度学习大讲堂”微信公众号～），该图形象的展示DL今年来的发展历程及关键节点：

![DL发展历史](https://img-blog.csdn.net/20160819150509668)

由图可以明显看出DL在从06年崛起之前经历了两个低谷，这两个低谷也将神经网络的发展分为了三个不同的阶段，下面就分别讲述这三个阶段

## **第一代神经网络（1958~1969）**

最早的神经网络的思想起源于**1943年的MCP人工神经元模型**，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示

![MCP神经元模型](https://img-blog.csdn.net/20160819151147686)

第一次将MCP用于机器学习（分类）的当属**1958年Rosenblatt发明的感知器（perceptron）算法**。该算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

然而学科发展的历史不总是一帆风顺的。

**1969年**，美国数学家及人工智能先驱**Minsky**在其著作中证明了感知器本质上是一种线性模型，**只能处理线性分类问题**，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。

## **第二代神经网络（1986~1998）**

第一次打破非线性诅咒的当属现代DL大牛**Hinton**，其在**1986年发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射**，**有效解决了非线性分类和学习的问题**。该方法引起了神经网络的第二次热潮。

**1989年**，Robert Hecht-Nielsen证明了MLP的**万能逼近定理**，即**对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近**该定理的发现极大的鼓舞了神经网络的研究人员。

也是在1989年，**LeCun发明了卷积神经网络-LeNet**，并将其用于数字识别，且取得了较好的成绩，不过当时并没有引起足够的注意。

值得强调的是在1989年以后由于没有特别突出的方法被提出，且NN一直缺少相应的严格的数学理论支持，神经网络的热潮渐渐冷淡下去。冰点来自于**1991年**，BP算法被指出存在**梯度消失问题**，即**在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习**，该发现对此时的NN发展雪上加霜。

**1997年，LSTM模型**被发明，尽管该模型在序列建模上的特性非常突出，但由于正处于NN的下坡期，也没有引起足够的重视。

## **统计学习方法的春天（1986~2006）**

**1986年，决策树方法**被提出，很快ID3，ID4，CART等改进的决策树方法相继出现，到目前仍然是非常常用的一种机器学习方法。该方法也是符号学习方法的代表。 
**1995年，线性SVM**被统计学家**Vapnik**提出。该方法的特点有两个：由非常完美的数学理论推导而来（统计学与凸优化等），符合人的直观感受（最大间隔）。不过，最重要的还是该方法在线性分类的问题上取得了当时最好的成绩。 
**1997年，AdaBoost**被提出，该方法是PAC（Probably Approximately Correct）理论在机器学习实践上的代表，也催生了集成方法这一类。该方法通过一系列的弱分类器集成，达到强分类器的效果。 
**2000年，KernelSVM**被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kernel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了NN时代。 
**2001年，随机森林**被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost更好的抑制过拟合问题，实际效果也非常不错。 
**2001年，一种新的统一框架-图模型**被提出，该方法试图统一机器学习混乱的方法，如朴素贝叶斯，SVM，隐马尔可夫模型等，为各种学习方法提供一个统一的描述框架。
## **第三代神经网络-DL（2006-至今）**

该阶段又分为两个时期：快速发展期（2006~2012）与爆发期（2012~至今）

### **快速发展期（2006~2012）**

**2006年，DL元年**。是年，Hinton提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。

**2011年，ReLU激活函数**被提出，该激活函数能够有效的抑制梯度消失问题。

**2011年，微软首次将DL应用在语音识别上**，取得了重大突破。

### **爆发期（2012~至今）**

**2012年，Hinton**课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络**AlexNet**一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。 

AlexNet的创新点： 

（1）首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题；（2）由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，**DL的主流学习方法也因此变为了纯粹的有监督学习**；（3）扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合；（4）首次采用GPU对计算进行加速；
**2013,2014,2015年**，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场

**2015年，Hinton，LeCun，Bengio论证了**局部极值问题对于DL的影响，结果是**Loss的局部极值问题对于深层网络来说影响可以忽略**。该论断也消除了笼罩在神经网络上的局部极值问题的阴霾。具体原因是**深层网络虽然局部极值非常多，但是通过DL的BatchGradientDescent优化方法很难陷进去，而且就算陷进去，其局部极小值点与全局极小值点也是非常接近，但是浅层网络却不然，其拥有较少的局部极小值点，但是却很容易陷进去，且这些局部极小值点与全局极小值点相差较大**。论述原文其实没有证明，只是简单叙述，严密论证是猜的。。。

**2015，DeepResidualNet发明**。分层预训练，ReLU和BatchNormalization都是为了解决深度神经网络优化时的梯度消失或者爆炸问题。但是在对更深层的神经网络进行优化时，又出现了新的**Degradation**问题，即”**通常来说，如果在VGG16后面加上若干个单位映射，网络的输出特性将和VGG16一样，这说明更深次的网络其潜在的分类性能只可能>=VGG16的性能，不可能变坏，然而实际效果却是只是简单的加深VGG16的话，分类性能会下降（不考虑模型过拟合问题）**“Residual网络认为这说明**DL网络在学习单位映射方面有困难**，因此设计了一个对于单位映射（或接近单位映射）有较强学习能力的DL网络，极大的增强了DL网络的表达能力。此方法能够轻松的训练高达150层的网络。

## 总结

从原理上解释为什么CNN要比传统的目标检测方法好？ 

（1）传统方法都是通过人工提取特征，需要在领域专家通过多年的积累和经验才能手工设计出来，DL方法是通过大量的数据，自动学习到能够反应数据差别的特征，更具有代表性 

（2）对于视觉识别来说，CNN分层提取的特征与人的视觉机理（神经科学）类似，都是进行边缘->部分->全体的过程。
以上。







