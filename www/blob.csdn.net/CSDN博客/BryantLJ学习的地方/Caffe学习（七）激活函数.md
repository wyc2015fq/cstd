# Caffe学习（七）激活函数 - BryantLJ学习的地方 - CSDN博客





2016年08月09日 20:26:41[遍地流金](https://me.csdn.net/u012177034)阅读数：3766








激活函数的起源是希望该函数能够对神经元进行建模，进而起到对输入的数据进行非线性响应的过程。

## 常用的激活函数

激活函数应该具有的性质： 

（1）非线性。线性激活层对于深层神经网络没有作用，因为其作用以后仍然是输入的各种线性变换。。 

（2）连续可微。梯度下降法的要求。 

（3）范围最好不饱和，当有饱和的区间段时，若系统优化进入到该段，梯度近似为0，网络的学习就会停止。 

（4）单调性，当激活函数是单调时，单层神经网络的误差函数是凸的，好优化。 

（5）在原点处近似线性，这样当权值初始化为接近0的随机值时，网络可以学习的较快，不用可以调节网络的初始值。 
**目前常用的激活函数都只拥有上述性质的部分，没有一个拥有全部的～～**

（1）**Sigmoid函数**


$f(x)=\frac{1}{1+e^{-x}}$

目前已被淘汰 

缺点： 
$\bullet$**饱和时梯度值非常小**。由于BP算法反向传播的时候后层的梯度是以乘性方式传递到前层，因此当层数比较多的时候，传到前层的梯度就会非常小，网络权值得不到有效的更新，**即梯度耗散**。如果该层的权值初始化使得$f(x)$处于饱和状态时，网络基本上权值无法更新。 
$\bullet$**输出值不是以0为中心值**。 

（2）**Tanh函数**


$tanh(x)=2\sigma(2x)-1$其中$\sigma(x)$为sigmoid函数，仍然具有饱和的问题。 

（3）**ReLU函数**


$f(x)=max(0,x)$Alex在2012年提出的一种新的激活函数。该函数的提出很大程度的解决了BP算法在优化深层神经网络时的梯度耗散问题 
**优点**： 
$\bullet$$x>0$时，梯度恒为1，**无梯度耗散问题，收敛快**； 
$\bullet$**增大了网络的稀疏性**。当$x<0$时，该层的输出为0，训练完成后为0的神经元越多，稀疏性越大，提取出来的特征就约具有代表性，泛化能力越强。**即得到同样的效果，真正起作用的神经元越少，网络的泛化性能越好**
$\bullet$运算量很小； 
**缺点**： 

如果后层的某一个梯度特别大，导致W更新以后变得特别大，导致该层的输入<0，输出为0，这时该层就会‘die’，没有更新。当学习率比较大时可能会有40%的神经元都会在训练开始就‘die’，因此需要对学习率进行一个好的设置。 

由优缺点可知$max(0,x)$函数为一个双刃剑，既可以形成网络的稀疏性，也可能造成有很多永远处于‘die’的神经元，需要tradeoff。 

（4）**Leaky ReLU函数**


$f(x)=\left\{\begin{matrix}1,(x<0)\\ \alpha x+1(x>=0)\end{matrix}\right.$

改善了ReLU的死亡特性，但是也同时损失了一部分稀疏性，且增加了一个超参数，目前来说其好处不太明确 

（5）**Maxout函数**


$f(x)=max(w_{1}^{T}x+b_{1},w_{2}^{T}x+b_{2})$泛化了ReLU和Leaky ReLU，改善了死亡特性，但是同样损失了部分稀疏性，每个非线性函数增加了两倍的参数
**真实使用的时候最常用的还是ReLU函数，注意学习率的设置以及死亡节点所占的比例即可**

Caffe中激活函数的设置都比较简单，不在赘述～～～




