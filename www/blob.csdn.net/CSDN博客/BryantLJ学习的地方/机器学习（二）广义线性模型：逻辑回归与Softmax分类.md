# 机器学习（二）广义线性模型：逻辑回归与Softmax分类 - BryantLJ学习的地方 - CSDN博客





2016年08月08日 17:56:19[遍地流金](https://me.csdn.net/u012177034)阅读数：2827








本文主要从**概率论及广义线性模型**的角度来谈线性回归，逻辑分类，Softmax分类三种常用学习方法的来源，基本假设及适用场景。

## 一.狭义线性模型及求解

狭义线性模型针对典型的回归问题 
**问题描述**：给定一组训练样本$（x_{i},y_{i}）,i\in(1,M)$，其中$x_{i}$为N维列向量，$y_{i}$为实数，M为训练样本个数。要求对于新输入的$x$，预测出合理的$y$。 
**模型假设**： 

（1）训练样本的观测误差$\epsilon_{i}$符合**正太分布**，即$y_{i}=y_{reali}+\epsilon_{i}$，其中$\epsilon_{i}\sim N(0,\sigma^{2})$，且所有$\epsilon$独立同分布。该假设的另一含义是**输出值$y_{i}$满足正态分布**，即$y_{i}\sim N(y_{reali},\sigma^{2})$

（2）正太分布的**均值与输入样本$x$满足线性关系**，即$y_{reali}=\theta^{T}*x_{i}$

（3）预测目标是y的期望，其中$y$满足$y\sim N(\theta^{T}x,\sigma^{2})$
**模型求解**

由上述两个假设可由极大似然法来估计出模型参数$\theta$。 

由题得训练样本集合的似然函数

$L=\prod_{i=1}^{M}P(y_{i}|x_{i};\theta)=\prod_{i=1}^{M} \frac{1}{\sqrt{2\pi}\sigma}\bullet e^{-\frac{1}{2\sigma^{2}}\sum{(y_{i}-\theta^{T}x_{i})^{2}}}$由上述似然函数的形式可得，似然函数极大化只需对$\sum{(y_{i}-\theta^{T}x_{i})^{2}}$求极小值即可。此即是**最小二乘法**的概率意义上的解释。也是目前处理回归问题最常用的方法-LinearRegression的原理。
## 二.广义线性模型GLM

狭义线性模型对与处理实数回归问题很有效，但是对于输出是类别或者是离散的变量时便无能为力了。广义线性模型希望能够将这种方法泛化到离散的类别上去使用。对此GLM对狭义线性模型的假设做了修改，使之能够适用于更多的问题

广义线性模型的组成： 

（1）真实输出y满足的指数概率分布$P(y;\eta)$，$\eta$为分布参数 

（2）线性预测器$\eta=\theta^{T}x$

（3）连接函数$g$，$E(y)=g^{-1}(\eta)=g^{-1}(\theta^{T}x)$。在狭义线性模型中的$g^{-1}(x)=x$。连接函数主要的工作就是将线性预测模型的$(-\infty,+\infty)$的输出变换到系统真实需要的输出域上去。理论上链接函数的定义可以是任意的，但是仍然要适应具体的分布和场景。
**模型假设:**

（1）样本观测值$y_{i}$只要满足带**参数$\eta$的指数分布簇**即可。即

$P(y;\eta)=b(y)e^{(\eta^{T}T(y)-a(\eta))}$常见的正态分布，二分类伯努利分布，多类多项分布，泊松分布等都可以转换成指数分布的形式。 

（2）分布**模型参数$\eta$与输入样本$x$呈线性关系**，即$\eta=\theta^{T}x$。狭义线性模型的假设是高斯分布的均值参数与输入样本x呈线性关系。 

（3）预测目标：$\eta$的任一充分统计量$T(y)$的期望$E(T(y))$，通常$T(y)=y$。 

注： 
$\bullet$充分统计量：$T(y)$是未知分布$p$参数$\theta$的充分统计量，当且仅当$T(y)$能够提供$\theta$的全部信息。比如在正太分布中，样本均值和方差就是未知的正态分布的充分统计量，因为这两个参数可以完全描述整个样本的分布特性。
现在感觉GLM并不能解释为什么逻辑回归要用到sigmoid函数（其他函数也能表示。。）Sigmoid和softmax函数只是广义线性模型中伯努利分布和多项分布所对应的链接函数的一种罢了

**最新更新：**
**逻辑回归为什么选择sigmoid**： 

（1）假设我们利用广义线性模型GLM来做预测。此步做出以下假设： 
$\bullet$预测输出假设满足伯努利分布，即满足指数分布族。假设$p(y==1)=\phi$，此时可有

$p(y)=\phi^{y}\phi^{1-y}=exp(log(\frac{\phi}{1-\phi})y+log(1-\phi))$$\bullet$假设本题中的指数分布的参数$\eta$与输入样本存在线性关系，即$\eta=\theta^{T}x$，此时有

$\eta=log(\frac{\phi}{1-\phi})=\theta^{T}x$可得$\phi=\frac{1}{1+e^{-\eta}}$即sigmoid的形式 

（2）至于为什么让伯努利分布的指数分布族形式的参数$\eta$与输入样本呈线性关系，好像是这种情况的信息熵最大吧～～～套用一句话就是，对于我们不确定的事物不要乱猜。比如对于掷骰子，最靠谱的方法就是认为每个数字出现的概率都是$\frac{1}{6}$，而不是武断的下结论认为某一个数出现的可能性是最大的～～ 
**为什么选择softmax**：由很多概率归一化的方法，为什么单单选择softmax函数? 

两个方面来看 

（1）从GLM与指数分布族来看，当假设输出满足多项分布时可直接推出预测的类别概率值为softmax函数 

（2）从算法优化及代码实现上来看，采用softmax+crossEntropyLoss组成softmax分类器，进行BP求导时非常简单，且形式与sigmoid几乎一样，可以共用。
**由GLM可知，LinearRegression，LogisticRegression，SoftmaxClassification三者都可以通过广义线性模型的指数分布族来解释，其参数求解优化过程都可以通过极大似然函数法来实现。当采用极大似然函数求解时，LogisticRegression等价于内积层+Sigmoid函数+BinraryCrossEntropy损失；SoftmaxClassification等价于内积层+softmax函数+CategoricalCrossEntropy损失（在caffe里叫做MultinomialLoss）；LinearRegression等价于内积层+均方差损失（caffe里叫做EuclideanLoss）**







