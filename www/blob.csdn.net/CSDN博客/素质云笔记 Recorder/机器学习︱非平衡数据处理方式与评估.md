
# 机器学习︱非平衡数据处理方式与评估 - 素质云笔记-Recorder... - CSDN博客

2017年07月23日 11:10:42[悟乙己](https://me.csdn.net/sinat_26917383)阅读数：5768


解决这一问题的基本思路是让正负样本在训练过程中拥有相同的话语权，比如利用采样与加权等方法。为了方便起见，我们把数据集中样本较多的那一类称为“大众类”，样本较少的那一类称为“小众类”。
解决方式分为：
![这里写图片描述](https://img-blog.csdn.net/20170723105911704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20170723105911704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
.
[
](https://img-blog.csdn.net/20170723105911704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
---一、相关方法总结

---1、采样

---采样方法是通过对训练集进行处理使其从不平衡的数据集变成平衡的数据集，在大部分情况下会对最终的结果带来提升。

---采样分为上采样（Oversampling，过采样）和下采样（Undersampling， 欠采样），上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。

---随机采样最大的优点是简单，但缺点也很明显。上采样后的数据集中会反复出现一些样本，训练出来的模型会有一定的过拟合；而下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。

---上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动，经验表明这种做法非常有效。

---——这一方式会加重过拟合！

---因为下采样会丢失信息，如何减少信息的损失呢？

---第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。简单的最佳实践是建立n个模型，每个模型使用稀有类别的所有样本和丰富类别的n个不同样本。假设想要合并10个模型，那么将保留例如1000例稀有类别，并随机抽取10000例丰富类别。然后，只需将10000个案例分成10块，并训练10个不同的模型。


---第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大，

---感兴趣的可以参考“Learning from Imbalanced Data”这篇综述的3.2.1节。


---.

---2、 数据合成

---数据合成方法是利用已有样本生成更多样本，这类方法在小数据场景下有很多成功案例，比如医学图像分析等。

---其中最常见的一种方法叫做SMOTE，它利用小众样本在特征空间的相似性来生成新样本。

---SMOTE为每个小众样本合成相同数量的新样本，这带来一些潜在的问题：一方面是增加了类之间重叠的可能性，另一方面是生成一些没有提供有益信息的样本。为了解决这个问题，出现两种方法：Borderline-SMOTE与ADASYN。

---Borderline-SMOTE的解决思路是寻找那些应该为之合成新样本的小众样本。即为每个小众样本计算K近邻，只为那些K近邻中有一半以上大众样本的小众样本生成新样本。直观地讲，只为那些周围大部分是大众样本的小众样本生成新样本，因为这些样本往往是边界样本。确定了为哪些小众样本生成新样本后再利用SMOTE生成新样本。

---ADASYN的解决思路是根据数据分布情况为不同小众样本生成不同数量的新样本。首先根据最终的平衡程度设定总共需要生成的新小众样本数量 G，然后为每个小众样本 xi 计算分布比例

---.

---3、 加权

---除了采样和生成新数据等方法，我们还可以通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同，如下图：

![这里写图片描述](https://img-blog.csdn.net/20170723110046461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
---[ ](https://img-blog.csdn.net/20170723110046461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

---横向是真实分类情况，纵向是预测分类情况，C(i,j)是把真实类别为j的样本预测为i时的损失，我们需要根据实际情况来设定它的值。

---[
](https://img-blog.csdn.net/20170723110046461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

---这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。

---.

---4、一分类

---对于正负样本极不平衡的场景，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等。

---.

---5、以正确的方式使用K-fold交叉验证

---值得注意的是，使用过采样方法来解决不平衡问题时应适当地应用交叉验证。这是因为过采样会观察到罕见的样本，并根据分布函数应用自举生成新的随机数据，如果在过采样之后应用交叉验证，那么我们所做的就是将我们的模型过拟合于一个特定的人工引导结果。这就是为什么在过度采样数据之前应该始终进行交叉验证，就像实现特征选择一样。只有重复采样数据可以将随机性引入到数据集中，以确保不会出现过拟合问题。

---K-fold交叉验证就是把原始数据随机分成K个部分，在这K个部分中选择一个作为测试数据，剩余的K-1个作为训练数据。交叉验证的过程实际上是将实验重复做K次，每次实验都从K个部分中选择一个不同的部分作为测试数据，剩余的数据作为训练数据进行实验，最后把得到的K个实验结果平均。

---.

---6、基于聚类的重抽样方法

---（1）首先分别对正负例进行K-means聚类

---（2）聚类之后进行Oversampling等系列方法

---举例说明，假设我们运行K-means方法分别对正负例进行了聚类，结果如下：

---正例三个簇，个数分别为：20 ， 5, 12 负例两个簇，个数分别为：4 ，6

---可以看出，正负例簇中个数最大的为20，所以正例其他两个簇通过oversampling都提高到20个实例，负例簇都提高到（20+20+20）/2=30 个实例。

---最后变为，正例三个簇：20,20,20 负例两个簇：30,30

---总结下这种基于聚类的抽样算法的

---优点：

---该算法不仅可以解决类间不平衡问题，而且还能解决类内部不平衡问题。

---.

---7、适应不平衡样本的模型

---所有之前的方法都集中在数据上，并将模型保持为固定的组件。但事实上，如果设计的模型适用于不平衡数据，则不需要重新采样数据，著名的XGBoost已经是一个很好的起点，因此设计一个适用于不平衡数据集的模型也是很有意义的。

---通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。例如，调整SVM以惩罚稀有类别的错误分类。

---.

---二、如何选择

---解决数据不平衡问题的方法有很多，上面只是一些最常用的方法，而最常用的方法也有这么多种，如何根据实际问题选择合适的方法呢：

---在正负样本都非常之少的情况下，应该采用数据合成的方式；

---在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；

---在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。

---采样和加权在数学上是等价的，但实际应用中效果却有差别。尤其是采样了诸如Random Forest等分类方法，训练过程会对训练集进行随机采样。在这种情况下，如果计算资源允许上采样往往要比加权好一些。

---另外，虽然上采样和下采样都可以使数据集变得平衡，并且在数据足够多的情况下等价，但两者也是有区别的。实际应用中，

---我的经验是如果计算资源足够且小众类样本足够多的情况下使用上采样，否则使用下采样，因为上采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。

---对于下采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。

---.

---参考：

---[干货｜如何解决机器学习中数据不平衡问题](https://mp.weixin.qq.com/s?__biz=MzA4NzE1NzYyMw==&mid=2247492055&idx=3&sn=76e4216a997199a6b2b76daa403ef000&chksm=903f1fcfa74896d92218c41814a7423c79b184fdff462129b446f39d2b3565ed7db0ee7419c9&mpshare=1&scene=1&srcid=0701s3q4k9QFeMkdYAiJcRs0#rd)

---[七招教你处理非平衡数据——避免得到一个“假”模型](https://mp.weixin.qq.com/s?__biz=MzA4NzE1NzYyMw==&mid=2247492105&idx=5&sn=80e53f326ac7d69a62c6fe64a803356e&chksm=903f1c11a7489507f117d0e6335baf86c8520929950fd64ff6cdddcfba61aacc054a0e80e38a&mpshare=1&scene=1&srcid=07052uXgb7dJowOTmjz8ycLh#rd)


