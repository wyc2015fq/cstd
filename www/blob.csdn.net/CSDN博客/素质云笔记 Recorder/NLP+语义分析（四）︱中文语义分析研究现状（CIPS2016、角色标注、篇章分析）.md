
# NLP+语义分析（四）︱中文语义分析研究现状（CIPS2016、角色标注、篇章分析） - 素质云笔记-Recorder... - CSDN博客

2017年02月18日 22:51:17[悟乙己](https://me.csdn.net/sinat_26917383)阅读数：22539


![这里写图片描述](https://img-blog.csdn.net/20170218223512499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 摘录自：CIPS2016 中文信息处理报告《第二章 语义分析研究进展、 现状及趋势》P14  CIPS2016>

> 中文信息处理报告下载链接：
> [http://cips-upload.bj.bcebos.com/cips2016.pdf](http://cips-upload.bj.bcebos.com/cips2016.pdf)
![这里写图片描述](https://img-blog.csdn.net/20170218223519222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 任何对语言的理解都可以归为语义分析的范畴。一段文本通常由词、句子和段落来构成，根据理解对象的语言单位不同， 语义分析又可进一步分解为词汇级语义分析、句子级语义分析以及篇章级语义分析。

> 语义分析的目标就是通过建立有效的模型和系统， 实现在各个语言单位 （包括词汇、句子和篇章等） 的自动语义分析，从而实现理解整个文本表达的真实语义。

> .

> .

> NLP词法、句法、语义、语篇综合系列：

> [NLP+词法系列（一）︱中文分词技术小结、几大分词引擎的介绍与比较](http://blog.csdn.net/sinat_26917383/article/details/52275328)

> [NLP+词法系列（二）︱中文分词技术及词性标注研究现状（CIPS2016）](http://blog.csdn.net/sinat_26917383/article/details/55682577)

> [NLP+句法结构（三）︱中文句法结构研究现状（CIPS2016）](http://blog.csdn.net/sinat_26917383/article/details/55682996)

> [NLP+语义分析（四）︱中文语义分析研究现状（CIPS2016）](http://blog.csdn.net/sinat_26917383/article/details/55683599)

> [NLP+语篇分析（五）︱中文语篇分析研究现状（CIPS2016）](http://blog.csdn.net/sinat_26917383/article/details/55683843)

> 公众号“素质云笔记”定期更新博客内容：
![这里写图片描述](https://img-blog.csdn.net/20180226155348545?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
> 一、词汇级

> 词汇层面上的语义分析主要体现在如何理解某个词汇的含义，主要包含两个方面：

> 第一，在自然语言中，一个词具有两种或更多含义的现象非常普遍。如何自动获悉某个词存在着多15种含义，以及假设已知某个词具有多种含义，如何根据上下文确认其含义，这些都是词汇级语义研究的内容。在自然语言处理领域，这又称为词义消歧。

> 第二，如何表示并学习一个词的语义，以便计算机能够有效地计算两个词之间的相似度。

> 1、词义消歧

> 词汇的歧义性是自然语言的固有特征。词义消歧根据一个多义词在文本中出现的上下文环境来确定其词义，作为各项自然语言处理的基础步骤和必经阶段被提出来。词义消歧包含两个必要的步骤:

> （ 1） 在词典中描述词语的意义;

> （ 2） 在语料中进行词义自动消歧。

> 详情见：
> [cips2016+学习笔记︱NLP中的消岐方法总结（词典、有监督、半监督）](http://blog.csdn.net/sinat_26917383/article/details/54882754)

> 2、词嵌入

> 高级词向量三部曲：

> 1、
> [NLP︱高级词向量表达（一）——GloVe（理论、相关测评结果、R&python实现、相关应用）](http://blog.csdn.net/sinat_26917383/article/details/54847240)

> 2、
> [NLP︱高级词向量表达（二）——FastText（简述、学习笔记）](http://blog.csdn.net/sinat_26917383/article/details/54850933)

> 3、
> [NLP︱高级词向量表达（三）——WordRank（简述）](http://blog.csdn.net/sinat_26917383/article/details/54852214)

> .

> .

> 二、句子级

> 1、浅层语义分析语义角色标注(SRL)

> 语义角色标注的任务明确，即给定一个谓词及其所在的句子，找出句子中该谓词的相应语义角色成分。语义角色标注的研究热点包括基于成分句法树的语义角色标注和基于依存句法树的语义角色标注。

> 以下以基于成分句法树的语义角色标注为例，任务的解决思路是以句法树的成分为单元，判断其是否担当给定谓词的语义角色。系统通常可以由三部分构成：

> 角色剪枝. 通过制定一些启发式规则，过滤掉那些不可能担当角色的成分。

> 角色识别. 在角色剪枝的基础上，构建一个二元分类器，即识别其是或不是给定谓词的语义角色。

> 角色分类. 对那些是语义角色的成分，进一步采用一个多元分类器，判断其角色类别。

> 在基于特征向量的方法中，
> 最具有代表性的 7 个特征，包括成分类型（ constituent type）、谓词子类框架（ subcategorization）、成分与谓词之间的路径（ parse tree path）、成分与谓词的位置关系（ constituent position）、谓词语态（ predicate voice）、成分中心词（ constituent head word）和谓词本身（ predicate）。

> 这７个特征随后被作为基本特征广泛应用于各类基于特征向量的语义角色标注系统中，同时后续研究也提出了其他有效的特征。

> 在角色识别和角色分类过程中， 无论是采用基于特征向量的方法,还是基于树核的方法,其目的都是
> 尽可能准确地计算两个对象之间的相似度。
> 基于特征向量的方法将结构化信息转化为平面信息， 方法简单有效；
> 缺点是在制定特征模板的同时， 丢弃了一些结构化信息。
> 同样， 基于树核的方法有效解决了特征维数过大的问题， 缺点是在利用结构化信息的同时会包含噪音信息， 另外计算开销远大于基于特征向量的方法。

> 2、深层语义分析

> 浅层语义分析主要围绕着句子中的谓词，为每个谓词找到相应的语义角色将整个句子转化为某种形式化表示
> ， 例如： 谓词逻辑表达式（包括 lambda 演算表达式）、基于依存的组合式语义表达式（ dependency-based compositional semantic

> representation） 等。 以下给出了 GeoQuery 数据集中的一个中英文句子对，以及对应的一阶谓词逻辑语义表达式：

> 中文: 列出 在 科罗拉多 州 所有 的 河流

> 英文: Name all the rivers in Colorado

> 语义表达式:  answer(river(loc_2(stateid(‘colorado’))))

> 那么方法主要有三类：

> 基于知识库的语义分析（图谱？）。

> 知识库中通过三元组等形式记录了一系列的事实。对某个给定的句子，语义分析通过某种转换技术，将句子分析为一系列知识库中已定义的元组，并构成一个实体关系图。

> 有监督语义分析

> 有监督的语义分析需要人工标注的语义分析语料支持。在人工标注的语义分析语料中，为每个自然语言句子人工标注了其语义表达式。

> 半监督或无监督语义分析

> 无监督的语义分析方法不需要利用人工标注的语义分析语料，仅利用知识库（或数据库）

> 中的实体名/关系名等，也不利用知识库中的记录的事实。无监督的语义分析方法虽然不使

> 用人工标注的语料，但通常会采用 EM 算法，在每轮算法迭代中，对句子进行语义分析，并

> 且选择置信度高的句子及其语义分析结果作为自训练数据集

> 深度语义分析主要面临如下二个关键问题。

> 普通文本到实体/关系谓词之间的映射

> 自然语言的一个主要特点在于其表达形式的丰富多样性，对同样的表达意思（如某个语义表达式），不仅可以使用不同的语言进行表达。 如何建立普通文本到实体/关系之间的映射是一个关键问题。

> 面向开放领域的语义分析

> 受标注语料的限制，目前的很多语义分析研究都限于某一特定领域。随着面向开放领域的知识库的构建及完善，如 Freebase 等，人工大规模标注涉及各领域的语义表达式是个费时费力的过程。为此，需要探索基于半监督或无监督的语义分析研究。

> .

> .

> 三、篇章级

> 篇章是指由一系列连续的子句、句子或语段构成的语言整体单位，在一个篇章中，子句、句子或语段间具有一定的层次结构和语义关系，篇章结构分析旨在分析出其中的层次结构和语义关系。

> 目前的篇章语义分析主要还是围绕着判定子句与子句的篇章语义关系。

> 1、基于 Penn Discourse TreeBank 的篇章分析

> 一个基19于 PDTB 的端对端的篇章分析通常划分为四个子任务， 分别是：

> （ 1） 篇章连接词识别；

> （ 2）论元(Argument)识别；

> （ 3） 显式篇章关系识别；

> （ 4） 隐式篇章关系识别。

> 下面我们分别阐述这四个子任务。

> 2、基于 RST 的篇章分析

> PDTB 是建立在宾州树库基础上的。基于 RST 的篇章分析主要包含有两个子任务： （ 1）篇章基本单元 (Element DiscourseUnit, 简称 EDU) 识别；（ 2） 篇章结构生成，即对每一个过程的输出采用自底向上方法，为功能子句对确定一个最可能的修辞关系

> 3、中文篇章分析

> 基于 RST 体系的标注。

> 中国传媒大学和南京师范大学均分别基于英文 RST 框架标注了中文语料，但他们的研究都表明英文 RST 的很多篇章关系无法在中文中找到与之对应的关系

> 基于 PDTB 体系的标注。

> 哈尔滨工业大学发布了HIT-CDTB 篇 章 分 析 语 料 ， 该 语 料 包 含 525 篇 标 注 文 本 ， 语 料 文 本 来 源 于OntoNotes4.0，覆盖了句群关系、复句关系、分句关系等多级信息。整体上，HIT-CDTB还是参照了英文 PDTB 的标注体系。

> 基于连接依存树的标注。

> 苏州大学发布了基于连接依存树的中文篇章结构表示体系的中文篇章分析语料（ Chinese Discourse TreeBank， CDTB），该标注体系借鉴了RST 和 PDTB 体系优点，并结合中文的特点。

> .

> 延伸一：语义角色标注

> 参考：
> [PaddlePaddle︱开发文档中学习情感分类（CNN、LSTM、双向LSTM）、语义角色标注](http://blog.csdn.net/sinat_26917383/article/details/54864760)

> 自然语言分析技术大致分为三个层面：词法分析、句法分析和语义分析。语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等。

> 请看下面的例子，“遇到” 是谓词（Predicate，通常简写为“Pred”），“小明”是施事者（Agent），“小红”是受事者（Patient），“昨天” 是事件发生的时间（Time），“公园”是事情发生的地点（Location）。
![这里写图片描述](https://img-blog.csdn.net/20170219134621483?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 语义角色标注（Semantic Role Labeling，SRL）以句子的谓词为中心，不对句子所包含的语义信息进行深入分析，只分析句子中各成分与谓词之间的关系，即句子的谓词（Predicate）- 论元（Argument）结构，并用语义角色来描述这些结构关系，是许多自然语言理解任务（如信息抽取，篇章分析，深度问答等）的一个重要中间步骤。在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元和它们的语义角色。

> .

> 延伸二：语言中二元组搭配的方式

> 来自知乎问答：
> [自然语言处理中，一般用什么方法来找语言中的固定搭配？](https://www.zhihu.com/question/27215318)

> 1、仅考虑相邻词首先，在一个语料库上训练一个Language Model就可以达到类似的事，楼主的问题就转换成找到一个词w,

> 最大化“鲜花”后面出现w的概率p(w|鲜花)。当然，LM用在这上面有点杀鸡用牛刀，只不过LM有很多现成的工具包，比如SRILM[3]

> Berkeleylm[4] 等等自己写的话，先中文分词，然后统计所有相邻词对(2-gram

> [5])的出现次数，从高到低排个序就好。中文分词包，推荐复旦[1] 或 Stanford

> parser[2]以上的方法简单可靠，只需要中文的分词加大量数据即可，手写代码即可。但缺点是只能统计相邻的词，对于鲜花 正在 盛开

> 这样的情况就无能为力。

> 2、利用Dependency Parser进行句法分析有一个东西叫做依存句法分析 Dependency

> Parser，可以找到句中的主谓宾关系，修饰，从句等等关系，可以部分解决【一】无法解决的不相邻词的常用搭配，即长距离依赖问题。

> 3、工具的话我推荐python下的NLTK，超方便！

> 文档：
> [http://www.nltk.org/howto/collocations.html](http://www.nltk.org/howto/collocations.html)

> 缺点是大语料下内存占用和耗时都太高

> 4、N-Gram模型，其中用的比较多的是Bi-Gram和Tri-Gram


