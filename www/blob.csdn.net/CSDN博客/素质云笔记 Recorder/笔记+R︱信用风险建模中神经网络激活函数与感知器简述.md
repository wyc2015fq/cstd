# 笔记+R︱信用风险建模中神经网络激活函数与感知器简述 - 素质云笔记/Recorder... - CSDN博客





2016年06月21日 15:29:30[悟乙己](https://me.csdn.net/sinat_26917383)阅读数：3407












**每每以为攀得众山小，可、每每又切实来到起点，大牛们，缓缓脚步来俺笔记葩分享一下吧，please~**![](https://img-blog.csdn.net/20161213101203247)


———————————————————————————


本笔记源于CDA-DSC课程，由常国珍老师主讲。该训练营第一期为风控主题，培训内容十分紧凑，非常好，推荐：[CDA数据科学家训练营](http://www.cda.cn/dsc/)





**——————————————————————————————————————————**




# 一、信用风险建模中神经网络的应用






申请评分可以将神经网络+逻辑回归联合使用。



《公平信用报告法》制约，强调评分卡的可解释性。所以初始评分（申请评分）一般用回归，回归是解释力度最大的。

神经网络可用于银行行为评级以及不受该法制约监管的业务（P2P）。其次，神经也可以作为申请信用评分的金模型。




金模型的使用：一般会先做一个神经网络，让预测精度（AUC）达到最大时，再用逻辑回归。

建模大致流程：

一批训练集+测试集+一批字段——神经网络建模看AUC——如果额定的AUC在85%，没超过则返回重新筛选训练、测试集以及字段；

超过则，可以后续做逻辑回归。
——[笔记︱风控分类模型种类（决策、排序）比较与模型评估体系（ROC/gini/KS/lift）](http://blog.csdn.net/sinat_26917383/article/details/51725102)



——————————————————————————————————————————





# 二、激活函数




神经网络模型中，激活函数是神经网络非线性的根源。




## 1、sigmoid函数=Logit




![](https://img-blog.csdn.net/20160621151653263)







其实就是逻辑回归的转化，神经网络=逻辑回归+变量的自动转化

如果激活函数是sigmoid的话，神经网络就是翻版的逻辑回归，只不过会自动转化（适合排序）





## 2、高斯型函数

![](https://img-blog.csdn.net/20160621151752123)




适合分类+聚类，识别类（欺诈行业很好，因为行为跟别人不一样，属于异常），在二维空间中就是等高线。


——————————————————————————————————————————





# 三、感知器




## 1、单感知器——无隐藏层




![](https://img-blog.csdn.net/20160621151825124)







Delta规则，w就是权重。很重要

单层感知器，相当于只要了神经网络的输入层以及输出层，比较简单，所以感知器其实相当于线性回归，也叫做线性神经网络，没有隐藏层


## 2、多层感知器——加入隐藏层




![](https://img-blog.csdn.net/20160621151901279)







两个隐藏层可以做任何复杂形状域。隐藏层因为属于黑箱，隐藏层越多，会产生过拟合现象（泛化能力不强），并且模型稳健性较差，但是要是模型调试的好，也是一匹“黑马”。

回归出现的所有错误（多重共线性（需进行变量筛选）、缺失值），神经网络都会出现，因为当激活函数为sigmoid时，等同于逻辑回归。


## 3、BP神经网络——多层感知器




BP神经网络对数据有严格要求，需要做极差标准化。

![](https://img-blog.csdn.net/20160621152149192)







△，小，就会摆动；大，乱跑；设置多少没有定论




——————————————————————————————————————————





# 四、BP神经网络-R语言实现——nnet包+AMORE包




BP神经网络需要对数据进行标准化，所以**建模之间切记要进行标准化**。



```
library(nnet)
help(package="nnet")
model_nnet<-nnet(y~., linout = F,size = 24, decay = 0.01, maxit = 100,trace = F,data = train) 
   #对分类数据预测需要加上y参数 
   #decay就是eta权重的调节，默认为0
   #linout=F默认，线性回归；T代表逻辑回归（激活函数只有一个sigmoid）
   #size就是隐藏层的个数，若size=0就是单感知器模型
```

linout=F代表线性回归，T代表逻辑回归（激活函数为sigmod）;


maxit代表最大循环迭代的次数，该值并不是越大越好，越大过拟合现象更严重，要调节在适当的数量。

size代表隐藏层大小，也跟迭代次数一样，层次越多过拟合现象加重，就会把训练集的很多噪声都拿来做建模，虽然训练集的精度高了，但是测试集的精度反而弱了，就是因为训练集噪声不适合于测试集的噪声。

BP神经网络调节模型精度AUC值的话：一般会选择调整maxit（最大迭代次数）+size（隐藏层大小）来调整最优精度。这里可以自编译一些函数来实现，CDA-DSC课程中就有一个自编译函数来进行选择。但是会耗费大量的运行速度。




AMORE包有待继续深入研究。





————————————————————————————




应用一：报错Error in nnet.default(x, y, w, entropy = TRUE, ...) 






```
Error in nnet.default(x, y, w, entropy = TRUE, ...) : 
  too many (1209) weights
```



      这个是因为隐藏层多了之后，运算不了，台式机不能运行那么多，所以要通过调整size的隐藏层个数来看效果如何。






















**每每以为攀得众山小，可、每每又切实来到起点，大牛们，缓缓脚步来俺笔记葩分享一下吧，please~**![](https://img-blog.csdn.net/20161213101203247)


———————————————————————————




