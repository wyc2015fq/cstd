# 数据预处理--克服数据不平衡 - wydbyxr的博客 - CSDN博客
2018年12月04日 09:50:45[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：251
  这种问题和业务需求也有很强的相关性，可能根据领域知识也能解决一些问题。
  一篇综述论文：《Learning from Imbalanced Data》。
# 不平衡数据评估指标
  1）单一评估指标、ROC曲线和PR曲线见机器学习：准确率(Precision)、召回率(Recall)、F值(F-Measure)、ROC曲线、PR曲线
  2）除此之外，Cost Function也可以作为一个指标。
  3）对于多分类，可以使用n个ROC曲线，即将其中一类当做正例，其余类当做负例。
  参考资料：
[https://blog.csdn.net/shine19930820/article/details/54143241](https://blog.csdn.net/shine19930820/article/details/54143241)
[https://blog.csdn.net/banbuduoyujian/article/details/54645345](https://blog.csdn.net/banbuduoyujian/article/details/54645345)
# 处理数据不平衡的方法(简单版)
  1）比较简单常用的，比如：
  数据少的时候常使用上采样（oversampling），复制观测值少的类的样本；
  数据多的时候常使用下采样（undersampling），去除观测值多的类的样本。
  2）通过算法生成不平衡样本
  经典的如 SMOTE（字面翻译 - 综合少数样本的过抽样技术，大概理解），使用两个或者多个样本的距离作为度量标准判断相似度，然后把其中一个样本加上随机噪声（或者叫扰动，此值实在相邻的样本的差异之间）来生成新样本。
  3）其他方法比如加权、用带惩罚的模型（比如 penalized-SVM 或者 penalized-LDA 等）。
  4）可以换个思路，把样本很不平衡问题换做异常点检测？或者用一分类（One-Class-SVM）？或许是考虑用 RandomForest 等对训练集随机采样的模型？
## 对于深度学习而言，过采样比欠采样要好，增加数据好比删除数据要好
# 如何克服不平衡(复杂版)
  解决方法主要分为两个方面。
  第一种方案主要从数据的角度出发，主要方法为抽样，既然我们的样本是不平衡的，那么可以通过某种策略进行抽样，从而让我们的数据相对均衡一些；
  第二种方案从算法的角度出发， 考虑不同误分类情况代价的差异性对算法进行优化，使得我们的算法在不平衡数据下也能有较好的效果。
## 1）取样方法：将不平衡数据通过某些方法变成平衡数据。
### a）随机过采样和欠采样
  随机过采样（Random Oversampling）：向minority中添加数据，可通过复制minority数据等，使得minority和majority数目相等。 —-添加重复数据，导致数据过拟合（overfit）
  随机欠采样（Random Undersampling）：从majority中减掉数据，使得minority和majority数目相等。 —-可能会丢失一些重要的数据。
### b）Informed Undersampling
  Informed Undersampling主要有3种算法：EasyEnsemble、BalanceCascade、K-nearest neighbour（KNN）和one-sided selection（OSS），主要目的是克服传统随机欠采样导致的数据丢失问题。
  EasyEnsemble：多次欠采样（放回采样）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。—-EasyEnsemble是非监督学习算法。
  BalanceCascade：先通过一次欠采样产生训练集，训练一个分类器，对于那些分类正确的majority样本不放回，然后对这个更小的majority样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。 —-BalanceCascade是监督学习算法。
  KNN：使用K近邻的方法挑选出一些K个样本，至于什么算是邻近的样本，每个算法有不同的定义。
### c）Synthetic Sampling with Data Generation
  synthetic minority oversampling technique（SMOTE）算法，算法在minority中，基于特征空间相似度，人工创造一些数据。
### d）Adaptive Synthetic Sampling
  为了克服SMOTE的缺点，Adaptive Synthetic Sampling方法被提出，主要包括：Borderline-SMOTE和Adaptive Synthetic Sampling（ADA-SYN）算法。
  Borderline-SMOTE：对靠近边界的minority样本创造新数据。其与SMOTE的不同是：SMOTE是对每一个minority样本产生综合新样本，而Borderline-SMOTE仅对靠近边界的minority样本创造新数据。
### e）Cluster-based Sampling Method
  基于聚类的采样算法（cluster-based sampling method，CBO）用来解决类内和类间数据的不平衡问题。比如K-means技术。
### f）Integration of Sampling and Boosting
  SMOTEBoost：基于SMOTE和Adaboost.M2的算法，其在boosting的每次迭代中都引入了SMOTE的方法，因此生成的每个分类器都集中于minority类，因为最后集成而得的分类器性能较好。
  DataBoost-IM：其将数据生成技术和Adaboost.M1结合，在没有牺牲majority类准确度的情况下，提高minority的预测率。DataBoost-IM根据类间difficult-to-learn比率去生成综合样本。这种方法在解决强不平衡数据方面有很好的性能，但是可能依赖于较适合的数据生成方法。
## 2）代价敏感方法：采样方法主要考虑正负例的分布，而代价敏感方法主要考虑误分类样本的代价，通过代价矩阵来度量。
### a）Cost-Sensitive Decision Trees
  代价敏感的决策树应用主要有三种形式：
  （i）代价敏感调整可以用于决策阈值，使用ROC刻画性能取值范围，ROC曲线的特征点用作最后的阈值。
  （ii）对于每个节点的拆分标准，可以作为代价敏感的考虑因素。
  （iii）可以在决策树上应用代价敏感的剪枝。在不平衡数据，直接移除掉概率低于某一特定阈值的节点，会发现移除的数据大多都是minority的数据，因此，应将重点放在概率估计上，使得剪掉的是positive的数据。
### b）Cost-Sensitive Neural Networks
  代价敏感性在神经网络上的应用主要有4个方面：
  （i）代价敏感变更可以应用到概率评估上。
  （ii）神经网络的输出也可以是代价敏感的。
  （iii）代价敏感变更适用于学习参数。
  （iv）最小误差函数可以用来预期代价。
# 克服数据不平衡的例子
  kaggle上的「座头鲸识别挑战」。
  在4251个训练图片中，有超过2000个类别中只有一张图片。还有一些类中有2-5个图片。现在，这是一个严重的不平衡类问题。我们不能指望用每个类别的一张图片对深度学习模型进行训练（虽然有些算法可能正是用来做这个的，例如 one-shot 分类问题，但我们现在忽略先这一点）。这也会产生一个问题，即如何划分训练样本和验证样本。理想情况下，您会希望每个类都在训练和验证样本中有所体现。
  方法：使用数据扩增，并进行过采样。先观察训练样本中的一些图像，这些图像都是鲸鱼的尾巴。因此，识别很可能与特定的图片方向有关。我也注意到在数据中有很多图像是黑白图片或只有R / B / G通道。
  故，以上代码块对不平衡类（数量小于10）中的每个图像都进行如下处理：
```
1.将每张图片的 R、G、B 通道分别保存为增强副本
2.保存每张图片非锐化的增强副本
3.保存每张图片非锐化的增强副本
```
  现在在每个不平衡类中都至少有了10个样本。我们继续进行训练。
