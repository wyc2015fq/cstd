# 最新论文阅读（2） - wydbyxr的博客 - CSDN博客
2018年06月01日 17:07:03[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：77
所属专栏：[深度学习--最新论文](https://blog.csdn.net/column/details/23683.html)
#Training RNNs as Fast as CNNs	
- 2017年9月
- Simple Recurrent Unit(SRU); NLP与语音识别任务; 加速
- ASAPP inc；MIT
　　通过有意简化状态计算并展现更多的并行性而提出了一个替代性的 RNN 实现，这一循环单元的运算和卷积层一样快，并且比 cuDNN 优化的 LSTM 快 5-10x。
　　该实现在诸如分类、问题回答、语言建模、机器翻译上证明了其有效性，并已在 PyTorch 和 CNTK1 中开源。
　　SRU实现：循环层之间增加highway连接;在将RNN正则化时，他们在标准的dropout外，增加了变分dropout(变分dropout在时间步长t与dropout使用相同的mask。然后，作者将和步骤t的神经门之间的连接全部丢弃，以这种方法来加速循环计算。相比之下，现有的RNN在实现时是要使用先前的输出状态的)。
#CNN-BLSTM（convolutional neural network combined with bidirectional long-short-term memory）
-
- Switchboard数据集上5.1%；语音识别
- Microsoft
#Stacked Deconvolutional Network for Semantic Segmentation	
- 2017年8月
- 没有特别的;就是把各种花样强行组合起来
-
提出了堆叠解卷积网络（SDN）；效果与其他语义分割网络相比最好；
　　stack（堆叠）的思想 + Densenet + FCN+hierarchical supervision
#Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex
- NIPS 2017 Spotlight	
- 贪心坐标下降 (Greedy Coordinate Descent, GCD)		
-
翻译过来就是，“通过在单纯形上软门限投影的加速随机贪心坐标下降”。GCD 被广泛用于机器学习里的求解稀疏优化的问题中。如果一个优化问题具有稀疏解，GCD 将比和它对应的算法 RCD 更为适合。"
#Enhanced Deep Residual Networks for Single Image Super-Resolution	
- 2017年7月	
- enhanced deep super-resolution network —— EDSR；训练方法；多尺度深度超分辨率 MDSR
- 韩国首尔大学
优化时去除了传统残差网络中的不必要模块。另一个原因就是，在使训练过程保持稳定的情况下，我们扩展了模型的规模。
　　NTIRE2017 超分辨率挑战赛的冠军和亚军；
　　过采用残差缩放（residual scaling ）方法来对大型模型进行稳定的训练。
　　去掉了BN和网络中的范围柔性（range flexibility）
