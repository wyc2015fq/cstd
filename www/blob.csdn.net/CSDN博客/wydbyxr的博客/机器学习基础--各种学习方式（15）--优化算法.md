# 机器学习基础--各种学习方式（15）--优化算法 - wydbyxr的博客 - CSDN博客
2018年07月14日 21:31:39[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：50
# 优化算法
### 对于优化问题的解决方法
　　1）确定方式 
　　2）近似方式，近似方法分为近似算法和启发式算法。
> 
1.近似算法通常可得到一个有质量保证的解； 
  2.启发式算法通常可找到在传统解决问题的经验中找到寻求一种面向问题的策略，之后用这种策略来在可行时间内寻找一个相对比较好的解，但对解的质量没有保证。贪心算法的确属于启发式算法的一种形式和应用。
# 连续优化（continuous optimization）算法
　　连续优化算法是机器学习最为常见的算法之一，其中包含一系列已知流行的算法，包括梯度下降、动量法、AdaGrad 和 ADAM 方法。
### 使用连续优化算法的原因
　　我们考虑过自动设计这些优化算法的问题，这么做有两个原因： 
　　1）首先，很多优化算法是在凸假设下设计的，但被应用到非凸目标函数上；通过在实际使用环境下学习，优化算法有望实现更好的性能。 
　　2）手动设计新的优化算法通常费时费力，可能需要数月或数年之久；学习优化算法可以减少手动设计的劳动量。
### 通常的工作方式
　　现有的连续优化算法通常的工作方式： 
　　1）它们通常以迭代的方式运行，并保持一定程度的迭代数量，并且每一次迭代都选取目标函数域内的一个点以搜索全局最小。 
　　2）在每个迭代中，算法都会使用固定的更新公式来计算步长向量，然后用它来修正迭代方向与下降步长。 
　　3）更新公式通常是在当前和过去的迭代中所评估历史梯度的函数，这些梯度都是目标函数对不同参数所求的偏导数。 
　　例如：在梯度下降中，更新公式是一些加权的负梯度（所加权为学习率）；在动量法中，更新公式是一些加权的指数移动均值。
