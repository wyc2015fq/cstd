# 集成学习之Boosting - wydbyxr的博客 - CSDN博客
2018年08月31日 17:57:31[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：136
个人分类：[机器学习具体算法](https://blog.csdn.net/wydbyxr/article/category/7945743)
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# Boosting
　　Boosting 指使用加权平均值使弱的学习器变强的一组算法。与 Bagging 不同，每个模型单独运行，最后在不偏向任何模型的前提下聚合输出结果。Boosting 是一项「团队工作」。每个模型决定下一个模型要关注的特征。 
　　Boosting 也需要 Bootstrapping。但是，这里还有一个区别。与 bagging 不同，boosting 为每个数据样本加权。这意味着一些样本运行的频率比其他样本高。 
　　当 Boosting 运行在模型中时，它追踪哪些数据样本是成功的，哪些不成功。输出结果分类错误最多的数据集会被赋予更高的权重。即这些数据更加复杂，需要更多次迭代才能恰当地训练模型。 
　　在实际的分类阶段中，Boosting 处理模型的方式也存在区别。Boosting 追踪模型误差率，因为更好的模型会获得更好的权重。这样，当「投票」（voting）出现时，结果更好的模型更有可能最终主导输出。 
　　boosting是”提升”的意思. 一般Boosting算法都是一个迭代的过程, 每一次新的训练都是为了改进上一次的结果。boosting在选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本。boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高(通常是边界附近的样本)，最后的分类器是很多弱分类器的线性叠加(加权组合)。 
## 实际使用
　　Boosting 和 Bagging 能够有效降低方差。集成方法通常优于单个模型。这就是那么多 Kaggle 获胜者使用集成方法的原因。 
　　但是，它们不适合所有问题，它们各自也有缺陷。Bagging 在模型过拟合时能够有效降低方差，但 Boosting 可能是二者中较好的选择。Boosting 更有可能导致性能问题，但它在模型欠拟合时也能有效降低偏差。 
　　这就需要经验和专业知识了！第一个模型能够成功运行可能比较容易，但是分析算法和它选择的所有特征非常重要。例如，如果一个决策树设置了特定的叶，那么这么设置的原因是什么呢？如果你无法用其他数据点或图支持它，它可能就不该实现。 
　　这不只是在不同的数据集上尝试 AdaBoost 或随机森林。我们需要根据算法的倾向和获得的支持来决定最终使用的算法。
## 与boosting的区别
　　不同点在于是采样方式。bagging:由于每次迭代的采样是独立的, 所以bagging可以并行。而boosting的采样或者更改样本的权重依赖于上一次迭代的结果, 在迭代层面上是不能并行的。  
　　不同点还在于带来的效果。Bagging主要减小了variance，而Boosting主要减小了bias，而这种差异直接推动结合Bagging和Boosting的MultiBoosting的诞生。
# adaboosting
　　这其实思想相当的简单, 大概是对一份数据, 建立M个模型(比如分类), 而一般这种模型比较简单, 称为弱分类器(weak learner). 每次分类都将上一次分错的数据权重提高一点, 对分对的数据权重降低一点, 再进行分类. 这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的效果。另外, 每个分类器的步长由在训练该分类器时的误差来生成。 
　　每次迭代的样本是一样的, 即没有采样过程, 不同的是不同的样本权重不一样. (当然也可以对样本/特征进行采样, 这个不是adaboosting的原意)。 
　　希望计算代价小，所以只用简单的分类器，但是又希望分类准确度高，于是把多个简单的分类器组合起来——聚弱为强，将多个弱分类器组合成一个强分类器，这就是AdaBoost方法的核心理念。通过AdaBoost方法来学习分类器，达到了以更小的计算代价换取同样的分类准确度的目的。 
　　采用AdaBoost方法由弱分类器构建强分类器，这是一个顺序执行的过程，换言之，一旦一个弱分类器被选中，其就必定会成为强分类器的组成部分，不允许反悔，这其实是假设增加弱分类器一定会使得强分类器的分类准确度更高，但是，这个假设并不总是成立。 
　　事实上，每次对弱分类器的选择只是依照当时的情况决定，而随着新的弱分类器被增加进来，从整体上来看，之前的选择未必最优。基于这样的想法，出现了允许回溯的FloatBoost方法。FloatBoost方法在选择新的弱分类器的同时，也会重新考查原有的弱分类器，如果去掉某个弱分类器之后强分类器的分类准确度得到了提升，那说明这个弱分类器带来了负面影响，应该被剔除。
# gradient boosting
　　Gradient Boosting就是梯度提升！！ 
　　每一次的计算是为了减少上一次的残差(residual), 而为了消除残差, 我们可以在残差减少的梯度 (Gradient)方向上建立一个新的模型. 所以说在Gradient Boost中, 每个新模型是为了使之前模型的残差往梯度方向减少, 与传统Boost对正确, 错误的样本进行加权有着很大的区别. (或者这样理解: 每一次建立模型是在之前建立模型损失函数的梯度下降方向。 
　　损失函数(loss function)描述的是模型的不靠谱程度。损失函数越大,  则说明模型越容易出错（其实这里有一个方差、偏差均衡的问题,  但是这里就假设损失函数越大, 模型越容易出错)。如果我们的模型能够让损失函数持续的下降,  则说明我们的模型在不停的改进,  而最好的方式就是让损失函数在其Gradient的方向上下降)。
　　与Adaboost不同的是，每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升。梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合（基函数）；提升算法通过迭代的选择一个扶梯度方向上的基函数来逐渐逼近局部极小值。
