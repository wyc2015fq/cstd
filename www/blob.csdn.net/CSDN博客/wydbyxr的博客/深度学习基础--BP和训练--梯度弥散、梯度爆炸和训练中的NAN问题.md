# 深度学习基础--BP和训练--梯度弥散、梯度爆炸和训练中的NAN问题 - wydbyxr的博客 - CSDN博客
2018年11月13日 09:50:29[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：155
# 梯度弥散、消失梯度问题（Vanishing Gradient Problem）
## 梯度消失问题的原因
  激活函数的梯度非常小的情况下会出现消失梯度问题。在权重乘以这些低梯度时的反向传播过程中，它们往往变得非常小，并且随着网络进一步深入而"“消失”"。这使得神经网络忘记了长距离依赖。这对循环神经网络来说是一个问题，长期依赖对于网络来说是非常重要的。
  这是由于每次梯度沿序列反向传播时会乘上一个W。当W的值很小时，多步反向传播后梯度就变得很小，这就说明了多步之前的信息只能起很小的作用。
  神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。
  梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0。造成学习停止。
## 梯度消失的例子
  在计算过程中，经常会用到激活函数，比如 Sigmoid 激活函数。残差在往前传播的过程中，每经过一个 Sigmoid 函数，就要乘以一个 Sigmoid 函数的导数值，残差值至少会因此消减为原来的 0.25 倍。神经网络每多一层，残差往前传递的时候，就会减少至少 3/4。如果层数太多，残差传递到前面已经为 0，导致前层网络中国呢的参数无法更新，这就是梯度消失。
## 解决深网络的梯度弥散问题
  1）使用更好的初始化方法，而非零初始或简单随机初始。
  2）使用Relu，而非sigmoid。
  3）使用batch normalization。
  但这三种方法只能解决一般前向网络的梯度弥散，对于RNN的问题，还是需要对结构进行改良，即使用LSTM或GRU。
# 梯度爆炸、梯度激增问题（Exploding Gradient Problem）
  这与消失的梯度问题完全相反，激活函数的梯度过大。
## 原因
  在反向传播期间，它使特定节点的权重相对于其他节点的权重非常高，这使得它们不重要。当W的值很大时，会导致反向传播时梯度越来越大（梯度爆炸），不利于网络训练。
  如果每一次梯度都大于1，则梯度越传越大，导致参数更新幅度很大，每次更新都很剧烈。	当权值过大，前面层比后面层梯度变化更快，会引起梯度爆炸问题。
## 解决
  这可以通过剪切梯度（clipping grad）来轻松解决，使其不超过一定值。
## sigmoid时，消失和爆炸哪个更易发生？
  量化分析梯度爆炸出现时a的树枝范围：因为sigmoid导数最大为1/4，故只有当abs(w)>4时才可能出现。
  由此计算出a的数值变化范围很小，仅仅在此窄范围内会出现梯度爆炸问题。而最普遍发生的是梯度消失问题。
# NaN trap（NAN陷阱）
  训练过程中，如果模型中的一个数字变成了 NaN，则模型中的很多或所有其他数字最终都变成 NaN。NaN 是「Not a Number」的缩写。
## 可能的原因
  1）cost函数中忘记在log的参数中加上一个极小值；
  2）计算过程中，出现了被除数为0的情况；
  3）梯度爆炸
  4）学习率设置的太大或者batchsize设置的太大，这个错误是因为logits输出太大变成INF，对这个取log就会在求梯度就会变成nan，nan是not a number 的缩写，表示不是一个有理数。所以除了调小学习率这个解决方案，另一个解决方案还可以给loss加正则化项。
  5）数据里有错误，数据本身就含有了nan的数据，错误的数据导致网络无法收敛。
