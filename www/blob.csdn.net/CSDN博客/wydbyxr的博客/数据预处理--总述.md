# 数据预处理--总述 - wydbyxr的博客 - CSDN博客
2018年12月03日 10:19:52[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：61
  围绕着要喂给神经网络的数据：收集数据、清洗数据、操作数据、给数据加标签、分析数据、做数据可视化等等。
# 数据预处理思想纲领
  基于对业务的理解，这种相对比较少一点，更多的还是基于数据本身。你可以用一些数据挖掘中常用的数据清理的方法，过滤异常值，过滤缺失严重的特征等等。
  有时候做预处理，最终还是需要一些反馈。比如从数据上看，可能需要扔掉某些东西。我们需要去试一试，扔掉这些东西之后，在最终的比赛中，提交的预测结果会不会变好。或者采用以结果导向的反馈，这样迭代去往下走。
  另外，每个比赛其实都有一些自己独特的数据，即使是同样领域的问题，数据的分布也可能差别很大。这时候之前的经验可能就不适用当前的问题，我们必须接受这样一个事实，需要针对数据重新去建立问题的解决方案。其实这还是一个偏实践的问题，实践推动着你往前走。
  你要真想玩，那你就得坚持，不断地去看论坛上的帖子，想想人家是怎么做的，再去改进自己的方案，那样总会迎来收获。
# 数据预处理
  数据本身决定了模型表现的上限，而模型和算法只是逼近这个上限，这和大部分数据挖掘教材中的 RIRO 原则是对应的，因此，对数据的预处理（和特征工程），要干的就是提高模型可逼近的上限，以及提高模型鲁棒性。
  1）怎么提高模型可逼近上限？
  提特征；特定领域还需要滤波和去噪；把干扰模型拟合的离群点扔掉。
  2）怎么提高模型的鲁棒性？
  数值型数据中的缺失值可以花式处理，默认值，平均值，中位值，线性插值；
  文本数据可以在字，词，句等粒度不同进行处理；
  图像数据进模型之前把图像进行 90，180，270 等不同角度翻转也是常见的处理方式了。
# 数值特征预处理
  1）连续型特征离散化
  将连续型特征离散化的一个好处是可以有效地克服数据中隐藏的缺陷：使模型结果更加稳定。例如，数据中的极端值是影响模型效果的一个重要因素。极端值导致模型参数过高或过低，或导致模型被虚假现象"“迷惑”"，把原来不存在的关系作为重要模式来学习。而离散化，尤其是等距离散，可以有效地减弱极端值和异常值的影响。
  例子：通过观察2.2节的原始数据集的统计信息，可以看出变量duration的最大值为4918，而75%分位数为319，远小于最大值，而且该变量的标准差为259，相对也比较大。因此对变量duration进行离散化。具体地，使用pandas.qcut()函数来离散化连续数据，它使用分位数对数据进行划分（分箱: bining），可以得到大小基本相等的箱子(bin)，以区间形式表示。然后使用pandas.factorize()函数将区间转为数值。
  2）规范化
  由于不同变量常常使用不同的度量单位，从数值上看它们相差很大，容易使基于距离度量的学习模型更容易受数值较大的变量影响。数据规范化就是将数据压缩到一个范围内，从而使得所有变量的单位影响一致。
```
for i in numeric_attrs:
        scaler = preprocessing.StandardScaler()
       data[i] = scaler.fit_transform(data[i])
```
  3）更细致的特征选择，如派生属性；
  4）解决数据不平衡问题，如代价敏感学习方法；
