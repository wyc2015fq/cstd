# 深度学习基础--卷积--1*1的卷积核与全连接的区别 - wydbyxr的博客 - CSDN博客
2018年11月14日 09:55:25[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：1130
# 1*1的卷积核与全连接的区别
  1*1的卷积核是输入map大小不固定的；而全连接是固定的。  1*1卷积的主要作用有以下两点：
  1）降维（ dimension reductionality ），inception中就是这个用。
  2）加入非线性，毕竟有激活。
  这里有个八卦是当年FCN得到CVPR’15 best paper honorable mention的时候，　Yann LeCun等人出来吐槽这个’FCN’的概念早就有了，AlexNet里面的fully connected layer (FC)本身就是个误导，因为FC layer可以看成是1x1的convolution, 本来就可以输入任意大小的图片。
## 更多的，卷积层跟全连接层的区别
  使得网络可以接受任意的输入的方法就是把全连接层变成卷积层，这就是所谓的卷积化。这里需要证明卷积化的等价性。
  直观上理解，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把卷积作用的区域扩大为整个输入，那就变成全连接了，我就不给出形式化定义了。所以我们只需要把卷积核变成跟输入的一个map的大小一样就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。
  举个例子，比如AlexNet，fc6的输入是256x6x6，那么这时候只需要把fc6变成是卷积核为6x6的卷积层就好了。
