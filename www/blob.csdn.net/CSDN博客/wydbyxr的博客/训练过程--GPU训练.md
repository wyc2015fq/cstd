# 训练过程--GPU训练 - wydbyxr的博客 - CSDN博客
2018年12月05日 14:09:23[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：73
个人分类：[算法的实际使用](https://blog.csdn.net/wydbyxr/article/category/8463718)
# 实际训练中，GPU的训练速度问题
  为什么Y2B的8M，8张普通TT为啥要跑几个月？
  因为GPU其实有60%的时间都是在等待数据填充完成，简直蠢。
## 解决方案
  1）换个固态硬盘试试？
  没用。问题出在系统总线上,一帧的数据量入显存后有1.58G
  2）cpu的io能力跟不上gpu，这问题常见.你是怎么解决的？
  用GPUDirect RDMA，让光纤网卡直通显卡显存。不经过CPU，直接数据交换，但是这时候，TT系列卡的劣势凸显得很厉害。
  TT卡的DMA是单工的，同时要么只能发，要么只能收。只有TESLA和quadro是双工的，可以同时收发。所以多路TT直接跑，跑几个月，一点都不冤枉，估计换DGX，也就是三四天的时间就完事。毕竟，DGX有NVLINK，卡间通讯能到300G。
# 分布式训练
  当前最优的分布式训练方式是通过参数服务器（Parameter Server）执行的同步随机梯度下降算法（SGD）。
  这是一种简单的分布式算法，其中存在一组节点，每个节点都拥有经过训练的神经网络的一个版本。这些节点通过一个服务器共享梯度和权重等信息。当试图扩展节点数量时，问题出现了。考虑到如今的问题涉及的巨大维度，当扩展节点数量（到什么级别）时，节点将不得不交流大量的信息。
# 多GPU训练
  目前的GPU特别适合跨GPU并行化，因为它们能够直接从另一个GPU的内存中读出和写入，不需要通过主机内存。
  一般有两种方法，一种是常用的数据并行，另一种是模型并行。
  1）模型并行指的是将一个完整的网络切分成不同块放在不同gpu上执行，每个gpu可能只处理某一张图的四分之一。
  采用模型并行很大程度上是因为显存不够放不下整个网络的数据，而现在gpu的功能性能提高，一个gpu已经能够很好的解决显存不够的问题，再加上模型并行会有额外的通信开销，因此开源框架采用了数据并行，用来提高并行度。
  我们采用的并行方案基本上是在每个GPU中放置一半核（或神经元），
  一个额外的技巧：GPU间的通讯只在某些层进行。
  这就是说，例如，第3层的核需要从第2层中所有核映射输入。然而，第4层的核只需要从第3层中位于同一GPU的那些核映射输入。选择连接模式是一个交叉验证的问题，但是这让我们可以精确地调整通信量，直到它的计算量在可接受的部分。
