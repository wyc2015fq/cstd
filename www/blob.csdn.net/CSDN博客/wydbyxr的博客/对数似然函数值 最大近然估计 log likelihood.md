# 对数似然函数值/最大近然估计/log likelihood - wydbyxr的博客 - CSDN博客
2018年10月20日 13:28:06[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：2642
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# 对数似然函数值/最大近然估计/log likelihood
  在参数估计中有一类方法叫做“最大似然估计”,因为涉及到的估计函数往往是是指数型族,取对数后不影响它的单调性但会让计算过程变得简单,所以就采用了似然函数的对数,称“对数似然函数”。
  根据涉及的模型不同,对数函数会不尽相同,但是原理是一样的,都是从因变量的密度函数的到来,并涉及到对随机干扰项分布的假设。
## 最大似然估计法的基本思想
  极大似然原理的直观想法是：一个随机试验如有若干个可能的结果A，B，C，…。若在一次试验中，结果A出现，则一般认为试验条件对A出现有利，也即A出现的概率很大。
  最大似然估计法的思想很简单：在已经得到试验结果的情况下，我们应该寻找使这个结果出现的可能性最大的那个X作为真X的估计。求X的极大似然估计就归结为求L(X)的最大值点，而由于对数函数是单调增函数，所以对L(X)取log。
  对log（L(X)）关于X求导数，并命其等于零，得到的方程组称为似然方程组。解方程组log（L(X)），又能验证它是一个极大值点，则它必是L(X)的最大值点，即为所求的最大似然估计。
## 理解对数似然估计函数值
  对数似然估计函数值一般取负值，实际值（不是绝对值）越大越好。
  1）第一，基本推理。对于似然函数，如果是离散分布，最后得到的数值直接就是概率，取值区间为0-1，对数化之后的值就是负数了；如果是连续变量，因为概率密度函数的取值区间并不局限于0-1，所以最后得到的似然函数值不是概率而只是概率密度函数值，这样对数化之后的正负就不确定了。
  2）第二，Eviews的计算公式解释。公式值的大小关键取之于残差平方和（以及样本容量），只有当残差平方和与样本容量的比之很小时，括号内的值才可能为负，从而公式值为正，这时说明参数拟合效度很高；反之公式值为负，但其绝对值越小表示残差平方和越小，因而参数拟合效度越高。
### 是不是解释变量减少了，log likelihood必然会变小？
  解释变量越多，因变量中被解释的部分就越多，对应的似然函数就越大，反之，解释变量少了，似然函数就会变小。你从对数似然函数的公式中也可以看出来，当变量更多的时候，比如从2个增加到3个，似然函数就可以在更大的空间范围内搜索最大值，所以3个解释变量得到的最大值肯定不会小于2个解释变量的情况。
### 解释变量个数不同，是不是log likelihood就没有可比性了？
  可以比较，似然函数本身的含义就是“当参数取某个值时，得到观测结果的可能性”。
### 根据目前这种后面的log likelihood变小的情况，能说log likelihood变量减少之后的模型要优于之前的模型么？
  不能，你看对数似然函数来决定是否需要采纳某个变量这是不符合逻辑的。选择某个变量进入方程原则上是应该看经济理论，比如在生产函数中，Y=f(L,K),L为劳动力，K为资本存量，为什么这个方程要有L和K，因为宏观经济学理论是这样的。又比如，还可以把人力资源成本加入到生产函数中，也是因为有相关的理论支持。当经济理论支持某个方程时候，多重共线性其实不算很大的问题。所以，你的出发点应该回到经济理论，而不是简单的看计量技术角度。
### 我的最终目的是想佐证：逐步回归之后精简了解释变量的模型要优于原来的模型（先不考虑经济理论）。目前看来log likelihood变小不能给我提供任何佐证了。那么，有没有别的指标可以表明新模型比旧模型要好？
  看F统计量和R^2值。
