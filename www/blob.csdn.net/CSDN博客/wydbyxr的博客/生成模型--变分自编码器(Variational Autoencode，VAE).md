# 生成模型--变分自编码器(Variational Autoencode，VAE) - wydbyxr的博客 - CSDN博客
2018年10月26日 09:39:33[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：579
# 变分自编码器(Variational Autoencode，VAE)
  Generative Models，Variational Autoencoder(VAE) 和 GAN 可以说是两座大山头。
  VAE包括两部分：编码器和解码器。编码器将数据分布的高级特征映射到数据的低级表征，低级表征叫作本征向量（latent vector）。解码器吸收数据的低级表征，然后输出同样数据的高级表征。
  参考资料：[http://geek.csdn.net/news/detail/230599](http://geek.csdn.net/news/detail/230599)
## 与标准自编码器有何不同？
  VAE的关键区别在于对本征向量（即编码器输出的向量）的约束。	
  如果是标准自编码器，那么我们主要关注重建损失（reconstruction loss）。
  而在变分自编码器的情况中，我们希望本征向量遵循特定的分布，通常是单位高斯分布（unit Gaussian distribution），即Encoder的Loss计算使用的是KL散度。即使下列损失得到优化：
  p(z′)∼N(0,I)
  其中,I 指单位矩阵（identity matrx），q(z∣X) 是本征向量的分布，其中。和由神经网络来计算。KL(A,B) 是分布 B 到 A 的 KL 散度。
## 标准自编码器与变分自编码器的不同（详解）
### 标准自编码器
  能学习生成紧凑的数据表达并重建输入数据，然而除了像去噪自编码器等为数不多的应用外，它的应用却极其有限。其根本原因在于自编码器将输入转换为隐含空间中的表达并不是连续的，使得其中的插值和扰动难以完成。
  例如利用MNIST数据集训练的自编码器将数据映射到2D隐含空间中，图中显示不同的分类之间存在着明显的距离。这使得解码器对于存在于类别之间的区域无法便捷的进行解码。如果你不想仅仅只是复现输入图像，而是想从隐含空间中随机的采样或是在输入图像上生成一定的变化，那此时一个连续的隐含空间就变得必不可少了。
  如果隐含空间不连续，那么在不同类别中间空白的地方采样后解码器就会生成非真实的输出。因为解码器不知道如何除了一片空白的隐含区域，它在训练过程中从未见到过处于这一区域的样本。
### 变分自编码器
  VAE具有与标准自编码器完全不同的特性，它的隐含空间被设计为连续的分布以便进行随机采样和插值，这使得它成为了有效的生成模型。它通过很独特的方式来实现这一特性，编码器不是输出先前的n维度向量而是输出两个n维矢量：分别是均值向量μ和标准差向量σ。
  这一随机生成意味着即使对于均值和方差相同的输入，实际的编码也会由于每一次采样的不同而产生不同的编码结果。其中均值矢量控制着编码输入的中心，而标准差则控制着这一区域的大小（编码可以从均值发生变化的范围）。
  通过采样得到的编码可以是这一区域里的任意位置，解码器学习到的不仅是单个点在隐含空间中的表示，而是整个邻域内点的编码表示。这使得解码器不仅仅能解码隐含空间中单一特定的编码，而且可以解码在一定程度上变化的编码，而这是由于解码器通过了一定程度上变化的编码训练而成。
  所得到的模型目前就暴露在了一定程度局域变化的编码中，使得隐含空间中的相似样本在局域尺度上变得平滑。理想情况下不相似的样本在隐含空间中存在一定重叠，使得在不同类别间的插值成为可能。但这样的方法还存在一个问题，我们无法对μ和σ的取值给出限制，这会造成编码器在不同类别上学习出的均值相去甚远，使它们间的聚类分开。最小化σ使得相同的样本不会产生太大差异。这使得解码器可以从训练数据进行高效重建。
## VAE的架构
![在这里插入图片描述](https://img-blog.csdn.net/20181025135323193?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 高斯过程与变分自动编码器(VAE)
  高斯过程（Gaussian Processes）曾经在 NIPS 中十分流行，有时也会被应用在图像问题上，但在深度学习时代却被「遗忘」了。
  在 pertaining 还是训练深度神经网络的唯一方法时，可变化自动编码器（VAEs）也曾是十分流行的。
  然而，随着对抗网络这类新技术的发展，人们愈加频繁地使用自动编码器。因为在内心深处，我们仍旧「盼望」着能有像编码器/解码器这样简单的网络，来赋予我们无监督学习的力量。
  人们对 VAE 进行了许多尝试，但是今天它已经不是那么主流了。
## 自编码器与不连续
  整个自编码器神经网络常常作为整体进行训练，其损失函数则定义为重建输出与原始输入之间的均方差/交叉熵，作为重建损失函数来惩罚网络生成与原始输入不同的输出。中间的编码作为隐藏层间链接的输出，其维度远远小于输出数据。
  参考资料：[http://www.a-site.cn/article/1668783.html](http://www.a-site.cn/article/1668783.html) （一篇文章告诉你「变分自编码器 (VAE)」的优秀）
