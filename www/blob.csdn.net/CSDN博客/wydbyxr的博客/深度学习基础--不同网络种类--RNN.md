# 深度学习基础--不同网络种类--RNN - wydbyxr的博客 - CSDN博客
2018年11月08日 13:30:28[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：107
# RNN
  全连接的DNN存在着一个问题——无法对时间序列上的变化进行建模。然而，样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。对了适应这种需求，出现了另一种神经网络结构——循环神经网络RNN。
  神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了(i-1)层神经元在该时刻的输出外，还包括其自身在(m-1)时刻的输出。
  RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度!正如我们上面所说，“梯度消失”现象又要出现了，只不过这次发生在时间轴上。
## Bidirectional RNNs(双向网络)的改进之处
  假设当前的输出(第t步的输出)不仅仅与前面的序列有关，并且还与后面的序列有关。
  为解决RNN的时间上的梯度消失，使用LSTM（长短时记忆模型)，通过门的开关实现时间上记忆功能，并防止梯度消失。
  对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。
  因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。
## 思考与总结
  RNN既然能继承历史信息，是不是也能吸收点未来的信息呢?因为在序列信号分析中，如果我能预知未来，对识别一定也是有所帮助的。
  因此就有了双向RNN、双向LSTM，同时利用历史和未来的信息。
  事实上，不论是那种网络，他们在实际应用中常常都混合着使用，比如CNN和RNN在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。
