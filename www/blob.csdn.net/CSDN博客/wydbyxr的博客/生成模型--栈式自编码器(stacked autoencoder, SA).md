# 生成模型--栈式自编码器(stacked autoencoder, SA) - wydbyxr的博客 - CSDN博客
2018年10月25日 13:34:57[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：541
# 栈式自编码器(stacked autoencoder, SA)
  即多个自编码器堆叠而成，是深度神经网络中的一种。
  例如：多个去噪自编码器的堆叠就变成了stacked denoised autoencoder (SDA)。
## 优点和缺点
  stacked denoised autoencoder (SDA)深度学习结构，和DBN类似 使用 无监督的网络“堆叠”起来的，他有分层预训练来寻找更好的参数，最后使用BP来微调网络。比dnn利用各种算法来初始化权值矩阵，从经验上来看是有帮助的。
  缺点也很明显，每层的贪婪学习权值矩阵，也带来了过长的训练时间。在大量的数据面前 dnn(relu)的效果已经不差于预训练的深度学习结构了。最终DBN也是看成是“生成模型”。
  CNN也没有pre-train过程，训练算法也是用BP。因为加入卷积可以更好的处理2D数据，例如图像和语音。并且目前看来相比其它网络有更好的表现。dnn/dbn/sda 等都是处理1D的数据。
## 训练过程
**无监督的逐层预训练+有监督的BP。**
  1）用原始输入数据作为输入，训练出（利用sparse autoencoder方法）第一个隐含层结构的网络参数，并将用训练好的参数算出第1个隐含层的输出。
  2）把步骤1的输出作为第2个网络的输入，用同样的方法训练第2个隐含层网络的参数。
  3）用步骤2 的输出作为多分类器softmax的输入，然后利用原始数据的标签来训练出softmax分类器的网络参数。
  4）计算2个隐含层加softmax分类器整个网络一起的损失函数，以及整个网络对每个参数的偏导函数值。
  5）用步骤1，2和3的网络参数作为整个深度网络（2个隐含层,1个softmax输出层）参数初始化的值，然后用lbfs算法迭代求出上面损失函数最小值附近处的参数值，并作为整个网络最后的最优参数值。
### 关于深度网络的学习几个需要注意的小点（假设隐含层为2层）
  1）利用sparse autoencoder进行预训练时，需要依次计算出每个隐含层的输出，如果后面是采用softmax分类器的话，则同样也需要用最后一个隐含层的输出作为softmax的输入来训练softmax的网络参数。
  2）由步骤1可知，在进行参数校正之前是需要对分类器的参数进行预训练的。且在进行参数校正(Finetuning )时是将所有的隐含层看做是一个单一的网络层，因此每一次迭代就可以更新所有网络层的参数。
## SAE与DBN的区别
  堆栈自编码网络的结构与DBN类似，由若干结构单元堆栈组成，不同之处在于其结构单元为自编码模型(auto—en—coder)而不是RBM。自编码模型是一个两层的神经网络，第一层称为编码层，第二层称为解码层。
