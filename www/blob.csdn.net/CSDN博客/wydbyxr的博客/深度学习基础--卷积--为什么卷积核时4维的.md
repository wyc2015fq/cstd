# 深度学习基础--卷积--为什么卷积核时4维的 - wydbyxr的博客 - CSDN博客
2018年11月15日 09:56:45[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：503
# 为什么卷积核时4维的
  因为本来就是4维的，input_channel*kernel_size*kernel_size*output_channel
  正常来说，参数的个数不是只和卷积核大小及数量有关吗，256个1通道的5*5的卷积核参数应该是256*1*5*5吧，和输入的特征图数量应该没有关系吧，请问该怎么理解？？？？
  你要联系BP网络看，BP里的节点数就是这里的第三维，而每一个节点的输入是二维的，权重就是卷积核二维的。
  你可能没有理解卷积的概念。比如通常我们说5*5的卷积核，其实都忽略了卷积核的第三个通道，该通道是和输入的feature_map的第3个维度有关的。
## 例子1
  比如我们data层输入了一张224*224*3的图片，然后我们第一个卷积层里面参数是kernel_size =5,output=96.那么我们卷积部分的参数应该是5*5*3*96,意思就是一个卷积核要同时“卷积”多个维度。
## 例子2
  这层卷积层总共设置32个神经元，也就是有32个卷积核去分别关注32个特征。窗口的大小是5×5,所以指向每个卷积层的权重也是5×5,因为图片是黑白色的，只有一个颜色通道，所以总共只有1个面，故每个卷积核都对应一组5*5*1的权重。
  因此，w权重的tensor大小应是[5,5,1,32]，而b权重的tensor大小应是[ 32 ]。
