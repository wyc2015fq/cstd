# 期望最大算法（EM算法） - wydbyxr的博客 - CSDN博客
2018年10月20日 12:02:49[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：177
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# 期望最大算法（EM算法）
  是一种以迭代的方式来解决一类特殊最大似然 (Maximum Likelihood) 问题的方法,这类问题通常是无法直接求得最优解。
  Expectation-Maximization 算法是统计学中用来给带隐含变量的模型做最大似然（和最大后验概率）的一种方法。EM算法的目标是使包含隐变量的数据集的后验概率或似然函数最大化，进而得到最优的参数估计。
  在上述存在隐变量的问题中，不能直接通过极大似然估计求出模型中的参数，EM算法是一种解决存在隐含变量优化问题的有效方法。
  EM 的应用特别广泛，经典的比如做概率密度估计用的 高斯混合模型(Gaussian Mixture Model)。
  我们知道，通过贝叶斯公式，可以发现后验概率中包含了似然函数和先验概率（忽略分母的那个evidence项），因此求最大后验概率的过程中包含了求极大似然估计的过程。因此虽然EM算法的目标是最大化后验概率或似然函数，而本质上就可以认为是最大化似然函数。因此下面我们直接讨论最大化似然函数。
  明白一个概念，机器学习所有的算法只有一个目标，那就是找出目标函数的最大(小)值，所以EM算法肯定不是凭空出现的，它也是为了找出某个问题的最大(小)值。
## EM算法理解的九层境界
EM 就是 E + M
EM 是一种局部下限构造
K-Means是一种Hard EM算法
从EM 到 广义EM
广义EM的一个特例是VBEM
广义EM的另一个特例是WS算法
广义EM的再一个特例是Gibbs抽样算法
WS算法是VAE和GAN组合的简化版
KL距离的统一
## 具体步骤
  EM算法是期望极大(Expectation Maximization)算法的简称，EM算法是一种迭代型的算法，在每一次的迭代过程中，主要分为两步：即求期望(Expectation)步骤和最大化(Maximization)步骤。
  1）我们首先固定一个θ（也就是随便给θ取个初始值），然后我们计算出隐变量z的取值的后验概率，就能让这个包含隐变量的似然函数变成传统意义上的似然函数，即只考虑参数θ的似然函数（这个过程称为E步）
  2）而最大化传统意义上的似然函数就不用啰嗦啦那就用传统的方法最大化呀最大化了以后就得到了当前的最优θ（这个过程称为M步）
  3）而得到了当前的最优θ以后，我们又可以重新计算出隐变量z的取值的后验概率，就能……总之就又可以E步，然后又M步，然后又E，又M……
  就这样一直重复，一直重复，直到似然函数的值不再变化，此时每个样本的Qi就是每个样本的标签，而此时的θ就是最终那个最优的θ啦~
  至此，理论上的EM算法完成了，最终得到的就是我们要估计的最优参数θ，顺便得到了每个样本的隐变量的取值。
