# 训练过程--正则化(regularization)技巧(包括L2正则化、dropout，数据增广，早停) - wydbyxr的博客 - CSDN博客
2018年12月06日 14:30:35[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：262
# 正则化（regularization）
  正则化是解决高方差问题的重要方案之一，也是Reducing Overfiltering（克服过拟合）的方法。
  过拟合一直是DeepLearning的大敌，它会导致训练集的error rate非常小，而测试集的error rate大部分时候很大。网络的拟合能力随之降低，这会使网络不容易过拟合到训练集。
## 1）L2正则化
  L2正则化倾向于使网络的权值接近0。这会使前一层神经元对后一层神经元的影响降低，使网络变得简单，降低网络的有效大小，降低网络的拟合能力。L2正则化实质上是对权值做线性衰减，所以L2正则化也被称为权值衰减（weight decay）。
## 2）随机失活（dropout）
  在训练时，随机失活随机选择一部分神经元，使其置零，不参与本次优化迭代。随机失活减少了每次参与优化迭代的神经元数目，使网络的有效大小变小。
  随机失活的作用有两点。
  1. 降低神经元之间耦合。因为神经元会被随机置零，所以每个神经元不能依赖于其他神经元，这会迫使每个神经元自身要能提取到合适的特征。
  2. 网络集成。随机失活可以看作在训练时每次迭代定义出一个新的网络，这些网络共享权值。在测试时的网络是这些网络的集成。
## 3）数据扩充（data augmentation）
  这实质是获得更多数据的方法。当收集数据很昂贵，或者我们拿到的是第二手数据，数据就这么多时，我们从现有数据中扩充生成更多数据，用生成的“伪造”数据当作更多的真实数据进行训练。以图像数据做分类任务为例，把图像水平翻转、移动一定位置、旋转一定角度、或做一点色彩变化等，这些操作通常都不会影响这幅图像对应的标记。并且你可以尝试这些操作的组合，理论上讲，你可以通过这些组合得到无穷多的训练样本。
## 4）早停（early stopping）
  随着训练的进行，当你发现验证集误差不再变化或者开始上升时，提前停止训练。
# Dropout
  dropout层一般用在FC层之后，每次forward的时候FC之前层的每个神经元会以一定的概率不参与forward，而backward的时候这些单元也不参与。这种方式使得网络强制以部分神经元来表示当前的图片，很大限度上降低过拟合。但是这样一定程度上会延长训练的时间，因为随机性不只是会打乱过拟合的过程，也会打乱正常拟合的过程。
  在test的时候，不使用dropout，使所有神经元参与运算，给他们的结果乘以0.5来作为输出值。
  其实，最后不乘0.5也是可以的。如果在分类的时候使用的不是原来的softmax，那么只要特征之间可以区分就行了，乘不同的系数只是放大或者缩小了这种差别。
  dropout 已经theoretically 证明等价于加 l2 regularizer 了。
  1）MAXOUT常与dropout一起用
  Maxout出现在ICML2013上，作者Goodfellow将maxout和dropout结合后，号称在MNIST, CIFAR-10, CIFAR-100, SVHN这4个数据上都取得了start-of-art的识别率。
  2）Dropout函数能减弱过拟合效应，所以当发现训练时的精度很高，但测试集上的精度上不去，就要使用它
  3）当特征比较稀疏，75%是0，使用dropout解决。
# 数据扩增
  简而言之就是对现有数据进行变换，使得总数据量得到提升。可以对图片进行集合变换，如平移，水平翻转等，论文中把原始图像缩放到256X256，然后分别取四个corner以及中间的224X224大小的patch，以及其flip后的patch来训练。论文还提到了对图片的RGB通道进行强度改变。即在训练集的RGB通道上做PCA，但是不降维，只取特征向量和特征值，对训练集上每张图片的每个像素加上值：
。。。
  其中和分别表示特征向量和特征值，表示高斯随机变量（均值为0，方差为0.1）
# 早期停止Early Stopping
  是另一种正则化方法，就是在训练集和验证集上，一次迭代之后计算它们上面的错误率，当在验证集上错误率最小，而没开始增大的时候就停止，因为再接着训练的话，训练集上误差应该是接着下降的，而验证集上的误差就会上升了，这时候就是要过拟合了，所以及时的停止，从而得到泛化最好的模型。
  所谓early stopping，即在每一个epoch结束时（一个epoch即对所有训练数据的一轮遍历）计算 validation data的accuracy，当accuracy不再提高时，就停止训练。这是很自然的做法，因为accuracy不再提高了，训练下去也没用。另外，这样做还能防止overfitting。
  早期停止和权重衰减之间的关系可以量化，所以关系a*q（其中a表示第几次迭代，而q表示学习率）扮演着正则化参数的角色，所以网络中有效的参数就会随着训练的过程而不断的增长。（其实就是网络中参数参与的会越来越多，这是一种形象的解释）。
## 具体做法
  那么，怎么样才算是validation accuracy不再提高呢？并不是说alidation accuracy一降下来，它就是“不再提高”，因为可能经过这个epoch后，accuracy降低了，但是随后的epoch又让accuracy升上去了，所以不能根据一两次的连续降低就判断“不再提高”。
  正确的做法是，在训练的过程中，记录最佳的validation accuracy，当连续10次epoch（或者更多次）没达到最佳accuracy时，你可以认为“不再提高”，此时使用early stopping。
  这个策略就叫“ no-improvement-in-n”，n即epoch的次数，可以根据实际情况取10、20、30….
