# NIN(network in network) - wydbyxr的博客 - CSDN博客
2018年11月27日 09:49:01[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：61
# NIN(network in network)
## 1）MLP卷积层
  利用多层mlp的微型网络，对每个局部感受野的神经元进行更加复杂的运算，而以前的卷积层，局部感受野的运算仅仅只是一个单层的神经网络罢了。
  提出了对卷积层的改进算法：MLP卷积层。Mlpconv层可以看成是每个卷积的局部感受野中还包含了一个微型的多层网络。一般来说mlp是一个三层的网络结构。NIN网络的mlp指的是局部感受野的，不是说1*1的卷积核等价于全连接。MLP与cnn相兼容，可以使用BP训练;并且MLP可自行深度化。
## 2）全局均值池化
  另一方面，传统的CNN最后一层都是全连接层，参数个数非常之多，容易引起过拟合（如Alexnet）,一个CNN模型，大部分的参数都被全连接层给占用了，故这篇paper提出采用了：全局均值池化，替代全连接层。
  传统的卷积神经网络卷积运算一般是出现在低层网络。对于分类问题，最后一个卷积层的特征图通过量化然后与全连接层连接，最后在接一个softmax逻辑回归分类层。
  然而，全连接层因为参数个数太多，往往容易出现过拟合的现象，导致网络的泛化能力不尽人意。于是Hinton采用了Dropout的方法，来提高网络的泛化能力。
  到了我们，我们提出采用全局均值池化的方法，替代传统CNN中的全连接层。与传统的全连接层不同，我们对每个特征图一整张图片进行全局均值池化，这样每张特征图都可以得到一个输出。另一方面它有一个特点，每张特征图相当于一个输出特征，然后这个特征就表示了我们输出类的特征。这样如果我们在做1000个分类任务的时候，我们网络在设计的时候，最后一层的特征图个数就要选择1000。
  在NIN的文章中表明，全连接层容易引起过拟合，去掉全连接层反而有助于精度提升。
