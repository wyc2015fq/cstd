# 深度学习基础--正则化与norm--Batch Normalization，简称BN - wydbyxr的博客 - CSDN博客
2018年11月20日 10:21:26[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：294
# Batch Normalization，简称BN
  一种正则化的方法。所谓的BN是指在数据经过一层进入下一层之前，需要对数据进行归一化，使之均值为0，方差为1。这样可以使得各层的参数量级上没有太大的差别。
## 优点
  1）加速训练
  2）减小权重的值的尺度的影响
  3）归一化所带来的噪声也有模型正则化的作用。
  bn的主要作用是控制数值区间，让比较深的网络训练起来稳定性比较好，更不容易爆炸。
## 缺点
  在训练时，因为要对数据进行scale，所以有很多矩阵乘法，导致训练时间过长。
## 实际使用
  1）理想情况下E和Var应该是针对整个数据集的，但显然这是不现实的。因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。
  参考资料：[http://blog.csdn.net/elaine_bao/article/details/50890491](http://blog.csdn.net/elaine_bao/article/details/50890491)
  2）使用BN的过程中，作者发现Sigmoid激活函数比Relu效果要好。
  均值和方差采用的是滑动平均的更新方式。因此，BN层共存储了3个数值：均值滑动和、方差滑动和、滑动系数和。
  3）在influence时，BN的参数是固定的。
## BN的局限性
  图像超分辨率和图像生成方面做了一些实践，发现在这类任务中，Batch Norm的表现并不好，加入了Batch Norm，反而使得训练速度缓慢，不稳定，甚至最后发散。
## 局限性分析
  Bn有着去除图像的对比度信息，利用图像的结构信息的功能。故在图像超分辨率、图像生成、GAN任务上不适用，而对风格转移很友好。
#### 分析1
  以图像超分辨率来说，网络输出的图像在色彩、对比度、亮度上要求和输入一致，改变的仅仅是分辨率和一些细节，而Batch Norm，对图像来说类似于一种对比度的拉伸，任何图像经过Batch Norm后，其色彩的分布都会被归一化，也就是说，它破坏了图像原本的对比度信息，所以Batch Norm的加入反而影响了网络输出的质量。
  虽然Batch Norm中的scale和shift参数可以抵消归一化的效果，但这样就增加了训练的难度和时间，还不如直接不用。
  基于这种想法，也可以从另外一种角度解释Batch Norm为何在图像分类任务上如此有效。图像分类不需要保留图像的对比度信息，利用图像的结构信息就可以完成分类，所以，将图像都通过Batch Norm进行归一化，反而降低了训练难度，甚至一些不明显的结构，在Batch Norm后也会被凸显出来（对比度被拉开了）。
  BN在classification当中的效果较好，主要原因是因为classification对于scale不敏感。但是superresolution这样图片到图片的变换，per image的scale还是一个有效信息。SRResNet时就发现去掉BN效果更好也更快，而且我也是把L2 loss改为了L1。
#### 分析2
  BN对于超分辨和去噪有很好的效果。
  1）对于去噪，每个batch的噪声输出分布是恒定的i.i.d高斯分布，这恰好很大程度上服从了BN的样本独立同分布假设，可以说BN最适合的应用就是去噪（输出得是噪声，最好还得是一个噪声水平）；
  2）对于超分辨，则没有去噪这个性质，输出的残差分布不是独立同分布的。因此相比较于超分辨，BN更适合去噪。在图像去噪领域，虽然超分辨CNN和去噪CNN的模型高度相似，且都属于回归类问题，但是BN在去噪CNN中却有更好的效果。
#### 分析3
  对于照片风格转移，为何可以用Batch Norm呢？
  原因在于，风格化后的图像，其色彩、对比度、亮度均和原图像无关，而只与风格图像有关，原图像只有结构信息被表现到了最后生成的图像中。
  Batch Norm会忽略图像像素（或者特征）之间的绝对差异（因为均值归零，方差归一），而只考虑相对差异，所以在不需要绝对差异的任务中（比如分类），有锦上添花的效果。而对于图像超分辨率这种需要利用绝对差异的任务，Batch Norm只会添乱。
## Batch Norm 的四大罪状
  1）如果 Batch Size 太小，则 BN 效果明显下降。
  2）对于有些像素级图片生成任务来说，BN 效果不佳；
  对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常 BN 是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用 BN 会带来负面效果，这很可能是因为在 Mini-Batch 内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。
  3）RNN 等动态网络使用 BN 效果不佳，且使用起来不方便
  对于 RNN 来说，尽管其结构看上去是个静态网络，但在实际运行展开时是个动态网络结构，因为输入的 Sequence 序列是不定长的，这源自同一个 Mini-Batch 中的训练实例有长有短。对于类似 RNN 这种动态网络结构，BN 使用起来不方便，因为要应用 BN，那么 RNN 的每个时间步需要维护各自的统计量，而 Mini-Batch 中的训练实例长短不一，这意味着 RNN 不同时间步的隐层会看到不同数量的输入数据，而这会给 BN 的正确使用带来问题。假设 Mini-Batch 中只有个别特别长的例子，那么对较深时间步深度的 RNN 网络隐层来说，其统计量不方便统计而且其统计有效性也非常值得怀疑。另外，如果在推理阶段遇到长度特别长的例子，也许根本在训练阶段都无法获得深层网络的统计量。综上，在 RNN 这种动态网络中使用 BN 很不方便，而且很多改进版本的 BN 应用在 RNN 效果也一般。
  4）训练时和推理时统计量不一致
  对于 BN 来说，采用 Mini-Batch 内实例来计算统计量，这在训练时没有问题，但是在模型训练好之后，在线推理的时候会有麻烦。因为在线推理或预测的时候，是单实例的，不存在 Mini-Batch，所以就无法获得 BN 计算所需的均值和方差，一般解决方法是采用训练时刻记录的各个 Mini-Batch 的统计量的数学期望，以此来推算全局的均值和方差，在线推理时采用这样推导出的统计量。虽说实际使用并没大问题，但是确实存在训练和推理时刻统计量计算方法不一致的问题。
