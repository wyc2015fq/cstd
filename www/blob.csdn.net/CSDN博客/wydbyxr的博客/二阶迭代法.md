# 二阶迭代法 - wydbyxr的博客 - CSDN博客
2018年09月30日 13:35:46[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：167
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# 二阶迭代法
  该优化方法基于牛顿法"	"其迭代方式如下：
  x←x−[Hf(x)]−1∇f(x)
  这里Hf(x)是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。∇f(x)是梯度向量，这和梯度下降中一样。
  直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。这个公式中没有学习率这个超参数，因此比一阶方法好得多。
## 缺点
  不使用该方法，原因有二：
  1）在实际深度学习应用中，上述更新方法很难运用，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。
  例如，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3725GB的内存。
  好消息是，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。最流行的是L-BFGS，该方法使用随时间的梯度中的信息来隐式地近似（不是将整个矩阵进行计算）。
  2）然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。
  在实践中，使用L-BFGS之类的二阶方法并不常见。基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。
