# NN模型设置--减小训练时的内存/显存占用 - wydbyxr的博客 - CSDN博客
2018年12月04日 10:40:03[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：189
# 1）kennel_stride
  一种技术是使用较大的步幅来进行卷积内核，也就是说，我们应用不是每个像素的平铺卷积，而是每两个或四个像素（stride为2或4），以便产生较少的输出数据。
  这通常用于输入层，因为这些使用大部分内存。
  当然，大stride通常是配合大的kenel_size一起使用；而使用大的kenel_size是为了扩大感受野。
  为什么在第一层的stride要这么大？
# 2）1*1kennel_size
  减少内存占用的另一个技巧是引入1×1的卷积内核层，减少通道。例如64x64x256输入可以通过96 1×1内核减少到64x64x96的输入。
# 3）pooling
  一个明显的技术是池化。一个2×2的池化层可以将该层的数据量减少4个，从而显着减少后续层的内存占用。
  下采样：(NCHW -> (1/4)*NCHW)
# 4）batch_size
  使用批量大小为64而不是128个内存消耗量。然而，培训也可能需要更长时间，特别是训练的最后阶段，使得准确的梯度变得越来越重要。大多数卷积操作也针对64或更大的小批量尺寸进行优化，从而从批量大小32开始，训练速度大大降低。因此，将小批量尺寸缩小到甚至低于32，只能作为最后的选择。
# 5）float32
  另一个经常被忽视的选择是更改卷积网络使用的数据类型。通过从32位切换到16位，可以轻松地将内存消耗量减半，而不会降低分类性能。在P100特斯拉卡上，这甚至会给你一个巨大的加速。
# 6）减少全连接层
  一般只留最后一层分类用的全连接层。
