# 最小二乘法（Least Square）和最大似然估计 - wydbyxr的博客 - CSDN博客
2018年10月20日 13:42:22[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：231
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# 最小二乘法（Least Square）
## 线性最小二乘（OLS，online Least Square）
  最小二乘，其实就是最小方差。
  找到一个（组）估计值，使得实际值与估计值的距离最小。本来用两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平方加总之后的值最小，称为最小二乘。“二乘”的英文为least square，其实英文的字面意思是“平方最小”。这时，将这个差的平方的和式对参数求导数，并取一阶导数为零，就是OLSE。
  投影变量p （拟合值）是在u （解释变量）张成的子空间中，距离v（被解释变量）最“近”的那个向量。这个“近”（距离的概念），是需要用内积来定义。而我说的 x.y = Exy’ 这种定义内积的方法，正好能推导出来用“方差”来定义距离的方法。所以投影得到了，最小二乘也实现了。
  OLS 是把所有变量扔到线性空间中，求线性投影的系数：它并不需要什么信息。
## 最小均方误差和最小二乘的区别
  针对的问题不一样，一个针对的是真实值到测量值的线性变换未知的情况下，另一个是已知的情况下。
  同理，递推最小二乘法和卡尔曼滤波的区别，即一个针对静态系统的估计，一个针对动态系统的估计。
# 最大似然估计（MLE）
  现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。
  也可以认为MLE是一种特殊情况下的Bayesian 估计，具体来说，就是在prior 是 diffuse （无知的）情况下，让posterior 分布取得极大值的系数值。最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
  MLE 是需要我们知道一个完整的理论模型 （否则P(observation|model) 根本就不知道是什么）。
# OLS（最小二乘）和MLE（最大似然）不同：
  由于一般大家接触的都是线性模型，所以二者区别不大。当模型无法变成线性状态时（比如censored data, logit/probit 之类的），此时OLS此时报告的仍然是线性投影，我们却没有用到这些“非线性”的信息，因此MLE的选项就好很多。
  不论任何时候，OLS报告的都是线性投影（准确的说，是对线性投影的“估计”值），都是 ““best linear predictor””。故当你加上了一些假设，（比如 在 y = x b + u 这样的理论模型中，你假设了 E(xu) = 0 这样的经典计量经济学假设），此时OLS报告的还是线性投影，只不过，这个线性投影正好等于模型中的"“b”"。
  如果在模型 y = x b + u 中，E(xu) != 0，不满足经典计量假设。那么此时你用上了OLS，得到的是y = x a + e 这样的模型，你是知道了a，而且很容易知道E(xe) = E(x(y-x a)) = x. (y-x a) = 0 （线性投影的垂直条件）。但是这个a 却不是你一开始设定模型时想要知道的b。
