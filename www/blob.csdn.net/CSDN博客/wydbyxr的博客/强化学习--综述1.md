# 强化学习--综述1 - wydbyxr的博客 - CSDN博客
2018年11月01日 11:01:15[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：127
# 强化学习
  即增强学习（Reinforcement Learning）,又称再励学习、评价学习。
  在这种学习方式中，模型先被构建，然后输入数据刺激模型，输入数据往往来自于环境中，模型得到的结果称之为反馈，使用反馈对模型进行调整。
## 强化学习、非监督学习、监督学习的关系
  在传统的机器学习分类中没有提到过强化学习，而在连接主义学习中，把学习算法分为三种类型，即非监督学习(unsupervised learning)、监督学习(supervised leaning)和强化学习。
  强化学习算法是另一组机器学习算法，这种学习算法介于在无监督和监督学习之间。
### 举例说明
  对于每个训练示例，
  1）在监督学习中有一个目标标签；
  2）在无监督学习中完全没有标签；
  3）强化学习包括时间延迟和稀疏标签——也就是“激励”。它与监督学习的区别在于反馈数据更多的来自于环境的反馈而不是由人指定。
## 应用领域
  在机器人控制机、控制系统、分析预测等领域有许多应用。
  例如，微软利用强化学习，通过“奖励”这个系统来为MSN.com的新闻故事选择标题，当更多的访问者点击链接时，这个系统会得到更高的分数。该系统试图根据设计者给出的规则来最大化其分数。当然，这意味着一个强化学习系统将对你明确奖励的目标进行优化，但这也有可能并不是你真正关心的目标(如终身客户价值)，因此正确明确目标和明确的目标是至关重要的。
  例如，围棋，即alphaGo，它的核心就是增强学习。
## 具体算法
  代表算法是Q-学习（Q-learning）和时序差分算法（Temporal difference learning）。		
  它可以分为间接学习和直接学习。		
  外部环境对系统输出结果只给出评价(奖或惩)而不是给出正确答案。每个agent会根据环境奖励学习自身行为。
  常用的模型是马尔可夫决策过程（MDP），然后使用动态规划解决MDP问题。	强化学习的目标是最大化累积奖赏，这一点与马可夫决策过程（MDP）的目标一致，因此强化学习也常常用MDP来建模。
## 挑战
  强化学习在实际问题上的广泛使用还面临诸多挑战，主要包括特征表示、搜索空间、泛化能力等方面的问题。
## 缺点
  1）一般来说，它只能在不需要推理和记忆的条件反射类的游戏中取得好的表现。相反，凡是能够在这些任务上表现优秀的模型，要么使用了先验信息指导，要么使用了演示（就像我们在之前的棋盘游戏的例子中提到的那样起作用）。即需要大量数据和明确的约束规则。
  例如，alpha GO，围棋任务具有确定性、离散型、静态性，而且是完全可观察的、信息完全可知的单智能体任务，可以被分成一节一节，开销较小、易于模拟、易于计分…… 而围棋问题唯一的挑战就是：巨大的分支空间。围棋可能是最困难（搜索空间巨大）的简单（限制较多）问题，但归根到底它还是一个简单问题。
  2）与人类相比，它还是需要大量的时间和经验去进行学习。
  3）样本利用率非常低;
  4）最终表现不够好,经常比不过基于模型的方法;
  5）好的奖励函数难以设计;
  6）难以平衡“探索”和“利用”,以致算法陷入局部极小;
  7）对环境的过拟合;
  8）灾难性的不稳定性。
## 目前深度增强学习的算法都可以包含在Actor-Critic框架下
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181101105803628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=,size_16,color_FFFFFF,t_70)
## 经典的强化学习
  经典强化学习的研究中，状态和动作空间均为有限集合，每一个状态和动作被分别处理。然而，一方面许多应用问题具有连续的状态和动作空间，例如机械臂的控制；另一方面即使对于有限状态空间，状态之间也并非没有联系，例如棋盘上走棋有位置关系。
  因此如何将状态赋予合适的特质表示将极大的影响强化学习的性能。这一方面的工作包括使用更好的特征编码方式等，而近期得益于深度学习技术的发展，特征可以更有效的从数据中学习，Google DeepMind的研究者在Nature上发表了基于深度学习和Q-Learning的强化学习方法Deep Q-Network。
  经典的强化学习研究多假设学习器处在稳定环境中，即MDP四元组都是固定不变的，学习器在这样的环境中进行学习，学习到的策略也只在相同的环境中进行使用和评价。
  经典的强化学习研究在车床控制、工业机器人等稳定环境中取得了成功的应用。
  然而随着强化学习的应用向更多的领域拓展，面对的环境更加复杂，以往的限定条件下的假设不再成立。例如在自动驾驶中，不同配置的车辆驾驶到各种各样的地形，某一种车辆在某一种地形上学习到的策略可能难以应对。
  有教授也发文指出限定条件下的强化学习算法面临领域过配问题(domain overfitting)，限定条件下设计的算法只能用于特定领域、而难以通用。对此问题，已出现了一些关于强化学习领域迁移的研究。
## 进化算法与增强学习
  对于监督学习来说，基于梯度的反向传播算法已经非常好，而且这一点可能短期内不会有什么改变。然而，在强化学习中，进化策略（Evolution Strategies, ES）似乎正在东山再起。因为强化学习的数据通常不是lid（独立同分布）的，错误信号更加稀疏，而且需要探索，不依赖梯度的算法表现很好。另外，进化算法可以线性扩展到数千台机器，实现非常快的平行训练。它们不需要昂贵的GPU，但可以在成百上千便宜的CPU机器上进行训练。
  2017年早些时候，OpenAI的研究人员证明了进化策略实现的性能，可以与Deep Q-Learning等标准强化学习算法相媲美。
## 深度学习与强化学习
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181101110101511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=,size_16,color_FFFFFF,t_70)
