# 深度学习基础--loss与激活函数--好的激活函数的性质 - wydbyxr的博客 - CSDN博客
2018年11月19日 10:00:57[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：30
# 好的激活函数的性质
  1）不会饱和。sigmoid和tanh激活函数在两侧尾端会有饱和现象，这会使导数在这些区域接近零，从而阻碍网络的训练。
  2）零均值。ReLU激活函数的输出均值不为零，这会影响网络的训练。
  3）容易计算。
  使用：最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout。
  另外，通常来说，很少会把各种激活函数串起来在一个网络中使用的。
