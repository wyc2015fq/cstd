# 机器学习基础--贝叶斯与Ln正则化、核范数与rank - wydbyxr的博客 - CSDN博客
2018年08月17日 10:32:42[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：151
# 频率派与贝叶斯派
  1）频率派：
  把需要推断的概率参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；"
  2）贝叶斯派：
  认为概率参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。于是就有了先验概率和后验概率。
  贝叶斯派的思考问题的固定模式：　　先验概率 + 样本信息 = 后验概率
# 贝叶斯与Ln正则化
  贝叶斯的角度，L2是先验为高斯分布下的先验最大估计，L1是先验为拉普拉斯分布下的后验最大估计。		
  Ln有助于减缓过拟合。过拟合有个普遍现象就是得到的系数估计的绝对值比真值大，这个你可以自己写个告诫多项式回归的仿真案例看看。所以缓解过拟合的一种方式就是把这些系数往0收缩，通过给优化目标（loss funciton）添加正则化向（主要是对参数绝对值的惩罚）使得如果有过大绝对值的系数出现，优化目标自身也会变得很大。因而使得整体达到最小的系数估计会比不加正则化像的系数估计绝对值要小（向0收缩了），过拟合也一定程度被缓释了。
# 核范数与rank（核范数与规则项参数选择）
  核范数||W|| * 是指矩阵奇异值的和，英文称呼叫Nuclear Norm。用来约束Low-Rank（低秩）。
  好了，低秩有了，那约束低秩只是约束rank(w)呀，和我们这节的核范数有什么关系呢？
  他们的关系和L0与L1的关系一样。因为rank()是非凸的，在优化问题里面很难求解，那么就需要寻找它的凸近似来近似它了。对，你没猜错，rank(w)的凸近似就是核范数||W||*。
## 秩
  例如：
![这里写图片描述](https://img-blog.csdn.net/20180817103122473?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
  上面的方程组有3个方程，实际上只有2个是有用的，一个是多余的，所以对应的矩阵的秩就是2了。
  既然秩可以度量相关性，而矩阵的相关性实际上有带有了矩阵的结构信息。
  如果矩阵之间各行的相关性很强，那么就表示这个矩阵实际可以投影到更低维的线性子空间，也就是用几个向量就可以完全表达了，它就是低秩的。
  所以我们总结的一点就是：如果矩阵表达的是结构性信息，例如图像、用户-推荐表等等，那么这个矩阵各行之间存在这一定的相关性，那这个矩阵一般就是低秩的。
　　如果X是一个m行n列的数值矩阵，rank(X)是X的秩，假如rank (X)远小于m和n，则我们称X是低秩矩阵。低秩矩阵每行或每列都可以用其他的行或列线性表出，可见它包含大量的冗余信息。
## 应用
  1）矩阵填充(Matrix Completion)：
　　我们首先说说矩阵填充用在哪。一个主流的应用是在推荐系统里面。我们知道，推荐系统有一种方法是通过分析用户的历史记录来给用户推荐的。
　　假设我们用一个“用户-影片”的矩阵来描述这些记录，例如下图，可以看到，会有很多空白的地方。如果这些空白的地方存在，我们是很难对这个矩阵进行分析的，所以在分析之前，一般需要先对其进行补全。也叫矩阵填充。
