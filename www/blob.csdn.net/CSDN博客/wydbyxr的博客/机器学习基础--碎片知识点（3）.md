# 机器学习基础--碎片知识点（3） - wydbyxr的博客 - CSDN博客
2018年08月17日 10:48:27[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：78
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# predictor
　　predictor是指：基于这个变量，我们可以预测另一个变量的值。也就是说，predictor是自变量。在英文专业术语中，自变量被称为predictor或者independent variable。相对应地，因变量的专业术语可以是dependent variable或者outcome variable。  统计学
# 机器学习术语
　　特征向量：模型接受的输入通常称为特征向量，用字母X代指。
　　标签：模型拟合的目标通常称为标签，用字母Y代指。
　　样本：通常听到的“样本”概念是特征向量+标签的组合，用d=(x,y)代指
　　数据集：就是很多个样本的集合，通常用D=（d1,d2,…dn）代指。
　　损失函数：计算单个样本上模型的“损失”的函数。
　　代价函数：计算整个数据集上模型的“代价”的函数。 
# 朴素贝叶斯思想
　　出现概率越大的样本就是越好的样本。知道思想后， 如何具体进行操作呢，如何估计出朴素贝叶斯公式中涉及到的概率呢？它会用频率估计概率的方法来把各个概率都估计出来，说的直白点就是数数。 
# 数据的处理方式
　　1）特征学习(feature learning)，又叫表示学习(representation learning)或者表征学习； 
　　表示学习的另一好处是高度抽象化的特征往往可以被应用于相关的领域上，这也是我们常说的迁移学习(transfer learning)的思路。 
　　从某个角度来看，表示学习有“嵌入式的特征选择”(embedded feature selection)的特性。建模的过程可以视为是一种嵌入式的特征选择。
　　2）特征工程(feature engineering)，主要指对于数据的人为处理提取，有时候也代指“洗数据”    狭义的特征工程指的是处理缺失值、特征选择、维度压缩等各种预处理手段
# 何为共线性, 跟过拟合有啥关联?
　　共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。 
　　两者的关系：共线性会造成冗余，导致过拟合。 
　　解决方法：排除变量的相关性／加入权重正则。
# 参数估计
　　这是统计学中的经典问题，解决的方法很多。最常用的有两种： 
　　1）最大似然估计方法。最大似然估计把参数看做是确定（非随机）而非未知的，最好的估计值是在获得实际观察样本的概率为最大的条件下得到的。
　　2）贝叶斯估计方法。贝叶斯估计则是把参数当做具有某种分布的随机变量，样本的观察结果使先验分布转换为后验分布，再根据后验分布修正原先对参数的估计。
　　虽然两者在结果上通常是近似的，但从概念上来说它们的处理方法是完全不同的。
## 先验分布和后验分布
　　先验概率可理解为统计概率，后验概率可理解为条件概率。 
　　1）先验分布：根据一般的经验认为随机变量应该满足的分布 
　　2）后验分布：通过当前训练数据修正的随机变量的分布，比先验分布更符合当前数据。后验分布往往是基于先验分布和极大似然估计计算出来的。 
　　3）似然估计：已知训练数据，给定了模型，通过让似然性极大化估计模型参数的一种方法
# 没有免费的午餐定理
　　在机器学习中存在一个普适定理–没有免费的午餐(No Free Lunch Theorem，NFL定理)。 
　　该定理可以进一步的引出一个普适的“守恒率”–对每一个可行的学习算法来说，它们的性能对所有可能的目标函数的求和结果确切地为零。即我们要想在某些问题上得到正的性能的提高，必须在一些问题上付出等量的负的性能的代价！比如时间复杂度和空间复杂度。 
　　平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。
# 辅助信息、边信息（Side Information）
　　是指利用已有的信息Y辅助对信息X进行编码，可以使得信息X的编码长度更短。 
　　例子：假设到马场去赌马，根据每个马的赔率可以得到一个最佳的投资方案。但是如果知道赌马的一些历史数据，比如上几场的胜负情况，那么可以得出一个更优的投资方案。赌马中的历史数据就是边信息。
# 大原则
　　不过也有一个普遍原则，即所有监督机器学习算法预测建模的基础。 
　　机器学习算法被描述为学习一个目标函数 f，该函数将输入变量 X 最好地映射到输出变量 Y，即Y = f(X)。
# 文本与图像的相互转化caption
　　问题： 
　　1）[batch size, sequence length, feature channels],图像和文本都是这种表达？ 
　　2）如果是句子，可以理解成 [batchsize, sequencelength, embedding_size]； 
　　3）如果是图像，不应该是 [batchsize, height, width, featurechannels] 吗？ 
　　4）如何将图像与 sequence_length 对应起来？
　　回答： 
　　　　句子变图像。for text: height (sequence length), width (embedding dim), channel (1 if use only one kind of embedding)。见下图 
![这里写图片描述](https://img-blog.csdn.net/20180817104309711?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
　　文本视为图像的例子可以看一下：[http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)
# 特征表达的方式——统计、hash、embedding
　　在机器学习领域的特征，一般是一个id，而且给这个id赋一个值。
　　1）这个特征值的表达一般可以通过统计的tf 或其他类似的数字表示。 
　　2）比较难于理解的是hash在特征表达里也能起到比较重要的作用。  通过对特征运用hash函数，获得对应的值。裁剪、分桶获取对应的位表示。可以用作特征，起到降维的作用。 
　　3）还有现在比较流行的是用embedding的方式，将特征映射到一定维度的实数空间里，相比hash不一样的是，这种做法一般是神经网络的迭代方式，而不是hash的非迭代云素昂方式，hash依赖的的hash核函数要设计的比较好（这个门槛比较高）。 embedding发热，主要源于word2vec的盛行， 在知识图谱领域演化了很多类似的TransE、TransR等方法。   
# embedding，向量化
　　数学上的概念，从一个空间映射到另外一个空间，保留基本属性。 
　　embedding 可以理解为比如降维，或者说把一些复杂难以表达的特征用相对来说可以用数学表达或者更易计算的形式来表达的一种映射。比如把单词转化成向量，把数字（的奇偶正负实复等性质）转化成n维矩阵。 
　　就是对特征进行固定长度的编码，比如对词进行固定长度的编码就是大名鼎鼎的word-embedding。
## word embedding
　　就是从一个高维空间（如：维度=词汇表长度=1M)，映射到低维度空间(如300)。
# 决策矩阵算法
　　参考资料：[https://www.ddvip.com/weixin/20170629A01UQ800.html](https://www.ddvip.com/weixin/20170629A01UQ800.html)
　　决策矩阵算法能系统地分析、识别和评估信息集和值之间关系的表现。这些算法主要用于决策。汽车是否需要制动或左转基于这些算法对物体的下一次运动的识别、分类和预测的置信度。 
　　决策矩阵算法是由独立训练的各种决策模型组合起来的模型，在某些方面，将这些预测结合起来进行总体预测，同时降低决策中错误的可能性。
## AdaBoosting就是其中最常用的算法
　　Adaptive Boosting算法也可以简称为AdaBoost，它是可以用于回归或分类的多种学习算法的组合。与任何其他机器学习算法相比，它克服了过度拟合问题，并且通常对异常值和噪声数据非常敏感。为了创建一个复合强大的学习器，AdaBoost需要经过多次迭代，因此，它具有适应性。学习器将重点关注被分类错误的样本，最后再通过加权将弱学习器组合成强学习器。 
　　AdaBoost有助于将弱阈值分类器提升为强分类器。上面的图像描绘了如何在一个可以理解性代码的单个文件中实现AdaBoost算法。该函数包含一个弱分类器和boosting组件。弱分类器尝试在数据维度之一中定位理想阈值，将数据分为2类。分类器通过迭代部分调用，并且在每个分类步骤之后，它改变了错误分类样本的权重。因此，它实际创建了级联的弱分类器，但性能像强分类器一样好。 
　　AdaBoost的老祖宗可以说是机器学习的一个模型，它的名字叫PAC(Probably Approximately Correct)。
# zero-shot learning
　　又称zero-shot recognition，ZS 
　　问题：当只能每一类只有很少的数据，但同时要求要将识别的范围扩大到很多类。 
　　解决的方法之一就是：扩大识别的一种方法是开发能够识别未知类别的模型，而不需要任何训练实例，即零镜头识别/学习。”          
# knowledge representation learning
　　人们构建的知识库通常被表示为网络形式,节点代表实体,连边代表实体间的关系。 
　　在网络表示形式下,人们需要设计专门的图算法存储和利用知识库,存在费时费力的缺点,并受到数据稀疏问题的困扰。最近,以深度学习为代表的表示学习技术受到广泛关注。 
　　表示学习旨在将研究对象的语义信息表示为稠密低维实值向量,知识表示学习则面向知识库中的实体和关系进行表示学习。该技术可以在低维空间中高效计算实体和关系的语义联系,有效解决数据稀疏问题,使知识获取、融合和推理的性能得到显著提升。”         
