# 训练过程--渐进式调整大小、动态批量大小 - wydbyxr的博客 - CSDN博客
2018年12月05日 14:18:16[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：101
# 渐进式调整大小
  有一种很简单有效的方法，经常用来处理过拟合效应和提高准确性，它就是训练小尺寸图像，然后增大尺寸并再次训练相同模型。
  最初减少训练图像大小使得卷积神经网络训练更有效地进行图像识别任务，逐步增加图片的大小可以加快训练速度。
  参考论文：《Testing the Efficient Network TRaining (ENTR) Hypothesis: initially reducing training image size makes Convolutional Neural Network training for image recognition tasks more efficient》
[fast.ai](http://fast.ai) 在 DAWNBench 竞赛中取得的主要进展是引入了渐进式图像尺寸调整来进行分类——在训练开始时使用小图像，随着训练的进行逐渐增加图像尺寸。采用这种做法，刚开始的时候模型非常不准确，但它可以很快看到大量图像并取得快速进展，在接下来的训练中，模型可以看更大的图像，学习更加细粒度的区别。
# 动态批量大小
  这一新研究还对一些中间的 epoch 使用更大的批量大小，以更好地利用 GPU RAM 并避免网络延迟。
  最近，腾讯发布了一篇很不错的研究论文（[https://arxiv.org/abs/1807.11205），显示可以在](https://arxiv.org/abs/1807.11205%EF%BC%89%EF%BC%8C%E6%98%BE%E7%A4%BA%E5%8F%AF%E4%BB%A5%E5%9C%A8) 2048 块 GPU 上用不到 7 分钟的时间训练 Imagenet。他们提到了一个 [fast.ai](http://fast.ai) 之前没有尝试过的方法，但这个方法非常合理：移除批归一化层上的权重衰减。使用这种方法可以从训练时间中再缩减几个 epoch。-（腾讯的论文还使用了 NVIDIA Research 开发的动态学习率方法 LARS，fastai 团队也开发了研究人员为 fastai 开发的，但还没有包含在这些结果中。）"
