# RNN--综述 - wydbyxr的博客 - CSDN博客
2018年10月31日 10:29:20[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：196
# RNN
  递归神经网络，即循环神经网络，Recurrent Neural Network，RNN。
## 如何学习RNN
  通过前期对RNN/LSTM的基本知识学习和语音系统方面的RNN应用学习，完成对LSTM的基本计算框架的分析和基本应用分析。
  具体要求如下：
  1）LSTM基本原理分析。
  2）LSTM构成形式分析，包括LSTM变种结构分析，基本要求包括共性特征分析、异性特征分析、基础单元等内容。
  3）单元级别详细数据分析，包括：门、神经元、input/output data、weight、存储开销、计算通路。
  4）基本网络应用形势分析。根据目前的应用领域（可以是语音领域、也可以是图像领域），选择某一种类型的LSTM单元，分析一个完整的LSTM应用。
  要求：网络结构拓扑结构明确清晰；明确算法应用或网络应用的operation flow；清楚input/output 的特性、数量；统计分析整个网络的网络层数、神经元个数、每一层权重数量、中间暂存数据量。即对某个RNN细节完全分析。
## 传统的RNN中有两种形式的Memory
  这两种Memory形式有不一样的结构、目的和容量（Capacity）。
  1）Short-Term Memory直接通过Hidden Vector来存放信息，容量是O(H)，这里H是Hidden Units的数量。
  2）Long-Term Memory通过现在的输入信息和Hidden Vector，来得到下一步的输出信息以及新的Hidden Vector，总共的容量是O(H2)+O(IH)+O(HO)，这里I和O是输入单元以及输出单元的数量。
  注意，传统的Long Short-Term Memory Networks（LSTM）依然只拥有O(H)的处理Short-Term Memory的能力。
## RNN和CNN的同异
### 不同点
  普通卷积神经网络（CNN）处理的是 “静态” 数据，样本数据之间独立，没有关系。
  循环神经网络（RNN）处理的数据是 “序列化” 数据。 训练的样本前后是有关联的，即一个序列的当前的输出与前面的输出也有关。比如语音识别，一段语音是有时间序列的，说的话前后是有关系的。
  总之，在空间或局部上有关联图像数据适合卷积神经网络来处理，在时间序列上有关联的数据适合用循环时间网络处理。但目前也会用卷积神经网络处理语音问题， 或自然言语理解问题，其实也是把卷积神经网络的计算方法用到这上面。
### 相同点
  从硬件相关的算子角度来看，CNN和RNN没有特殊的算子。
## 普通 RNN 的不足之处
  首先是神经网络里面的计算，可以大致分为三类：函数合成，函数相加，加权计算。
  特别容易发生梯度消失或梯度爆炸
  在计算过程中，经常会用到激活函数，比如 Sigmoid 激活函数。残差在往前传播的过程中，每经过一个 Sigmoid 函数，就要乘以一个 Sigmoid 函数的导数值，残差值至少会因此消减为原来的 0.25 倍。神经网络每多一层，残差往前传递的时候，就会减少至少 3/4。如果层数太多，残差传递到前面已经为 0，导致前层网络中国呢的参数无法更新，这就是梯度消失。
  RNN难以训练，就在2014年，还没有多少人对RNN的可训练性抱有信心，在过去的十年中，有很多工作表明RNN十分难以训练。有证据表明，大量不同的RNN架构，其表现力是同等的，任何性能上的差异都是由于某些架构比其他架构更容易优化导致的。
