# 最新论文阅读（9）--Learning to Segment Every Thing - wydbyxr的博客 - CSDN博客
2018年06月07日 20:19:55[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：326
所属专栏：[深度学习--最新论文](https://blog.csdn.net/column/details/23683.html)
# Learning to Segment Every Thing
```
- 2017年11月  
- 迁移学习，半监督；实例分割(instance-segmentation)；Mask^X R-CNN
- 伯克利；FAIR；Kaiming He； Ross Girshick
```
　　是「Mask RCNN后续工作」，结合weakly supervision和weight transferring等技术，做到真实场景下更加泛化的示例级别物体分割。 
　　部分监督的实例分割算法：1）给定一个类别集，其中一小部分具备实例掩码标注，其他类别仅具备边界框标注；2）实例分割算法利用该数据构建一个能够分割该集合中所有目标类别实例的模型。由于训练数据是大量的强标注样本（带有掩码）和少量弱标注样本（只有边界框标注）的混合，该任务被认为是部分监督式的。 
　　算法：基于Mask R-CNN，提出一种新的部分监督式训练范式，加上一种全新的权重迁移函数；称为Mask^X R-CNN。 
　　实验：使用Visual Genome数据集中的Box注释和COCO数据集中80个类的mask来检测和分割3000种。 
### 背景
　　在实践中，典型的实例分割系统只能关注小部分视觉信息，一般约为 100 个目标类别。该限制的主要原因是实例分割算法需要强大的监督系统，而此类监督数据很难收集新的类别，且比较昂贵。相比之下，边界框标注更丰富，也没有那么昂贵。 
　　这就引出了一个问题：在不对所有类别提供完整的实例分割标注情况下，我们还可以训练高质量实例分割模型吗？该论文以此为动机，引入了一个新型部分监督实例分割任务，提出了一种用于解决该问题的新型迁移学习方法。 
　　部分监督方法的主要优势是允许我们利用现有数据集的两种类型：大量类别具有边界框标注的数据集如 Visual Genome 和少量类别具备实例掩码标注的数据集如 COCO，构建大规模实例分割模型。这促使研究者将顶尖的实例分割方法扩展至数千个类别，这对现实世界应用部署至关重要。 
　　如下图，训练部分监督的实例分割模型：类别子集（绿色框）在训练过程中有实例掩码标注；其他类别（红色框）只有边界框标注。上图显示的是使用 COCO 中 80 个类别的掩码标注对来自 Visual Genome 的 3000 个类别进行训练后的输出。 
![这里写图片描述](https://img-blog.csdn.net/20180607201559780?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
### 算法细节
　　提出了一种基于 Mask R-CNN的新型迁移学习方法。 
　　Mask R-CNN将实例分割问题分解为边界框目标检测和掩码预测的子任务。这些子任务由联合训练的专门网络「heads」来处理。该方法背后的理念是经过训练后，边界框头部的参数对每个物体类别的嵌入进行编码，该嵌入表征使类别的视觉信息迁移至部分监督的mask头部。”
### Mask^X R-CNN
![Mask^X R-CNN](https://img-blog.csdn.net/20180607201746808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
　　如上图是Mask^X R-CNN 方法的详细描述。Mask^X R-CNN 没有直接学习mask参数 w_seg，而是使用学得的权重迁移函数 T，利用对应的检测参数 w_det 预测类别的分割参数 w_seg。 
　　在训练中，T 仅需要集 A 中类别的掩码数据，测试阶段中它可应用于集 A ∪ B 中所有类别。 
　　研究者还使用补充性（complementary）全连接多层感知机（MLP）增强掩码头部标签。
### 评估与实验
　　研究者在两种设置中对该方法进行了评估。 
　　在第一种设置中，使用 COCO 数据集将部分监督实例分割任务模拟成一种在具备高质量标注和评估指标的数据集上构建量化结果的方式。具体来说，将 COCO 类别分割成带有掩码标注的子集和系统只能获取边界框标注的余子集。由于 COCO 数据集仅包含少量语义分割类别（80 个），因此定量评估是精确可信的。实验结果证明该方法在强基线上改善了结果，在没有训练掩码的类别上的 mask AP 实现了 40% 的增长。 
　　在第二种设置中，研究者使用 Visual Genome (VG) 数据集在 3000 个类别上训练大规模实例分割模型。VG 包含大量目标类别的边界框标注，但是定量评估很有难度，因为很多类别存在语义重叠（如近义词）、标注不够详尽，造成难以度量其查准率和查全率。此外，VG 不使用实例掩码进行标注。相反，研究者使用 VG 提供大规模实例分割模型的量化输出。模型输出如图 1 和图 5 所示。
