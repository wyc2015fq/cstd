# 深度学习基础--BP和训练--常用的梯度下降 - wydbyxr的博客 - CSDN博客
2018年11月12日 13:46:54[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：54
# 常用的梯度下降
  梯度下降是线性回归的一种(Linear Regression)
# 1）Adam
  Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
## 具体实现:
  需要:步进值 ?, 初始参数 θ, 数值稳定量δ，一阶动量衰减系数ρ1, 二阶动量衰减系数ρ2
  其中几个取值一般为：δ=10^-8,ρ1=0.9,ρ2=0.999
  中间变量：一阶动量s，二阶动量r,都初始化为0。
  每步迭代过程:
  1.  从训练集中的随机抽取一批容量为m的样本{x1,…,xm},以及相关的输出yi
  2. 计算梯度和误差,更新r和s,再根据r和s以及梯度计算参数更新量。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181112115000161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZGJ5eHI=,size_16,color_FFFFFF,t_70)
# 2）批梯度下降算法
  计算每一个θ值都需要遍历计算所有样本。
  缺点是：速度慢；一般要设定最大迭代数和收敛参数；十分依靠初始点和步长
# 3）随机梯度下降算法SGD
  因为每次计算梯度都需要遍历所有的样本点。这是因为梯度是J(θ)的导数，而J(θ)是需要考虑所有样本的误差和 。所以接下来又提出了随机梯度下降算法(stochastic gradient descent )。
  随机梯度下降算法，每次迭代只是考虑让该样本点的J(θ)趋向最小，而不管其他的样本点，这样算法会很快，但是收敛的过程会比较曲折，整体效果上，大多数时候它只能接近局部最优解，而无法真正达到局部最优解。
  sgd是当前权重减去步长乘以梯度，得到新的权重。
## 优缺点
  优点：1. 训练速度快。从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。2. 适合用于较大训练集的case。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了。
  缺点：随机梯度下降过早的结束了迭代，使得它获取的值只是接近局部最优解，而并非像批梯度下降算法那么是局部最优解。
## SGD与GD比较
  Prat I: 相对于非随机算法，SGD 能更有效的利用信息，特别是信息比较冗余的时候。
  Prat II: 相对于非随机算法， SGD 在前期迭代效果卓越。
  Prat III: 如果样本数量大，那么 SGD的Computational Complexity 依然有优势。
# 随机平均梯度法(Stochasitc Average Gradient)
  我们知道sgd是当前权重减去步长乘以梯度，得到新的权重。而其实，SAG就是SGD with momentum（带动量的随机梯度下降）的姊妹版本。
  sag中的a，就是平均的意思，具体说，就是在第k步迭代的时候，我考虑的这一步和前面n-1个梯度的平均值，当前权重减去步长乘以最近n个梯度的平均值。n是自己设置的，当n=1的时候，就是普通的sgd。
  想法非常的简单，在随机中又增加了确定性，类似于mini-batch sgd的作用，但不同的是，sag又没有去计算更多的样本，只是利用了之前计算出来的梯度，所以每次迭代的计算成本远小于mini-batch sgd，和sgd相当。效果而言，sag相对于sgd，收敛速度快了很多。这一点上面的论文中有具体的描述和证明。
  随机梯度下降被发明出来的原因是因为它的下降速度快，可以减少迭代的次数，但是不容易收敛是它的缺点。在有些特定的情况下呢，需要更多的迭代（更多的计算复杂度）才能达到收敛条件，反而可能不如正常的梯度下降来得好。
  为了避免这个情况呢，随机平均梯度法就诞生啦，保证了随机梯度下降原汁原味的优点（下降快），同时又利用平均的特性，让梯度下降能更快达到收敛条件，减少迭代次数。
  综上，这个方法呢，以后估计还会改进吧，毕竟平均这个特性太“low”了，不过思想倒是可取的，跟马尔科夫链有点异曲同工，说不定下一个算法就是“马尔科夫链随机梯度下降”呢。
## SAG与SGD 比较
  1）SAG是对过去k次的梯度求均值。
  2）SGD with momentum是对过去所有的梯度求加权平均（权重成指数衰减）。
# 小批量梯度下降
  小批量和大批量训练之间的权衡。基本原则是较大的批量每次迭代会变慢，较小的批量可以加快迭代过程，但是无法保证同样的收敛效果。	
  之前是整个数据集为一个batch。
