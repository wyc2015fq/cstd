# 集成学习之Bagging - wydbyxr的博客 - CSDN博客
2018年08月31日 17:51:22[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：76
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# Bagging
　　可以看成是一种圆桌会议, 或是投票选举的形式. 通过训练多个模型, 将这些训练好的模型进行加权组合来获得最终的输出结果(分类/回归)。即Bagging predictor 是一种生成多个预测器版本然后生成聚合预测器的方法。一般这类方法的效果, 都会好于单个模型的效果. 在实践中, 在特征一定的情况下, 大家总是使用Bagging的思想去提升效果。 
　　训练时, 使用replacement的sampling方法（有放回的）, sampling一部分训练数据k次并训练k个模型；预测时, 使用k个模型。 
　　如果为分类, 则让k个模型均进行分类并选择出现次数最多的类(每个类出现的次数占比可以视为置信度)；如为回归, 则为各类器返回的结果的平均值. 此时, Bagging算法可以认为每个分类器的权重都一样。 
　　参考资料：[http://blog.csdn.net/zimenglan_sysu/article/details/48025509](http://blog.csdn.net/zimenglan_sysu/article/details/48025509)
## 核心思想在于采样
　　Bagging 实际上指 Bootstrap Aggregator。 
　　本质上，所有这些模型同时运行，然后对哪个假设最准确进行投票。这有助于降低方差，即减少过拟合。 
　　Bagging的作用：降低只在训练数据上准确率较高的模型的方差——这种情况也叫作过拟合。过拟合即函数过于拟合数据。通常原因在于实际的公式过于复杂，无法考虑每个数据点和异常值。另外，容易过拟合的另一种算法是决策树。使用决策树构建的模型需要非常简单的启发式方法。决策树由一系列特定顺序的 if-else 语句组成。因此，如果把一个数据集变更成新的数据集，则新数据集可能在底层特征中与之前的数据集存在一些偏差或区别。该模型不可能准确。原因在于数据无法非常好地拟合数据（前向声明）。
　　如何解决过拟合问题呢？使用采样和替换数据的方法在数据中创建自己的方差来规避这个问题，同时测试多个假设（模型）。通过使用多个样本（很可能由不同属性的数据组成）来减少噪声。  直到每个模型提出一个假设。这些模型使用投票法（voting）进行分类，用平均法进行回归。这里「Aggregating」和「Bootstrap Aggregating」将发挥作用。每个假设具备相同的权重。这是 Bagging 和 Boosting 方法的区别之一。 
## 与random forest区别
　　随机森林就是在bagging的基础上做了修改。 
　　随机森林可以既处理属性是离散的量, 比如ID3算法, 也可以处理属性为连续值得量, 比如C4.5算法. 
　　这里的random就是指：1）生成树时随机：boostrap中的随机选择样本(有放回的随机选择样本)；2）单个树的分支的随机：random subspace的算法中从所有属性/特征即中随机选择k个属性/特征, 每棵树节点分裂时, 从这随机的k个属性/特征, 选择最优的。（当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。） 
　　问：如何确定树的棵数? 
　　答：先选取一个比较大的数, 然后用二分法逐步减少树的数目, 分别计算相应的点画图, 看哪个过拟合了, 哪个变得平滑, 就确定这个数。但是这个是在验证集上检验。
# 与boosting的区别
　　boosting是”提升”的意思. 一般Boosting算法都是一个迭代的过程, 每一次新的训练都是为了改进上一次的结果。boosting在选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本。boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高(通常是边界附近的样本)，最后的分类器是很多弱分类器的线性叠加(加权组合)。 
　　不同点在于是采样方式。bagging:由于每次迭代的采样是独立的, 所以bagging可以并行。而boosting的采样或者更改样本的权重依赖于上一次迭代的结果, 在迭代层面上是不能并行的。 
　　不同点还在于带来的效果。Bagging主要减小了variance，而Boosting主要减小了bias，而这种差异直接推动结合Bagging和Boosting的MultiBoosting的诞生。
