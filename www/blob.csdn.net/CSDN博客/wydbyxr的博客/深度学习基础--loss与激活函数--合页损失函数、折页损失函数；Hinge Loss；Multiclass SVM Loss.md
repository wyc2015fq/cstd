# 深度学习基础--loss与激活函数--合页损失函数、折页损失函数；Hinge Loss；Multiclass SVM Loss - wydbyxr的博客 - CSDN博客
2018年11月16日 10:26:37[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：75
# 合页损失函数、折页损失函数；Hinge Loss；Multiclass SVM Loss
  Hinge Loss是一种目标函数（或者说损失函数）的名称，有的时候又叫做max-margin objective。用于分类模型以寻找距离每个样本的距离最大的决策边界，即最大化样本和边界之间的边缘。
  其最著名的应用是作为SVM的目标函数。
## 定义
  在二元分类中，hinge 损失函数按以下方式定义：
  loss=max(0,1−(y′* t))
  其中，t 是真实的标签，-1 或+1。
  y’是分类器模型的预测值（-1到1之间），计算公式是：y′=b+w_1 * x_1+ w_2* x_2+…w_n*x_n
  其含义为，y的值在-1到1之间就可以了，并不鼓励 |y|>1 ，即并不鼓励分类器过度自信，让某个可以正确分类的样本距离分割线的距离超过1并不会有任何奖励。从而使得分类器可以更专注整体的分类误差。
## 变种
  实际应用中，一方面很多时候我们的y的值域并不是[-1,1]，比如我们可能更希望y更接近于一个概率，即其值域最好是[0,1]。另一方面，很多时候我们希望训练的是两个样本之间的相似关系，而非样本的整体分类。
  其变种的公式为：
  loss(y,y′)=max(0,m−y+y′)
  其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）
  即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。
#### 例子
  我们想训练词向量，我们希望经常同时出现的词，他们的向量内积越大越好；不经常同时出现的词，他们的向量内积越小越好。
  则我们的hinge loss function可以是：
  loss(w,w+,w−)=max[0,1−（w*w+）+（w * w−）]
  其中，w是当前正在处理的词， w+ 是w在文中前3个词和后3个词中的某一个词， w− 是随机选的一个词。
