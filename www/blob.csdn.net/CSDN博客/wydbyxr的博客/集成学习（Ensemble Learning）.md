# 集成学习（Ensemble Learning） - wydbyxr的博客 - CSDN博客
2018年08月31日 17:30:31[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：627
所属专栏：[经典机器学习算法](https://blog.csdn.net/column/details/28812.html)
# 集成学习（Ensemble Learning）
　　集成学习是机器学习中一个非常重要且热门的分支，是用多个弱分类器构成一个强分类器，其哲学思想是“三个臭皮匠赛过诸葛亮”。一般的弱分类器可以由决策树，神经网络，贝叶斯分类器，K-近邻等构成。这些算法可以是不同的算法，也可以是相同的算法。已经有学者理论上证明了集成学习的思想是可以提高分类器的性能的，比如说统计上的原因，计算上的原因以及表示上的原因。 
　　集成方法的研究点集中在使用什么模型以及这些模型怎么被组合起来。 
　　基本分类器之间的整合方式，一般有简单投票，贝叶斯投票，基于D-S证据理论的整合，基于不同的特征子集的整合。 
　　如果弱学习器与强学习器是等价的, 当强学习器难以学习时(如强学习器高度非线性等), 问题就可以转化为这样的学习问题。学习多个弱分类器(弱分类器容易学习), 并将多个弱分类器组合成一个强分类器(与原来的强学习器等价)。 
　　它有RF（Random Forests）和GBDT两大杀器，它有嫁接法、集成半监督学习的最新进展能够提升学习效果。 
　　集成学习在众多的机器学习/数据挖掘竞赛中往往探囊取物。 
　　用好集成学习有两个关键点：1）怎么训练每个算法？2）怎么融合每个算法？围绕这两个关键点，有很多方法提出来，极具代表性就是大家熟知的bagging和boosting方法，其中Bagging和boosting也是当今两大杀器RF（Random Forests）和GBDT（Gradient Boosting Decision Tree）之所以成功的主要秘诀。 
　　多角度自适应集成学习模型将在传统集成学习模型的基础上，从多个不同角度加入自适应学习过程，从而获取最优的集成学习模型。        
## 集成学习的主要算法
　　有3个主要的算法：boosting、bagging、stacking。 
　　1）boosting的弱分类器形成是同一种机器学习算法，只是其数据抽取时的权值在不断更新，每次都是提高前一次分错了的数据集的权值，最后得到T个弱分类器，且分类器的权值也跟其中间结果的数据有关。 
　　2）Bagging算法也是用的同一种弱分类器，其数据的来源是用bootstrap算法得到的。 
　　3）Stacking算法分为2层，第一层是用不同的算法形成T个弱分类器，同时产生一个与原数据集大小相同的新数据集，利用这个新数据集和一个新算法构成第二层的分类器。 
　　更详细地：Boosting；Bootstrapped Aggregation (Bagging)；AdaBoost；Stacked Generalization (blending)；Gradient Boosting Machines (GBM)；Random Forest
#### Bagging方法
　　给定一个大小为n的训练集 D，Bagging算法从中均匀、有放回地选出 m个大小为 n’ 的子集Di，作为新的训练集。在这 m个训练集上使用分类、回归等算法，则可得到 m个模型，再通过取平均值、取多数票等方法综合产生预测结果，即可得到Bagging的结果。      
#### Boosting 方法
　　加入的过程中，通常根据它们的上一轮的分类准确率给予不同的权重。加和弱学习者之后，数据通常会被重新加权，来强化对之前分类错误数据点的分类，其中一个经典的提升算法例子是AdaBoost。       
#### Stacking 方法
　　将训练好的所有基模型对整个训练集进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。 
　　Stacking：划分训练数据集为两个不相交的集合，在第一个集合上训练多个学习器（初级学习器），在第二个集合上测试这几个学习器，把第三步得到的预测结果作为输入，把正确的回应作为输出，训练一个高层学习器（次级学习器）。 
　　stacking中一个模型的完整流程，stacking中同一层通常包含多个模型，假设还有Model2: LR，Model3：RF，Model4: GBDT，Model5：SVM，对于这四个模型，我们可以重复以上的步骤，在整个流程结束之后，我们可以得到新的A2,A3,A4,A5,B2,B3,B4,B5矩阵。 
　　stacking和bagging的区别：stacking中的各个模型(基分类器)追求的是“准而不同”。stacking中的子模型一般应该是独立准确，而不同的基学习器之间有所差异。  
　　stacking和blending的区别：stacking和blending的区别应该在于数据的划分。blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。
## 异态集成学习和同态集成学习
　　集成学习按照基本分类器之间的关系可以分为异态集成学习和同态集成学习。异态集成学习是指弱分类器之间本身不同，而同态集成学习是指弱分类器之间本身相同只是参数不同。               
## 怎样形成不同的基本分类器呢？
　　主要从以下5个方面得到： 
　　1）基本分类器本身的种类，即其构成算法不同； 
　　2）对数据进行处理不同，比如说boosting,bagging,stacking, cross-validation,hold-out test； 
　　3）对输入特征进行处理和选择； 
　　4）对输出结果进行处理，比如说有的学者提出的纠错码； 
　　5）引入随机扰动；
## 目前有的一般性实验结论
　　1）Boosting方法的集成分类器效果明显优于bagging,但是在某些数据集boosting算法的效果还不如单个分类器的。 
　　2）Boosting算法一定程度上依赖而数据集，而bagging对数据集的依赖没有那么明显。 
　　3）Boosting算法不仅能够减少偏差还能减少方差，但bagging算法智能减少方差，对偏差的减少作用不大。” 
　　4）使用随机化的人工神经网络初始权值来进行集成的方法往往能够取得和bagging同样好的效果。
## 缺点
　　集成学习模型的性能往往受到外在环境（如：样本空间和属性空间）和内在环境（基本分类器的参数和基本分类器的权重）的影响。            
## 趋势
　　主要有两个方向： 
　　1）集成学习模型的优化 
　　2）集成学习模型的并行化。 
　　在大数据时代，集成学习非常适合用于多元数据融合和挖掘，在集成学习里，集成器由一组单一的学习模型所构成，每一个学习模型都可以对应每一个来源的数据，并自动地提取该数据源所蕴含有价值规律。       
## 集成学习与深度学习
　　企业做机器学习追求的不是用无限的资源做尽可能好的效果，而是如何充分利用有限资源，获得最好效果。 
　　假设企业只有两台机器，如何用这两台机器获得最好的效果呢？如果采用集成学习，用两台机器跑五个模型，就要把两台机器分成五份，每个模型只能用0.4台机器去跑，因此跑的数据量就有限。那如果换种方式，不用集成学习，就用一个模型去跑，就能跑5倍的数据。 
　　通常5倍的数据量能比集成学习有更好的效果。在工业界比较少会应用集成学习，主要是因为工业界绝大多数的场景都是资源受限，资源受限时最好的方式是想办法放进去更多的数据。 
　　集成学习因为跑更多的模型导致只能放更少的数据，通常这种效果都会变差。
