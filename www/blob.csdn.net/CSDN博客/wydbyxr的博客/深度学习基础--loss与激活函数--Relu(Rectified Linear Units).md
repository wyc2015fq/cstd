# 深度学习基础--loss与激活函数--Relu(Rectified Linear Units) - wydbyxr的博客 - CSDN博客
2018年11月19日 10:03:51[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：137
# ReLu(Rectified Linear Units)，即修正线性单元
  它是不饱和的、线性的函数。可以认为是一种特殊的maxout。
## Relu的优点
  1）采用sigmoid和tanh等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大；而采用Relu激活函数，整个过程的计算量节省很多。
  2）对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，梯度变化太缓慢，导数趋于0，这种情况会造成信息丢失），减缓收敛速度。vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一。相反，Relu的gradient大多数情况下是常数，有助于解决深层网络的收敛问题。
  3）Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。
## ReLU 的缺点
  训练的时候很”脆弱”，很容易就”die”了。
  举个例子：一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，即这个神经元的梯度就永远都会是0。
  实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都”dead”了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。
  后来就有了各种的变种，Leaky-ReLU、P-ReLU、R-ReLU。
## Relu、sigmoid和tanh的比较
  sigmoid和tanh是饱和的、非线性的函数，导致了它们与relu的gradient特性不同，最终导致relu效果更好。
  饱和其实就是看函数自变量x很大的时候其函数值如果变动很小，那么就称其为饱和。
  sigmoid和tanh的缺点之一是计算所耗时间比较长，在CIFAR-10数据集上，训练到25%的错误率，ReLU的速度是tanh的6倍。还有一个缺点是对于该层输入的数据最好是要做归一化，否则当逐层累积后输入数据可能会变得很大，导致激励函数的输出值变动不大，非线性的性质被削弱。而ReLU则没有这种问题。
## 具体应用
  把除了最外层输出用的sigmoid函数的其他所有用到sigmoid函数的地方全都改为ReLu函数，然后把学习速率调低。
