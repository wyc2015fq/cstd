# 深度学习基础--不同网络种类--可微分编程；Differentiable Programming - wydbyxr的博客 - CSDN博客
2018年11月11日 10:00:29[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：245
个人分类：[深度学习基础](https://blog.csdn.net/wydbyxr/article/category/6829999)
# 可微分编程；Differentiable Programming
  lecun说"深度学习已死，可微分编程万岁！",即深度学习这个词已死，该有新的名词可微分编程来替代它了。
  深度学习的本质是可微分编程，那么，就把神经网络当函数用吧！
  一个程序本身当成一个神经网络，然后自己调节参数。实现真正的可微分编程需要的就是自动化调参，于是乎，贝叶斯方法开始大量用于深度学习。
## 传统编程方法与可微分编程(Differentiable Programming)
  1）传统的编程方法：程序员人工写出每一行代码，机器按照给定的代码运行程序，根据输入数据X得到运算结果Y。
  2）Differentiable Programming：让程序自己写程序。程序员不写代码，或者仅写出少量high-level的代码，但是提供大量输入数据X与对应运算结果Y的例子。神经网络根据提供的数据集，自动学出从输入数据X到最终运算结果Y的映射（既整个程序）；或者结合程序员提供的high-level的代码，用神经网络作为中间函数，补全得到整个程序。
## 更具体地解释可微分编程
  最近，有一些研究者转向了“不整齐”的神经网络。这些神经网络并没有分出明显的层次，反而是，神经节点之间可以跨层次连接，不同的神经网络之间也可以相互连接，它们之间的连接也可以是各种函数。Hinton的Capsule网络，也类似于这个思想。还有一个课题叫AutoML，研究如何自动生成神经网络的结构。但是不论网络结构怎么改变，有效的训练方法还是那几种，例如BP算法和遗传算法。
  其中最常用的BP算法，就是以求“微分”为基础的。只要一个函数可以求微分，那就可以用BP算法去训练（训练能否成功是另一回事）。
  LeCun的原文中提到，现在越来越多的人用“过程式”的方法定义神经网络，就像是定义以往的程序一样。这句话的意思是，人们开始“设计”神经网络的结构，就像是我们以往的程序设计一样。举个例子，以往我们编程，要对一串数字进行排序，我们一般会写一个sort函数，然后我们就可以多次调用这个sort函数。神经网络也是一样，比方说我们有个神经网络，它可以对一串数字进行排序，那么我们也可以多次调用它（题外话：在Neural Turing Machines这篇论文中，就训练了可以对数字进行排序的神经网络）。这样的神经网络，就像是我们传统编程概念中的函数一样，可以被重复调用。唯一的不同点是，以往的程序是固定的；而神经网络是动态的，是可以被训练的。
  例如capsule，以前CNN每个节点的权值是标量，Hinton改为用向量，将来不排除可以用张量。但本质上的架构，其实确实是可微分编程。Lecun要提出改名也无可厚非，毕竟这是对DeepLearning续命的好方法啊，可惜一波不明就里的媒体直接拿来做标题榜。
## 可能产生的影响
  可以看出来，其实任何神经网络都可以像搭积木一样拼出来，这个积木只需要一个条件，就是可以微分，这种可微分是做BP的必要条件, 而且可微分有一个很好的特点，就是链式法则，这样决定了所有的微分可以组合起来。除了这个之外，和普通编程没有什么区别。我想这就是可微分编程的来历。
  想一想，这对以后的编程有什么影响，以后写神经网络。直接提供给你几个可以微分的模块(就像Drag and Drop UI 里面的这种模块), 你简单的组合起来, 加上loss function, 就可以形成你的神经网络了。
  我觉得有点像Keras现在做的，但是现在Keras只提供了有限的Cell。以后的模块应该只要满足模块可微分就可以了，这个比现在的Keras Cell 要多的多得多。
  1）简单的神经网络：一层加一层，每一层有N个节点，forward过程做加权，backward过程，因为每个节点都是可以微分的，做反向传播(BP)。
  2）CNN，每一层改为一个3维或者4维的层级，每一块扫描，每一块可微分。forward加权，因为每个节点可微分，做BP。
  3）RNN，每个层可以向自己做微分。
  4）GAN，两个网络，每层都可以微分，特殊的loss函数。
  算法越来越向全自动的方向发展，例如自适应的batch、自适应梯度…
