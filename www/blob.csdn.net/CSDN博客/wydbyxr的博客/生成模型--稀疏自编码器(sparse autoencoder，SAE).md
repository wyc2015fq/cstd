# 生成模型--稀疏自编码器(sparse autoencoder，SAE) - wydbyxr的博客 - CSDN博客
2018年10月25日 13:42:52[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：599
# 稀疏自编码器(sparse autoencoder，SAE)
  在自动编码的基础上加上稀疏性限制，就可得到稀疏自动编码器（Sparse AutoEncoder）。
  用来约束自动编码器重构的方法，是对其损失函数施加约束。比如，可对损失函数添加一个正则化约束，这样能使自编码器学习到数据的稀疏表征。
  一般用来学习特征，以便用于像分类这样的任务。稀疏正则化的自编码器必须反映训练数据集的独特统计特征，而不是简单地充当恒等函数。以这种方式训练，执行附带稀疏惩罚的复现任务可以得到能学习有用特征的模型。
  对于稀疏自编码器而言，在隐含层中，加入了L1正则化作为优化阶段中损失函数的惩罚项。与香草自编码器相比，这样操作后的数据表征更为稀疏。
  稀疏性约束在深度学习算法优化中的地位越来越重要，主要与深度学习特点有关。
  大量的训练参数使训练过程复杂，且训练输出的维数远比输入的维数高，会产生许多冗余数据信息。加入稀疏性限制，会使学习到的特征更加有价值，同时这也符合人脑神经元响应稀疏性特点。
