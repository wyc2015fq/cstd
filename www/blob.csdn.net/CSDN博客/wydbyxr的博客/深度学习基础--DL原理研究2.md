# 深度学习基础--DL原理研究2 - wydbyxr的博客 - CSDN博客
2018年11月02日 10:18:59[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：41
# 深度学习神经网络需要更深而非更广
  《On the Number of Linear Regions of Deep Neural Networks》中有解释。
  这篇文章证明了，在神经元总数相当的情况下，增加网络深度可以使网络产生更多的线性区域。
深度的贡献是指数增长的，而宽度的贡献是线性的。
  折纸”这个比喻大概是这么一回事。首先我们知道一个ReLU单元y=ReLU(Wx+b)会在输入空间中产生一个超平面Wx+b=0，把输入空间分成两个部分，在负半空间上输出值y=0，在正半空间上输出y=Wx+b。这是一个分片线性函数，有两个线性区域。如果输入空间是二维的，这就好比拿一张纸，沿Wx+b=0折了一下，折出两个区域。
  两个隐层的网络。第二层的一个ReLU单元会在第二层的输入空间，也就是第一层输出的值域上，形成一个超平面，并将第一层的值域分成两个区域。而我们注意到，第一层可能会把不同的输入值映射到相同的输出值，因此在第一层的值域上多一个区域，就可能在输入空间上多出好几个区域。这就好比拿一张纸，折几下，折出几个区域，然后不打开，再折一下。那么最后折的折一下，就会在之前折出的多个区域中新增一个区域。
  一个大小为N的全连接层，后跟一个ReLU非线性，可以将一个向量空间切割成N个分段线性块。添加第二个ReLU层，进一步将空间细分为N个以上的块，在输入空间中产生N^　2个分段线性区域，3个层就是N^3。
  使用ReLU非线性单元的神经网络在数学上相当于一个分片线性函数，线性区域越多，神经网络的非线性性就越强，也就更有可能在实际任务中取得好的效果。
