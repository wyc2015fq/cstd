# NN模型设置--L1/L2正则化 - wydbyxr的博客 - CSDN博客
2018年12月04日 10:21:17[whitenightwu](https://me.csdn.net/wydbyxr)阅读数：61
# 正则化的理解
  规则化函数Ω有多重选择，不同的选择效果也不同，不过一般是模型复杂度的单调递增函数——模型越复杂，规则化值越大。	
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181204101837129.png)
  正则化含义中包含了权重的先验知识，是一种对loss的惩罚项（regularization term that penalizes parameters）。
  L2正则化是权重符合正态分布的先验；L1则是权重符合拉普拉斯分布的先验（对参数引入 拉普拉斯先验 等价于 L1正则化）。
  正则化参数等价于对参数引入先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项 对应后验估计中的 先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。
## 正则化的超参数
  这其中除了损失函数，正则项以外，还包括参数λ，称为hyper-parameters​（超参）。该参数用于平衡损失函数和规则项的（还有一种理解是“用于权衡bias和variance”），λ越大，则正则项的作用越重要，就是说相比于拟合训练数据，更希望模型能够满足约束条件Ω(w)的特性，反之亦然。
  举个极端的例子，如果λ=0，规则化项不起作用，就是希望模型能够最小化损失函数项，当模型拟合了所有训练数据时损失函数项最小，但这样得到的模型对于新的数据泛化能力就非常差，产生了过拟合。​
由此可知，需要协调λ的值，使得模型即具有较小的训练误差，又具有很好的泛化能力。最常用的方法是交叉验证，实验不同的λ值。
# L1和L2的比较
  1）L1 正则化向目标函数添加正则化项，以减少参数的值总和；
  2）L2 正则化中，添加正则化项的目的在于减少参数平方的总和。
  根据之前的研究，L1 正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于 0，因此它常用于特征选择设置中。机器学习中最常用的正则化方法是对权重施加 L2 范数约束。
  对于 L2 范数来说，权重会被约束在一个 L2 范数的球体中，而对于 L1 范数，权重将被限制在 L1 所确定的范围内。
  我们常说的 “small n, large p problem”。（我们一般用 n 表示数据点的个数，用 p 表示变量的个数 ，即数据维度。当  的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话， p 越大，通常会导致模型越复杂，但是反过来 n 又很小，于是就会出现很严重的 overfitting 问题。
  假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予高的权重；而L1正则倾向于选择影响较大的参数，而舍弃掉影响较小的那个。实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的计算量。
  既然 L1和 L2正则各自都有自己的优势，那我们能不能将他们 combine 起来？
  可以，事实上，大牛早就这么玩过了。
