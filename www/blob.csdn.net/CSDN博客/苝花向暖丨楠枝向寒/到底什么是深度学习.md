# 到底什么是深度学习 - 苝花向暖丨楠枝向寒 - CSDN博客

2018年10月11日 22:55:00[苝花向暖丨楠枝向寒](https://me.csdn.net/weixin_40247263)阅读数：208


转自 [http://www.360doc.com/content/17/0213/06/37113458_628587558.shtml](http://www.360doc.com/content/17/0213/06/37113458_628587558.shtml)

深度学习其实跟VR很像，他们都不是全新的概念，却在这几年因为硬件进步而死灰复燃。深度学习是机器学习的一种方式，也可以说是目前人工智能的主流，今年击败世界棋王的Google AlphaGo，2011年夺得益智问答比赛大奖的IBM Watson都是最佳代言。

要设计出比天才还厉害的电脑，一定是比天才还聪明的人啰?答案是：不，建构一套深度学习的网络，其实没有想像中困难，只要看完这篇文章，就能够有基本的了解，再搭配网络资源自学一下，甚至就可以开始建立自己的深度学习网络。

如果你想要深度学习「深度学习」，又能快速搞懂它到底在深什么东西，看这篇文章就对了，那我们开始啰!(ㄟ跑错棚了吧)

(本文内容来自资料科学年会2016 议程，未加注来源之图片取自台湾大学电机系助理教授李宏毅之简报)

什么是深度学习?深度学习其实很简单，就跟把大象放进冰箱一样，只需三个步骤：「打开冰箱、放进大象、关上冰箱门。」专攻语音辨识领域深度学习的台大电机系教授李宏毅说，「深度学习也只要三个步骤：建构网络、设定目标、开始学习，说穿了就是这么简单。」

Deep Learning 研究生的心得：其实就像在玩积木一样，尝试各种堆叠的方法。(Keras 是一款深度学习的开发套件)

「简单说，深度学习就是一个函数集，如此而已。」李宏毅说，类神经网络就是一堆函数的集合，我们丢进去一堆数值，整个网络就输出一堆数值，从这里面找出一个最好的结果，也就是机器运算出来的最佳解，机器可以依此决定要在棋盘上哪一点下手，人类也可以按照这个建议作决策。

这段叙述也许太过抽象，我们可以先具体的说明一下函数。

![](https://img-blog.csdn.net/20181011225122902?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDI0NzI2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

神经网络的基本架构就长这样，每一层都有很多神经元(neuron)，上一层的output 就是下一层的input，最终得出一组final output。

但现实世界中，这个函数集更复杂，而且要等好几年，才能看到结果。在程式设计里，我们不但可以很快看到结果，还可以告诉机器，这个结果no good，请调整函数内容，给我其他结果。这个过程，就是所谓的「学习」，经过大量的训练过程，最终机器就能找到一个最佳函数，得出最佳解。

以AlphaGo 为例，团队设定好神经网络架构后，便将大量的棋谱资料输入，让AlphaGo 学习下围棋的方法，最后它就能判断棋盘上的各种状况，并根据对手的落子做出回应。

![](https://img-blog.csdn.net/20181011225225272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDI0NzI2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

如果让AlphaGo 学习《棋灵王》的棋谱，当对方下了第一手5 之五，AlphaGo 就会知道下在天元是最佳解!

「AlphaGo很厉害，但是它只能下棋，它的架构就是为了围棋而存在的，要拿去开车就必须要重新设计」李宏毅表示，深度学习并不是万能的人工智慧，它其实只能针对特定的需求来设计，现在的各种酷炫应用都还在原始阶段，还有很多需要人类去定义、设计，未来当机器可以自己定义架构时，就更加值得期待。

听起来一点都不简单啊!其实，深度学习的概念早在90 年代就存在，那时是以类神经网络的概念发表，但是当时的电脑运算能力不足，因此效率不彰，导致后来只要提到神经网络，就没人关注，直到近几年换上深度学习的名字才卷土重来。

Google 内部的研发专案使用深度学习的比例，在这两年疯狂暴增。图片来源：SIGMOD/Jeff Dean

深度学习的网络架构层层叠叠，说这个东西很简单的肯定是疯子，但事实上它又真的很简单，因为这是「机器学习」啊。

一般来说，我们看到十几层的网络，就会想到每一层要怎么设定各自的权重(weight)跟变数(parameter)，然后还要互相串连，并且运算在极庞大的资料库上。假设这个网络每一层有1000 个神经单元(neuron)，那每一层中间就有100 万个权重，叠加越多层越可怕，这怎么会简单?

如果所有的细节都要人类去设定，那就不叫做「机器学习」了，这个系统厉害的地方就在于，在神经网络里的千百万个数值细节，都是机器自己学出来的，人类要做的事情就是给他「规则」跟海量的学习资料，告诉机器什么答案是对的，中间的过程完全不用操心。

举例来说，Google就尝试让机器手臂自己学习如何抓握不规则物品，与其透过人类不断去修正每个动作的精准度，还不如让机器自己学习，最后让失败率下降了16%。

在目前主要的深度学习架构里，人类要担心的重点只有一个：「Gradient Descent」，中文勉强译做梯度下降法。我们可以把深度学习想像成有一百万个学生同时在写答案，他们每个人都有不同的思考方式，最后每个人都交出一个不同的答案(一个数字)。将所有的答案跟标准答案相减之后(技术上称为loss)，画成一条折线图(或是复杂一点的3D图)，离标准答案最接近的那个答案，就会在这张图的最低点，深度学习的目标就是要找到这个最低点。

最低点代表什么呢?代表写出这个答案的学生，拥有最接近正确答案的思考方式，接下来我们让其他学生向这位学生学习，并继续测试，是否都能回答正确。理论上，随着测试次数越多，正确率就会越高，表示这个机器已经通过测试，可以投入实战分析了。

![](https://img-blog.csdn.net/20181011225301142?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDI0NzI2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

这条曲线代表所有的output 跟正确答案的差(Loss)，因此最低点就最接近正确答案。机器随机选取其中一个点，然后观察两旁的斜率来逼近最低点，你发现其中的难题了吗?

看到这里，如果你想跟朋友炫耀一下什么是「深度学习」，就跟他说「很简单啊，就是在找loss最小的点嘛，Gradient Descent，懂?」

深度学习越深越好吗?深度学习之所以厉害就在于它堆叠了很多层，因此很多人会好奇，神经网络越多层就越好吗?这个问题的答案跟「头大的人就比较聪明吗」差不多：不一定。

![](https://img-blog.csdn.net/2018101122532016?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDI0NzI2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

从错误率(红字)来看，神经网络叠得越深似乎越好，但可以注意它的架构也不是简单的堆叠，而是变得很复杂。

虽然从这几年的一些机器学习竞赛结果来看，似乎越多层就能得到更低的错误率，去年Residual Network堆到152层，错误率也低到3.57%(ImageNet资料库的测试结果)。但是堆叠上百层的神经网络，常常会导致「Vanishing Gradient」，也就是因为每一层运算让数值不断收敛，导致最后的output越来越小，跟正确答案相减之后也就看不到显著的最小值，看起来到处都是最小值。

那你可能又会问，像前面说的，把所有的答案列出来跟标准答案比对，怎么会找不到最小值呢?事实上，我们面对的可能不只一百万个答案，很可能是千万或上亿个答案，实作上几乎不可能列出所有的答案去「穷举」它，而是用随机抽选的方式，在线上找一个点，然后比对它旁边的数值，看看是否更低，慢慢去贴近最低点，像在爬楼梯一样渐渐往下所以才称为gradient。

这会遇到一种状况，当系统以为找到了最低点，但其实越过山丘，还会发现更低点;或者开局就落在高原上，附近超平坦的，就觉得应该是最低点了，其实远方还有人在谷底等候。

神经网络叠加的越多层，这个问题就会越明显，因此需要设计不同的架构，跟特殊的运算过程，才能避免找不到最低点。有时候反而layer 少一点，正确率还更高。

「目前我们尝试的语音辨识模型，大概叠8层，是一个C/P值满高的选择。」李宏毅说，深度学习目前其实还在神农尝百草的阶段，甚至是寒武纪大爆发的时代，虽然很精彩，但是其实水准还很低阶。也许不久后，就有人找出比Gradient Descent更有效的方法，那现在所学的很多帮忙找出最低点的技术就没用了，但这也代表我们离高阶人工智慧又更近一步。

图像辨识、语音分析和种种可能在学习深度学习的时候，我脑中一直浮现古老的丰年果糖广告，爸爸骄傲的说「爸爸头脑比电脑好啊」的画面。从理论来看，人脑确实很强大，强大到机器也要模仿人脑来变得更聪明。

「人类的五感随着演化过程互有消长，我们的嗅觉退化得很快，视觉则要处理最庞大的资料量。」专研影像辨识、智慧监控的Umbo CV 技术长张秉霖指出，人脑处理一张图像的资料量可能高达一百万个位元，如果我们要用机器进行影像辨识，就一定要参考人脑的运作方式。

在深度学习中有很多方式去辨识图像，其实作法跟人脑很像，第一层先处理基本的线条，然后再慢慢组合成一些形状，最后就能判读出图形的意义，就像2012年的Google Brain就能够从庞大的图形资料中，分辨出猫脸跟人脸的不同。

「但是俗话说得好，一张图胜过千言万语，不同人对同一张图片可能有不同的解读，这是因为每个人对这张图的背景知识认知不同。」张秉霖说，要让机器图像辨识再更上一层楼，就要让机器看懂图像背后的脉络，而要让机器看懂脉络，就需要让它吸收大量知识，最好的办法就是让机器学会人类的语言，就可以学习到更多背景知识。

![](https://img-blog.csdn.net/20181011225343509?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDI0NzI2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

清华大学电机系孙民博士指出，深度学习应用在图像分析上，目前机器已经能够用文字描述场景。

语音辨识面临的问题则恰恰相反，之前的语音分析是将语音转成文字，然后用文字进行语意分析，进而推理出这段语音的意思，这样的作法无法判断声音情感，常常会误判;新的作法则是将整段语音，丢到资料库里进行比对，找出最相近的声纹，来理解这段话的意思。

透过深度学习，机器正在变得越来越聪明，人工智能的运用也更加广泛。目前已经有一些案例，像是美国的梅西百货(Macy's)、Hilton McLean饭店还有乔治亚理工学院，也尝试运用IBM Watson来担任服务员、柜台接待与课堂助教，透过回答一些简单的问题，来减轻人类的负担，人类就能专心处理那些棘手的难题。

![](https://img-blog.csdn.net/20181011225401883?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDI0NzI2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Cornell大学的研究团队利用深度学习设计出的应用，首先输入一张画家的作品，再输入任意照片，机器就会用那位画家的风格来重绘照片。

不难想像，深度学习在未来会运用在更多领域，甚至还有人尝试用Watson来抓神奇宝贝。不过，深度学习的原理与实作的门槛并不太高，真正的难处不在深度学习本身，而是在于如何将人类要解决的问题用数字来表达，并设计成机器可以学习的架构，如何用数字来表示猫脸?用数字来描述围棋?这些都需要人类去定义，因此深度学习要进一步发展，最需要的其实是人才，剩下的，就是机器的事了。

