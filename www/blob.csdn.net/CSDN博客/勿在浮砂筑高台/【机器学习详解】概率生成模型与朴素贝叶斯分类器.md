# 【机器学习详解】概率生成模型与朴素贝叶斯分类器 - 勿在浮砂筑高台 - CSDN博客





2016年03月31日 23:39:08[勿在浮砂筑高台](https://me.csdn.net/luoshixian099)阅读数：9627
所属专栏：[机器学习详解](https://blog.csdn.net/column/details/ml-theory.html)









**转载请注明出处**[http://blog.csdn.net/luoshixian099/article/details/51028244](http://blog.csdn.net/luoshixian099/article/details/51028244)

# **1.概率生成模型**

首先介绍生成模型的概念，然后逐步介绍采用生成模型的步骤。

## **1.1概念**

> 
**即对每一种类别$C_k$分别建立一种数据模型$p(x|C_k)$，把待分类数据x分别带入每种模型中，计算后验概率$p(C_k|x)$，选择最大的后验概率对应的类别。**


假设原始数据样本有K类，生成学习算法是通过对原始数据类$p(x|C_k)$与$p(C_k)$建立数据类模型后,采用贝叶斯定理从而得出后验概率$p(C_k|x)$。对待分类样本x分别计算属于每个类别的后验概率$p(C_k|x)$，取最大可能的类别。

$arg\ max(k)=p(C_k|x)=\frac{p(x,C_k)}{p(x)}=\frac{p(x|C_k)p(C_k)}{\sum_j{p(x|C_j)p(C_j)}}$
- 
**二分类的情况:（K=2）**


$p(C_1|x)=\frac{p(x,C_1)}{p(x)}=\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}=\frac{1}{1+exp(-\alpha)}=\sigma(\alpha)$其中$\alpha=ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$;函数$\sigma(\alpha)=\frac{1}{1+exp(-\alpha)}$称为sigmoid函数。

- 
**多类的情况：(K>2)**

多分类的情况，是二分类的扩展，称为softmax函数。同样采用贝叶斯定理：

$p(C_k|x)=\frac{p(x|C_k)p(C_k)}{\sum_j{p(x|C_j)p(C_j)}}=\frac{exp(\alpha_k)}{\sum_jexp(\alpha_j)}$

其中$\alpha_k=lnp(x|C_k)p(C_k)$。
## **1.2高斯分布假设**

对于连续变量x，我们首先假设给定具体类条件下数据密度函数$p(x|C_k)$分布服从多维高斯分布，同时所有类别$p(x|C_k)$具有相同的协方差矩阵$\sum$： 
![这里写图片描述](https://img-blog.csdn.net/20160331215413863)

二维高斯分布，相同方差，不同期望的三个图形。 
![这里写图片描述](https://img-blog.csdn.net/20160331220819056)- **二分类情况K=2**

把多维高斯分布公式带入上述对应的贝叶斯公式得： 
![这里写图片描述](https://img-blog.csdn.net/20160331215938099)
**注意到sigmoid函数参数是关于数据x的线性函数**

下图是2维数据的高斯分布图形： 
![这里写图片描述](https://img-blog.csdn.net/20160331222739235)- **多分类的情况K>2**

多维高斯分布函数带入softmax函数得： 
![这里写图片描述](https://img-blog.csdn.net/20160331222536969)
**注意：$\alpha_k(x)$也是关于样本数据x的线性函数**
> 
**实际上，无论是连续型数据还是下面将要介绍的离散型数据（朴素贝叶斯分类），只要假设的分布属于指数簇函数，都有广义线性模型的结论。**

**K=2时为sigmoid函数：参数$\lambda$为模型的固有参数**
![这里写图片描述](https://img-blog.csdn.net/20160331225413542)
**K>2时为softmax函数：**
![这里写图片描述](https://img-blog.csdn.net/20160331225427652)
## **1.3模型参数的求解**

在假设了数据类密度函数$p(x|C_k)$的情况下，下面需要对模型的参数进行求解。例如，上述假设了数据为高斯分布，需要计算先验概率$p(C_k)$及参数$\mu_k,\sum$ .我们采用最大化释然函数的方法求解： 

考虑二分类的情况：样本数据为$ (x_n,t_n)$，样本总量为N,$t_n=1$属于$C_1$类,总数为$N_1$；$t_n=0$属于$C_2$类，总数为$N_2$.假设先验概率$p(C_1)=\pi$;则$p(C_2)=1-\pi$

释然函数：![这里写图片描述](https://img-blog.csdn.net/20160331231702504)

分别求偏导数并令为0，得： 
![这里写图片描述](https://img-blog.csdn.net/20160331232128397)
![这里写图片描述](https://img-blog.csdn.net/20160331231934865)
![这里写图片描述](https://img-blog.csdn.net/20160331232020130)
![这里写图片描述](https://img-blog.csdn.net/20160331232006130)
# **2.朴素贝叶斯分类器(NBC)**

## **2.1概念**

朴素贝叶斯分类器是生成学习算法的一种。考虑一个样本$x=(x_1,x_2,x_3...x_D)$，有D个特征，每个特征$x_i$取值为有限的离散值，这时需要对$p(x|y)$建立模型。朴素贝叶斯算法做了一种很强的假设：即给定类别y=c的情况下，每种特征之间相互独立，即有$p(x_1|y,x_2)=p(x_1|y)$;$p(x_1,x_2|y)=p(x_1|y)p(x_2|y)$所以有： 
![这里写图片描述](https://img-blog.csdn.net/20160401204401289)
**条件类概率$p(x|y)$可根据数据类型建立不同的形式：**- 当样本数据x取实数值为时,采用高斯分布：$p(x|y=c,\theta)=\prod_{j=1}^{D}N(x_j|\mu_{jc},\sigma^2_{jc})$
- 当每种特征$x_j\in\{0,1\}$时，采用伯努利分布$p(x|y=c,\theta)=\prod_{j=1}^{D}Ber(x_j|\mu_{jc})$
- 当每种特征取值$x_j\in\{1,2,3,...,K\}$,可以采用multinoulli distribution：$p(x|y=c,\theta)=\prod_{j=1}^{D}Cat(x_j|\mu_{jc})$

## **2.2文本分类**

朴素贝叶斯虽然做了很强的特征独立性假设，却对在文本分类的情况效果很好。 

首先收集所有样本数据中出现过的词，建立一个有序字典，长度为D。对待分类文本x依据字典建立一个长度为D词向量,$x=(x_1,x_2,x_3,....,x_D)$,每种特征$x_j\in\{0,1\}$。即$x_j=1$表示字典中第j个词在此文本中出现过；反之，$x_j=0$表示字典中第j个词没有在文本中出现过,采用伯努利分布$p(x,y)=p(y)p(x|y)=p(y)\prod_{j=1}^{D}Ber(x_j|\mu_{jc})$。 

定义：$\phi_{i|y=0}=p(x_i=1|y_i=0)$,$\phi_{i|y=1}=p(x_i=1|y_i=1)$,$\phi_{y}=p(y=1)$

释然函数： 
![这里写图片描述](https://img-blog.csdn.net/20160401221724356)

最大释然估计得： 
![这里写图片描述](https://img-blog.csdn.net/20160401221810965)

训练出模型后，对待分类样本根据贝叶斯定理，计算每种类别的后验概率，选择最大的后验概率类别： 
![这里写图片描述](https://img-blog.csdn.net/20160401223142799)
## **2.3拉普拉斯平滑**

在对文本分类的情况下，假如我们训练分类器采用的训练文本所有$x_j$都为0时，这时模型参数$\phi_{j|y=0}=0$,$\phi_{j|y=1}=0$，这时如果需要对待一个文本x分类且$x_j=1$，根据上述朴素贝叶斯方法，得到每种后验概率都为0,即$p(y=1|x)=0,P(y=0|x)=0$。这是由于上述乘法的缘故，根本原因是$\phi_{j|y=0}=0$,$\phi_{j|y=1}=0$。由于样本量有限，预测某个事件的发生概率为0，也是不准确的。 

为了解决这种情况，可以模型参数的分子加上1，同时保持和为1，，称为拉普拉斯平滑。 
![这里写图片描述](https://img-blog.csdn.net/20160401225758699)
![这里写图片描述](https://img-blog.csdn.net/20160401225741684)
参考：PRML&&MLAPP














