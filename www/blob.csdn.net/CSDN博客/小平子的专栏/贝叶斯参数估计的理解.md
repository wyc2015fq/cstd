# 贝叶斯参数估计的理解 - 小平子的专栏 - CSDN博客





2016年12月03日 13:51:54[阿拉丁吃米粉](https://me.csdn.net/jinping_shi)阅读数：7645标签：[贝叶斯估计																[共轭先验																[参数估计																[机器学习](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)




# 极大似然估计

贝叶斯估计是参数估计中的一种方法，以贝叶斯思想为基础，而贝叶斯思想在机器学习中经常用到。机器学习中常涉及贝叶斯网络，最终的问题都是转化为参数求解。贝叶斯参数估计是这些问题的基础版本。前方高能预警，本文的讲解比较理论。

实际问题中我们会有很多数据，比如一篇文章中每个单词的词频等。我们得到的数据通常用$X$表示，也称为样本。我们还会假设这些数据服从某一个分布，例如最常用的正态分布，这时可以将问题表示为$X \sim N(\mu, \sigma)$，$\mu$和$\sigma$表示正态分布的两个参数。如果这两个参数知道了，这个分布就确定了，从而可以知道数据$X$的许多性质。最常用的参数估计方法是极大似然（或最大似然估计）估计。

一般的最大似然法求解两个参数的基本步骤是：
- 假设每个样本$X_i$是独立同分布（iid）的，即每一个样本都有$X_i \sim N(\mu, \sigma)$.
- 求所有样本$X$的联合分布 

因为是iid，所以$X$的联合分布等于每个样本$X_i$的概率密度函数的乘积，即：$L(\mu, \sigma^2; \boldsymbol{x}) = f(\boldsymbol{x}) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left\{-\sum_{i=1}^n \frac{(x_i - \mu)^2}{2\sigma^2}\right\}$
- 对上述联合分布的概率密度函数取对数，即：$\ell(\mu, \sigma^2; \boldsymbol{x}) = \log L(\mu, \sigma^2; \boldsymbol{x}) = -\frac{n}{2}\log\left(2\pi\sigma^2\right) -\sum_{i=1}^n \frac{(x_i - \mu)^2}{2\sigma^2}$
- 对上述函数分别求$\frac{\partial \ell}{\partial \mu}$和$\frac{\partial \ell}{\partial \sigma^2}$并另它们等于0，进而求得极值
- 分别对$\mu$和$\sigma^2$求二阶偏导，验证极值是最大值

上述步骤是极大似然法的求解步骤，用到的信息都是已知样本的信息。但是通常在估计参数时我们可能已经对参数有了一个大概的了解，比如已经知道$\mu$和$\sigma^2$的取值范围。仅仅知道取值范围还太简单，有时会更进一步假设$\mu$和$\sigma^2$的取值服从某个分布，这样问题就变成了在正态分布中，要估计期望$\mu$和方差$\sigma^2$的值，但与极大似然法不同的是，我们事先已经知道了$\mu$和$\sigma^2$的取值是服从某种分布的，这个信息如果不用到参数估计中那真是太浪费了，于是问题变成：**如何将这两个参数的分布结合到参数估计当中去呢？**贝叶斯估计解决了这个问题。

# 贝叶斯估计（Bayes Estimation）

上述提到的在估计参数之前对参数已经有了了解称为参数的**先验知识**。贝叶斯估计即在估计过程中将先验知识也考虑了进去，博众家之长总是好的。**先验知识可以是一个具体的值，也可以是取值范围（函数）**。实际应用中，通常会将参数的先验知识视作一个分布，那么这个参数就会有一个概率密度函数，这个pdf叫做参数的**先验概率**。

一般待估计的一维参数用$\theta$表示，多维用粗体$\boldsymbol{\theta}$. **先验概率密度函数用符号$\pi(\theta)$表示**。样本的概率密度函数用$f(x|\theta)$表示，其中加入$\theta$是表示该pdf跟$\theta$有关，同时说明要估计的参数是$\theta$.

贝叶斯估计涉及到三个基本概念，他们长的很像：
- **损失函数**（Loss Funcition）
- **风险函数**（Risk Function）
- **贝叶斯风险**（Bayes Risk）

贝叶斯估计的目的是**结合参数的先验知识，使得估计出来的参数能令贝叶斯风险达到最小**。简单说就是最小化贝叶斯风险。

下面解释这三个概念。

## 损失函数

在参数估计问题中，评价估计的好坏就是看估计出来的参数与真值的**差距**有多小。估计出来的参数通常用$\hat{\theta}$表示，参数的真值用$\theta$表示。那么这个**差距**如何定义呢？实际上，这个差距就是损失函数。

损失函数有好几种：
- 

$L(\hat{\theta}, \theta) = (\hat{\theta} - \theta)^2$
- 

$L(\hat{\theta}, \theta) = |\hat{\theta} - \theta|$
- 

$\begin{split} L(\hat{\theta}, \theta) = \begin{cases}	0 &\mbox{if $|\theta - \hat{\theta}| \leqslant \Delta$} \\	1 &\mbox{if $|\theta - \hat{\theta}| > \Delta$}\end{cases}\end{split}$

上述是三种常用的损失函数。可以看到当估计值与真实值无限接近时，损失函数都会无限接近0，相当于没有损失. 损失函数中的估计值$\hat{\theta}$是通过样本计算出来的。比如正态分布中的$\mu$，我们可以用样本均值来估计$\mu$，即$\hat{\mu} = \frac{1}{n}\sum_{i=0}^n X_i = \bar{\boldsymbol{X}}$。我们也常常用样本方差来估计$\sigma^2$. 但是注意到$\hat{\mu}$和$\hat{\sigma}^2$的值都跟样本的个数有关，即都跟$n$有关。如果$\frac{1}{n}\sum_{i=0}^n X_i$是$\mu$的估计值，那$\frac{1}{n-1}\sum_{i=0}^{n-1} X_i$是$\mu$的估计值吗？那$\frac{1}{n-6}\sum_{i=0}^{n-6} X_i$呢？可以看到$n$不一样，估计值也不一样，到底用哪个$n$（用多少个样本）来计算损失函数呢？这时容易（其实不容易！）想到，既然损失函数可以因为$\hat{\theta}$的不同而有不同的值，那就求平均啊！通过判断损失的平均值的大小来判断参数估计得好不好。求平均是很自然的想法，但平均通常是相对样本来说的，如果是总体，我们通常说期望。这时就要引入**风险函数**了。

## 风险函数

其实风险就是**损失函数关于$\hat{\theta}$的期望**。

既然估计值$\hat{\theta}$是随$n$的变化而变化的，那也可以将$\hat{\theta}$视为随机变量，进而可以计算损失函数的期望。于是风险函数的定义如下：



$R(\hat{\theta}, \theta) = E_{\hat{\theta}} \left[L(\hat{\theta}, \theta) \right]$

$E_{\hat{\theta}}$表示对$\hat{\theta}$求期望（因为真值是固定的，不是变量）。我们的目标是：**求出一个$\hat{\theta}$，使得风险最小（最小化风险）**。

举个栗子，估计期望$\mu$，假设损失函数是$L(\hat{\mu}, \mu) = (\hat{\mu} - \mu)^2$，计算当$\hat{\mu} = \frac{1}{n}\sum_{i=0}^n X_i$时的风险是多少？



$E \left[L(\hat{\mu}, \mu) \right] = E(\bar{\boldsymbol{X}} - \mu)^2 = E\bar{\boldsymbol{X}}^2 - 2\mu E\bar{\boldsymbol{X}} + \mu^2 \ \ \mbox{(记住这个例子)}$

算出上式中的$E\bar{\boldsymbol{X}}^2$和$E\bar{\boldsymbol{X}}$就可以知道风险是多少啦！其实上式的风险是0，已经是最小值了。这个结果说明当使用样本均值来估计总体期望时，风险最低。

到目前为止我们并没有用到前面提过的先验知识$\pi(\theta)$。仅仅定义了损失函数和风险函数。上式的风险函数求出了一个具体的值，但很多情况没有那么简单，求出的风险是一个函数表达式，而不是值。如果求出来的风险是一个值，其实可以不用贝叶斯估计（上面也一直没有用到$\theta$的任何先验知识）。贝叶斯估计通常用于风险不可以直接比较的情况。

这时就要引入**贝叶斯风险**的概念了。贝叶斯风险中就用到了$\theta$的先验分布$\pi(\theta)$.

## 贝叶斯风险

我们注意到风险函数$R(\hat{\theta}, \theta)$是$\hat{\theta}$的函数，前面也提到真值$\theta$是一个固定的值，不是变量。**但是**！我们又提到，我们会在估计$\theta$之前知道了一些$\theta$的知识，比如说$\theta$的分布$\pi(\theta)$（这个分布是怎么知道的呢？下文有讲解）。既然$\theta$是有一个概率分布的，**那么此时$\theta$也变成一个随机变量了**，所以$R(\hat{\theta}, \theta)$同时是$\hat{\theta}$和$\theta$的函数。这时要怎么使用$\pi(\theta)$呢？下面定义贝叶斯风险。

（注意！下面要积分了！）

贝叶斯风险： 


$Bayes\ Risk = E_{\theta} \left[R(\theta, \hat{\theta})\right] = \int R(\theta, \hat{\theta})\pi(\theta) d\theta$

上式表示：**贝叶斯风险是风险函数在$\theta$上的期望（时刻注意到期望是通过积分来定义的）**。

回顾一下期望的定义。如果随机变量用$\boldsymbol{X}$表示，其概率密度函数是$f(\boldsymbol{x})$，那么$\boldsymbol{X}$的期望$E\boldsymbol{X} = \int xf(x)dx$. 对照期望的积分形式，可以看到$\pi(\theta)$就是$\theta$的概率密度函数，随机变量是$\theta$. $R(\theta, \hat{\theta})$是关于随机变量$\theta$的一个函数，所以Bayes Risk就是在求风险函数的期望。总的来说：**风险函数是损失函数关于$\hat{\theta}$的期望，而贝叶斯风险是风险函数关于$\theta$的期望。**所以贝叶斯风险是一个双期望。更进一步，$\hat{\theta}$是关于随机变量$X$的函数（$\hat{\theta}$总是通过$\boldsymbol{X}$求出来的呀！），所以也可以说：**风险函数是损失函数关于$\boldsymbol{x}$的期望**。这样就将风险与$\theta$的先验知识结合起来了。

之所以叫贝叶斯风险是因为引入了一个先验分布$\pi(\theta)$. 『先验』这个词本身就是贝叶斯理论的一部分。

# 如何最小化贝叶斯风险（贝叶斯估计）

终于到达贝叶斯估计的核心了，即如何找到一个$\hat{\theta}$，使得贝叶斯风险最小。问题转化为求$ \int R(\theta, \hat{\theta})\pi(\theta) d\theta$的最小值。

这是一个积分形式的函数，如果知道$R(\theta, \hat{\theta})$和$\pi(\theta)$的形式或许可以求出最小值。但其实仅仅根据这个抽象的形式，也是可以求出最小值的形式的。

下面要开始积分了！不想看推导过程可以直接看最后结论。

**为了跟一些书的符号统一，下面引入一个新的符号：$a = \hat{\theta}$. 即用字母$a$来表示参数的估计值$\hat{\theta}$.**

字母$a$表示action，表示采取某一动作求得$\theta$的估计。**在参数估计问题中，$a$就是$\hat{\theta}$.** 所以$R(\theta, \hat{\theta}) = R(\theta, a)$

因为$R(\theta, a)$的本质也是求期望，先将其转换为积分的形式。



$R(\theta, a) = E_a\left[L(\theta, a)\right]  = \int L(\theta, a(x))f(x|\theta) dx$

因为$a$是$x$的函数，所以对$a$求期望就是在$x$上求期望，进而转换为对$x$求积分。同时$x$的概率密度函数是已知的，记为$f(x|\theta)$，表示$x$的pdf跟$\theta$有关，也表示**条件概率密度函数**，一石二鸟。

既然在$\theta$给定的条件下，$x$的条件pdf是$f(x|\theta)$（此时已经将$\theta$当成随机变量看待了），而且$\theta$的分布又是$\pi(\theta)$，那么根据条件概率的定义，可以求出$x$的边缘概率密度函数：



$f(x) = \int f(x|\theta) \pi(\theta)\ d\theta \ \ \mbox{(边缘概率定义)}$

所以有：



$\begin{split}\int R(\theta, a)\pi(\theta) d\theta &= \int_{\theta} \left[\int_x L(\theta, a(x))\ f(x|\theta)\ dx\right] \pi(\theta)\ d\theta  & \mbox{(带入上式展开)}\\&= \int_{\theta} \int_x L(\theta, a(x)) \frac{f(x|\theta) \pi(\theta)}{f(x)} f(x)\ dx\ d\theta & \ \mbox{(除以一个$f(x)$再乘以一个$f(x)$结果不变)} \\&= \int_{\theta} \int_x L(\theta, a(x)) \pi(\theta|x)\ f(x)\ dx\ d\theta & \ \mbox{(贝叶斯定理)} \\&= \int_x \left[\int_{\theta} L(\theta, a(x)) \pi(\theta|x)\ d\theta \right] f(x)\ dx\ & \ \mbox{(交换积分顺序)} \\\end{split}$

到这里再也无法化简了，那就来分析上式中最后一项。

通过贝叶斯定理，我们求出了$\pi(\theta|x)$. 观察中括号中的式子，$\pi(\theta|x)$是一个pdf，会发现中括号中的这一项很像期望的定义。实际上中括号这一项就是**在$\pi(\theta|x)$上求损失函数$L(\theta, a(x))$的期望。**

要记得贝叶斯估计的目的：**求出$\hat{\theta}$（在这里就是$a(x)$），使得贝叶斯风险最小（即$\int R(\theta, a)\pi(\theta) d\theta$最小）**。

中括号中的那一项是对$\theta$积分，积分后不会有$\theta$，从而整个式子只剩下$x$. 而回想一下贝叶斯估计的目的，$x$并不是我们要关心的。所以贝叶斯估计就是要计算中括号一项，使得中括号内的积分最小，最终还是回到了损失函数上。

刚才说到中括号一项看起来像是期望的定义，其实这一项就叫posterior expected risk. 记作：



$\int_{\theta} L(\theta, a(x)) \pi(\theta|x)\ d\theta = E_{\pi} L(\theta, a(x))$

$E_{\pi}$表示在$\pi(\theta|x)$上求期望。$\pi(\theta|x)$就叫做**$\theta$的后验分布**，即在知道数据$x$后$\theta$的分布。所以贝叶斯估计就是：**求$\hat{\theta}$，使得损失函数在$\theta$的后验分布上的期望最小。**

这样我们发现，$L(\theta, a(x))$是自己设计的，比如前面提到的那三种，如果知道$\pi(\theta)$，根据贝叶斯定理，容易求出$\pi(\theta|x)$；而$\pi(\theta)$又是我们自己定义的（先验知识嘛，肯定是事先就知道了的），所以这个posterior expected risk不难求得。至于如何计算后验分布，后面有讲解。

## 结论

贝叶斯参数估计的步骤：
- 拿到数据，知道数据的分布，记为$f(\boldsymbol{x}|\theta)$，要估计的参数记为$\theta$
- 定义损失函数$L(\theta, \hat{\theta})$
- 定义$\theta$的先验知识或先验分布$\pi(\theta)$
- 根据贝叶斯定理求出后验分布$\pi(\theta|\boldsymbol{x}) = \frac{f(\boldsymbol{x}|\theta)\pi(\theta)}{f(\boldsymbol{x})}$
- 最小化如下式子：$\arg \min \int L(\theta, \hat{\theta}) \pi(\theta|\boldsymbol{x})\ d\theta = E_{\pi} L(\theta, \hat{\theta})$

上面最后一步，涉及积分以及最小值求解，看起来十分麻烦。所幸，在特定的损失函数形式下，上面最后一步可以化简，并不需要完全用到积分。下面一部分的讲解就是在三种特定损失函数形式下贝叶斯估计的计算方法。

理论总是简单的，上述的求解过程后面附有例子，可以温习一下。

# 三种常用损失函数的贝叶斯估计计算

## Square Error

square error就是 


$L(\theta, a) = (\theta - a)^2$

下面这直接给出结论： 
**若损失函数是square error，那么当$a$等于$\theta$在$\pi(\theta|x)$上的期望时，贝叶斯风险最小。**
上述结论证明涉及到其它知识，略过。

上述结论说明，如果知道$\pi(\theta|x)$的形式，那么只需求$\int\theta\pi(\theta|x)\ d\theta$就可以了，实际上就是期望。例如，如果$\pi(\theta|x)$是正态分布，$\pi(\theta|x) \sim N(\eta, \tau)$，那么$a = \eta$，$\eta$就是参数$\theta$的估计。

## Absolute Error

Absolute error就是 


$L(\hat{\theta}, \theta) = |\hat{\theta} - \theta|$

下面直接给出结论： 
**若损失函数是square error，那么当$a$等于数据$X$的中位数时，贝叶斯风险最小。**

这个结论说明：如果使用square error作为损失函数，连$\theta$的后验分布都不用求了。

## Uniform Error

uniform error的形式为： 


$\begin{split} L(\hat{\theta}, \theta) = \begin{cases}	0 &\mbox{if $|\theta - \hat{\theta}| \leqslant \Delta$} \\	1 &\mbox{if $|\theta - \hat{\theta}| > \Delta$}\end{cases}\end{split}$

下面直接给出结论： 
**若损失函数是uniform error且$\Delta$很小，当$a = \arg \max \pi(\theta|x)$时，即$a$等于$\theta$后验分布的最大值时，贝叶斯风险最小。**

上述结论说明，在uniform error的情况下，如果知道$\pi(\theta)$的形式，那么求它的最大值即可。如果$\pi(\theta|x)$是正态分布，$\pi(\theta|x) \sim N(\eta, \tau)$，正态分布的最大值在均值处取得，所以$\theta$的估计值为$\eta$，与square error一样。

# 如何确定先验分布（先验知识）与后验分布

前面一直提到先验知识或者先验分布，偶尔混用。这是因为$\theta$的先验知识$\pi(\theta)$可以有很多种形式，可以是一个数，可以是离散的几个数，也可以是个概率分布函数，此时就叫后验分布。

但是对于后验分布，它只能是一个概率分布形式，即$\pi(\theta|x)$必须满足概率密度函数的定义，而$\pi(\theta)$却不一定。原因是因为最后求贝叶斯风险最小值的函数形式只涉及到$\pi(\theta|x)$，没有涉及到$\pi(\theta)$，所以$\pi(\theta)$取什么值在数学上无所谓，但是会对结果造成影响。

很多应用中$\pi(\theta)$会取一个概率密度函数。下面介绍一种$\pi(\theta)$的取法：**共轭先验**。

## 共轭先验（Conjugate Prior）

**共轭先验**这四个字指的不是一个分布，而是指一大类分布，比如指数族分布。下面给出共轭分布不太严谨的数学定义：

设资料$X$有概率密度函数$F$：$X \sim F(x| \theta)$. $\theta$的先验分布$\pi(\theta)$属于某个分布族$P$：$\pi(\theta) \in P$. 如果对任意$\theta$，$\theta$的后验分布 $\pi(\theta|x)$也属于分布族$P$，那么$P$就叫做$F$的共轭先验。

**白话解释：**

如果找到一个$\pi(\theta)$，它是$F$的共轭先验，那么$\theta$的后验分布$\pi(\theta|x)$和先验分布$\pi(\theta)$会有一样的形式，即同属于分布族$P$。注意共轭是指$\pi(\theta)$与$f(x|\theta)$共轭。

前面说到贝叶斯估计最终需要计算出$\theta$的后验分布，$\theta$的先验分布$\pi(\theta)$分布是已知的。如果$\pi(\theta)$于$f(x)$共轭，那么$\pi(\theta)$与$\pi(\theta|x)$会有一样的形式，这样不就会很方便求解$\pi(\theta|x)$？事实上就是这样的，因此在设计先验分布的时候常常会设计成与$f(x)$共轭，后面计算会方便。

但是看定义凭空想出一个共轭先验蛮难的。幸运的是我们可以证明，**所有属于指数族分布的$f(x)$，都可以求出它的共轭先验分布$\pi(\theta)$的具体形式**。而大部分常见的分布都属于指数族分布（Exponential Family），比如正态分布，指数分布，二项分布，泊松分布，Beta分布，Gamma分布等等。

下面给出常见的共轭先验：
|中文名称|$f(x|\theta)$|$\pi(\theta)$|
|----|----|----|
|正态分布|$N(\mu, \sigma^2)$|$\pi(\mu) \sim N(\eta, \tau)$|
|正态分布|$N(\mu, \sigma^2)$|$\pi(\sigma^2) \sim inverse\ Gamma$|
|指数分布|$Exp(\theta)$|$\pi(\theta) \sim inverse\ Gamma$|
|二项分布|$Bin(n, p)$|$\pi(p) \sim Beta(\alpha, \beta)$|
|泊松分布|$Poi(\lambda)$|$\pi(\lambda) \sim \Gamma(\alpha, \beta)$|

如果你的样本的分布是上面表中第二列中的一项，那么就可以将先验分布设计成第三列中对应的分布。所以在才会在那么多算法或实际问题中将某个参数的分布定义成Gamma或Beta这种“奇怪”的形式，原因就是为了求解方便。

## 后验分布的计算

贝叶斯估计的落脚点之一是求解参数的后验分布$\pi(\theta|x)$. 本部分讲解求解$\pi(\theta|x)$的步骤。

先给出公式： 


$\pi(\theta|x) = \frac{f(x|\theta)\pi(\theta)}{f(x)}$

贝叶斯理论中是将参数作为随机变量看待的，因此样本的概率密度函数通常写成$f(x|\theta)$的形式，表示$\theta$是已知的，而$f(x)$就是$x$的边缘密度概率函数，这个需要计算，而且的确是可以计算出来的。

### 泊松分布的例子

这个例子很理论，但胜于比较典型。

问题：已知$X_1, X_2, ..., X_n$是iid的，服从泊松分布，$X_i \sim Poi(\lambda)$. $\lambda$是要估计的参数。  

（1） 利用$\lambda$的共轭先验求$\lambda$的后验分布 

（2） 求在square error下$\lambda$的贝叶斯估计$\hat{\lambda}_{bayes}$
**求解（1）**

依题意，我们可以写出$X_i$的概率密度函数： 


$f(x|\lambda) = \frac{e^{-\lambda}\lambda^x}{x!}$
因为$X_i$是独立同分布，所以它的联合概率密度函数是： 


$f(\boldsymbol{x}|\lambda) = \frac{e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n (x_i!)} \ \mbox{(连乘)}$

由之前的表格可知，泊松分布的共轭先验分布是Gamma分布，Gamma分布的参数是$\alpha$和$\beta$，这两个参数可以当成是已知的。所以先验分布可以写成： 


$\pi(\lambda) = \frac{\beta^\alpha \lambda^{\alpha-1} e^{-\lambda \beta}}{\Gamma(\alpha)}$

求解$X$的边缘概率密度函数$f(x)$ （下面积分过程其实很简单，但也实在太碍眼了，可以略过不看） 


$\begin{split}f(\boldsymbol{x}) &= \int_0^\infty f(\boldsymbol{x}|\lambda) \pi(\lambda) d\lambda\  \mbox{（连续函数需要积分，如果是离散就就求和）}\\&=  \int_0^\infty\frac{e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n (x_i!)} \frac{\beta^\alpha \lambda^{\alpha-1} e^{-\lambda \beta}}{\Gamma(\alpha)} d\lambda \\&= \frac{\beta^\alpha}{\prod_{i=1}^n (x_i!) \Gamma(\alpha)} \int_0^\infty \frac{\lambda^{\sum_{i=1}^n x_i + \alpha - 1}}{e^{(n+\beta)\lambda}} d\lambda \\&= \left(\frac{1}{n+\beta}\right)^{\sum_{i=1}^n x_i + \alpha} \frac{\beta^\alpha}{\prod_{i=1}^n (x_i!) \Gamma(\alpha)} \int_0^\infty \frac{\left((n+\beta)\lambda\right)^{(\sum_{i=1}^n x_i + \alpha) - 1}}{e^{(n+\beta)\lambda}} d(n+\beta)\lambda \\&=   \frac{\beta^\alpha}{\prod_{i=1}^n (x_i!) \Gamma(\alpha)} \Gamma(\sum_{i=1}^n x_i + \alpha) \left(\frac{1}{n+\beta}\right)^{\sum_{i=1}^n x_i + \alpha}\end{split}$

根据贝叶斯定理求解$\lambda$的后验分布： 


$\begin{split}\pi(\lambda|\boldsymbol{x}) &= \frac{f(\boldsymbol{x}|\lambda) \pi(\lambda)}{f(\boldsymbol{x})} \\&= \frac{e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n (x_i!)} \frac{\beta^\alpha \lambda^{\alpha-1} e^{-\lambda \beta}}{\Gamma(\alpha)} \frac{\prod_{i=1}^n (x_i!) \Gamma(\alpha) (n+\beta)^{\sum_i x_i + \alpha}}{\beta^\alpha  \Gamma(\sum_{i=1}^n x_i + \alpha) } \\&= \frac{e^{-(n+\beta)\lambda} \lambda^{\sum_i x_i + \alpha - 1} (n+\beta)^{\sum_i x_i + \alpha}}{\Gamma(\sum_{i=1}^n x_i + \alpha)}\end{split}$

上面的式子很复杂，但其实它是一个Gamma分布： 


$\pi(\lambda|\boldsymbol{x})  \sim \Gamma\left(\sum_{i=1}^n x_i + \alpha, n+\beta \right)$

上面的求解过程还是太复杂，其实有更简便的方法。因为共轭先验分布是Gamma分布，所以后验分布肯定也是Gamma，我们可以直接凑出后验分布的形式，但不是很直观，略过。

**求解（2）**

Square error下的贝叶斯估计就是后验分布的期望。对于$X \sim \Gamma( \alpha, \beta )$，$EX = \frac{\alpha}{\beta}$. 所以对于本问题： 


$\hat{\lambda}_{bayes} =  \frac{\sum_{i=1}^n x_i + \alpha}{n+\beta}$
可以看到先验分布中$\lambda$服从参数为$\alpha$和$\beta$的Gamma分布，在观察到一些数据后，$\lambda$仍然是服从Gamma分布的，只不过参数得到了修正，变成了$\sum_{i=1}^n x_i + \alpha$和$n+\beta$. 这就是贝叶斯估计的思想，先假设参数服从某个分布，可能会有偏差。有偏差不要紧，我们将观察到的数据（样本）带入贝叶斯估计的过程便可以修正这些偏差。

### 二项分布的例子

问题：已知$X_1, X_2, ..., X_n$是iid的，服从伯努利（Bernouli），$X_i \sim Ber(r)$. $r$是要估计的参数。  

（1） 利用$r$的共轭先验求$r$的后验分布 

（2） 求在square error下$r$的贝叶斯估计$\hat{r}_{bayes}$
**求解（1）**

依题意，可以写出数据的分布： 


$f(\boldsymbol{x}|r) = r^{\sum x_i} (1-r)^{n-\sum x_i} \mbox{ (单个pdf连乘)}$

二项分布的共轭先验是Beta分布，长这个样子： 


$\pi(r) \sim Beta(\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma({\alpha}) + \Gamma({\beta})} r^{\alpha-1} (1-r)^{\beta - 1}$

不像第一个例子，这里不根据$\pi(\theta|\boldsymbol{x}) = \frac{f(\boldsymbol{x}|\theta)\pi(\theta)}{f(\boldsymbol{x})}$来求后验分布，而是用比较简单的拼凑法来求。

注意到$\pi(\theta|\boldsymbol{x}) = \frac{f(\boldsymbol{x}|\theta)\pi(\theta)}{f(\boldsymbol{x})}$的分母与参数$\theta$无关，因此可以认为$\pi(\theta|\boldsymbol{x})$的分布近似于$f(\boldsymbol{x}|\theta)\pi(\theta)$的形式。



$\begin{split}\pi(r|\boldsymbol{x}) &\propto \pi(r) \times f(\boldsymbol{x}|r) \\&\propto r^{\alpha - 1} (1-r)^{\beta - 1} \times r^{\sum x_i} (1-r)^{n-\sum x_i} \mbox{(其余部分与参数无关，省略)} \\&\propto r^{\sum x_i+\alpha - 1} (1-r)^{n-\sum x_i+\beta - 1} \\& \propto \frac{1}{B(\alpha + \sum x_i, n-\sum x_i + \beta)}r^{\sum x_i+\alpha-1}(1-r)^{n-\sum x_i+\beta-1} \mbox{(凑出Beta分布的形式)}\end{split}$

上式中： 


$B(\alpha + x, n-x + \beta) = \frac{\Gamma(\alpha + x + n - x + \beta)}{\Gamma(\alpha + x) + \Gamma(n - x +\beta)} = \frac{\Gamma(\alpha + \beta + n)}{\Gamma(\alpha+ x) + \Gamma(n-x+\beta)}$.

所以最终后验分布的形式是： 


$\pi(r|\boldsymbol{x}) \sim Beta(\alpha + \sum x_i, n-\sum x_i+\beta)$

**求解（2）**

Square error下的贝叶斯估计是后验分布的期望，在这里即为$Beta(\alpha + x, n-x+\beta)$的期望。其期望如下： 


$\hat{r}_{bayes} = E(r|X) = \frac{\alpha+\sum x_i }{\alpha+\sum x_i + n-x + \beta} = \frac{\alpha+\sum x_i }{\alpha + \beta + n}$

#### 二项分布贝叶斯估计的实际背景（点击率的贝叶斯平滑）

这个例子有实际应用的背景。$X_i$可以认为是一件商品或一则广告，对于网页上的广告，用户看到了算是一次曝光（impression或exposure），看到之后用户只有点击与不点击两种情况，点击取1，不点击取0，那么$\sum x_i$的实际意义就是点击次数，而$n$就是曝光次数。令$C=\sum x_i$，$I=n$，$\frac{C}{I}$就是点击率了，用$r$表示点击率，那么上式中的$\hat{r}_{bayes}$就是点击率的贝叶斯估计，也称为点击率的贝叶斯平滑，式中的$\alpha$和$\beta$是平滑参数。这两个平滑参数可以方便地从历史数据中计算（估计）得到。](https://so.csdn.net/so/search/s.do?q=参数估计&t=blog)](https://so.csdn.net/so/search/s.do?q=共轭先验&t=blog)](https://so.csdn.net/so/search/s.do?q=贝叶斯估计&t=blog)




