# 周志华《机器学习》 学习笔记（三） 经验误差、过拟合与评估方法 - HJ - CSDN博客
2018年02月04日 21:46:58[FZH_SYU](https://me.csdn.net/feizaoSYUACM)阅读数：704
所属专栏：[机器学习](https://blog.csdn.net/column/details/19571.html)
一、经验误差
通常我们把分类错误的样本数占样本总数的比例称为“错误率”，即如果在m个样本中有a个样本分类错误，则错误率E = a / m；相应的，1 – a / m称为“精度”（accuracy），即“精度 = 1 - 错误率”。更一般地，我们把学习器的实际预测输出与样本的真是输出之间的差异称为“误差”，学习器在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差称为“泛化误差”。
显然，我们希望得到泛化误差小的学习器。然而，我们事先并不知道新样本是什么样，实际能做的是努力使经验误差最小化。
二、过拟合
我们是实际希望的，是在新样本上能表现得很好的学习器。为了达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样才能在遇到新样本时作出正确的判别。然而，当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。这种现象在机器学习中称为“过拟合”。与“过拟合”相对的是“欠拟合”，这是指对训练样本你的一般性质尚未学好。图2.1给出了关于过拟合与欠拟合的一个便于直挂能理解的类比：
![](https://img-blog.csdn.net/20180204214320874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
有很多因素可能导致过拟合，其中最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则通常是由于学习能力低下而造成的。
欠拟合比较容易克服，例如在决策树学习中扩展分支、在神经网络学习中增加训练轮数等，而过拟合则很麻烦。在后面的学习中，我们将看到，过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施。
另外，必须认识的一点是，过拟合是无法彻底避免的，我们所能做的只是“缓解”，或者说减小其风险。
关于这一点，可大致这样理解：机器学习面临的问题通常是NP难甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性地证明了“P=NP”；因此，只要相信“P≠NP”，过拟合就不可避免。
三、评估方法
通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而作出选择。为此，需使用一个“测试集”来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。
现在，我们只有一个包含m个样例的数据集D={(x1,y1),(x2,y2),…,(xm,ym)}，既要训练，又要测试，怎样过才能做到呢？答案是：通过对D进行适当的处理，从中产生出训练集S和测试集T。下面介绍几种常见的做法。
1.留出法
“留出法”直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即D=S∪T，S∩T=∅。在S上训练处模型后，用T来评估其测试误差，作为对泛化误差的估计。
以二分任务为例，假定D包含1000个样本，将其划分为S包含700个样本，T包含300个样本，用S行训练后，如果模型在T上有90个样本分类错误，那么其错误率为（90/300）×100%=30%，相应的，精度为1-30%=70%。
需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似。如果从采样的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”。
另一个需注意的问题是，即便在给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。因此，单次使用留出法到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。
此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境：若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练处的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性。这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试。
2.交叉验证法
“交叉验证法”先将数据集D划分为k个大小相似的互子集，即。每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样的到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。
显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证”。k最常用的取值是10，此时称为10折交叉验证。图2.2给出了10折叉验证的示意图： 
![](https://img-blog.csdn.net/20180204214430401?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。为减少因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折叉验证结果的均值。
缺点比较：我们希望评估的是用D训练处的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。
3.自助法
“自助法”是针对上述缺点的一个比较好的解决方案，它直接以自助采样法为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集D’：每次随机从D中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集D’，这就是自助采样的结果。
显然，D中有一部分样本会在D’中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采样中始终不被采到的概率![](https://img-blog.csdn.net/20180204214523539?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，取极限得到![](https://img-blog.csdn.net/20180204214542338?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D’中。
于是我们可将D’用作训练集，D\D’用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。
这样的测试结果，称为“包外估计”
自助法在数据集较小、难以有效划分训练/测试集时很有用；而在初始数据量足够时，留出法和交叉验证法更常用一些。
4.调参与最终模型
在进行模型评估与选择时，除了要对使用学习算法进行选择，还需对算法参数进行设定，这就是通常所说的“参数调节”或简称“调参”。
在现实中常用的做法，是对每个参数选定一个范围和变化步长。显然，有的时候选定的参数值往往不是“最佳”值，但这是在计算开销和性能估计之间进行折中的结果，通过这个折中，学习过程才变得可行。事实上，即便在进行这样的折中后，调参往往仍很困难。
另外，需注意的是，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为“验证集”。
