# 周志华《机器学习》  学习笔记（二） 假设空间与归纳偏好 - HJ - CSDN博客
2018年02月04日 17:18:21[FZH_SYU](https://me.csdn.net/feizaoSYUACM)阅读数：714
所属专栏：[机器学习](https://blog.csdn.net/column/details/19571.html)
一、假设空间
归纳（induction）与演绎（deduction）是科学推理的两个大基本手段。前者是从特殊到一半的泛化（generalization）过程，即从具体的事实归结出一半性规律；后者则是从一般到特殊的“特化”（specialization）过程，即从基础原理推演出具体状况。
归纳学习有狭义和广义之分，广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念（concept），因此亦称为“概念学习“或”概念形成“。
概念学习中最基础的是布尔概念学习，即对“是”“不是”这样的可表示为0/1布尔值的目标概念的学习。
我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确定，假设空间及其规模大小就确定了。
![](https://img-blog.csdn.net/20180204171524200?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
在对假设空间进行操作的时候，可以有许多策略对这个假设空间进行搜索，例如自顶向下、从一般到特殊，或是自底向上、从特殊到一般，搜索过程中可以不断上删除与正例不一致的假设、和（或）与反例一致的假设。最终将会获得与训练集一致（即对所有训练样本能够进行正确判断）的假设，这就是我们学得的结果。需要注意的是，现实问题中我们常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，我们称之为“版本空间”（version space）。例如，在西瓜问题中，与图1.1训练集所对应的版本空间如下图1.2所示
![](https://img-blog.csdn.net/20180204171546859?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
二、归纳偏好
对于一个具体的学习算法而言，它必须要产生一个模型。这个时候，学习算法本身的“偏好”就会起到关键的作用。例如，若我们的算法喜欢“尽可能特殊”的模型，则它会选择“好瓜<->（色泽=*）∧（根蒂=蜷缩）∧（敲声=浊响）”；但若我们的算法喜欢“尽可能一般”的模型，并且由于某种原因它更“相信”根蒂，则它会选择“好瓜<->（色泽=*）∧（根蒂=蜷缩）∧（敲声=*）”。机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”或简称为“偏好”
![](https://img-blog.csdn.net/20180204171600914?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行在选择启发性或“价值观”。那么，有没有一般性的原则来引导算法确立“正确的”偏好呢？“奥卡姆剃刀”是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。
事实上，归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。
假设学习算法ξa基于某种归纳偏好产生了对应于曲线A的模型，学习算法ξb基于另一种归纳偏好产生对应于曲线B的模型
![](https://img-blog.csdn.net/20180204171649205?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
不过很遗憾，对于一个学习算法ξa，若它在某些问题上比学习算法ξb好，则必然存在另一些问题，在哪里ξb比ξa好。又去的是，这个结论对任何算法均成立。证明如下:
![](https://img-blog.csdn.net/20180204171705594?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
不要怕，这个公式最后会简化成这个样子![](https://img-blog.csdn.net/20180204171722344?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)。进而我们可以得出，总误差与学习算法无关！对于任意两个学习算法ξa和ξb，我们都有![](https://img-blog.csdn.net/20180204171754320?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVpemFvU1lVQUNN/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)也就是说，无论学习算法ξa多聪明、学习算法ξb多笨拙，它们的期望性能是相同的！这就是“没有免费的午餐”定理（简称NFL定理）。
我们需要注意到，NFL定理有一个重要前提：所有“问题”出现的机会相同、或所有问题同等重要。但实际情形并不是这样。很多时候，我们只关注自己正在试图解决的问题（例如某个具体应用任务），希望为它找到一个解决方案，至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心。
例如，为了快速从A地到达B地，如果我们正在考虑的A地是南京鼓楼、B地是南京新街口，那么“骑自行车”是很好的解决方案；这个方案对A地是南京鼓楼、B地是北京新街口的情形显然很糟糕，但我们对此并不关心。
NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好“
毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的相对优劣，必须要针对具体的学习问题；在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用。
