# 神经网络中激活函数的比较 - HJ - CSDN博客
2018年10月22日 23:42:18[FZH_SYU](https://me.csdn.net/feizaoSYUACM)阅读数：97
一、神经网络的训练过程
主要是两类，一类是从输入层开始，上一层神经元到当前层神经元的传播方式，称为前向传播，另一类则是从最后一层开始，后一层的神经元得到结果与实际的结果形成一个偏差通过梯度下降的方法改变当前层的权重；
**前向传播**：上一层的神经元与本层的神经元有连接，那么本层的神经元的激活等于上一层神经元的权值进行加权和运算，最后通过一个非线性函数（**激活函数**）如ReLU、Sigmoid等函数，最后得到的结果就是本层神经元的输出；逐层逐神经元通过该操作向前传播，最终得到输出层的结果；
**反向传播**：由最后一层开始，逐层向前传播进行权值的调整，前向传播得到的结果与实际的结果得到一个偏差，然后通过梯度下降法的思想，通过偏导数与残差的乘积通过最后一层逐层向前去改变每一层的权重；
通过不断的前向传播和反向传播不断调整神经网络的权重，最终到达预设的迭代次数或者对样本的学习已经到了比较好的程度后，就停止迭代；
二、神经网络中提到的激活函数
激活函数的作用：一般来说，对于每一次上一层得到的一个类似于$h_l$=$w^T_l$$h_{l-1}$的线性函数，假如一直如此迭代下去，那么最终得到的结果也只是线性的。因此在每一次前向传播的过程中，得到的$h_l$都会放到激活函数中进行优化。常见的激活函数以及比较如下：
**1、Sigmoid函数：  $f(x)$=$1\over1+e^{-y}$**
特点：
（1）饱和使梯度消失，其导数都小于0.25，那么在进行反向传播的时候，梯度相乘的结果会慢慢的趋近于0；除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意，如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习了；
（2）输出不是“零为中心”，一个多层的Sigmoid神经万罗，如果你的输入$x$都是正数，那么在反向传播中$w$的梯度传播到网络的某一处时，权值的变化要么全正要么全负；
（3）指数函数的计算是比较消耗计算资源的；
**2、tanh函数： $tanh(x)$=2Sigmoid($2x$)-1=$1-e^{-2x}\over1+e^{-2x}$**
特点：解决了Sigmoid的输出非“零为中心”的问题，但是其他1和3两个问题并没有很好的解决；
**3、ReLU函数 也叫修正线性单元  $f(x)$=$max(0,x)$**
特点：
（1）解决了梯度消失的问题，至少$x$在正区间内，神经元不会饱和；另外，由于ReLU线性、非饱和的形式，在SGD中能够快速收敛；不仅如此，计算速度也远比指数函数的计算快很多；
（2）ReLU的输出不是“零为中心的”；随着训练的进行，可能会出现神经元死亡，权重无法更新的情况，这种神经元的死亡是不可逆转的死亡。从而导致了训练数据多样化的丢失；
三、神经网络中传播出现的不稳定以及解决办法
问题：
在靠近输入层的隐藏层中或会**梯度消失**，或会**梯度爆炸**。这种不稳定性才是深度神经网络中基于梯度学习的根本问题，其原因是前面层上的梯度是来自后面层上梯度的乘积，当存在过多的层时，就会出现梯度不稳定场景；
描述：
**梯度消失**：通常所用的Sigmoid函数能够将负无穷到正无穷的数映射到0和1之间，并且对这个函数求到的结果是$f'(x)$=$f(x)(1-f(x))$，因此两个0到1之间的数相乘，得到的结果就会变得很小了；神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新；
**梯度爆炸**：由于初始化权值过大，前面层会比后面层变化得更快，就会导致权值越来越大，梯度爆炸的现象就发生了；
**解决办法**：
用ReLU激活函数来替代Sigmoid函数；
ReLU函数可以表示为$f(x)$=$max(0,x)$，它更符合神经元的激活原理，它的一个平滑解析函数为$f(x)$=$m(1+e^x)$，被称为Softplus function；Softplus的微分就是logistic函数$f(x)$=$1\over1+e^{-x}$；ReLU具备引导适度稀疏的能力，从函数图形上看，ReLU比Sigmoid更接近生物学的激活模型；
