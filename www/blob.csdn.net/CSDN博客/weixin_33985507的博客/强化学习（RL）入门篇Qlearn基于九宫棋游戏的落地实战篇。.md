# 强化学习（RL）入门篇Qlearn基于九宫棋游戏的落地实战篇。 - weixin_33985507的博客 - CSDN博客
2018年02月13日 14:55:00[weixin_33985507](https://me.csdn.net/weixin_33985507)阅读数：35
1.真正开始看强化学习也是在不久之前，与一位业界大家交流之后，发现了许多新天地，所以尝试着落地了其中的一些小的demo，试图在真正自己基于具体case编程中学习领悟。
2.理论部分，对强化学习理解尚浅。粗糙的理解一下，监督学习，无监督学习与强化学习之间的关系。（1）监督学习是基于过往的样本（experience），找寻特征之间的关联关系或者说找寻出一个函数或者一组函数，来表示label（通俗一点说来表示这个东西的好坏程度）。那么监督学习就决定了他强依赖与所找寻的experience，如果找的experience就木有最佳情况出现，那么所得到的function也就不能最佳刻画事物的关联关系。无监督学习更像是从一堆事物中找出某些关联关系，按照这种关联关系把数据有序化或者组织化。RL比他们的优点就是可以交互学习，不需要依赖之前的experience。可以随机构造experience，然后在不停的往最大化回报函数的角度进行。
3.qlearn的解释，说说自己的理解。qlearn用于解决一类有有穷状态的最大化回报问题，比如我的demo落地的代码，九宫棋游戏状态有穷状态一共9 ＋ 9*8 ＋ 9*8*7....在刨除一些终止状态（就是某一方赢棋了）状态剩余的大概有549945个状态。这些状态大致有三种类型，赢棋，输棋和未分胜负状态。我们定义他们的回报分别是很大的正数（50），一般的正数（10）和一个很小的正数（1）。所以这里的q矩阵是549945*549945个状态。当然1一个状态最多与8个状态是连通，因为在下棋的时候每下一步棋盘就会少一个棋子位置（棋盘一共有九个位置）借用如下公式进行迭代：Q(state, action) = R(state, action) + Gamma * MaxQ(next state, all actions)。具体不做额外解释，最好的解释就是看代码。现在我将github 代码公布在下面。
part1:https://github.com/wangyue11190/QLearnOffline， 这部分代码是找到九宫棋所有的状态，以及用上述迭代公式去迭代矩阵。代码纯python。
part2:https://github.com/wangyue11190/QLearnGame  这部分代码是九宫棋的游戏代码，电脑方是用的qlearn训练的q矩阵，这个代码可以在mac 上直接编译运行，代码是C++。由于这个游戏比较简单，感觉发挥不出RL的威力，大部分时间都是平局。在此期待自己能够在2018年多学一点RL， 写一个五子棋。
我有一个问题：我在训练完q矩阵之后，如果我是按每一步最大值状态选取的话，往往在某些步骤，地方已经是2连，此时电脑也不会去堵截，而是去形成自己的2连。所以我在c＋＋代码里关于棋子位置选择，加了一个规则，如果选择最大化q矩阵的同时，下一步就可能导致自己失败，那么就去堵对方。这个问题可能是我训练的时候方法不对，请大神指正。
