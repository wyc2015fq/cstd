# 机器学习导论（张志华）：主元分析 - 去向前方的博客 - CSDN博客





2018年10月02日 08:54:25[Big_quant](https://me.csdn.net/lvsehaiyang1993)阅读数：276








# 前言

这个笔记是北大那位老师课程的学习笔记，讲的概念浅显易懂，非常有利于我们掌握基本的概念，从而掌握相关的技术。

# basic concepts

$$exp(-tz^{\frac{1}{2}}) =\int exp(-tuz) dF(u)$$
$$z=||x||^2$$
$$exp(-t||x||),exp(-t||x||).$$

The product of P.D is P.D

eul distance transformed into another space to get the distance.
$$||\phi(x)-\phi(y)||^2_2$$

Part2 unsuperrised learning

CB dimensionlity reduction.
# PCA(Principal Component Analysis)

Population PCA

Def. if $$\overline x \subset R^p \quad is\quad a\quad random \quad vector, \quad with   \quad mean:u \quad and \quad covariance \quad matrix \sigma$$

then the PCA is
$$\overline x-&gt; \overline y=U^t(x-u)$$

when U is orthgonal.
# Spectral Decompistion

Thm,
$$If x-&gt;N(\mu,\sigma)$$ Then,$$y~N(0,n)$$

(2)$$E(y_0)=0,$$

(3)$$Cov(Y_m,Y_i)=0 for i !=j $$

(4)$$y \quad is\quad a \quad orthangonal \quad transform \quad x \quad is \quad uncorrelation \quad but \quad ot \quad squre. $$

(5)$$Var(Y_i)=\sigma_i$$
# Sample Principal Component

$$Let X=[\overline x_1 ...\overline x_n]^T be\quad a \quad n*p $$

# sample data matrix

$$\overline x=\frac{1}{n} \sum_{x=1}^n \overline x_i,$$
$$S=\frac{1}{n}X^THX$$
$$H:I_n=\frac{1}{n}I_nI_n$$

reduce the data to k-dimension ,you get the first k element.

keep most information,PCA.suppos.
# SVD

$$U=eigenvectorof(AA^T)$$
$$D=\sqrt{AA^T}$$
$$V=eigenvector(A^TA)$$
# PCO(Principal Coordinate Analysis)

$$S=X^THX$$

power equal : HH=H
$$B=HXX^TH$$

variance matrix

AB=BA

Non-zero eigenvector are equal.








