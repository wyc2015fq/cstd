# 奇异值分解讨论及其实现的计算步骤 - 去向前方的博客 - CSDN博客





2018年10月01日 20:54:05[Big_quant](https://me.csdn.net/lvsehaiyang1993)阅读数：2203








# 前言

在看一个教程的发现对奇异值分解不太熟悉，因此从新搜索了相关资料，然后，总结成这个咯。

一般来说，想要获得低维的子空间，最简单的是对原始的高维空间进行线性变换（当然了，非线性也是可以的，如加入核函数，比较著名的就是KPCA）。SVD和PCA呢，都实现了降维与重构，但是呢，思路不太一样，老师课上提了一次，以前看的迷迷糊糊的，这次下定决心，怎么都要搞清楚这两个概念。

SVD（singular value decomposition），线性代数里也叫作奇异值分解，可以用来计算矩阵的特征值（奇异值）和特征向量（我知道表达不准确，但我也不想知道正确的说法）。

SVD大多数情况下用来提取一个矩阵的主特征值，可以用于图像的压缩和去噪。举个例子，将一个血细胞示意图像奇异值分解，
```
Im=imread('timg3.jpg');
figure(1);
Im=rgb2gray(Im);
imshow(Im,[]);
set(gca,'position',[0 0 1 1])
[m,n]=size(Im);Im=double(Im);r=rank(Im);
[s,v,d]=svd(Im);
Im2=s(:,:)*v(:,1:100)*d(:,1:100)';
figure(2);
imshow(mat2gray(Im2));
imwrite(mat2gray(Im2),'1.jpg')
set(gca,'position',[0 0 1 1])
```

![在这里插入图片描述](https://img-blog.csdn.net/20181001204715635?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

原图像共有10001个奇异值，这里选用100个奇异值重建
![在这里插入图片描述](https://img-blog.csdn.net/20181001204806894?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

奇异值分解的公式为
$$A=UDV^t$$

矩阵sigma中的最大的特征值所对应的U和V中的列向量和行向量，也就包含了图像中最多的信息。而且这种信息应该是低频的信息，特征值越大，表明原矩阵中的列向量的在该方向上的投影长度越大，也就是相关性越大，通过小的特征值重构出来的成分往往是高频噪声，因为在这些方向上，原矩阵中各个向量的相关性很小。

为什么说这些，其实我就想说，SVD也是一种实用的降维手段。下面说说是具体怎么降维的，降得是什么维？

因为正常我们会把每一个样本排成一个列向量，原始的矩阵C（m*n）的行数指的样本的维数m，列数就是样本的个数n，，现实生活中m<<n，因此rank（C）<=m。一个m维的向量，使用m个基向量就可以将其完备的表示出来，SVD就是寻找这些基向量。我们进行奇异值分解后，注意U的列向量和V的行向量都是单位向量，这不是明摆着的基向量么。将U中的列向量看做基，这些基地原矩阵C的列向量的基底，并且对于C中所有的列向量来说，这些矩阵在基向量u1上的投影长度最长，也就是说相关度最高，因为u1向量对应的sigma值最大，因此可以看做具体请看下图。
![在这里插入图片描述](https://img-blog.csdn.net/20181001205028123?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
看完上面的图，有没有疑问，有就对了，我也是刚想到的一个问题，就拿c1向量来说，把他span成基向量的表达方式时，每个基向量前面的系数不仅有sigma，还有v1(1)等系数，经过这些系数的加权之后，那u1这个主向量还能保证吗？

答：可以保证，给大家一个思路，v*向量是单位向量，剩下来的大家应该知道了吧。

PCA（Principal Component Analysis）主成分分析最常用的一种降维方法，出发点是对于正交属性空间中的样本点，如何使用一个超平面对所有的样本进行恰当的表达。PCA在CSDN上已经被讲了n次了，有就不详细讲了，就理一理思路，大家感兴趣可以搜其他大牛的博客看看。一般来说，有两个出发点，1）样本点到这个超平面的距离足够近（重构性，最大程度的代表样本点），2）样本点在这个超平面上的投影尽可能分开（可分性，样本点在降维空间内可以区分）下面使用第二个指导思想简单的推导一下
![在这里插入图片描述](https://img-blog.csdn.net/20181001205045512?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

利用PCA进行降维的过程中，我们只需要按照特征值的大小顺序，将原来的样本挨个往特征向量投影即可，至于选几个向量，一般来说需要调参。

Andrew ng教程中又提及了白化（whiten）的概念，目的就是为了把特征向量的特征值变成1，这样就消除了主方向的影响，具体可以看 [http://ufldl.stanford.edu/wiki/index.php/Whitening。](http://ufldl.stanford.edu/wiki/index.php/Whitening%E3%80%82)

矩阵的奇异值是一个数学意义上的概念，一般是由奇异值分解（Singular Value Decomposition，简称SVD分解）得到。如果要问奇异值表示什么物理意义，那么就必须考虑在不同的实际工程应用中奇异值所对应的含义。

奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于的权重。奇异值的几何含义为：这组变换后的新的向量序列的长度

奇异值分解，就是把矩阵分成多个“分力”

奇异值的大小，就是各个“分力”的大小
设X是一个n*m的数据矩阵（在此不把它理解成变换），每一列表示一个数据点，每一行表示一维特征。

对X做主成分分析（PCA）的时候，需要求出各维特征的协方差，这个协方差矩阵是。

（其实需要先把数据平移使得数据的均值为0，不过在此忽略这些细节）

PCA做的事情，是对这个协方差矩阵做对角化：

可以这样理解上式右边各项的物理意义：用一个均值为0的多维正态分布来拟合数据，则正交矩阵P的每一列是正态分布的概率密度函数的等高线（椭圆）的各个轴的方向，而对角矩阵的对角线元素是数据在这些方向上的方差，它们的平方根跟椭圆各个轴的长度成正比。

现在来看数据矩阵X的奇异值分解：，其中U、V各列是单位正交的，S是对角阵，对角元非零。

由此式可以得到。

也就是说，SVD中的矩阵U相当于PCA中的矩阵P，不过仅保留了的非零特征值对应的那些特征向量，而（也只保留了非零特征值）。

所以，SVD中的U代表了X中数据形成的正态分布的轴的方向（一组单位正交基），代表了这些轴的长度（分布的标准差）。

那么V呢？可以把US放在一起看成一个由伸缩和旋转组成的坐标变换（不包括平移），数据矩阵X是由数据矩阵经此变换得来的，而的各列（V的各行）则服从标准正态分布。这也就是说，的各维特征（的各行，V的各列）是互不相关的且各自的方差均为1，也就是说V的各列是单位正交的。

现在换一个角度，把X中的各行看作数据，那么就也有了新的理解。

现在，的各行（V的各列）就成了X的各行形成的正态分布的轴向（单位正交基），是这些轴的长度，而U中的各行数据服从标准正态分布，U的各列单位正交。

可以看到，对于这个式子，无论是把X的各行还是各列看成数据，都能解释U、V各列的单位正交性，但它们的单位正交性的含义不同（一个是单位正交基，一个是标准正态分布）。其中S除以数据个数的平方根后是标准正态分布在各个轴上的标准差，从两个角度看得到的标准差是成比例的。
本文来自 芦金宇 的CSDN 博客 ，全文地址请点击：[https://blog.csdn.net/ch1209498273/article/details/78385248?utm_source=copy](https://blog.csdn.net/ch1209498273/article/details/78385248?utm_source=copy)

问题模型：

对下面的矩阵进行SVD运算：
![在这里插入图片描述](https://img-blog.csdn.net/20181001203029825?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
步骤1：

![在这里插入图片描述](https://img-blog.csdn.net/20181001203039787?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

步骤2：
![在这里插入图片描述](https://img-blog.csdn.net/20181001203048986?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
步骤3：构建对角矩阵S
![在这里插入图片描述](https://img-blog.csdn.net/20181001203144718?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![在这里插入图片描述](https://img-blog.csdn.net/20181001203203118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![在这里插入图片描述](https://img-blog.csdn.net/20181001203213142?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2c2VoYWl5YW5nMTk5Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
通过上面的图可以看出来A被分解了之后又被近似的还原过来。

# reference

[svd](https://blog.csdn.net/zhangdadadawei/article/details/50929574)

《机器学习》周志华

《The Matrix Cook book》 version：November 15,2012（强烈推荐的手头工具书，涉及矩阵的求导之类的直接套公式就行












