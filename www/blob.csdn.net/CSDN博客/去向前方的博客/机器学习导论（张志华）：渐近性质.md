# 机器学习导论（张志华）：渐近性质 - 去向前方的博客 - CSDN博客





2018年09月24日 20:54:19[Big_quant](https://me.csdn.net/lvsehaiyang1993)阅读数：99








# 前言

这个笔记是北大那位老师课程的学习笔记，讲的概念浅显易懂，非常有利于我们掌握基本的概念，从而掌握相关的技术。

# basic concepts

a two-class problem can be assumped as a  Bernoulli distribution

$$\overline Z  ~~ M(z|\theta,n_1+n_m)$$

## reproducing kernels

1 Cover’s theorem

A complex pattern-classification problem cast in a high-dimensional space nonlincedy is more likely to be linearly seperable than in a low-dimension space.

## kernel

Def 1.1 Let  $$x \subseteq R^P$$ be a non empty set: A function K:x*x -> R is called a kerne; over x.

Def 1.2A kernel k is called symmetric of $$k(x_i,x_j)=k(x_j,x_i)$$ for any $$x_i and x_j \subseteq X$$

Def1.3  A symmetric kernel is positive definite if $$\sum_{j,k=1}^n\alpha_j\alpha_kK(X_i,X_k) \geq 0 $$

for any $$n \subset N$$ and $${{ \alpha_1,...,\alpha_n}}$$.

we call the symmetric K a conditional.P.D if +
$$\sum_{j,k=1}^n\alpha_j\alpha_kK(X_i,X_k) \geq 0 $$
$$for \quad all \quad n \geq z $$,$${x_1,x_k}\subset X$$
Def 1.4 if k is C.P.D then we call -k negative definite.
$$ k(x_i,x_j) \geq 0 $$
$$\phi(x,y)=||x-y||^2_2 $$




