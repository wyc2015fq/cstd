# 机器学习导论（张志华）：核定义 - 去向前方的博客 - CSDN博客





2018年09月26日 21:32:59[Big_quant](https://me.csdn.net/lvsehaiyang1993)阅读数：78








# 前言

这个笔记是北大那位老师课程的学习笔记，讲的概念浅显易懂，非常有利于我们掌握基本的概念，从而掌握相关的技术。

# Reproducing Kernel （2000-2010）

$$K:x*x-&gt;R$$
$$x \subset R^2$$

# cauchy-schwartz inequality

$$K^2(x_i,x_j) \leq K(x_i,x_j)K(x_j,x_I)$$

# diag matrix

$$K_{n*m} D=Diag(K(x_i,x_j)).K(X_n,X_n)$$

# property

The P.D kernels are closed under sum,direct product,tensor product,pointwise limit,and composition with a power series,$$\sum_{n-&gt;-\infin}^{n-&gt;+\infin}a_nx_n $$ with $$a_n \geq 0$$

for all $$b \subset N$$.



