# 机器学习导论（张志华）：正定核应用 - 去向前方的博客 - CSDN博客





2018年09月30日 20:41:36[Big_quant](https://me.csdn.net/lvsehaiyang1993)阅读数：93








# 前言

这个笔记是北大那位老师课程的学习笔记，讲的概念浅显易懂，非常有利于我们掌握基本的概念，从而掌握相关的技术。

# basic concepts

If a function is positive definite，then matrix  is P.S.D.
$${x_1,,,,x_n} \subset  X =&gt; K_0(x_i,x_j)=g(x_i)g(x_j)$$
$$=&gt; k_0 =[g(x_1),..,g(x_n)]&#x27; *[g(x_1 ),..., g(x_n)]$$
# Thm:

Let F be a probalility measure on the half low Pat such that $$0&lt; \int _0^{\infin} s dF(s)&lt;\infin $$

and l(F,u)=$$\int_0^{\infin} exp(-ts\phi)dF$$ is P.D for all t>0;

example:

polynomial kernel.

RBF Gauss kernel.

two advantages:$$ 1.low dimension-&gt; \infin dimension$$
$$2.normalize.$$
# Levy  distribution

$$(B/2*pi)^1/2 exp(sqrt(2B))| f(s)=sqrt(t/2*pi)u^-{3/2}exp(-t/2u)du$$

if $$\phi (x)=K^{+1/2}$$
$$K^{\frac{1}{2}}* K^{\frac{1}{2}}=K^T$$
# Thm

let $$k X*X -&gt; R$$ be a P.D kernel then exists a HILBERT space H and from x->H such that $$\phi(H)$$
$$\forall x,y \subset x,K(x,y)=&lt;\phi(x),\phi(y)&gt;$$ three kernels.






