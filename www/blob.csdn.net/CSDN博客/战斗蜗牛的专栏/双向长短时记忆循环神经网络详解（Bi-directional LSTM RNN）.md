# 双向长短时记忆循环神经网络详解（Bi-directional LSTM RNN） - 战斗蜗牛的专栏 - CSDN博客





2017年05月12日 11:15:31[vbskj](https://me.csdn.net/vbskj)阅读数：805








## 1. Recurrent Neural Network (RNN)


尽管从多层感知器（MLP）到循环神经网络（RNN）的扩展看起来微不足道，但是这对于序列的学习具有深远的意义。循环神经网络（RNN）的使用是用来处理序列数据的。在传统的神经网络中模型中，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题是无能为力的。比如，预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。循环神经网络（RNN）指的是一个序列当前的输出与之前的输出也有关。具体的表现形式为网络会对前面的信息进行记忆，保存在网络的内部状态中，并应用于当前输出的计算中，即隐含层之间的节点不再无连接而是有链接的，并且隐含层的输入不仅包含输入层的输出还包含上一时刻隐含层的输出。理论上，循环神经网络能够对任何长度的序列数据进行处理，但是在实践中，为了减低复杂性往往假设当前的状态只与前面的几个状态相关。


下图展示的是一个典型的循环神经网络（RNN）结构。

![这里写图片描述](https://img-blog.csdn.net/20160721140234598)


图1 循环神经网络（RNN）结构



将循环神经网络（RNN）可视化的一种有效方法是考虑将其在时间上进行展开，得到如图2结构。

![这里写图片描述](https://img-blog.csdn.net/20160721140844855)


图2 循环神经网络（RNN）在时间上展开



图中展示的是一个在时间上展开的循环神经网络（RNN），其包含输入单元（Input Units）, 输入集标记为{}，输出单元（Output
 Units），输出集标记为{}，以及隐含单元（Hidden
 Units），输出集标记为{}，这些隐含单元完成了最为主要的工作。图中，从输入层连接到隐含层的权值被标记为U，从隐含层到自己的连接权值被标记为W，从隐含层到输出层的权值被标记为V。注意，在每一个时步的时候同样的权值被再使用。同时，为了表示清晰，偏置的权值在这里被忽略。


在循环神经网络中（RNN），有一条单向流动的信息流是从输入单元到达隐含单元的，与此同时，另一条单向流动的信息流从隐含单元到达输出单元。在某些情况下，循环神经网络（RNN）会打破后者的限制，引导信息从输出单元返回隐含单元，这些被称为“Back projections”，并且隐含层的输入还包括上一层隐含层的输出，即隐含层内的节点是可以自连也可以互连的。


对于图2网络的计算过程如下：


1)  表示第t，t=1,
 2, 3, … 步的输入


2)  为隐含层第t步的状态，它是网络的记忆单元。 根据当前输入层的输出和上一步隐含层的输出进行计算  ，其中f一般为非线性的激活函数，如tanh或ReLU（后面会用到LSTM），在计算  时，即第一个的隐含状态，需要用到 ，但其并不存在，在现实中一般被设置为0向量。


3)  是第t步的输出，


需要注意的是：


隐含层状态被认为是网络的记忆单元。包含了前面所有步的隐含层状态。而输出层的只与当前步的有关。在实践中，为了降低网络的复杂度，往往只包含前面若干步而不是所有步的隐含层输出。


在传统神经网络中，每一个网络层的参数是不共享的。而在循环神经网络（RNN）中，每输入一步，每一层各自都共享参数U,V,W，其反映着循环神经网络（RNN）中的每一步都在做相同的事，只是输入不同。因此，这大大降低了网络中需要学习的参数。具体的说是，将循环神经网络（RNN）进行展开，这样变成了多层的网络，如果这是一个多层的传统神经网络，那么到之间的U矩阵与到之间的U是不同的，而循环神经网络（RNN）中的却是一样的，同理对于隐含层与隐含层之间的W、隐含层与输出层之间的V也是一样的。


图中每一步都会有输出，但是每一步都要有输出并不是必须的。比如，我们需要预测一条语句所表达的情绪，我们仅仅需要关系最后一个单词输入后的输出，而不需要知道每个单词输入后的输出。同理，每步都需要输入也不是必须的。循环神经网络（RNN）的关键之处在于隐含层，隐含层能够捕捉序列的信息。


最后，对于整个循环神经网络（RNN）的计算过程如下：

向前推算（Forward pass）：


对于一个长度为T的输入x，网络有I个输入单元，H个隐含单元，K个输出单元。定义为t时刻的第i个输入，定义和分别表示为t时刻网络单元j的输入以及t时刻单元j非线性可微分激活函数的输出。对于完整序列的隐含单元我们可以从时间t
 = 1开始并通过递归的调用如下公式得到：


![这里写图片描述](https://img-blog.csdn.net/20160721154151471)



与此同时，对于网络的输出单元也可以通过如下公式计算出：


![这里写图片描述](https://img-blog.csdn.net/20160721154224863)


向后推算（Forward pass）：


如同标准的反向传播（Backpropagation），通过时间的反向传播（BPTT）包含对链规则的重复应用。具体的说是，对于循环网络，目标函数依赖于隐含层的激活函数（不仅通过其对输出层的影响，以及其对下一个时步隐含层的影响），也就是：


![这里写图片描述](https://img-blog.csdn.net/20160721154253707)



对于全部的序列项能够从时刻t
 = T通过递归的使用上面的公式计算得到。最后，在每一个时步对于隐含层单元的输入和输出的权值是相同的，我们这个序列求和来得到关于每个网络权值的导数。


![这里写图片描述](https://img-blog.csdn.net/20160721154332427)


## 2. Bi-directional Recurrent Neural Network (BRNN)


如果能像访问过去的上下文信息一样，访问未来的上下文，这样对于许多序列标注任务是非常有益的。例如，在最特殊字符分类的时候，如果能像知道这个字母之前的字母一样，知道将要来的字母，这将非常有帮助。同样，对于句子中的音素分类也是如此。


然而，由于标准的循环神经网络（RNN）在时序上处理序列，他们往往忽略了未来的上下文信息。一种很显而易见的解决办法是在输入和目标之间添加延迟，进而可以给网络一些时步来加入未来的上下文信息，也就是加入M时间帧的未来信息来一起预测输出。理论上，M可以非常大来捕获所有未来的可用信息，但事实上发现如果M过大，预测结果将会变差。这是因为网路把精力都集中记忆大量的输入信息，而导致将不同输入向量的预测知识联合的建模能力下降。因此，M的大小需要手动来调节。


双向循环神经网络（BRNN）的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络（RNN），而且这两个都连接着一个输出层。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一个时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层（w1, w3），隐含层到隐含层自己（w2, w5），向前和向后隐含层到输出层（w4, w6）。值得注意的是：向前和向后隐含层之间没有信息流，这保证了展开图是非循环的。

![这里写图片描述](https://img-blog.csdn.net/20160721152411328)


图3 双向循环神经网络（BRNN）在时间上展开



对于整个双向循环神经网络（BRNN）的计算过程如下：

向前推算（Forward pass）：


对于双向循环神经网络（BRNN）的隐含层，向前推算跟单向的循环神经网络（RNN）一样，除了输入序列对于两个隐含层是相反方向的，输出层直到两个隐含层处理完所有的全部输入序列才更新：

![这里写图片描述](https://img-blog.csdn.net/20160721152522777)

向后推算（Backward pass）：


双向循环神经网络（BRNN）的向后推算与标准的循环神经网络（RNN）通过时间反向传播相似，除了所有的输出层项首先被计算，然后返回给两个不同方向的隐含层：

![这里写图片描述](https://img-blog.csdn.net/20160721153358055)

## 3. Long Short-Term Memory (LSTM)


循环神经网路（RNN）在工作时一个重要的优点在于，其能够在输入和输出序列之间的映射过程中利用上下文相关信息。然而不幸的是，标准的循环神经网络（RNN）能够存取的上下文信息范围很有限。这个问题就使得隐含层的输入对于网络输出的影响随着网络环路的不断递归而衰退。因此，为了解决这个问题，长短时记忆（LSTM）结构诞生了。与其说长短时记忆是一种循环神经网络，倒不如说是一个加强版的组件被放在了循环神经网络中。具体地说，就是把循环神经网络中隐含层的小圆圈换成长短时记忆的模块。这个模块的样子如下图所示：

![这里写图片描述](https://img-blog.csdn.net/20160721153646583)


图4 长短时记忆模块



关于这个单元的计算过程如下所示：

向前推算（Forward pass）：


Input Gate:


![这里写图片描述](https://img-blog.csdn.net/20160721155340409)



![这里写图片描述](https://img-blog.csdn.net/20160721155411887)



通过上图可以观察有哪些连接了 Input Gate: t 时刻外面的输入， t-1 时刻隐含单元的输出， 以及来自 t-1 时刻 Cell 的输出。 累计求和之后进行激活函数的计算就是上面两行式子的含义了。


Forget Gate：


![这里写图片描述](https://img-blog.csdn.net/20160721155604538)



![这里写图片描述](https://img-blog.csdn.net/20160721155740244)



这两行公式的计算意义跟上一个相同，Forget Gate的输入来自于t时刻外面的输入，t-1时刻隐含单元的输出，以及来自t-1时刻Cell的输出。


Cells：


![这里写图片描述](https://img-blog.csdn.net/20160721155838105)



![这里写图片描述](https://img-blog.csdn.net/20160721155914886)



这部分有些复杂，Cell的输入是：t时刻Forget Gate的输出 * t-1时刻Cell的输出 + t时刻Input Gate的输出 * 激活函数计算（t时刻外面的输入 + t-1时刻隐含单元的输出）


Output Gate:


![这里写图片描述](https://img-blog.csdn.net/20160721160004936)



![这里写图片描述](https://img-blog.csdn.net/20160721160038904)



这部分就同样好理解了：Output Gate的输入是：t时刻外面的输入，t-1时刻隐含单元的输出以及t时刻Cell单元的输出。


Cell Output:


![这里写图片描述](https://img-blog.csdn.net/20160721160557708)



最后，模块的输出是t时刻Output Gate的输出 * t时刻Cell单元的输出。

向后推算（Forward pass）：


![这里写图片描述](https://img-blog.csdn.net/20160721160828868)



Cell Output:


![这里写图片描述](https://img-blog.csdn.net/20160721160930659)



Output Gate:


![这里写图片描述](https://img-blog.csdn.net/20160721161014636)



Cells：


![这里写图片描述](https://img-blog.csdn.net/20160721161103160)



![这里写图片描述](https://img-blog.csdn.net/20160721161136316)



Forget Gate：


![这里写图片描述](https://img-blog.csdn.net/20160721161415189)



Input Gate:


![这里写图片描述](https://img-blog.csdn.net/20160721161455455)


