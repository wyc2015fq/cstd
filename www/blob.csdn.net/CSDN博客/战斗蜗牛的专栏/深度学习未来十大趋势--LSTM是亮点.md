# 深度学习未来十大趋势--LSTM是亮点 - 战斗蜗牛的专栏 - CSDN博客





2016年07月29日 21:50:07[vbskj](https://me.csdn.net/vbskj)阅读数：1379









本周，我在加拿大蒙特利尔参加了NIPS（Neural Information Processing Systems，神经信息处理系统）2015年论坛。这是一次令人难以置信的经历，就像从信息海洋中汲水一样。特别感谢我的雇主Dropbox派遣我参加这场会议（我们正在招人）。




这里是本周我注意到的一些趋势；注意到这些趋势更偏向于深度学习和强化学习（reinforcement
 learning），因为它们是我在这次论坛中参加的主要部分。






神经网络框架变得越来越复杂而精密





在感知、语言翻译等等方面的大部分最先进的神经网络框架正在发展并且不在仅仅关于简单前馈式（feed forward）框架或者卷积式框架（convolutional）。特别地，它们正在混合并匹配不同的神经网络技术如LSTMs、卷积、自定义目标函数、多皮层柱（multiple cortical columns）等等。



所有最酷的系统都在使用 LSTMs





大部分最先进的系统都将LSTMs纳入到系统中，以使系统具有捕捉重复模式的记忆力。



“注意力模型”在升温





一些系统，但不是全部，开始放到“注意力模型”的背景中，或者说让神经网络在完成任务的过程中试图学习在哪里放置其“注意力”。这些还不是一个正规神经网络流水线中的一部分，但是已经时不时的出现在模型中了。



神经图灵机仍然有趣，但并没有影响到实际工作





神经网络图灵机（Neural Turing Machines）的研究，或者说能够有差异地训练一个神经网络来学习算法，仍然有趣，但是还没有应用到实际工作中。它们还很复杂并且目前只能解决玩具问题（toy problems）。



计算机视觉和自然语言处理，会变得几乎不可分离——在电脑视觉和自然语言处理的领域的深度学习正在互相融合





卷积神经网络第一次出现是在电脑视觉中，但是现在用于一些自然语言处理（NLP）中了，LSTMs和主流对递归神经网络使用的倾向性，第一次做出引人注目的成果是在NLP任务中——如序列到序列的翻译（sequence-to-sequence translation），然而现在通过修剪被纳入到电脑视觉神经网络任务中。





另外，电脑视觉和NLP的交叉部分再加上在如图片捕捉任务中使用到的常见的嵌入（embeddings）技术，还很热门。



符号微分法越来越重要




随着神经网络框架和它们的目标函数可以自定义，同时也变得越来越复杂，人为手动提取它们反向传播中的梯度变得越来越难，也容易出错。最新的工具包如谷歌的TensorFlow有了自动符号微分，所以你可以构建你的框架和目标函数，在训练过程中工具包会在众多的碎片中自动地找出正确的微分来保证误差梯度可以反向传播。



神经网络模型压缩带来了越来越多令人惊喜的结果





多个团队展示了不同的方式来剧烈地压缩一个训练过的模型的权重数量：二值化（binarization）、固定浮点（fixed floating point）、迭代剪枝（iterative pruning）和微调措施（fine tuning steps）等等更多。





这些方法为许多应用带来了可能：有可能将很复杂的模型适配到手机上，例如，与云端无延迟的对话来得到结果，如语音识别。另外，如果我们能够高帧率的快速查询一个模型（因为它的空间和计算运行时间成本很低，如30 FPS），那么在移动装置上使用复杂的、训练好的神经网络模型来完成接近实时的新类型电脑视觉任务就有可能了。





NIPS展示了这些压缩技术，但是我没有看到任何人应用它们。我觉得我们在2016年可能见到相应的应用。



深度学习和强化学习的交叉在继续




虽然今年NIPS没有展示关于强化学习的主要结果，但是深度强化学习研究讨论室只剩下站立的地方，他们展示了深度神经网络和强化学习的计划能力两者结合给人带来的令人兴奋的可能。





在这个领域一些令人兴奋的工作正在发生，如端对端机器人，使用深度学习和强化学习来完成原始传感器数据到实际动作执行器的直接过度。我们正从过去的只是分类一步步发展到试图理解如何在方程中加入计划和行动。还有更多的工作要做，但是早期工作很令人兴奋。



如果你没使用批量归一化，那么现在应该开始了




批量归一化（batch normalization）正被考虑成为神经网络工具包的一个标准部分，并在论坛的整体工作过程中作为参考（reference）。



神经网络和产品应用，应该携手同行





你需要让研究人员创造新的神经网络方法，而且也有途径将这些方法快速扩展到实际应用产品中。谷歌的TensorFlow是数据库中很少做到这一点的平台之一：研究人员可以快速创造新的网络拓扑如图像，然后这些能够扩展在不同的配置中——如使用像Python或C++主流程序语言的单个设备、多个设备或者是移动设备中。





然而，注意到TensorFlow还在早期阶段；Caffe现在倒是能使用。TensorFlow的单装置表现不如其他的构架；谷歌也宣称不久他们会公布一个使用Kubernetes和gRPC的分布式版本但是分布式训练尚未发挥作用；并且使用TensorFlow目前还不能在亚马逊的AWS上运行。尽管如此，TensorFlow的前景可期。



