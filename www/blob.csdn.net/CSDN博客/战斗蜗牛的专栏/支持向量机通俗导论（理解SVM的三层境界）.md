# 支持向量机通俗导论（理解SVM的三层境界） - 战斗蜗牛的专栏 - CSDN博客





2015年04月19日 15:15:28[vbskj](https://me.csdn.net/vbskj)阅读数：799
个人分类：[paper](https://blog.csdn.net/vbskj/article/category/942420)









> 
> 
> 
###   支持向量机通俗导论（理解SVM的三层境界）





作者：July ；致谢：pluskid、白石、JerryLead。
出处：结构之法算法之道blog。



### 前言


    动笔写这个支持向量机(support vector machine)是费了不少劲和困难的，原因很简单，一者这个东西本身就并不好懂，要深入学习和研究下去需花费不少时间和精力，二者这个东西也不好讲清楚，尽管网上已经有朋友写得不错了(见文末参考链接)，但在描述数学公式的时候还是显得不够。得益于同学白石的数学证明，我还是想尝试写一下，希望本文在兼顾通俗易懂的基础上，真真正正能足以成为一篇完整概括和介绍支持向量机的导论性的文章。


    本文在写的过程中，参考了不少资料，包括《支持向量机导论》、《统计学习方法》及网友pluskid的支持向量机系列等等，于此，还是一篇学习笔记，只是加入了自己的理解和总结，有任何不妥之处，还望海涵。全文宏观上整体认识支持向量机的概念和用处，微观上深究部分定理的来龙去脉，证明及原理细节，力保逻辑清晰
 & 通俗易懂。




    同时，阅读本文时建议大家尽量使用chrome等浏览器，如此公式才能更好的显示，再者，阅读时可拿张纸和笔出来，把本文所有定理.公式都亲自推导一遍或者直接打印下来（可直接打印网页版或本文文末附的PDF，享受随时随地思考、演算的极致快感），在文稿上演算。


    Ok，还是那句原话，有任何问题，欢迎任何人随时不吝指正 & 赐教，感谢。






## 第一层、了解SVM


    支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。

### *1.1*、分类标准的起源：Logistic回归




    理解SVM，咱们必须先弄清楚一个概念：线性分类器。


    给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（ wT中的T代表转置）：

![](https://img-blog.csdn.net/20131107201104906)

可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。


    Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。


    假设函数

> 
> 
> 
> ![](https://img-my.csdn.net/uploads/201304/05/1365174192_8325.png)




    其中x是n维特征向量，函数g就是logistic函数。


    而![](https://img-my.csdn.net/uploads/201304/05/1365174223_1807.png)的图像是



> 
> 
> 
> 


![](https://img-my.csdn.net/uploads/201304/05/1365174236_6175.png)


> 
> 

> 







    可以看到，将无穷映射到了(0,1)。


    而假设函数就是特征属于y=1的概率。



> 
> 
> 


![](https://img-my.csdn.net/uploads/201304/05/1365174921_9452.png)




    从而，当我们要判别一个新来的特征属于哪个类时，只需求![](https://img-my.csdn.net/uploads/201304/05/1365175136_8232.png)即可，若![](https://img-my.csdn.net/uploads/201304/05/1365175136_8232.png)大于0.5就是y=1的类，反之属于y=0类。

    此外，![](https://img-my.csdn.net/uploads/201304/05/1365175136_8232.png)只和![](https://img-my.csdn.net/uploads/201304/05/1365175161_1760.png)有关，![](https://img-my.csdn.net/uploads/201304/05/1365175169_2349.png)>0，那么![](https://img-my.csdn.net/uploads/201304/05/1365175178_8905.png)，而g(z)只是用来映射，真实的类别决定权还是在于![](https://img-my.csdn.net/uploads/201304/05/1365175189_9269.png)。再者，当![](https://img-my.csdn.net/uploads/201304/05/1365175205_2324.png)时，![](https://img-my.csdn.net/uploads/201304/05/1365175215_8446.png)=1，反之![](https://img-my.csdn.net/uploads/201304/05/1365175233_9165.png)=0。如果我们只从![](https://img-my.csdn.net/uploads/201304/05/1365175266_3733.png)出发，希望模型达到的目标就是让训练数据中y=1的特征![](https://img-my.csdn.net/uploads/201304/05/1365175288_2654.png)，而是y=0的特征![](https://img-my.csdn.net/uploads/201304/05/1365175299_9597.png)。Logistic回归就是要学习得到![](https://img-my.csdn.net/uploads/201304/05/1365175329_6408.png)，使得正例的特征远大于0，负例的特征远小于0，而且要在全部训练实例上达到这个目标。



    接下来，尝试把logistic回归做个变形。首先，将使用的结果标签y = 0和y = 1替换为y = -1,y = 1，然后将![](https://img-my.csdn.net/uploads/201304/05/1365175711_9116.png)（![](https://img-my.csdn.net/uploads/201304/05/1365175723_3132.png)）中的![](https://img-blog.csdn.net/20140826150648949)替换为b，最后将后面的![](https://img-my.csdn.net/uploads/201304/05/1365175737_9557.png)替换为![](https://img-my.csdn.net/uploads/201304/05/1365175737_9557.png)（即![](https://img-my.csdn.net/uploads/201304/05/1365175756_2693.png)）。如此，则有了![](https://img-my.csdn.net/uploads/201304/05/1365175767_8636.png)。也就是说除了y由y=0变为y=-1外，线性分类函数跟logistic回归的形式化表示![](https://img-my.csdn.net/uploads/201304/05/1365175792_4997.png)没区别。

    进一步，可以将假设函数![](https://img-my.csdn.net/uploads/201304/05/1365175830_5193.png)中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：

> 
> 
> ![](https://img-my.csdn.net/uploads/201304/05/1365175998_9759.png)


### *1.2*、线性分类的一个例子


    下面举个简单的例子，如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是 -1 ，另一边所对应的y全是1。

> 



> 
> 
> 
![](https://img-blog.csdn.net/20140829134124453)




    这个超平面可以用分类函数![](https://img-blog.csdn.net/20131107201211968)表示，当f(x) 等于0的时候，x便是位于超平面上的点，而f(x)大于0的点对应 y=1 的数据点，f(x)小于0的点对应y=-1的点，如下图所示：

> 
> 
> 
![](https://img-blog.csdn.net/20140829134548371)


注：有的资料上定义特征到结果的输出函数![](https://img-blog.csdn.net/20131120103601656)，与这里定义的![](https://img-blog.csdn.net/20131107201211968)实质是一样的。为什么？因为无论是![](https://img-blog.csdn.net/20131120103601656)，还是![](https://img-blog.csdn.net/20131107201211968)，不影响最终优化结果。下文你将看到，当我们转化到优化![](https://img-my.csdn.net/uploads/201210/25/1351141837_7366.jpg)的时候，为了求解方便，会把yf(x)令为1，即yf(x)是y(w^x
 + b)，还是y(w^x - b)，对我们要优化的式子max1/||w||已无影响。

    （有一朋友飞狗来自Mare_Desiderii，看了上面的定义之后，问道：请教一下SVM functional margin 为![](https://img-blog.csdn.net/20131111154113734)=y(wTx+b)=yf(x)中的Y是只取1和-1
 吗？y的唯一作用就是确保functional margin的非负性？真是这样的么？当然不是，详情请见本文评论下第43楼）
    当然，有些时候，或者说大部分时候数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在(不过关于如何处理这样的问题我们后面会讲)，这里先从最简单的情形开始推导，就假设数据都是线性可分的，亦即这样的超平面是存在的。


    换言之，在进行分类的时候，遇到一个新的数据点x，将x代入f(x) 中，如果f(x)小于0则将x的类别赋为-1，如果f(x)大于0则将x的类别赋为1。


    接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。

### *1.3*、函数间隔Functional margin与几何间隔Geometrical margin 

在超平面w*x+b=0确定的情况下，|w*x+b|能够表示点x到距离超平面的远近，而通过观察w*x+b的符号与类标记y的符号是否一致可判断分类是否正确，所以，可以用(y*(w*x+b))的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。


    定义函数间隔（用![](https://img-blog.csdn.net/20140829135049264)表示）为：



![](https://img-blog.csdn.net/20131107201248921)




    而超平面(w，b)关于T中所有样本点(xi，yi)的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面(w, b)关于训练数据集T的函数间隔：

> 
> 
















