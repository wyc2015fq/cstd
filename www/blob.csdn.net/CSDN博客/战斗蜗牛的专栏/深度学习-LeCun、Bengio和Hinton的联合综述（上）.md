# 深度学习-LeCun、Bengio和Hinton的联合综述（上） - 战斗蜗牛的专栏 - CSDN博客





2016年07月29日 21:55:52[vbskj](https://me.csdn.net/vbskj)阅读数：373








**【编者按】**三大牛Yann LeCun、Yoshua Bengio和Geoffrey Hinton在深度学习领域的地位无人不知。为纪念人工智能提出60周年，最新的《Nature》杂志专门开辟了一个[“人工智能
 + 机器人”专题](http://www.nature.com/nature/journal/v521/n7553/index.html#insight) ，发表多篇相关论文，其中包括了Yann LeCun、Yoshua Bengio和Geoffrey Hinton首次合作的这篇综述文章“[Deep Learning](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)”。本文为该综述文章中文译文的上半部分，深入浅出地介绍了深度学习的基本原理和核心优势。

原文摘要：深度学习可以让那些拥有多个处理层的计算模型来学习具有多层次抽象的数据的表示。这些方法在许多方面都带来了显著的改善，包括最先进的语音识别、视觉对象识别、对象检测和许多其它领域，例如药物发现和基因组学等。深度学习能够发现大数据中的复杂结构。它是利用BP算法来完成这个发现过程的。BP算法能够指导机器如何从前一层获取误差而改变本层的内部参数，这些内部参数可以用于计算表示。深度卷积网络在处理图像、视频、语音和音频方面带来了突破，而递归网络在处理序列数据，比如文本和语音方面表现出了闪亮的一面。


机器学习技术在现代社会的各个方面表现出了强大的功能：从Web搜索到社会网络内容过滤，再到电子商务网站上的商品推荐都有涉足。并且它越来越多地出现在消费品中，比如相机和智能手机。


机器学习系统被用来识别图片中的目标，将语音转换成文本，匹配新闻元素，根据用户兴趣提供职位或产品，选择相关的搜索结果。逐渐地，这些应用使用一种叫深度学习的技术。传统的机器学习技术在处理未加工过的数据时，体现出来的能力是有限的。几十年来，想要构建一个模式识别系统或者机器学习系统，需要一个精致的引擎和相当专业的知识来设计一个特征提取器，把原始数据（如图像的像素值）转换成一个适当的内部特征表示或特征向量，子学习系统，通常是一个分类器，对输入的样本进行检测或分类。特征表示学习是一套给机器灌入原始数据，然后能自动发现需要进行检测和分类的表达的方法。**深度学习就是一种特征学习方法，把原始数据通过一些简单的但是非线性的模型转变成为更高层次的，更加抽象的表达。通过足够多的转换的组合，非常复杂的函数也可以被学习。**对于分类任务，高层次的表达能够强化输入数据的区分能力方面，同时削弱不相关因素。比如，一副图像的原始格式是一个像素数组，那么在第一层上的学习特征表达通常指的是在图像的特定位置和方向上有没有边的存在。第二层通常会根据那些边的某些排放而来检测图案，这时候会忽略掉一些边上的一些小的干扰。第三层或许会把那些图案进行组合，从而使其对应于熟悉目标的某部分。随后的一些层会将这些部分再组合，从而构成待检测目标。**深度学习的核心方面是，上述各层的特征都不是利用人工工程来设计的，而是使用一种通用的学习过程从数据中学到的。**


深度学习正在取得重大进展，解决了人工智能界的尽最大努力很多年仍没有进展的问题。它已经被证明，它能够擅长发现高维数据中的复杂结构，因此它能够被应用于科学、商业和政府等领域。除了在图像识别、语音识别等领域打破了纪录，它还在另外的领域击败了其他机器学习技术，包括预测潜在的药物分子的活性、分析粒子加速器数据、重建大脑回路、预测在非编码DNA突变对基因表达和疾病的影响。也许更令人惊讶的是，深度学习在自然语言理解的各项任务中产生了非常可喜的成果，特别是主题分类、情感分析、自动问答和语言翻译。我们认为，**在不久的将来，深度学习将会取得更多的成功，因为它需要很少的手工工程，它可以很容易受益于可用计算能力和数据量的增加。目前正在为深度神经网络开发的新的学习算法和架构只会加速这一进程。**

# 监督学习


机器学习中，不论是否是深层，最常见的形式是监督学习。试想一下，我们要建立一个系统，它能够对一个包含了一座房子、一辆汽车、一个人或一个宠物的图像进行分类。我们先收集大量的房子，汽车，人与宠物的图像的数据集，并对每个对象标上它的类别。在训练期间，机器会获取一副图片，然后产生一个输出，这个输出以向量形式的分数来表示，每个类别都有一个这样的向量。我们希望所需的类别在所有的类别中具有最高的得分，但是这在训练之前是不太可能发生的。通过计算一个目标函数可以获得输出分数和期望模式分数之间的误差（或距离）。然后机器会修改其内部可调参数，以减少这种误差。这些可调节的参数，通常被称为权值，它们是一些实数，可以被看作是一些“旋钮”，定义了机器的输入输出功能。在典型的深学习系统中，有可能有数以百万计的样本和权值，和带有标签的样本，用来训练机器。为了正确地调整权值向量，该学习算法计算每个权值的梯度向量，表示了如果权值增加了一个很小的量，那么误差会增加或减少的量。权值向量然后在梯度矢量的相反方向上进行调整。我们的目标函数，所有训练样本的平均，可以被看作是一种在权值的高维空间上的多变地形。负的梯度矢量表示在该地形中下降方向最快，使其更接近于最小值，也就是平均输出误差低最低的地方。


在实际应用中，大部分从业者都使用一种称作随机梯度下降的算法（SGD）。它包含了提供一些输入向量样本，计算输出和误差，计算这些样本的平均梯度，然后相应的调整权值。通过提供小的样本集合来重复这个过程用以训练网络，直到目标函数停止增长。它被称为随机的是因为小的样本集对于全体样本的平均梯度来说会有噪声估计。这个简单过程通常会找到一组不错的权值，同其他精心设计的优化技术相比，它的速度让人惊奇。训练结束之后，系统会通过不同的数据样本——测试集来显示系统的性能。这用于测试机器的泛化能力——对于未训练过的新样本的识别能力。


当前应用中的许多机器学习技术使用的是线性分类器来对人工提取的特征进行分类。一个2类线性分类器会计算特征向量的加权和。当加权和超过一个阈值之后，输入样本就会被分配到一个特定的类别中。从20世纪60年代开始，我们就知道了线性分类器只能够把样本分成非常简单的区域，也就是说通过一个超平面把空间分成两部分。


但像图像和语音识别等问题，它们需要的输入-输出函数要对输入样本中不相关因素的变化不要过于的敏感，如位置的变化，目标的方向或光照，或者语音中音调或语调的变化等，但是需要对于一些特定的微小变化非常敏感（例如，一只白色的狼和跟狼类似的白色狗——萨莫耶德犬之间的差异）。在像素这一级别上，两条萨莫耶德犬在不同的姿势和在不同的环境下的图像可以说差异是非常大的，然而，一只萨摩耶德犬和一只狼在相同的位置并在相似背景下的两个图像可能就非常类似。

![](http://img.ptcms.csdn.net/article/201506/01/556b858e8685e_middle.jpg?_=4335)



图1 多层神经网络和BP算法


- **多层神经网络（用连接点表示）可以对输入空间进行整合，使得数据（红色和蓝色线表示的样本）线性可分**。注意输入空间中的规则网格（左侧）是如何被隐藏层转换的（转换后的在右侧）。这个例子中只用了两个输入节点，两个隐藏节点和一个输出节点，但是用于目标识别或自然语言处理的网络通常包含数十个或者数百个这样的节点。获得C.Olah
 ([http://colah.github.io/](http://colah.github.io/))的许可后重新构建的这个图。
- **链式法则告诉我们两个小的变化（x和y的微小变化，以及y和z的微小变化）是怎样组织到一起的。**x的微小变化量Δx首先会通过乘以∂y/∂x（偏导数）转变成y的变化量Δy。类似的，Δy会给z带来改变Δz。通过链式法则可以将一个方程转化到另外的一个——也就是Δx通过乘以∂y/∂x和∂z/∂y（英文原文为∂z/∂x，系笔误——编辑注）得到Δz的过程。当x，y，z是向量的时候，可以同样处理（使用雅克比矩阵）。
- **具有两个隐层一个输出层的神经网络中计算前向传播的公式。**每个都有一个模块构成，用于反向传播梯度。在每一层上，我们首先计算每个节点的总输入z，z是前一层输出的加权和。然后利用一个非线性函数f(.)来计算节点的输出。简单期间，我们忽略掉了阈值项。神经网络中常用的非线性函数包括了最近几年常用的校正线性单元（ReLU）f(z)
 = max(0,z)，和更多传统sigmoid函数，比如双曲线正切函数f(z) = (exp(z) − exp(−z))/(exp(z) + exp(−z)) 和logistic函数f(z) = 1/(1 + exp(−z))。
- **计算反向传播的公式。**在隐层，我们计算每个输出单元产生的误差，这是由上一层产生的误差的加权和。然后我们将输出层的误差通过乘以梯度f(z)转换到输入层。在输出层上，每个节点的误差会用成本函数的微分来计算。如果节点l的成本函数是0.5*(yl-tl)^2, 那么节点的误差就是yl-tl，其中tl是期望值。一旦知道了∂E/∂zk的值，节点j的内星权向量wjk就可以通过yj
 ∂E/∂zk来进行调整。




一个线性分类器或者其他操作在原始像素上的浅层分类器不能够区分后两者，虽然能够将前者归为同一类。这就是为什么浅分类要求有良好的特征提取器用于解决选择性不变性困境——提取器会挑选出图像中能够区分目标的那些重要因素，但是这些因素对于分辨动物的位置就无能为力了。为了加强分类能力，可以使用泛化的非线性特性，如核方法，但这些泛化特征，比如通过高斯核得到的，并不能够使得学习器从学习样本中产生较好的泛化效果。**传统的方法是手工设计良好的特征提取器，这需要大量的工程技术和专业领域知识。但是如果通过使用通用学习过程而得到良好的特征，那么这些都是可以避免的了。这就是深度学习的关键优势。**


深度学习的体系结构是简单模块的多层栈，所有（或大部分）模块的目标是学习，还有许多计算非线性输入输出的映射。栈中的每个模块将其输入进行转换，以增加表达的可选择性和不变性。比如说，具有一个5到20层的非线性多层系统能够实现非常复杂的功能，比如输入数据对细节非常敏感——能够区分白狼和萨莫耶德犬，同时又具有强大的抗干扰能力，比如可以忽略掉不同的背景、姿势、光照和周围的物体等。



# 反向传播来训练多层神经网络




在最早期的模式识别任务中，研究者的目标一直是使用可以训练的多层网络来替代经过人工选择的特征，虽然使用多层神经网络很简单，但是得出来的解很糟糕。直到20世纪80年代，使用简单的随机梯度下降来训练多层神经网络，这种糟糕的情况才有所改变。只要网络的输入和内部权值之间的函数相对平滑，使用梯度下降就凑效，梯度下降方法是在70年代到80年代期间由不同的研究团队独立发明的。 


用来求解目标函数关于多层神经网络权值梯度的反向传播算法（BP）只是一个用来求导的链式法则的具体应用而已。**反向传播算法的核心思想是：目标函数对于某层输入的导数（或者梯度）可以通过向后传播对该层输出（或者下一层输入）的导数求得（如图1）。**反向传播算法可以被重复的用于传播梯度通过多层神经网络的每一层：从该多层神经网络的最顶层的输出（也就是改网络产生预测的那一层）一直到该多层神经网络的最底层（也就是被接受外部输入的那一层），一旦这些关于（目标函数对）每层输入的导数求解完，我们就可以求解每一层上面的（目标函数对）权值的梯度了。


很多深度学习的应用都是使用前馈式神经网络（如图1），该神经网络学习一个从固定大小输入（比如输入是一张图）到固定大小输出（例如，到不同类别的概率）的映射。从第一层到下一层，计算前一层神经元输入数据的权值的和，然后把这个和传给一个非线性激活函数。当前最流行的非线性激活函数是rectified linear unit(ReLU)，函数形式：f(z)=max(z,0)。过去的几十年中，神经网络使用一些更加平滑的非线性函数，比如tanh(z)和1/(1+exp(-z))，但是ReLU通常会让一个多层神经网络学习的更快，也可以让一个深度网络直接有监督的训练（不需要无监督的pre-train）。


达到之前那种有pre-train的效果。通常情况下，输入层和输出层以外的神经单元被称为隐藏单元。隐藏层的作用可以看成是使用一个非线性的方式打乱输入数据，来让输入数据对应的类别在最后一层变得线性可分。


在20世纪90年代晚期，神经网络和反向传播算法被大多数机器学习团队抛弃，同时也不受计算机视觉和语音识别团队的重视。人们普遍认为，**学习有用的、多级层次结构的、使用较少先验知识进行特征提取的这些方法都不靠谱。确切的说是因为简单的梯度下降会让整个优化陷入到不好的局部最小解**。


实践中，如果在大的网络中，不管使用什么样的初始化条件，局部最小解并不算什么大问题，系统总是得到效果差不多的解。最近的理论和实验表明，局部最小解还真不是啥大问题。相反，解空间中充满了大量的鞍点（梯度为0的点），同时鞍点周围大部分曲面都是往上的。所以这些算法就算是陷入了这些局部最小值，关系也不太大。


2006年前后，CIFAR（加拿大高级研究院）把一些研究者聚集在一起，人们对深度前馈式神经网络重新燃起了兴趣。研究者们提出了一种**非监督的学习方法**，这种方法可以创建一些网络层来检测特征而不使用带标签的数据，这些网络层可以用来重构或者对特征检测器的活动进行建模。通过预训练过程，深度网络的权值可以被初始化为有意思的值。然后一个输出层被添加到该网络的顶部，并且使用标准的反向传播算法进行微调。这个工作对手写体数字的识别以及行人预测任务产生了显著的效果，尤其是带标签的数据非常少的时候。


使用这种与训练方法做出来的第一个比较大的应用是关于语音识别的，并且是在GPU上做的，这样做是因为写代码很方便，并且在训练的时候可以得到10倍或者20倍的加速。2009年，这种方法被用来映射短时间的系数窗口，该系统窗口是提取自声波并被转换成一组概率数字。它在一组使用很少词汇的标准的语音识别基准测试程序上达到了惊人的效果，然后又迅速被发展到另外一个更大的数据集上，同时也取得惊人的效果。从2009年到到2012年底，较大的语音团队开发了这种深度网络的多个版本并且已经被用到了安卓手机上。对于小的数据集来说，无监督的预训练可以防止过拟合，同时可以带来更好的泛化性能当有标签的样本很小的时候。一旦深度学习技术重新恢复，这种预训练只有在数据集合较少的时候才需要。


然后，还有一种深度前馈式神经网络，这种网络更易于训练并且比那种全连接的神经网络的泛化性能更好。这就是卷积神经网络（CNN）。当人们对神经网络不感兴趣的时候，卷积神经网络在实践中却取得了很多成功，如今它被计算机视觉团队广泛使用。



