# 深度学习的训练加速 - 战斗蜗牛的专栏 - CSDN博客





2018年02月07日 15:38:50[vbskj](https://me.csdn.net/vbskj)阅读数：1837









http://blog.csdn.net/hust_tsb/article/details/52916365






深层模型训练需要各种技巧，例如网络结构的选取，神经元个数的设定，权重参数的初始化，学习率的调整，Mini-batch的控制等等。即便对这些技巧十分精通，实践中也要多次训练，反复摸索尝试。此外，深层模型参数多，计算量大，训练数据的规模也更大，需要消耗很多计算资源。如果可以让训练加速，就可以在同样的时间内多尝试几个新主意，多调试几组参数，工作效率会明显提升，对于大规模的训练数据和模型来说，更可以将难以完成的任务变成可能。这一节就谈谈深层模型的训练加速方法。



### GPU加速




矢量化编程是提高算法速度的一种有效方法。为了提升特定数值运算操作（如矩阵相乘、矩阵相加、矩阵-向量乘法等）的速度，数值计算和并行计算的研究人员已经努力了几十年。矢量化编程强调单一指令并行操作多条相似数据，形成单指令流多数据流（SIMD）的编程泛型。深层模型的算法，如BP，Auto-Encoder，CNN等，都可以写成矢量化的形式。然而，在单个CPU上执行时，矢量运算会被展开成循环的形式，本质上还是串行执行。




GPU（Graphic Process Units，图形处理器）的众核体系结构包含几千个流处理器，可将矢量运算并行化执行，大幅缩短计算时间。随着NVIDIA、AMD等公司不断推进其GPU的大规模并行架构支持，面向通用计算的GPU（General-Purposed GPU, GPGPU）已成为加速可并行应用程序的重要手段。得益于GPU众核（many-core）体系结构，程序在GPU系统上的运行速度相较于单核CPU往往提升几十倍乃至上千倍。目前GPU已经发展到了较为成熟的阶段，受益最大的是科学计算领域，典型的成功案例包括多体问题（N-Body
 Problem）、蛋白质分子建模、医学成像分析、金融计算、密码计算等。




利用GPU来训练深度神经网络，可以充分发挥其数以千计计算核心的高效并行计算能力，在使用海量训练数据的场景下，所耗费的时间大幅缩短，占用的服务器也更少。如果对针对适当的深度神经网络进行合理优化，一块GPU卡可相当于数十甚至上百台CPU服务器的计算能力，因此GPU已经成为业界在深度学习模型训练方面的首选解决方案。



### 数据并行




数据并行是指对训练数据做切分，同时采用多个模型实例，对多个分片的数据并行训练。








图13 数据并行的基本架构 [17]




要完成数据并行需要做参数交换，通常由一个参数服务器（Parameter Server）来帮助完成。在训练的过程中，多个训练过程相互独立，训练的结果，即模型的变化量ΔW需要汇报给参数服务器，由参数服务器负责更新为最新的模型W’ = W – η ∙ ΔW，然后再将最新的模型W’分发给训练程序，以便从新的起点开始训练。




数据并行有同步模式和异步模式之分。同步模式中，所有训练程序同时训练一个批次的训练数据，完成后经过同步，再同时交换参数。参数交换完成后所有的训练程序就有了共同的新模型作为起点，再训练下一个批次。而异步模式中，训练程序完成一个批次的训练数据，立即和参数服务器交换参数，不考虑其他训练程序的状态。异步模式中一个训练程序的最新结果不会立刻体现在其他训练程序中，直到他们进行下次参数交换。




参数服务器只是一个逻辑上的概念，不一定部署为独立的一台服务器。有时候它会附属在某一个训练程序上，有时也会将参数服务器按照模型划分为不同的分片，分别部署。



### 模型并行




模型并行将模型拆分成几个分片，由几个训练单元分别持有，共同协作完成训练。当一个神经元的输入来自另一个训练单元上的神经元的输出时，产生通信开销。








