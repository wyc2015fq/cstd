# 快速入门机器学习 图解机器学习10张经典图片 - 战斗蜗牛的专栏 - CSDN博客





2016年08月03日 21:19:42[vbskj](https://me.csdn.net/vbskj)阅读数：3343









入门机器学习, 总有几张图片, 令人印象深刻. 以下是十张经典图片, 图解机器学习, 非常有 启发性:


1.  训练错误和测试错误。这张图告诉我们训练错误越小，不一定是最好的。训练误差和测试误差要达到一个平衡，才是最好的。下图展示了[ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) 图
 2.11, 训练错误和测试错误与模型复杂度的关系.
![Test and training error](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-11.35.49-AM-1-300x208.png)Test
 and training error


2. “[欠拟合”和”过拟合”](http://blog.csdn.net/yhdzw/article/details/22733317).
 出自[PRML](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) 图1.4.
 下图数据点是从绿色曲线生成的. 拟合参数是M,  通过M得到的模型是红色曲线. 可见, 如果M过小, 得到的模型不够复杂, 不能还原真实模型, 也就是”欠拟合”. 如果M太大, 得到的曲线复杂度过高, 也不能真实还原模型, 也就是”过拟合”. 猜到了吧? 还是要在”欠拟合”和”过拟合”之间找到一个平衡呀~
![Under and overfitting](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-11.35.49-AM-2-300x220.png)Under
 and overfitting


3. 奥卡姆剃刀(Occam’s razor). 出自[ITILA](http://www.inference.phy.cam.ac.uk/itila/) 图
 28.3. 为什么贝叶斯推断包含着[奥卡姆剃刀的原理](http://baike.baidu.com/view/646319.htm) ?
 下图展示了为什么复杂的模型会变得低效。横轴代表了贝叶斯理论的汇报模型在可能数据集上被准确预测的可能性。 代表了使用复杂模型情况下，数据集被准确预测的概率和置信度(Evidence)； 代表了使用较简单模型情况下，数据集被准确预测的概率和置信度(Evidence)。可见复杂模型在预测时，一些数据置信度，或者信心很高，但是其实整体准确度，
 不如模型 。说什么来着？
 模型复杂度也要平衡哦~

![Occam's razor:](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-11.29.59-AM-300x144.png)Occam’s
 razor:


4. 特征结合。(1) 为什么投影后的特征看起来相关, 而离散个体看起来无关 ? (2) 为什么线性模型会失效? 来自Isabelle Guyon的[特征抽取教程](http://clopinet.com/isabelle/Projects/ETH/).
![Feature combinations](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-12.49.50-PM-300x296.png)Feature
 combinations


5. 无关特征. 下图中, 使用y轴作为特征区分样本, 但是, 看起来是不是右图更容易混淆? 混淆就是因为多了x轴的干扰.
![Irrelevant features](http://nooverfit.com/wp/wp-content/uploads/2016/05/irrelevant-features-1-300x97.png)Irrelevant
 features


6. 升维. 一个非线性的问题, 在升维之后, 可以变为一个线性问题. 如下图, 想象样本是从一维的曲线生成的, 一定是个非线性问题. 但是如果把这个曲线看做是二维的, 马上就可以用线性的基础函数划分开了.
 这就是SVM([支持向量机](http://baike.baidu.com/view/541845.htm))的理论. 来自Andrew
 Moore的[SVM
 教程](http://www.autonlab.org/tutorials/svm.html) .
![Basis functions](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-12.21.41-PM-276x300.png)Basis
 functions


7. 判别模型和生成模型. 来自[PRML](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) 图
 1.27. 左图是只使用先验的判别模型, 而右图是使用了后验的生成模型, 绿色垂直线代表最有信心的分界线.
![Discriminative vs. Generative](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-12.51.35-PM-300x142.png)Discriminative
 vs. Generative


8. 损失函数. 非常多的机器学习算法可以看做优化损失函数的过程. 来自 [PRML](http://research.microsoft.com/en-us/um/people/cmbishop/prml/)图
 7.5. 蓝线: SVM中的hinge error function. 绿线: 均方错误. 黑线:错分率. 红色: log函数回归错误.
![Loss functions](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-12.47.03-PM-300x239.png)Loss
 functions


9. 最小方差的几何图解. 来自 [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) 图
 3.2. y轴在平面上的投影表示最小方差估计.
![Geometry of least squares](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-5.21.38-PM-300x213.png)Geometry
 of least squares


10. 稀疏性. 为什么 Lasso (L1 范数正则) 容易给出稀疏解 ? (即 权重向量有更多的零值). 来自[ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) 图
 3.11. 图中红色的等高线是平方误差项等值线, 可以理解为在等高线上的解是误差相等的. 左图蓝色方形线是L1范数等值线, 右图蓝色圆形线是L2范数等值线. 可见L1范数更可能得到轴上权重值为0的解. 即, 得到的解更容易稀疏.
![Sparsity](http://nooverfit.com/wp/wp-content/uploads/2016/05/Screen-Shot-2014-02-14-at-11.40.12-AM-300x172.png)Sparsity





