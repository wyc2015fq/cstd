# IO多路复用之epoll函数详解 - z69183787的专栏 - CSDN博客
2016年10月27日 11:26:03[OkidoGreen](https://me.csdn.net/z69183787)阅读数：477
个人分类：[IO-多路复用](https://blog.csdn.net/z69183787/article/category/6479531)
**epoll**
在linux的网络编程中，很长的时间都在使用select来做事件触发。在linux新的内核中，有了一种替换它的机制，就是epoll。
　　相比于select，**epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。**因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。
　　相对于select和poll来说，epoll更加灵活，**没有描述符限制**。
epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个**事件表**中，**这样在用户空间和内核空间的****copy只需一次。**
**epoll接口**
epoll操作过程需要三个接口，分别如下：
#include<sys/epoll.h>
int epoll_create(int size);
int epoll_ctl(int epfd,int op, int fd,struct epoll_event*event);
int epoll_wait(int epfd,struct epoll_event*events,int maxevents,int timeout);
首先要调用**epoll_create建立**一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。
**epoll_ctl**可以**操作**上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等。
**epoll_wait**在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时，就返回用户态的进程。
**int epoll_create(intsize);**
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。
　　需要注意的是，**当创建好****epoll句柄后，它就是会占用一个fd值**，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，**必须调用****close()关闭**，否则可能导致fd被耗尽。
**int epoll_ctl(int epfd,int op, int fd, structepoll_event *event);**
epoll的**事件注册函数**，它不同与select()是在监听事件时告诉内核要监听什么类型的事件epoll的事件注册函数. 
　　它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是**在这里先注册要监听的事件类型**。
　　第一个参数是epoll_create()的返回值，第二个参数表示动作，用三个宏来表示：
EPOLL_CTL_ADD：注册新的fd到epfd中；
EPOLL_CTL_MOD：修改已经注册的fd的监听事件；
EPOLL_CTL_DEL：从epfd中删除一个fd；
　　第三个参数是需要监听的fd。
　　第四个参数是告诉内核需要监听什么事，struct epoll_event结构如下：
struct epoll_event{
 __uint32_tevents; /* Epoll events */
 epoll_data_tdata; /* User data variable */
};
events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET：将EPOLL设为边缘触发(Edge
 Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
**int epoll_wait(intepfd, struct epoll_event * events, intmaxevents, int timeout);**
等待事件的产生，类似于select()调用。
　　参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size。
　　参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。
**epoll_wait范围之后应该是一个循环，遍历所有的事件。**
　　我们调用epoll_ wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，**因为内核已经在****epoll_ctl中拿到了要监控的句柄列表。**
　　所以，实际上在你调用epoll_ create后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。
　　在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。
**epoll实现机制**
epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的**内核高速****cache区**，用于安置每一个我们想监控的socket。
**这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。**
　　这个内核高速cache区，就是**建立连续的物理内存页**，然后在之上**建立****slab层**。
**简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。**
epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。
　　这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，**还会再建立一个****list链表，用于存储准备就绪的事件.**
　　当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。
　　而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已。
**那么，这个准备就绪list链表是怎么维护的呢？**
　　当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，**还会给内核中断处理程序注册一个回调函数**，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。
　　所以，当一个socket上有数据到了，**内核在把网卡上的数据****copy到内核中后就来把socket插入到准备就绪链表里了**。
**如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。**
**执行epoll_ create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。**
**工作模式**
epoll对文件描述符的操作有两种模式：LT（level
 trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：
**LT模式：**
　　当epoll_ wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。
**ET模式：**
　　当epoll_ wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。
ET模式在很大程度上减少了epoll事件**被重复触发**的次数，因此效率要比LT模式高。
**epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。**
　　那么ET模式是怎么做到的呢？
**ET模式的原理**
　　当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_
 wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表。
　　最后，epoll_ wait检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。
　　所以，非ET的句柄，只要它上面还有事件，epoll_ wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。
**优点**
[IO多路复用之](http://blog.csdn.net/lixungogogo/article/details/52219951)select函数总结
[IO多路复用之poll函数总结](http://blog.csdn.net/lixungogogo/article/details/52226434)
前两篇文章中我们说过select和poll的缺点，现在我们谈谈epoll的优点：
**1）支持一个进程打开大数目的socket描述符(FD)**
select最不能忍受的是一个进程所打开的FD是有一定限制的，由FD_SETSIZE设置，默认值是1024/2048。对于那些需要支持的上万连接数目的IM服务器来说显然太少了。这时候你一是可以选择修改这个宏然后重新编译内核。不过 epoll则没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat
 /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。
**2）IO效率不随FD数目增加而线性下降**
　　传统的select/poll另一个致命弱点就是当你拥有一个很大的socket集合，不过由于网络延时，任一时间只有部分的socket是”活跃”的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对”活跃”的socket进行操作—这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。那么，只有”活跃”的socket才会主动的去调用 callback函数，其他idle状态socket则不会，在这点上，epoll实现了一个”伪”AIO，因为这时候推动力在Linux内核。
**3）使用mmap加速内核与用户空间的消息传递。**
　　这点实际上涉及到epoll的具体实现了。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重要，在这点上，epoll是通过内核与用户空间mmap同一块内存实现的
