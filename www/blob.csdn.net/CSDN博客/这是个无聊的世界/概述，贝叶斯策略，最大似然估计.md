# 概述，贝叶斯策略，最大似然估计 - 这是个无聊的世界 - CSDN博客





2016年05月11日 09:15:53[lancelot_vim](https://me.csdn.net/lancelot_vim)阅读数：1068








# 概述，贝叶斯策略，最大似然估计

标签： 模式分类

@author lancelot-vim

## 绪论

#### 宽度和数量直方图：

![宽度与数量直方图.png-138.7kB](http://static.zybuluo.com/lancelot-vim/9hm26r49oothh1n2lifeqmmv/%E5%AE%BD%E5%BA%A6%E4%B8%8E%E6%95%B0%E9%87%8F%E7%9B%B4%E6%96%B9%E5%9B%BE.png)

#### 光泽度和数量直方图：

![光泽度与数量直方图.png-137.3kB](http://static.zybuluo.com/lancelot-vim/tl2j0hbk9z721avjuyj6rll6/%E5%85%89%E6%B3%BD%E5%BA%A6%E4%B8%8E%E6%95%B0%E9%87%8F%E7%9B%B4%E6%96%B9%E5%9B%BE.png)

#### 宽度-光泽度联合分类图：

![宽度-光泽度联合分类.png-176.6kB](http://static.zybuluo.com/lancelot-vim/2g7jpm83n1fp83g69rcuexwi/%E5%AE%BD%E5%BA%A6-%E5%85%89%E6%B3%BD%E5%BA%A6%E8%81%94%E5%90%88%E5%88%86%E7%B1%BB.png)

#### 简单归纳：
- 从单一特征得到的分类一般不强
- 将单一特征组合起来成多特征分类能得到更强的分类器
- 分类器模型简单（如图中红色线条）会比较弱，分类器太强（如图中蓝色线条）可能会过分类
- 以上问题，可能会存在如果鲈鱼分错，可能不会有太大的问题，但反之可能造成很大的影响

#### 问题:
- 如何选择特征
- 如何选择分类器
- 分类之后如何采取行动

#### 处理方案流程图：

Created with Raphaël 2.1.0输入 (物理信号)传感器 (输入信号，模拟信号、数字信号等)预处理 (分割，组织，对单词、字母、图像去除背景等操作)特征提取 (平移不变性、旋转不变性、尺度不变性，三维问题、遮挡问题、透视失真等）分类 （同一类别特征值波动， 不同类别的差异，特征丢失）后处理 （上下文信息改善分类，根据风险选择策略）

## 贝叶斯决策论

### 引言

#### 条件概率密度与贝叶斯公式

![条件概率密度与贝叶斯公式.png-90.5kB](http://static.zybuluo.com/lancelot-vim/i0raf3tuprsdmhx1jpkgsqhy/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F.png)

#### $P(w_1) = \frac{2}{3} $, $P(w_2) = \frac{1}{3} $时的后验概率：

![后验概率图.png-84kB](http://static.zybuluo.com/lancelot-vim/ahen2emqw7m0ww0rrfksff08/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E5%9B%BE.png)

#### 误差定义：



$ p(error)=\left\{\begin{aligned}p(w_{1}|x) &   & x \in w_{2} \\p(w_{2}|x) &  & x \notin w_{2} \\\end{aligned}\right.$

总误差为：  $ P(error) = \int_{-\infty}^{\infty} p(error,x)dx = \int_{-\infty}^{\infty} p(error|x)p(x)dx $

对 $ \forall x $, 若 $ p(error|x) $ 尽量小， 那么 $ P(error) $就尽量小， 所以令  $ p(error|x) = min[p(w_{1}|x), p(w_{2}|x)] $

### 连续特征的贝叶斯决策论
- 允许使用多于一个的特征
- 允许使用两种类别以上的情形
- 允许有其他行为而不仅仅只是判定类别
- 通过引入一个更一般的损失函数来代替误差概率

以下4个约定： 

1. $ \{w_1, w_2, w_3,... w_c\} $ 表示c个类别(class) 

2. $ \{\alpha_1, \alpha_2, \alpha_3.... \alpha_a \} $ 表示a中行动(action) 

3. $\lambda(\alpha_i|w_j)$ 表示类别为$ w_j $，采取行为$\alpha_i$的损失 

4. $\vec{x}$表示d维的特征
根据贝叶斯公式： $p(w_j|\vec{x}) = \frac{p(\vec{x} | w_j)p(w_j)}{p(\vec{x})}$

若观测到$\vec{x}_0$,采取行为$\alpha_i$，则损失为：$R( \alpha_i | \vec{x}_0)$ = $ \sum_{j=1}^{c}\lambda(\alpha_i   | w_j)p(w_j|\vec{x}_0)$

总损失为: $R = \int R(\alpha(\vec{x}) | \vec{x})P(\vec{x})d\vec{x} $

若选择$\alpha(\vec{x}）$使得：$R(\alpha_i | \vec{x})$对每个$\vec{x}$尽可能小，则风险函数最小化

### 对于二分类问题

约定： 

1. $\alpha_1$ 对应于$w_1$

2. $\alpha_2$ 对应于$w_2$

3. $\lambda_{ij} = \lambda(\alpha_i | w_j)$ 表示损失
则损失函数方程为： 


$ \left\{\begin{aligned}R(\alpha_1 | \vec{x}) = \lambda_{11}p(w_1|\vec{x}) + \lambda_{12}p(w_2|\vec{x})\\R(\alpha_2 | \vec{x}) = \lambda_{21}p(w_1|\vec{x}) + \lambda_{22}p(w_2|\vec{x})\end{aligned}\right.$

若$R(\alpha_1 | \vec{x}) < R(\alpha_2|\vec{x})$， 即$(\lambda_{21} - \lambda_{11})p(w_1|\vec{x}) > (\lambda_{12} - \lambda_{22})p(w_2|\vec{x}) $ ，将该类别判为$w_1$

若$\lambda_{21} > \lambda_{11} $且 $\frac{p(\vec{x} | w_1)}{p(\vec{x} | w_2)} > \frac{\lambda_{12} - \lambda{22}}{\lambda_{21} - \lambda{11}}\frac{P(w_2)}{P(w_1)}  = \theta$， 将该类别判为$w_1$, 如下图 
![似然比图.png-75.2kB](http://static.zybuluo.com/lancelot-vim/ite66nekvlwcazto2svqkuri/%E4%BC%BC%E7%84%B6%E6%AF%94%E5%9B%BE.png)
#### 极小化极大原则

##### 总损失：

$R = \int_{R1}[\lambda_{11}p(\vec{x}|w_1)P(w_1) + \lambda_{12}p(\vec{x}|w_2)P(w_2)]d\vec{x}  \  \qquad + \int_{R_2}[\lambda_{21}p(\vec{x}|w_1)P(w_1) + \lambda_{22}p(\vec{x}|w_2)P(w_2)]d\vec{x}$

由于$P(w_2) = 1 - P(w_1)$, $\int_{R_1} = 1 - \int_{R_2}$得：

$R[P(w_1)] = \lambda_{22} + (\lambda_{12} - \lambda_{22})\int_{R1}p(\vec{x}|w_2)d\vec{x} + \\  \qquad \qquad \quad P(w_1)[(\lambda_{11} - \lambda_{22}) + (\lambda_{21} - \lambda{11})\int_{R2}p(\vec{x}|w_1)d\vec{x} - (\lambda_{12} - \lambda_{22})\int_{R1}p(\vec{x}|w_2)d\vec{x}]$

令$(\lambda_{11} - \lambda_{22}) + (\lambda_{21} - \lambda{11})\int_{R2}p(\vec{x}|w_1)d\vec{x} - (\lambda_{12} - \lambda_{22})\int_{R1}p(\vec{x}|w_2)d\vec{x} = 0$

可得$R_1$,$R_2$， 以及极小化极大误差：$R_{mm} = \lambda_{22} + (\lambda_{12} - \lambda_{22})\int_{R1}p(\vec{x}|w_2)d\vec{x} = \lambda_{11} + (\lambda_{21} - \lambda_{11})\int_{R2}p(\vec{x}|w_1)d\vec{x} $

##### 极小化极大描述图：

## ![极小化极大描述图.png-62kB](http://static.zybuluo.com/lancelot-vim/p8qijdzjcqzi6uogh1qqkxe7/%E6%9E%81%E5%B0%8F%E5%8C%96%E6%9E%81%E5%A4%A7%E6%8F%8F%E8%BF%B0%E5%9B%BE.png)

### 分类器、判别函数和判定面

#### 定义：

一般我们认为对于所有的$j \neq i$,有$g_i(\vec{x}) > g_j(\vec{x})$，则认为该特征向量$\vec{x}$的类型为$w_i$

#### 一般流程如下图：

![分类决策流程图.png-79.3kB](http://static.zybuluo.com/lancelot-vim/74akzjdez5wv3iic304343de/%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

#### 一般判决函数选择：
- $g_i(\vec{x}) = P(w_i|\vec{x})=\frac{p(\vec{x}|w_i)P(w_i)}{\sum_{j=1}^cp(\vec{x}|w_j)P(w_j)} $
- $g_i(\vec{x}) = p(\vec{x}|w_i)P(w_i) $
- $g_i(\vec{x}) = \ln p(\vec{x}|w_i) + \ln P(w_i)$

### 正态判别函数

对于正态分布，通常我们取判别函数为$g_i(\vec{x}) = \ln p(\vec{x}|w_i) + \ln P(w_i)$，根据正态分布密度函数可得： 
$g_i(\vec{x})=-\frac{1}{2}(\vec{x}-\vec{u}_i)^T\Sigma^{-1}_{i} (\vec{x}-\vec{u}_i) - \frac{d}{2}\ln 2\pi - \frac{1}{2}\ln |\Sigma_i| + \ln P(w_i)$

#### 情况1 ： $\Sigma_i = \sigma^2I$

对此情况，$|\Sigma_i| = \sigma^{2d} $, $ \Sigma_i^{-1}=\frac{I}{\sigma^2}   $,由此简化判别函数为：

$g_i(\vec{x})=-\frac{||\vec{x}-\vec{u}_i||}{2\sigma^2} + \ln P(w_i) = -\frac{1}{2\sigma^2}[\vec{x}^T\vec{x} - 2\vec{u_i}^T\vec{x} + \vec{u_i}^T\vec{u_i}] + \ln P(w_i)$

显然$\vec{x}^T\vec{x}$对所有的i是相等的，所以可以简化$g_i$为线性判别函数:$g_i(\vec{x}) = \vec{w_i}^T\vec{x} + w_{i0}$

其中$\vec{w_i} = \frac{1}{\sigma^2}\vec{u_i}$, $w_{i0} = \frac{-1}{2\sigma^2}\vec{u}_i^T\vec{u}_i + \ln P(w_i)$

对于$i \neq j $,令$g_i = g_j$,得：$\vec{w}^T(\vec{x} - \vec{x}_0 ) = 0  $,其中$\vec{w} = \vec{u}_i- \vec{u}_j ,\vec{x}_0  = \frac{1}{2}(\vec{u}_i + \vec{u}_j) - \frac{\sigma^2}{||\vec{u}_i - \vec{u}_j||}\ln \frac{P(w_i)}{P(w_j)}(\vec{u}_i - \vec{u}_j)$

由$\vec{w}$可见，判别面为数据的法平面，当$P(w_i) = P(w_j) $时，正好是中垂面

![情形一示意图.png-50.6kB](http://static.zybuluo.com/lancelot-vim/s6bufo5d3aln0d5bplyfe5xr/%E6%83%85%E5%BD%A2%E4%B8%80%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

#### 情况2 ： $\Sigma_i = \Sigma$

判别函数可重写为： $g_i(\vec{x}) = -\frac{1}{2}(\vec{x}-\vec{u}_i)^T\Sigma^{-1}_i(\vec{x}- \vec{u}_i) + \ln P(w_i)$

由同样的方法可得： 
$\vec{w} = \Sigma^{-1}(\vec{u}_i - \vec{u}_i), x_0 = \frac{1}{2}(\vec{u}_i + \vec{u}_j)-\frac{\ln [P(w_i)]/P(w_j)]}{(\vec{u}_i - \vec{u}_j)^T\Sigma^{-1}(\vec{u}_i - \vec{u}_j)}(\vec{u}_i-\vec{u}_j) $

由$\vec{w}$可见，判别面为数据马氏距离的法平面，当$P(w_i) = P(w_j) $时，正好是马氏距离中垂面

## 最大似然估计

假设每个分类有数据集$D_1, D_2 ... D_c$的样本分别都是根据独立同分布的$p(\vec{x}|w_j)$抽取的，概率分布形式已知，但参数未定，约定未知参数符号为$\vec{\theta}_j $,那么可以写出最大似然函数：$L(D_j|\vec{\theta}_j) = \Pi_{k=1}^np(\vec{x}_k | \vec{\theta}_j)$

我们认为发生的事情为是概率最大的事，所以目标为求得使得$L(D_j|\vec{\theta}_j)) $最大的$\vec{\theta}_j$, 一般情况，为了计算方便，我们使用似然函数的对数函数即$l(\vec{\theta}_j) = \ln L $

![最大似然估计示意图.png-96.2kB](http://static.zybuluo.com/lancelot-vim/cos8zpum8lo3htjexleoitry/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

### 高斯解
- $u$未知：$\hat{u} = \frac{1}{n}\sum_{k = 1} ^n\vec{x}_k $
- $u,\Sigma$未知：$\hat{u} = \frac{1}{n}\sum_{k = 1} ^n\vec{x}_k , \hat{\Sigma} = \frac{1}{n}\sum_{k = 1}^n(\vec{x}_k-\hat{u})(\vec{x}_k-\hat{u})^T$






