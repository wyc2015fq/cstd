# 大数据学习笔记 - kittyzc的博客 - CSDN博客





2018年09月09日 14:30:43[IE06](https://me.csdn.net/kittyzc)阅读数：64








# 1. docker

# 1.1 概念

Docker：就是操作系统中的chroot。可以理解为你在写一篇论文的时候新建了一个文件夹，所有的东西都在这个文件夹里面。

Host：主机，运行容器的机器。比如我们自己的电脑。

Image：镜像，文件的层次结构，包含如何运行容器的元数据。可以比成一个论文模板。

Container：容器，从镜像启动，包含正在运行的程序的进程。可以比成我们电脑里打开的论文。run相当于打开论文模板，修改后commit相当于保存论文。start，stop就相当于打开论文、关闭论文。一个container等于一个image+运行指令等参数

Registry：镜像仓库。可以比成论文模板库。pull就相当于从论文库下载模板到自己的电脑上。

Volume：容器外的存储。可以比成论文里面链接的文件夹。

Dockerfile：用于创建镜像的脚本。可以比成普通的word程序。
# 1.2 使用

运行docker Linux内核版本需要在3.8以上，内核为2.6的系统需要先升级内核，不然会特别卡

yum -y install docker-io：安装docker

service docker start：启动docker

docker version：查看版本

cat /var/log/docker：查看日志
首先，image和container都有id和name:tag两种方式。不加tag的话默认是latest。

搜索查看类命令：

docker info：查看docker相关信息

docker search [IMAGE]：查找镜像库

docker pull [IMAGE]  拉取镜像

docker images/docker image ls：查看本地镜像

docker ps :查看容器状态；-a，所有；-l，最近。

docker logs [CONTAINER]：查看容器标准输出

docker port [CONTAINER]：查看网络端口映射情况
启动关闭类命令：

docker run -it [IMAGE] [COMMAND]：启动交互界面，运行镜像的命令；使用exit退出

docker run -d [IMAGE] [COMMAND]：启动后台模式；-P:将容器内部使用的网络端口映射到我们使用的主机上。-name：给容器命名

docker build [IMAGE]：通过Dockerfile创建镜像

docker start/stop/restart/attach [CONTAINER/ID]：操作容器，其中attach是切换到相应容器

docker commit -m -a [CONTAINER] [NEW IMAGE]：容器保存为镜像

docker rmi [IMAGE] ：删除镜像

docker rm [CONTAINER] ：删除容器

docker ps -a -q/docker container ls -a：查看所有容器ID

docker rm $(docker ps -a -q)：删除所有的容器

docker inspect [CONTAINER|IMAG]：查看容器或镜像的详细信息
仓库搭建：
[https://blog.csdn.net/u010397369/article/details/42422243](https://blog.csdn.net/u010397369/article/details/42422243)
[https://blog.csdn.net/boling_cavalry/article/details/78818462](https://blog.csdn.net/boling_cavalry/article/details/78818462)
仓库使用：

docker push username/repository:tag

docker run username/repository:tag
# 2. Hadoop

# 2.1 部署集群

首先你需要有几台服务器，将它们的name和ip都记录下来，然后将它们记录到**每一台机器**的/etc/hosts文件中，比如：

> 
192.168.119.128   namenode

192.168.119.129   datanode1

192.168.119.130   datanode2
然后需要配置这几台机器之间的免密登录，这一步非常非常容易出错，一定要当心。首先先生成每台机器的密钥：

> 
ssh-keygen  -t   rsa   -P  ‘’


然后将所有机器下/root/.ssh/id_rsa.pub的内容粘贴到authorized_keys文件，放在每台机器的/root/.ssh/目录下。

然后各个机器之间ssh来回登录一下，将首次免密登录需要输入的yes给搞定（不来回登录一下的话，之后hadoop安装可能会报错）。

# 2.2 安装hadoop

安装hadoop时记住自己的用户名，不要随便用sudo！用sudo建立的目录是管理员权限，普通用户无法创建文件。

在**每一台机器上**创建hadoop的工作文件夹，比如：

> 
mkdir  /root/hadoop

mkdir  /root/hadoop/tmp

mkdir  /root/hadoop/var

mkdir  /root/hadoop/dfs

mkdir  /root/hadoop/dfs/name

mkdir  /root/hadoop/dfs/data
然后下载一个hadoop放到namenode机器上然后解压缩，依次修改如下文件：
- hadoop-2.8.0/etc/hadoop/slaves，添加datanode的名字

```
datanode1
datanode2
```
- hadoop-2.8.0/etc/hadoop/hadoop-env.sh，添加

```
export   JAVA_HOME=/usr/java/jdk1.8.0_66
```
- hadoop-2.8.0/etc/hadoop/core-site.xml，在configuration之间添加

```
<property>
     <name>hadoop.tmp.dir</name>
     <value>/root/hadoop/tmp</value>
</property>
<property>
     <name>fs.default.name</name>
     <value>hdfs://namenode:9000</value>
</property>
```
- hadoop-2.8.0/etc/hadoop/hdfs-site.xml，在configuration之间添加

```
<property>
   <name>dfs.name.dir</name>
   <value>/root/hadoop/dfs/name</value>
</property>
<property>
   <name>dfs.data.dir</name>
   <value>/root/hadoop/dfs/data</value>
</property>
<property>
   <name>dfs.replication</name>
   <value>2</value>
</property>
<property>
      <name>dfs.permissions</name>
      <value>false</value>
</property>
```
- 复制hadoop-2.8.0/etc/hadoop/mapred-site.xml.template为mapred-site.xml，添加

```
<property>
   <name>mapred.job.tracker</name>
   <value>hdfs://namenode:49001</value>
</property>
<property>
      <name>mapred.local.dir</name>
       <value>/root/hadoop/var</value>
</property>
<property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
</property>
```
- hadoop-2.8.0/etc/hadoop/yarn-site.xml，添加

```
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>namenode</value>
</property>
<property>
    <name>yarn.resourcemanager.address</name>
    <value>${yarn.resourcemanager.hostname}:8032</value>
</property>
<property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>${yarn.resourcemanager.hostname}:8030</value>
</property>
<property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>${yarn.resourcemanager.hostname}:8088</value>
</property>
<property>
    <name>yarn.resourcemanager.webapp.https.address</name>
    <value>${yarn.resourcemanager.hostname}:8090</value>
</property>
<property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>${yarn.resourcemanager.hostname}:8031</value>
</property>
<property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>${yarn.resourcemanager.hostname}:8033</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>8182</value>
    <discription>单个任务最大可申请内存,默认8182MB</discription>
</property>
<property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
</property>
<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>2048</value>
    <discription>该节点可使用内存</discription>
</property>
```

然后使用scp命令将hadoop文件夹复制到其他所有节点上。

# 2.3 启动hadoop

执行初始化脚本：

> 
hadoop-2.8.0/bin/hadoop  namenode  -format


启动hadoop：

> 
hadoop-2.8.0/sbin/start-all.sh


可使用如下命令查看启动后的状态：

> 
hadoop-2.8.0/bin/hadoop dfs admin -report查看hadoop状态


使用50060端口可以查看hadoop overview，使用8088端口可以查看hadoop集群情况。

# 3. Spark

首先安装好hadoop，然后下载spark on hadoop，解压缩到namenode上。将spark-2.1.1-bin-hadoop2.7/conf/spark-env.sh.template改为spark-env.sh，然后添加：

```
export JAVA_HOME =/usr/java/jdk1.8.0_66
export HADOOP_HOME =/root/hadoop-2.8.0
export HADOOP_CONF_DIR =$HADOOP_HOME/etc/hadoop
export SPARK_HOME = /root/spark-2.3.1-bin-hadoop2.7
export SPARK_MASTER_IP =namenode
export SPARK_EXECUTOR_MEMORY =4G
```

然后将spark拷贝到各个节点，真的是简单到不行。

在namenode的sbin目录下使用如下命令来启动python环境的spark，参数根据自己集群的情况进行修改。

> 
pyspark  --master yarn --deploy-mode client --num-executors 10 --driver-memory 8g --executor-memory 16g --executor-cores 4 --conf “spark.driver.host=namenode”


可以使用8080查看spark集群情况，用4040查看spark任务。

为了方便使用，可以修改/etc/profile：

```
export HADOOP_HOME=/root/hadoop-2.8.0
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export SPARK_HOME=/root/spark-2.3.1-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH
```

# 4. Hive

这里介绍一下hive安装的步骤，主要参考 [https://blog.csdn.net/jssg_tzw/article/details/72354470](https://blog.csdn.net/jssg_tzw/article/details/72354470)

hive需要把元数据存储在数据库中，默认的derby不能多进程访问，因此这里使用mysql来存储。centos2.7 默认是没有mysql的，需要手动安装mysql。可以使用wget方式安装：

```
wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm 
rpm -ivh mysql-community-release-el7-5.noarch.rpm
yum -y install mysql mysql-server mysql-devel
```

启用服务并修改密码：

```
systemctl start mysqld.service
mysql -uroort

mysql>set password for root@localhost = password('123');
```

下载hive，上传到namenode服务器上，解压，同时创建一个/home/deploy/hive/tmp文件夹用于存放临时文件。在集群上建立如下文件夹：

```
hadoop fs -mkdir -p /user/hive/warehouse
hadoop  fs -chmod 777 /user/hive/warehouse 
hadoop fs -mkdir -p /tmp/hive/
hadoop  fs -chmod 777 /tmp/hive/
```

修改/etc/profile，增加两行：

```
export HIVE_HOME=/home/deploy/apache-hive-2.1.1-bin
export HIVE_CONF_DIR=${HIVE_HOME}/conf
```

执行source /etc/profile使其生效。

进入conf文件夹，执行：

```
cp hive-default.xml.template hive-site.xml
```

将hive-site.xml中的${system:java.io.tmpdir}替换为hive的临时目录，将${system:user.name}都替换为root。

搜索javax.jdo.option.ConnectionURL，将该name对应的value修改为MySQL的地址，例如我修改后是：

```
<name>javax.jdo.option.ConnectionURL</name>  
<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
```

搜索javax.jdo.option.ConnectionDriverName，将该name对应的value修改为MySQL驱动类路径，例如我的修改后是：

```
<name>javax.jdo.option.ConnectionDriverName</name> 
<value>com.mysql.jdbc.Driver</value>
```

搜索javax.jdo.option.ConnectionUserName，将对应的value修改为MySQL数据库登录名：

```
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
```

搜索javax.jdo.option.ConnectionPassword，将对应的value修改为MySQL数据库的登录密码：

```
<name>javax.jdo.option.ConnectionPassword</name>
 <value>123</value>
```

搜索hive.metastore.schema.verification，将对应的value修改为false：

```
<name>hive.metastore.schema.verification</name>
<value>false</value>
```

下载mysql驱动 [https://pan.baidu.com/s/1pLsA48F](https://pan.baidu.com/s/1pLsA48F)

上传到hive的lib文件夹下。

开启防火墙，添加3306端口：
```
systemctl start firewalld
firewall-cmd --permanent --add-port=3306/tcp
```

这样就能使用hive了。

如果要用spark连接hive，则将hive-site.xml放到spark的conf文件夹下，并添加

```
<property>
   <name>hive.metastore.uris</name>
   <value>thrift://IP:9083</value>
</property>
```

然后hive --service metastore&启用后台服务。

接着将mysql驱动放到spark的jars目录下，在pyspark中输入：

```
hive = SparkSession.builder.enableHiveSupport().getOrCreate()
```

即可在pyspark中使用hive。

# 5. Kafka

[官方地址](https://www.apache.org/dyn/closer.cgi?path=/kafka/2.0.0/kafka-2.0.0-src.tgz)下载kafka后，解压缩

> 
tar -zxvf kafka_2.0.0-0.0.0.tar.gz


进入文件夹：

> 
cd kafka_2.0.0-0.0.0


开启zookeeper和kafka

> 
bin/zookeeper-server-start.sh config/zookeeper.properties >/dev/null 2>&1

bin/kafka-server-start.sh config/server.properties >/dev/null 2>&1 &


输入jps命令，可以看到Kafka和QuorumPeerMain两个进程，成功搞定！

创建一个test话题：

> 
bin/kafka-create-topic.sh -zookeeper localhost:2181 -topic test


然后打开生产界面：

> 
bin/kafka-console-producer.sh -broker-list localhost:9092 -topic test


在另一个窗口打开消费界面：

> 
bin/kafka-console-consumer.sh -zookeeper localhost:2181 -topic test -from-beginning


在生产界面输入的所有文字，都会在消费界面出现，成功！














