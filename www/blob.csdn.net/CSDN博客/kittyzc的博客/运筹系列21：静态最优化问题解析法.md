# 运筹系列21：静态最优化问题解析法 - kittyzc的博客 - CSDN博客





2019年03月14日 16:50:36[IE06](https://me.csdn.net/kittyzc)阅读数：81
所属专栏：[运筹学算法python实现](https://blog.csdn.net/column/details/26511.html)









在求解最优化问题$\min f(x)$时，如果使用了目标函数的导数，则称为解析法，否则称之为直接法。直接法包括0.618法、多项式插值法等。本篇介绍解析法，定义$g(x)=\nabla f(x)$，$G(x)=\nabla^2 f(x)$。解析法主要分两类：线搜索和置信域搜索两种。

# 1. 无约束最优化问题

首先是**线搜索方法**，基本思路是先确定下降方向d，然后确定步长$\alpha$。时刻记住函数的泰勒展开近似：$f(x+\alpha d)=f(x)+\alpha d g(x)+o(x)$。**精确线搜索**：规定了方向$d_k$后，在$d_k$上$f(x_k+\alpha d_k)$取得最小值时的$\alpha$称为精确线搜索；**非精确线搜索**：保证每次迭代后$f$下降，但并不保证下降最多。非精确线搜索可以用一阶函数控制变化范围，比如一般要求步长满足Armijo准则作为上界：$f(x_k+\alpha d_k)\le f(x_k)+\rho g_k^Td_k\alpha$，此外还要满足如下的准则：Goldstein准则：$f(x_k+\alpha d_k)\ge f(x_k)+(1-\rho) g_k^Td_k\alpha$，Wolfe准则：$g(x_k+\alpha d_k)^Td_k \ge\sigma g^T_kd_k$，强Wolfe准则：$|g(x_k+\alpha d_k)^Td_k |\le -\sigma g^T_kd_k$
**信赖域方法**是限制在一定的范围内，使用近似函数（例如二阶泰勒展开）同时确定方向和步长；如果合适就加大限制范围，如果不合适就减小限制范围。

下面是具体的方法清单：
**最速下降法(SD)** 的方向$d_k=-g_k/||g_k||$，步长为在方向$d_k$上取最小值。可以理解为用线性方程去拟合曲面，只能确定下降方向。
**基本Newton法**的方向$d_k=-G^{-1}_kg_k$，步长为1。可以理解$G_k$度量下的最速下降方向。用二次函数去拟合曲面，可以求得下降最多的点。由于要算二阶导的逆，并且二阶导有可能奇异、不正定，所以一般不用。
**阻尼Newton法**的方向$d_k=-G^{-1}_kg_k$，步长为在方向$d_k$上取最小值（或使用Wolfe准则求步长）。基本Newton法是拟合曲面的最小值，而阻尼Newton法是优化问题的最小值。
**LM法**（Levenberg-Marquardt）用于处理迭代过程中，二阶导不正定、矩阵奇异的情况。LM法的方向$d_k=-(G_k+v_kI)^{-1}g_k$，$v_k$不断乘以2直到$G_k+v_kI$正定。LM法得到的方向在Newton方向和负梯度方向之间浮动。
**拟Newton法**：用一阶导数计算$H_k$来近似$G_k^{-1}$（或者用$B_k$来近似$G_k$）。拟牛顿法的条件是$H_{k+1}(g_{k+1}-g_k)=x_{k+1}-x_k$，则方向$d_k=-H_{k}g_k$。其中，对称秩1公式是假设$H_{k+1}=H_k+\beta uu^T$，称为SR1算法，是一个自对偶的算法，简化表达如下：$\Delta H=\frac{(\Delta x-H\Delta g)(\Delta x-H\Delta g)^T}{(\Delta x-H\Delta g)\Delta g}$。对称秩2公式是假设$H_{k+1}=H_k+\beta uu^T+\gamma vv^T$，称为**DFP**法，简化表达为：$\Delta H=\frac{\Delta x\Delta x^T}{\Delta x^T\Delta g}-\frac{H\Delta g\Delta g^T H}{\Delta g^T H\Delta g}$。**BFGS**公式是针对$B_k$来进行修正的公式，也是秩2算法，是DFP算法的对偶算法。DFP和BFGS以一定的比例结合起来的算法称为**Broyden族公式**。
**BB**法虽然是负梯度方法，但是步长使用拟牛顿法的条件去求解。
**共轭梯度法**使用一阶导信息，迭代产生正交的共轭方向（比如二次优化里面的Gram-Schmidt正交化过程）。共轭梯度的迭代方向为$d_k=-g_k+\beta_{k-1}d_{k-1}$，其中FR方法的$\beta_{k-1}=\frac{g_k^Tg_k}{g_{k-1}^Tg_{k-1}}$，PRP方法的$\beta_{k-1}=\frac{g_k^T(g_k-g_{k-1})}{g_{k-1}^Tg_{k-1}}$，PRP$^+$方法的$\beta_{k-1}=\max\{\frac{g_k^T(g_k-g_{k-1})}{g_{k-1}^Tg_{k-1}},0\}$。共轭下降法的$\beta_{k-1}=-\frac{g_k^Tg_k}{d_{k-1}^Tg_{k-1}}$，DY法的$\beta_{k-1}=\frac{g_k^Tg_k}{d_{k-1}^T(g_k-g_{k-1})}$。
接下来是最小二乘问题(LS)求解方法。LS是一类非常重要的问题，下面的方法都是针对这个问题提出的。一般方法是将一阶最优条件转化为一个无约束最优化问题，然后用牛顿法或者置信域法去求解。
**Gauss-Newton法**用于求解非线性最小二乘（LS）问题。LS问题如此定义：$\min f(x)=\frac{1}{2}r(x)^Tr(x),x \in R^n$，问题主要来源于数据拟合问题。定义$r(x)$的Jacobi矩阵：$J(x)=[\nabla r^T_0,...,\nabla r^T_m]^T$，$S(x)=\Sigma_{i=1}^mr_i(x)\nabla^2r_i(x)$，则$f(x)$的梯度$g(x)=J(x)^Tr(x)$，Hesse阵$G(x)=J(x)^TJ(x)+S(x)$，Newton法求解优化方向方程为$(J_k^TJ_k+S_k)d_k=-J_k^Tr_k$。

当$S_k$比较小时，称问题为小剩余量二乘问题，这种情况下忽略$s_k$并使用Newton法迭代的方法称为Gauss-Newton法，即$J_k^TJ_kd_k=-J_k^Tr_k$。当搜索长度$\alpha_k=1$时称为基本GN方法；带线搜索的GN方法称为阻尼GN方法；

LS问题也可以使用**正交化方法**求解。将Newton法方程转化为优化问题$\min ||J_kd+r_k||^2$，方法为对$J_k$进行QR分解，求解$R_kd=-Q_k^Tr_k$作为迭代方向。
**LMF方法**是一种基于LM求解LS问题的方法。类似正交化方法，也是将Newton方程转化为最小化问题，然后使用信赖域方法求解，然后用KKT条件再转化为方程组：$(J^T_kJ_k+v_kI)d_k=-J^T_kr_k$，$v_k(\Delta ^2_k-||d_k||^2)=0$，然后看实际函数和近似函数变化的比值，超过一定数值(比如0.75)则扩大范围，低于一定数值(比如0.25)则缩小范围。

上述问题也可以使用**Dogleg**方法求解，若$\Delta _k$很小，则使用SD方向与信赖域边界的交点；若$\Delta _k$很大，则使用GN方向在信赖域内部确定优化点；否则在SD和GN中间选一个方向并求解其与信赖域边界的交点。
# 3. 约束最优化问题

定义问题：$\min f(x)$

s.t.$g(x)\ge 0$
$h(x)=0$
**Lagrange函数**：$L(x,\lambda,\mu)=f(x)-\lambda g(x)-\mu h(x)$
**KKT条件**：约束最优化问题最优解的一阶必要条件：$\nabla_xL=0$（定常方程）；$g(x)\ge 0$，$h(x)=0$（原始可行）；$\lambda \ge 0$（对偶可行）；$\lambda g(x)=0$（互补剩余）。最后三个条件可以写作$\min \{\lambda, g(x)\}=0$，求出来的点称为KKT点。对于凸规划问题，KKT点就是全局最优解（凸性相当于二阶最优条件）。
**罚函数法**也是一种将约束最优化问题转为无约束最优化问题的一种方法。**外点罚函数法**把约束的平方加到目标函数上，不断提高惩罚因子（比如说$\{10^k\}$）直至求解结果进入可行域或者满足一定的终止条件；**内点罚函数法**又叫障碍函数方法，把约束的倒数（或者约束的负对数）加到目标函数上，不断降低障碍因子直至到达边界或满足一定条件。
**乘子罚函数法**又叫拉格朗日罚函数法，结合了Lagrange函数的乘子惩罚和外点罚函数法的约束平方惩罚。
**序列二次规划（SQP）方法**是解决一般等式约束最优化问题的方法，Lagrange函数为$L(x,\lambda)=f(x)-\lambda c(x)$。Lagrange-Newton法使用Newton法求解上面函数的KKT稳定点，是一个二次收敛的方法。每一步的迭代问题可以转化为一个二次规划问题（并可以使用拟牛顿法求解），不断迭代进行求解。





