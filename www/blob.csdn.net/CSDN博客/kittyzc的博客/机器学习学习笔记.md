# 机器学习学习笔记 - kittyzc的博客 - CSDN博客





2019年04月08日 12:49:28[IE06](https://me.csdn.net/kittyzc)阅读数：84








# 1.  基本概念

狭义上的**机器学习**是一种使用特定模型对样本数据进行分析，然后进行预测与决策的工具。注意机器学习的范围限定在**对样本进行分析**的范围。

机器学习模型主要分类如下：1.  按样本是否带有标签，分为**有监督**和**无监督**两类；2. 按预测值是离散还是连续，分为**分类**和**回归**两类。

机器学习**过拟合问题**：模型在训练集上误差很小，在测试集上误差很大。解决方法主要有：1. 留出法：数据留出一部分数据作为测试集。2. 交叉验证法：将数据划分为k个子集，每次用一个子集作为测试集。3. booststrap方法：每次随机从大小为m的样本中复制一个，共复制m次。复制的集合作为训练集，剩下的样本作为测试集。
**模型评估标准**有：- 错误率：错分/总数。错分包括假正和假反两种情况。
- 准确率（accuracy）：对分/总数。对分包括真正和真反两种情况。
- 查准率（precision）：真正/预测正。预测正=真正+假正。
- 查全率（recall）：真正/实际正。实际正=真正+假反。
- P-R曲线：查准 vs 查全，一般来说是个负相关的曲线。查准=查全时的点成为平衡点（Break-Even Point，BEP）
- F1（调和平均）：2/(1/查准+1/查全)。更加广义的是$F_\beta = (1+\beta^2)/(1/$查准$+\beta^2/$查全$)$
- AUC：首先定义ROC（Receiver Operating Characteristic）曲线，真正率vs假正率。真正率=真正/实际正；假正率=假正/实际假。按照预测结果进行排序，一般来说是一个从(0,0)到(1,1)的凹函数。下面的面积叫AUC（Area Under Curve），用来评估模型的优劣。

这里简单分析一下模型评估标准。

一般我们首先想到的判断标准是错误率和准确率，然而在数据不均衡的情况下，这两个指标问题很大，比如正：负=99：1的情况，只要把所有的样本都预测为正的，就可以得到99%的准确率。

因此我们需要将正样本和负样本在分母分开，得到灵敏度和特异度两个指标，将这两个指标结合起来形成了ROC曲线。这两个比例不受正负样本数量的影响，因此可以放心用在不均衡的样本中。

在数据平衡时，常常使用另外一个常用的PR曲线和F1指标。首先引入查全率（也叫召回率），观察的是正样本中预测正确的比例，这个值是反映覆盖量的一个指标；在此基础上引入查准率（也叫精准率），反映预测性能的一个指标。一般来说覆盖量越大，预测性能越差。因此引入PR曲线和F1指标，
# 2. 线性模型

**线性回归**：给定一系列的$(x_i,y_i)$，求$w$和$b$，使得$wx+b$与$y$尽量接近。线性回归模型的目标函数一般是最小化均方误差(SSE)：min $\Sigma _i(y_i-(wx_i+b))^2$。因为$y = wx+b$是线性的超平面，因此这个模型称为线性回归模型。若拟合函数为$y = e^{wx+b}$，则称为**对数线性回归**。更一般的，若$g(y) = wx+b$，则称之为广义线性模型，$g$称为联系函数。
**逻辑回归**是分类模型。定义对数似然函数：$p(x)=1/(1+e^{-(wx+b)})$，$p$可以理解为样本等于1的概率，$1-p$为样本等于0的概率。目标函数为最大化“对数似然”： $\max \Sigma _i(y_ip(x_i)+(1-y_i)(1-p(x_i)))$，这样就可以用回归模型来处理分类问题了。这个模型叫做“对数几率回归”（也叫“逻辑回归”），虽然叫回归，但实际是做二分类的。
**线性判别分析（LDA）**，也称Fisher判别分析，原理是寻找超平面y=wx+b，使得投影到这个平面上的两个类别的数据，同类尽量接近、异类尽量远离。定义类内散度矩阵：$S_w=||x_0-u_0||^2+||x_1-u_1||^2$、类间散度矩阵：$S_b=||u_0-u_1||^2$，其中$u_0,u_1$分别是$x_0,x_1$的均值向量。则目标函数为 $\max J=(w^TS_bw)/(w^TS_ww)$。

多分类问题可以由二分类问题推广得到，主要有3种：1. 一对一：将N个类别两两配对，分别做二分类。2. 一对其余：将N个类别和剩下的其余类别分别配对，做二分类。3. 多对多：需要进行编码，每次是多个类别和其余类别分别配对做二分类。
**支持向量回归（SVR）**：目标函数为$\min_{w,b} ||w||^2/2+C\Sigma _il(f(x_i)-y_i)$。另外，和第1节相反，在判断的时候直接使用$f(x)$就行，不需要再加上额外的函数$g$了。使用支持向量回归的好处在于，求解的结果具有稀疏性，最终的$f$仅仅由少数几个支持向量就可以决定。
# 2.  神经网络模型

1943年提出的**M-P模型**：$y=f(\Sigma x)$，$f$是激活函数，大于$\theta$时为1，其他为0。

1956年提出**感知机模型**：$y=f(wx-\theta)$，$f$是激活函数，常见的有Sigmoid函数、reLu函数。令$x'=(x,-1),w'=(w,\theta)$，则上面的函数可以表示为$y=f(w',x')$。感知机按照下公式迭代更新：$w_i'=w_i-\eta \Delta y_i x_i$
**多层神经网络**：层之间全连接，层内无连接，跨层无连接。这个是经典的神经网络模型。

90年代之后，因为SVM的成功，以及之后随即森林、adaboost等算法的兴起，神经网络的研究不再火热，直到2006年深度学习再次兴起。
**误差逆传播（BP）算法**用于训练多层神经网络。更新的原则是：**正向计算值，逆向计算误差**，然后使用梯度法更新每个神经元的参数。下面简单推导反向传播的公式：

用均方差来定义误差$E=\Sigma_j\Delta y_j^2/2$，并且有$y_j=f(\theta_j)$，$\theta _j= \Sigma_h w_{h,j}b_h$，则均方差$E$对倒数第二层参数$w_{h,j}$的负梯度为：$-\eta \Delta y_j f'(\theta_j)b_h$（这四项分别为：学习率、均方差偏导、激活函数偏导、线性函数偏导）。$b_h=f(\alpha _h)$，$\alpha_h=\Sigma _iv_{i,h}x_i$，均方差对倒数第三层参数$v_{i,h}$的负梯度为：$-\eta \Sigma_j(\Delta y_jf'(\theta_j) w_{h,j})f'(\alpha _h)x_i$。

令$g_j=-\Delta y_j f'(\theta_j)$，$e_h=\Sigma_j(g_jw_{h,j}) f'(\alpha _h)$，则可以简单表示为：$\Delta w_{h,j}=\eta g_jb_h$，$\Delta v_{i,h}=\eta e_hx_i$。

同样的，若还有一层，则令$c_i=\Sigma_h (e_hv_{i,h})f'(\lambda_i)$，有$\Delta m_{k,i}=\eta c_ia_k$，依次类推。

防止过拟合的策略主要有2种：1. **早停策略**：验证集误差上升，则停止；2. **正则化**：加上复杂度的惩罚项，比如加上所有权值的平方。
# 3. 决策树

决策树分支原则主要有3个：
- ID3（Iterative Dichotomiser，迭代二分器）。使用香农熵$Ent(D) =-\Sigma p_klog_2p_k$，信息增益表示为$Gain(D,a) = Ent(D)-\Sigma |D^v|/|D|*Ent(D^v)$，其中$a$的取值为$\{a^1,a^2,...,a^V\}$。分支原则选信息增益最大的。
- C4.5（Classifier 4.5）。在ID3的基础上，计算$-Gain(D,a)/\Sigma (|D^v|/|D|log_2(|D^v|/|D|))$，称为信息增益率。分支原则选信息增益率最大的。
- CART（Classification and Regression Tree，分类回归树）。使用Gini Index（基尼系数），$Gini(D) = \Sigma\Sigma p_kp_{k'}=1-\Sigma p_k^2$，分支原则选基尼系数最小的。

# 4. 贝叶斯模型

所谓**后验概率**，指的是从结果推原因的概率；而**先验概率**则是原因本身的概率。**贝叶斯公式**就指后验概率用先验概率来表达的公式：$P(y|x)$ = $P(y)P(x|y)/P(x)$。
**贝叶斯分类模型**是最小化期望错分代价：$\min E_x[C(h(x)|x)]$，其中$h(x)$是贝叶斯分类模型得到的结果，$C$是代价函数。若代价函数是错分为1，正确分类为0，则目标函数变为：$\min E_x(1-p(h(x)|x))$，等价于$\max E_xp(h(x)|x)$，等价于对每个$x$，$\max p(h(x)|x)$。贝叶斯分类器不需要进行训练，其数据主要是用来估算概率值。由贝叶斯公式可以从历史数据中计算$p(y_i)$与$p(x|y_i)$，计算得到所有的$p(y_i|x)$，然后选取值最大的一个$y_i$作为$h(x)$即可。为什么不能直接从历史数据估算$p(y_i|x)$呢？因为一般来说标签集合类别很少，而样本集合类别非常多，因此每个标签里面数据很多，而每个样本里面数据往往比较少，不足以作为概率估计。
**朴素贝叶斯**（Naive Bayes，NB）假设$x$的特征相互之间是独立的，这样$P(x|y_i)$就可以拆分为每个特征的乘积：$P(x|y_i)=P(x_1|y_i)*P(x_2|y_i)*...*P(x_n|y_i)$，这样就把问题拆解为不同分类结果下每个特征的概率分布估计了，这一般是可以用历史数据来估算的。

具体来说，假设训练数据分别为$(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$，要求预测$x_j$对应的$y$，其中$x$有很多分量，可能取值非常多。计算过程为：

1）计算各个类别的先验概率并取对数：$log(p(y_i))=log(n_i)-log(n)$，其中$n_i$是$y_i$出现的次数。

2）计算各类别下各特征属性条件概率并取对数：
$log(p(x_{kj}|y_i))=log(n_{kij})-log(n_i)$，其中$n_{kij}$为$(x_{kj},y_i)$出现次数，$k$为分量下标

3）将$x_j$对应的各分量的概率对数相加。现在假设x有2个分量：
$log(p(x_j|y_i)*p(y_i))= log(n_{1ji})+log(n_{2ji})-log(n_i)-log(n)$

4）目标函数为$max_i$${p(y_i|x_j)}$，等价于$max_i$$(log(n_{1ji})+log(n_{2ji})-log(n_i))$
# 5. 支持向量机

支持向量机的学习路线：从回归问题到二分类问题，最大化间隔，max 1/||w||，min ||w||2/2，拉格朗日对偶问题，KKT条件，SMO算法，软间隔。

**LS问题**：给定一系列$(x_i,y_i)$，求线性函数$f(x) = wx + b$，使得$\min \Sigma(f(x) - y)^2$。如果y不是连续值，而是离散的分类结果，该怎么处理？特别的，在二分类问题中该怎么处理？简单，将连续的y转化为离散的-1和1就行了。

转换方法有很多，比如最简单的：
$g = -1$ if $y <0$
$g = 1$ if $y≥ 0$

为了方便以后求导，这里用logistic函数进行转换，这里记作$g(y)$。logistic函数的形状可以参考[这里](https://blog.csdn.net/bitcarmanlee/article/details/51154481)。模型变为：给定一系列($x,y$)，求线性函数$f(x) = wx + b$，使得$\min \Sigma (g(f(x)) - y)^2$。
**支持向量机问题**：LS问题只能用数值最优化去求解，为了更好的给出结果，我们使用另外一个损失函数，将优化问题改为“最大化间隔”，详情可以参考[这里](https://blog.csdn.net/v_july_v/article/details/7624837)，求解模型变为：给定一系列(x,y)，求线性函数$f(x) = wx + b$，使得
$\max_{w,b} \min_i d$

s.t. $d ≤ yf/||w||$
$yf(x)$称为函数间隔，$yf(x)/||w||$称为几何间隔。
$||w||$是$w$的L2范数，也就是常说的欧式空间几何长度，详细解释可以参考[这里](https://blog.csdn.net/shijing_0214/article/details/51757564)。修改后的模型称为支持向量机。

**算法推导**
$f(x) = wx+b$与$||w||$可以进行等比例缩放，令$L = w^2/2 - \lambda (yf-1)$，因此对目标函数做点小变换题：
$\min_{w,b}\max_{\lambda_i} L$ ，转化为对偶问题：$\max_{\lambda_i}\min_{w,b} L$，最优点KKT条件求解并带入原问题：$\max_{\lambda_i} L = Σ_iλ_i - (Σ_{i,j}λ_iλ_jy_iy_jx_ix_j)/2$。1998年，Microsoft Research的John C. Platt在[论文](https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F69644%2Ftr-98-14.pdf)中提出针对上述问题的解法：SMO算法，它很快便成为最快的二次规划优化算法，特别是在针对线性SVM和数据稀疏时性能更优。简单来说，步骤是：

循环执行下述步骤：

(1) 选取两个$λ_i$和$λ_j$（可以选择违反KKT条件最严重的，也可以选择距离最大的）

(2) 求解问题：
$\max_{\lambda_i,\lambda_j} L(λ_i, λ_j)$

s.t. $\Sigma_i λ_iy_i = 0$

直到L无法再优化。具体的推导和步骤还可以参考[这里](https://www.cnblogs.com/xxrxxr/p/7538430.html)。
**核函数方法**

当遇到线性不可分的问题时，我们可以简单的用一个新的一次变量代替高次变量，这样相当于把低维的特征空间映射到高维空间。

比如说$y = x+x^2$就可以用$y = x_1+x_2$代替，其中$x_1 = x，x_2 = x^2$，特征空间由一维的$[x]$变为了二维的$[x_1,x_2]$。定义一个非线性映射：$φ(x)$，比如说上面例子里面$φ(x) = [x, x^2]$，我们可以对(φ(x),y)使用支持向量机。将决策规则中的$f(x)=wx + b$改为$f(x) = w \varphi (x) + b$ ，则$L(λ) = Σ_iλ_i - (Σ_{i,j}λ_iλ_jy_iy_j\langle x_i,x_j\rangle)/2$。

为避免爆炸性的计算，定义核函数$K(x,z) = \langleφ (x),φ(z)\rangle$为计算两个向量在隐式映射过后的空间中的内积的函数，得到$L(λ) = Σ_iλ_i - (Σ_{i,j}λ_iλ_jy_iy_jK(x_i,x_j))/2$。我们其实并不需要内积展开的显式结构，只需要有不同x下的内积的值就行了，因此使用核函数的形式事先在低维上进行计算，而将实质上的分类效果表现在了高维上。
**软间隔和正则化**

有的时候问题不一定是完全线性可分的，这时候需要引入软间隔的概念，以允许一些出错的样本。其实就是引入罚函数，目标函数变为$\min ||w||^2/2+C\Sigma _il(y_if(x_i))$。$l$函数常见的有0/1损失、hinge损失、指数损失、对率损失等。特别的，使用hinge函数$l(z) = max(0,1-z)$，得到的对偶问题比硬间隔的对偶问题只有唯一的约束条件差别：$0\leq \lambda \leq C$。

更一般的，目标函数可以表示为$\min \Omega(f)+C\Sigma _il(y_i,f(x_i))$，前面的部分称为结构风险，也可以称作正则化项，后面的称为经验风险，C是罚函数，也称为正则化常数。正则化项常常使用$L_p$范数。












