# 自然语言处理核心内容 - kittyzc的博客 - CSDN博客
2018年08月28日 14:32:38[IE06](https://me.csdn.net/kittyzc)阅读数：131
# 1. 统计模型

一般我们学习语言的时候，都是从**语法规则+字/词汇**开始学习的，这也是传统的语言处理方法。现代方式则是基于统计学的，简单来说，就是：**说的人多了，就是正确的话**（可以是词汇，可以是句子，可以是文章）。换种方式表达，就是：**出现的概率多了，就是正确的话**。用数学语言表达，则是：$SSS$出现的概率$p(S)p(S)p(S)$越大表示$SSS$越正确。

## 1.1 单词概率

单词概率最简单的方法就是直接用频度估计。但是会有如下问题：有些单词、词组在语料库中没有出现过~因此我们要考虑这个问题，留一些概率给没有出现过的这些单词、词组、句子……使用古德-图灵估计(Good-Turing Estimate)，具体的方法如下：
- 统计语料库中出现$rrr$次的单词，共$NrN_rNr​$个。
- 设置阈值$TTT$（一般设置为8-10），如果$0&lt;r&lt;T0&lt;r&lt;T0<r<T$，则单词概率的估算值从$r/Nr/Nr/N$变为$Nr+1∗(r+1)/(Nr∗N)N_{r+1}*(r+1)/(N_r*N)Nr+1​∗(r+1)/(Nr​∗N)$，即由$r+1r+1r+1$的数据来平滑估计$rrr$的数据。
- 这样可以得到出现0次的单词的概率估计值从0变为$Σr&lt;T(r−(r+1)∗Nr+1/Nr)/N\Sigma _{r&lt;T} (r -  (r+1)*N_{r+1}/N_r)/NΣr<T​(r−(r+1)∗Nr+1​/Nr​)/N$。对于二元组的概率估计，也使用同样的方法进行处理。

## 1.2 n元模型

如何评估一个句子的概率呢？用数学表达式表示为：$S=(w1,w2,...,wn)S=(w_1,w_2,...,w_n)S=(w1​,w2​,...,wn​)$，$wiw_iwi​$是单词，我们需要估计$p(S)p(S)p(S)$。最简单的模型是：假设句子内的单词之间没有关联关系，那么$p(S)=p(w1)∗p(w2)∗...p(wn)p(S)=p(w_1)*p(w_2)*...p(w_n)p(S)=p(w1​)∗p(w2​)∗...p(wn​)$。单词的概率评估是比较好操作的，直接用频度估算概率即可。

显然上述假设是不成立的，不然我们语法都白学了，只要去记单词就好了。在此基础上做一点简单的修改，即假设每个单词只和前面的单词有关系，即$p(S)=p(w1)∗p(w2∣w1)∗...p(wn∣wn−1)p(S)=p(w_1)*p(w_2|w_1)*...p(w_n|w_{n-1})p(S)=p(w1​)∗p(w2​∣w1​)∗...p(wn​∣wn−1​)$，这是一个马尔科夫链，对应的模型交二元模型（Bigram Model）。如果跟前面$n−1n-1n−1$个单词有关系，则称为**n元模型**（n-gram Model）。实际中一般假设$n=3n=3n=3$。

统计语言模型最重要的数据就是公式里面的条件概率了，所有这些条件概率称为模型的参数。条件概率不好直接评估，需要转化为联合概率和边缘概率进行计算。公式为：$p(wi∣wi−1)=p(wi,wi−1)/p(wi−1)p(w_i|w_{i-1}) = p(w_i,w_{i-1})/p(w_{i-1})p(wi​∣wi−1​)=p(wi​,wi−1​)/p(wi−1​)$。联合概率$p(wi,wi−1)p(w_i,w_{i-1})p(wi​,wi−1​)$用两个单词一起出现的频度来估计，边缘概率$p(wi−1)p(w_{i-1})p(wi−1​)$用单个单词出现的频度来估计。
## 1.3 单词数学表达：词袋模型（BOW）和词向量模型（word2vec）

word embedding，是一种将单词映射为向量的方法，形成了一个词典大小*向量长度的矩阵，每一行代表对应的单词的向量。

如果忽略所有句子结构，对词进行统计的模型称为**词袋模型**，是一种计数型的字典型表示方法。词向量是one-hot型的，句子向量由词向量叠加起来，字典有多大，词向量和句子向量就有多长。TF-IDF也可以看做是词袋模型的变种，TF首先是对计数进行了归一化，IDF则是考虑进了单词对文档的区分度。

接着是**word2vec**模型，使用了句子上下文的信息，对词向量进行了压缩。

首先是**神经概率语言模型**（NPLM, Neural Probabilistic Language Model），其核心是使用神经网络训练方法。NPLM用前n-1个单词拼接为输入，第n个词为输出作为训练数据，模型有一个隐藏层。
**C&W模型**以上下文n个单词的one-hot拼接作为输入，以得分作为输出（包含中心词为1，不包含中心词为0）。
**CBOW**（连续词袋）则是以上下文组成的单词组作为输入，中心词作为输出进行训练，去除了隐藏层，并且使用词向量的平均值代替了拼接。
**skip-gram**模型是以中心词作为输入，上下文组成的单词组作为输出进行训练。
## 1.4 句子数学表达：隐马尔科夫过程（HMM）

假设我们接收到的文章是无法理解的句子组合$o1,o2,...o_1,o_2,...o1​,o2​,...$，怎样把它们转化为我们能够理解的句子组合$s1,s2,...s_1,s_2,...s1​,s2​,...$？

同样使用概率模型，求解最大的$p(s1,s2,...∣o1,o2,...)p(s_1,s_2,...|o_1,o_2,...)p(s1​,s2​,...∣o1​,o2​,...)$即可。首先将上式转化为$p(o1,o2,...∣s1,s2,...)∗p(s1,s2,...)/p(o1,o2,...)p(o_1,o_2,...|s_1,s_2,...)*p(s_1,s_2,...)/p(o_1,o_2,...)p(o1​,o2​,...∣s1​,s2​,...)∗p(s1​,s2​,...)/p(o1​,o2​,...)$，分母是个固定项不用计算，只需要计算分子的两项就行了。

紧接着将问题建模成隐马尔科夫过程（Hidden Markov Model），即假设$p(s1,s2,...)=p(s1)∗p(s2∣s1)∗p(s3∣s2)∗...p(s_1,s_2,...)=p(s_1)*p(s_2|s_1)*p(s_3|s_2)*...p(s1​,s2​,...)=p(s1​)∗p(s2​∣s1​)∗p(s3​∣s2​)∗...$称为转移概率，另外$p(o1,o2,...∣s1,s2,...)=p(o1∣s1)∗p(o2∣s2)∗...p(o_1,o_2,...|s_1,s_2,...)=p(o_1|s_1)*p(o_2|s_2)*...p(o1​,o2​,...∣s1​,s2​,...)=p(o1​∣s1​)∗p(o2​∣s2​)∗...$称为发射概率

隐马尔科夫过程模型的训练需要有人工标记，工作量非常大，因此常用的办法是使用Veterbi算法，不断更新参数。
## 1.5 线性链条件随机场（CRF）

线性链CRF和HMM基本类似，主要是原来的有向图变为无向图。

# 2 单词类

## 2.1 分词

中文语言在统计单词频度时有一个基本问题：单词如何划分？传统的方法是使用维护词典的**规则分词**：1. 最大正向匹配：切分m（最长可能字符数）个字符，从多到少依次看有没有匹配上的单词；2. 最大逆向匹配：相反；3. 双向最大匹配：正向、逆向各匹配一次，选取取词数较少的作为匹配结果。

现代语言处理使用**统计分词**方法解决。简单来说，是先估算好所有单词、词组的概率，那么句子S的分词方法就是使得$p(w1,w2,...,wn)p(w_1,w_2,...,w_n)p(w1​,w2​,...,wn​)$最大的划分方法。枚举分词效率比较低，实际一般使用动态规划+维特比（Viterbi）算法来求解。

## 2.2 词性标注

最简单的方法莫过于将语料库的高频词性作为预测的词性，稍复杂的方法是将问题看做序列标注问题，用隐马尔科夫模型等进行标注。jieba分词使用词典匹配和HMM共同的方式。

## 2.3 命名实体标注

传统方法是词典法，现代方法是使用统计方法在词汇形态处理的任务中单独处理，称为NER（named entities recognition）。使用CRF，可以定义一系列特征函数（转移函数、状态函数）。

## 2.4 关键词提取

首先是**TF-IDF**（词频-逆文档频次算法）。TF是一篇文章中词频越多越好，IDF则是在全局中出现的次数越少越好（有区分度）。TF-IDF=TF*IDF。其中IDF在计算时需要用到拉普拉斯平滑：$IDFi=log⁡(∣D∣1+∣Di∣)IDF_i=\log(\frac{|D|}{1+|D_i|})IDFi​=log(1+∣Di​∣∣D∣​)$

其次是**Text-Rank**，使用窗口的概念，将固定范围内的词链接起来，使用$Wi=(1−d)+dΣj∈IniWjOutiW_i=(1-d)+d\Sigma_{j\in In_i}\frac{W_j}{Out_i}Wi​=(1−d)+dΣj∈Ini​​Outi​Wj​​$
**LSA**（潜在语义分析）：使用BOW模型将每个文档表示为向量，然后拼接成词-文档矩阵，进行SVD分解，然后将中间的矩阵选择一部分。
**LDA**方法假设先验、后验是D分布，则统计语料符合多项式分布
## 2.5 自动摘要

同样使用**Text-Rank**方法，注意要计算句子之间两两相似度作为权重乘到得分上。相似度可以用余弦相似度。

# 3. 语句类

## 3.1 句法分析

PCFG，是一种生成式的方法，计算句法概率树，然后选择概率最大的那个。

## 3.2 情感分析

一种有效的方法是基于词典的词袋方法，将词典中的单词标注为积极和消极两类，然后进行匹配计算。

另一种方法是使用传统机器学习方法，如SVM、NB等。

最后是使用深度学习的方法（主要是LSTM和GRU方法）。
## 3.3 地址分析

地址分析常常用有限状态机方法，这是一个有向图，是根据已知文法，对句子建立的模型。首先对语句进行分词，将句子拆分为省、市、县、街道等信息，然后词语之间的前后关系存在限制要求，可以构成一张图。如果能从起点走到终点，那么句子就是表达正确的地址，否则是表达错误的地址。

# 4. 信息论

## 4.1. 信息度量方法：信息熵

香农熵：$H=−Σp∗log(p)H = -\Sigma p*log(p)H=−Σp∗log(p)$，即最小编码长度。

对于二元模型，有$H(X∣Y)=−Σp(x,y)log(p(x∣y))H(X|Y) = -\Sigma p(x,y)log (p(x|y))H(X∣Y)=−Σp(x,y)log(p(x∣y))$，有$H(X)≥H(X∣Y)H(X)\geq H(X|Y)H(X)≥H(X∣Y)$，即二元模型的信息大于一元模型。

定义互信息：$I(X;Y)=Σp(x,y)log(p(x,y)/(p(x)∗p(y)))I(X;Y) = \Sigma p(x,y)log(p(x,y)/(p(x)*p(y)))I(X;Y)=Σp(x,y)log(p(x,y)/(p(x)∗p(y)))$，并且有$I(X;Y)=H(X)−H(X∣Y)I(X;Y) = H(X)-H(X|Y)I(X;Y)=H(X)−H(X∣Y)$。
## 4.2. 信息编码

目前一般使用MD5和SHA-1算法来对信息进行简化编码（同时加密），编码的过程也就是进行哈希映射的过程。有时将编码映射后的值称为信息指纹。

编码的一个应用：判别文章/视频是否相同。一般是按照相同的规则提取关键词/关键帧，然后一一比较其哈希值。还可以使用相似哈希（Simhash）来计算。首先进行分词，然后计算每个词的hash编码与权重（比如TF-IDF），然后将哈希编码拆分成每一位和权重进行映射计算（比如二进制哈希码，可以将每一位的0变成-1后，求每一位的加权和，然后再映射为0-1。这两步分别称为扩展和收缩）。

第二个应用是加密：现代的加密算法一般都是用到费马小定理。比如RSA算法：给定公开密钥E和参数N，信息X的编码方法为：$Y=XE&VeryThinSpace;mod&VeryThinSpace;NY = X^E\bmod NY=XEmodN$，解码方法为使用私钥D：$X=YD&VeryThinSpace;mod&VeryThinSpace;NX = Y^D\bmod NX=YDmodN$。其中$N=P∗QN=P*QN=P∗Q$为两个质数的乘积，$M=(P−1)∗(Q−1)M = (P-1)*(Q-1)M=(P−1)∗(Q−1)$，而密钥满足$E*D = k*M +1 $。可以证明。可以证明。可以证明$(Y^D\bmod N) = (X^{k*M+1}\bmod N)=(X\bmod N) = X$。

第三个应用是拼音输入法：如果是一个字一个字输入，那么26个字母的信息量为log26，每个字的信息量大概是10，输入一个汉字需要敲键盘$10/log26≈2.110/log26\approx2.110/log26≈2.1$。以词为单位统计，每个字的信息量大概是8，平均只需要敲1.7次键盘。基于上下文相关性的话，每个字的信息量可以压缩到6，这时候只需要1.3次。当然这是极限，实际一般都是用自然的拼音输入法。在输入过程中，输入法会自动对拼音进行分词，然后把问题建立成一个隐马尔科夫过程，直接用动态规划方法求解最短路问题。






