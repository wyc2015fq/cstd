# 强化学习系列10：无模型的直接策略搜索 - kittyzc的博客 - CSDN博客





2019年01月07日 20:10:40[IE06](https://me.csdn.net/kittyzc)阅读数：81
所属专栏：[强化学习系列](https://blog.csdn.net/column/details/33845.html)









# 1. 从值函数到策略函数

前面介绍的方法都是基于值函数$v$或者$q$来计算的（用定义或者bellman方程），策略根据$\pi(s)=\arg \max_a q(s,a)$得到。有时我们可以直接建立状态与策略之间的函数，即$\pi(s)=f_{\theta}(s)$，比如说建立一个神经网络，网络参数为$\theta$，输入是状态$s$，输出为每一个行动的概率$p$。直接策略搜索的有点是：直接、简单、易收敛。

同样在直接策略搜索的领域内，也分有模型与无模型两类，无模型算法中，确定性策略一般用DDPG方法，随机策略主要是TRPO、A2C等；有模型算法中，主要是使用GPS等算法。这里首先介绍无模型的算法。

详细的介绍可以参考这篇博客：[https://blog.csdn.net/jinzhuojun/article/details/78007628](https://blog.csdn.net/jinzhuojun/article/details/78007628)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190226094528863.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tpdHR5emM=,size_16,color_FFFFFF,t_70)
# 2. 随机策略梯度法

首先简单推导一下策略梯度更新方法：

定义采样策略为$\Pi$，采样轨迹为$\tau$，总体回报为$r(\tau)$，则目标函数为最大化$J_{\pi}=E_{\tau\sim\pi}[r(\tau)]$，梯度为$\nabla J_{\pi}=\Sigma[r(\tau)\nabla \pi(\tau)]=\Sigma[\pi(\tau)r(\tau)\nabla \log\pi(\tau)]=E_{\tau\sim\pi}[r(\tau)\nabla \log\pi(\tau)]$。将$\tau$展开即可得到上式等于$E[\Sigma_t r_t\Sigma_t\nabla\log\pi(a_t|s_t)]$。令$G=\Sigma_t r_t$，这个公式可以看做是以长期回报$G$为加权权重，对所有的($s,a$)样本进行最大似然估计的公式，非常简洁：$E[G\nabla log\pi]$。

将期望用蒙特卡洛采样法进行替换，用当前策略采样$N$条轨迹，并将权重从长期期望改成从当前时间开始的期望值。则$\nabla J_{\pi}=\frac{1}{N}\Sigma_{i} G_i\nabla \log\pi(a_{i,t}|s_{i,t})$ ，然后进行更新：$\Delta{\theta}=\alpha\nabla J_{\pi}$，其中$\alpha$是学习率。

在实际过程中，我们进行交互的次数往往是有限的，学习成本很高，样本不足会造成策略波动太大的问题，业界发展了一些新的方法来处理这个问题。
## 2.1 Actor-Critic方法

在Actor-Critic方法中，我们采用几种措施来降低算法的方差，首先是使用独立的模型代替轨迹的长期回报（比如TD-error，$g_t= \Sigma_{i=1...n}\gamma^i r_{t+i}+v(s_{t+n})-v(s_t)$），称为Critic；原来用于行动的策略模型称为Actor。公式变为：$\nabla J_{\pi}=E[\Sigma_t g_t\nabla\log\pi(a_t|s_t)]$。

其次，在实际更新时又有异步和同步两种方法。分别称为A3C(Asynchronous Advantage Actor-Critic)和A2C(Advantage Actor-Critic)。OpenAI在官方博客中提到A2C效果比A3C要好。

再者，目标函数中加入策略的熵，以增加不确定性进行一定量的探索。具体来说，就是加上$\beta\nabla_{\theta}H(\pi(s_t))$。

Baselines上的案例如下：
```python
import gym
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines import A2C

# multiprocess environment
n_cpu = 4
env = SubprocVecEnv([lambda: gym.make('CartPole-v1') for i in range(n_cpu)])
model = A2C(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=25000)
obs = env.reset()
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render()
```

## 2.2 TRPO：置信区域策略优化

在上面的算法中，学习率$\alpha$是固定的。我们想要自动选择合适的步长避免模型震荡，这里就要用到TRPO方法。TRPO全称trust region policy optimization，其目标是每次更新策略后，回报函数的值不能变差。其核心是如下公式：$J_{\pi'}=J_{\pi}+E_{\pi'}[\Sigma_t\gamma^tA_{\pi}]$。

注意其中$\pi'$和$\pi$分别代表新策略和旧策略，后面计算期望的时候，用新策略采样，用旧策略计算优势函数$A_{\pi}$。经过一系列tricky的处理之后，求解函数变为：
$\min \nabla L_{x}*(\theta-x)$

s.t. $\frac{1}{2}(\theta-x)^TI_{\theta}(\theta-x)\le\delta$

其中$L_x=E[\frac{\pi_x}{\pi_{_{\theta}}}A_{\theta}]$，$I_{\theta}$为fisher阵，TRPO算法使用共轭梯度法计算fisher阵。

Baselines中的使用方法和A2C类似，把模型名字换掉即可。
## 2.3 GAE：泛化优势估计

GAE(Generalized Advantage Estimation)可以看做是AC方法的拓展。我们把2.1节中的权重再进行替换。我们将n步TD-error记作$G_t^n$，则新的方法$G_t^{GAE(\lambda)}=G_t^1+\gamma \lambda G_t^2+(\gamma \lambda )^2G_t^3+......)=\Sigma_i(\gamma \lambda)^iG_{t+i}^1$。当$\lambda=0$时，权重为1阶TD-error；当$\lambda=1$时，权重为蒙特卡洛和价值函数之间的差值。

Baselines中的使用方法和A2C和TROP类似，把模型名字换掉即可。

## 2.4 PPO：近似策略优化

PPO(Proximal Policy Optimization)对TRPO方法进行了优化，将置信区域的约束条件改到了目标函数中，$L_x=E[\frac{\pi_x}{\pi_{_{\theta}}}A_{\theta}]$变为：$E[ \min \{\frac{\pi_x}{\pi_{_{\theta}}}A_{\theta},clip(\frac{\pi_x}{\pi_{_{\theta}}},1-\epsilon,1+\epsilon)A_{\theta} \}]$，相当于给$\frac{\pi_x}{\pi_{_{\theta}}}$加了2把锁。此外，PPO还将值函数的目标函数和策略模型的熵加入到了目标中。

# 3. 无模型确定策略

这里的确定策略指的是我们的输出不再是不同行动的概率值，而是直接给出一个具体的行动。

确定策略的信息量比随机策略要少，但是在连续决策空间问题上，只能使用确定策略。比如我们要通过强化学习去得到词汇的32维词向量，每一维都有无穷多的连续取值，我们不可能给出它们每一个概率（除非进行离散化），只能给出最合适的一个词向量值。

## 3.1 DDPG：深度确定性策略梯度法

DDPG以AC为基础，借鉴使用DQN的replay buffer和target network来进行学习。首先，Actor和Critic都有自己的Behavior和Target Network，其次不同于DQN的隔一段时间同步一次，在DDPG中每次都是更新一点点，称为soft方法。







