# 模型训练 - qq_30006593的博客 - CSDN博客





2017年08月29日 14:36:24[lshiwjx](https://me.csdn.net/qq_30006593)阅读数：345








# 白化

减均值：使分布以0为中心 

除以方差：自然图像本身就是，不用除

# BN和dropout

[区别联系](http://forum.ai100.com.cn/blog/thread/ml-2016-08-12-4007583222243708/)
[BN](https://www.zhihu.com/question/38102762)：论文中提到了四个优点
- 使用更高的学习率
- 不用太担心初始化
- 像一个正则项，减少dropout的需求，但不矛盾
- 使更快收敛

[实现](http://blog.csdn.net/silent56_th/article/details/53998028)：weight和bias对应gamma和beta，running mean 和 var对应内部的累计均值和方差，配合momentum在训练时使用，直接在测试时使用。affine控制是否学习scale和shift，即weight和bias。

# GPU设置

[设置使用的gpu个数](http://www.cnblogs.com/darkknightzh/p/6591923.html)

# 训练技巧

[val和train的gap，lr。。。](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)

# 优化方法的选择

[各种方法的比较，推荐adam](http://shuokay.com/2016/06/11/optimization/)
[简短版本](http://blog.csdn.net/BVL10101111/article/details/72614711)

# Loss

[均方差和交叉熵](http://neuralnetworksanddeeplearning.com/chap3.html#saturation2_anchor)


$  C = \frac{(y-a)^2}{2}$
[熵和交叉熵](http://colah.github.io/posts/2015-09-Visual-Information/)：熵就是最优编码，交叉熵就是用p的最优编码来编码q的平均编码长度。 


$H(p) = \sum_x p(x)\log_2\left(\frac{1}{p(x)}\right)$


$H_p(q) = \sum_x q(x)\log_2\left(\frac{1}{p(x)}\right)$
# Softmax

[softmax 层的输出是一个概率分布。在许多问题中，我们可以很方便地将输出激活值看作是神经网络认为结果是的概率。](http://neuralnetworksanddeeplearning.com/chap3.html#softmax)


$  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}}$




