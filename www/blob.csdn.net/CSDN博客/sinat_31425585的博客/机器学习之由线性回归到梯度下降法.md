# 机器学习之由线性回归到梯度下降法 - sinat_31425585的博客 - CSDN博客
2018年10月03日 20:17:55[Mirror_Yu_Chen](https://me.csdn.net/sinat_31425585)阅读数：103
    一谈到机器学习，我们可能会思考，什么是机器学习，到底学习什么？
**    一、机器学习学什么？**
    我们先来看一张图，如图1所示，这张图来自Andrew Ng机器学习课程第二章，什么意思？我总结一下：
    1） 机器学习目的：建立输入（x）到输出（y）之间的映射关系（h）；
    2） 模型建立：模型建立过程中，我们需要根据经验，对要建立模型做一些假设（Hypothesis），比如说，这里假设输入（房屋面积）和输出（房屋价格）之间是满足线性关系的，这样模型的类型就确定了；
    3） 机器学习学的内容：学习内容，其实就是学习我假定模型的参数，例如，这里我们假定是线性的，那么相当于就是假定了一个线性方程![y=W^{T}X+b](https://private.codecogs.com/gif.latex?y%3DW%5E%7BT%7DX&plus;b)，我们学习的过程，其实就是得到确切的模型参数![W^{T},b](https://private.codecogs.com/gif.latex?W%5E%7BT%7D%2Cb)，**学习的本质是学习模型的参数。**
![](https://img-blog.csdn.net/20181003172738608?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                         图1 机器学习流程示意图（摘自参考资料[1]）
**    二、如何学？**
    在说如何学之前，我们先聊一下我们是如何去判别一件事情完成的好坏程度。我们在衡量一件事情完成好坏之前，自身会有一个期望结果（预期值），而这件事情有一个实际结果（真实值），这样，我们就可以通过这种实际值与预期值之间的偏差，就可以很好的来判断这件事情的好坏。
** 2.1 损失函数**
    在机器学习中，同样也是类似的，机器学习中有一个专门的名词：loss（cost），描述loss（cost）使用loss function（损失函数）或error function（误差函数）。
    这里，我们看一下一个具体模型的loss计算过程，如图2所示。
![](https://img-blog.csdn.net/20181003204912933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                      图2 Loss 直观解释（摘自参考资料[1]）
    期望值与预测值之间的偏差就对应于蓝色线段，预测总的偏差等于所有蓝色线段长度之和，所以，Loss function可以定义如下：
![L(\theta )=(h_{\theta (x)}- y)^{2}](https://private.codecogs.com/gif.latex?L%28%5Ctheta%20%29%3D%28h_%7B%5Ctheta%20%28x%29%7D-%20y%29%5E%7B2%7D)
    衡量整个样本集中的损失，我们采用cost function（代价函数）：
![J(\theta )=\frac{1}{2m}\sum_{i=0}^{m}(h_{\theta (x_{i})}- y_{i})^{2}](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29%3D%5Cfrac%7B1%7D%7B2m%7D%5Csum_%7Bi%3D0%7D%5E%7Bm%7D%28h_%7B%5Ctheta%20%28x_%7Bi%7D%29%7D-%20y_%7Bi%7D%29%5E%7B2%7D)
**    2.2 学习目标**
    如果当前模型能够完美的拟合所有样本点，这个时候有：   ![h_{\theta (x_{i})}= y_{i},i=1,2,...,m](https://private.codecogs.com/gif.latex?h_%7B%5Ctheta%20%28x_%7Bi%7D%29%7D%3D%20y_%7Bi%7D%2Ci%3D1%2C2%2C...%2Cm)，  对应的![J(\theta )=0](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29%3D0)，如图3所示。
![](https://img-blog.csdn.net/20181003211021230?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                      图3 模型完美拟合样本（摘自参考资料[1]）
    因此，如果我们能够最小化损失函数![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)，让![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)足够小，那么这个时候参数![\theta](https://private.codecogs.com/gif.latex?%5Ctheta)对应模型就能较好的拟合样本集，我们就能得到一个我们希望得到的模型，因此，如何学习问题其实就等价于如何来最小化损失函数![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)，学习的目标就是最小化损失函数![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)：![min_{\theta }J(\theta )](https://private.codecogs.com/gif.latex?min_%7B%5Ctheta%20%7DJ%28%5Ctheta%20%29)
    对于求解的问题，我们可以考虑直接求偏导，然后令之为0，就可以得到解：
![\theta =(X^{T}X)^{-1}X^{T}Y](https://private.codecogs.com/gif.latex?%5Ctheta%20%3D%28X%5E%7BT%7DX%29%5E%7B-1%7DX%5E%7BT%7DY)
    实际求解过程中，当![X^{T}X](https://private.codecogs.com/gif.latex?X%5E%7BT%7DX)可逆且![X](https://private.codecogs.com/gif.latex?X)维度较小时，可以采用这种解析法来进行求解，但是当![X^{T}X](https://private.codecogs.com/gif.latex?X%5E%7BT%7DX)不可逆，或![X](https://private.codecogs.com/gif.latex?X)维度较高时，求矩阵的逆比较难求，因此采用考虑另外一种思路：梯度下降法。
    解析法实现代码：
```python
import numpy as np
import matplotlib.pyplot as plt
class linearRegression(object):
    def __init__(self, pathOfData):
        self._pathOfData = pathOfData
        self._numFeat = len(open(self._pathOfData).readline().split('\t')) - 1
        self._dataMat = []
        self._labelMat = []
        self._ws = []
        
    def loadDataSet(self):
        fr = open(self._pathOfData)
        for line in fr.readlines():
            lineArr = []
            curLine = line.strip().split('\t')     # get the elments of every row
            for i in range(self._numFeat):
                lineArr.append(float(curLine[i]))  
            self._dataMat.append(lineArr)             # data
            self._labelMat.append(float(curLine[-1]))  # label
        
    def standRegress(self):
        self._dataMat = np.mat(self._dataMat)
        self._labelMat = np.mat(self._labelMat).T
        xTx = self._dataMat.T * self._dataMat
        if(np.linalg.det(xTx) == 0.0):
            print('This matrix is singular, cannot do inverse')
            return
        self._ws = xTx.I * (self._dataMat.T * self._labelMat)
        
pathOfData = './/data//ex0.txt'
LR = linearRegression(pathOfData)
LR.loadDataSet()
LR.standRegress()
# show the result
fig = plt.figure()
ax = fig.add_subplot(111)
# firstly, flatten the matirx, then convert to an array
ax.scatter(LR._dataMat[:,1].flatten().A[0], LR._labelMat[:,0].flatten().A[0])
xCopy = LR._dataMat.copy()
xCopy.sort(0)
yHat = xCopy * LR._ws
ax.plot(xCopy[:,1],yHat)
plt.show()
```
   效果如下：
![](https://img-blog.csdn.net/20181009160458303?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**    2.3 梯度下降法**
    我们先看一下损失函数长什么样，损失函数的图像如图4所示。我们需要找寻全局最小值，直接求解可能太过于困难，因此我们考虑将问题进行分解，每一次优化一点，让模型变得更好一点，通过一次次迭代，最终得到一个最优的模型参数。梯度下降法就是按照这个思路设计的，每一次对参数![](https://private.codecogs.com/gif.latex?)![\theta](https://private.codecogs.com/gif.latex?%5Ctheta)的调整，都会使得损失函数![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)变小，这样多次调整之后，我们就能使得![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)足够小，最终得到理想的模型参数。
![](https://img-blog.csdn.net/20181003213012948?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                        图4 损失函数图像（摘自参考资料[1]）
    那么，具体怎么做？首先回忆一下我们本科学习过的泰勒级数展开式，我们先看一下一阶的泰勒展式：
![f(x_{k}+\delta )=f(x_{k})+f(x_{k})^{'}\delta](https://private.codecogs.com/gif.latex?f%28x_%7Bk%7D&plus;%5Cdelta%20%29%3Df%28x_%7Bk%7D%29&plus;f%28x_%7Bk%7D%29%5E%7B%27%7D%5Cdelta)
    类似的，对于一个向量![X_{k}](https://private.codecogs.com/gif.latex?X_%7Bk%7D)有：
![f(X_{k}+d_{k} )=f(X_{k})+g(X_{k})^{T}d_{k}](https://private.codecogs.com/gif.latex?f%28X_%7Bk%7D&plus;d_%7Bk%7D%20%29%3Df%28X_%7Bk%7D%29&plus;g%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D)
    这个式子说明了什么问题？我们先将这个式子做一下移项变形，有：
![f(X_{k}+d_{k} )-f(X_{k})=g(X_{k})^{T}d_{k}](https://private.codecogs.com/gif.latex?f%28X_%7Bk%7D&plus;d_%7Bk%7D%20%29-f%28X_%7Bk%7D%29%3Dg%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D)
    这样就明显了，如果我们沿着![d_{k}](https://private.codecogs.com/gif.latex?d_%7Bk%7D)方向做一个改变，会引起![g(X_{k})^{T}d_{k}](https://private.codecogs.com/gif.latex?g%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D)大小变化，我们再分析一下![g(X_{k})^{T}d_{k}](https://private.codecogs.com/gif.latex?g%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D)：
![g(X_{k})^{T}d_{k}=||g(X_{k})||*||d_{k}||*cos\alpha](https://private.codecogs.com/gif.latex?g%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D%3D%7C%7Cg%28X_%7Bk%7D%29%7C%7C*%7C%7Cd_%7Bk%7D%7C%7C*cos%5Calpha)
    我们希望损失函数![J(\theta )](https://private.codecogs.com/gif.latex?J%28%5Ctheta%20%29)尽可能的变小，因此要求![g(X_{k})^{T}d_{k}](https://private.codecogs.com/gif.latex?g%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D)尽可能的大，而当![\alpha =0](https://private.codecogs.com/gif.latex?%5Calpha%20%3D0)时，![g(X_{k})^{T}d_{k}](https://private.codecogs.com/gif.latex?g%28X_%7Bk%7D%29%5E%7BT%7Dd_%7Bk%7D)取得最大值![||g(X_{k})||*||d_{k}||](https://private.codecogs.com/gif.latex?%7C%7Cg%28X_%7Bk%7D%29%7C%7C*%7C%7Cd_%7Bk%7D%7C%7C)，这个时候![g(X_{k}),d_{k}](https://private.codecogs.com/gif.latex?g%28X_%7Bk%7D%29%2Cd_%7Bk%7D)同向，这也是为什么梯度下降法要沿着梯度方向来做的原因。
    我们可以再进一步，将问题进行简化，假设当前模型为：![h_{\theta (x_{i})}=\theta _{1}x](https://private.codecogs.com/gif.latex?h_%7B%5Ctheta%20%28x_%7Bi%7D%29%7D%3D%5Ctheta%20_%7B1%7Dx)，损失函数的图像如图5所示。
![](https://img-blog.csdn.net/20181003223638951?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                    图5 梯度下降法示意图（摘自参考资料[1]）
    梯度下降的过程为：
![Repeat \ until \ converge:](https://private.codecogs.com/gif.latex?Repeat%20%5C%20until%20%5C%20converge%3A)
![\theta _{j}:=\theta _{j}-\alpha \frac{\partial J(\theta ) }{\theta _{j}}](https://private.codecogs.com/gif.latex?%5Ctheta%20_%7Bj%7D%3A%3D%5Ctheta%20_%7Bj%7D-%5Calpha%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta%20%29%20%7D%7B%5Ctheta%20_%7Bj%7D%7D)
    其中，![\alpha](https://private.codecogs.com/gif.latex?%5Calpha)为学习率，在经过一次次迭代之后，偏导数会变得越来越小，直至接近于0，这个时候，![\alpha](https://private.codecogs.com/gif.latex?%5Calpha)乘以这个偏导数接近于0，算法就自然收敛了。需要注意的是，如果学习率过大，可能会导致cost function曲线出现上升趋势或循环上升下降，如图6所示，这个时候，需要降低学习率![\alpha](https://private.codecogs.com/gif.latex?%5Calpha)，但是如果![\alpha](https://private.codecogs.com/gif.latex?%5Calpha)过小，又会极大的增加学习所需要的时间，因此，需要选择合适的学习率![\alpha](https://private.codecogs.com/gif.latex?%5Calpha)。
![](https://img-blog.csdn.net/20181006211205910?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                      图6 学习率过大，对应代价函数曲线（摘自参考资料[1]）
**    三、梯度下降法做线性回归**
    偏导数求解：
![](https://img-blog.csdn.net/2018100323202833?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
    注意，对应于一般的参数方程![\theta_{2}x_{2} + \theta _{1}x_{1}+\theta_{0}=0](https://private.codecogs.com/gif.latex?%5Ctheta_%7B2%7Dx_%7B2%7D%20&plus;%20%5Ctheta%20_%7B1%7Dx_%7B1%7D&plus;%5Ctheta_%7B0%7D%3D0)对应于我们常见的![ax+by+c=0](https://private.codecogs.com/gif.latex?ax&plus;by&plus;c%3D0)，![\theta _{0}=c,x_{0} = 1](https://private.codecogs.com/gif.latex?%5Ctheta%20_%7B0%7D%3Dc%2Cx_%7B0%7D%20%3D%201)，这里将所有的参数表示成一个向量只是为了方便表示，其实本质上并没有什么区别，只是表示方法的不同而已。
**    四、代码实现**
```python
import numpy as np
import matplotlib.pyplot as plt
class linearRegression(object):
    def __init__(self, pathOfData, iterations, learningRate):
        self._pathOfData = pathOfData
        self._iterations = iterations
        self._learningRate = learningRate
        self._numFeat = len(open(self._pathOfData).readline().split('\t')) - 1
        self._dataMat = []
        self._labelMat = []
        self._ws = 0.1*np.random.randn(2, 1)
        
    def loadDataSet(self):
        fr = open(self._pathOfData)
        for line in fr.readlines():
            lineArr = []
            curLine = line.strip().split('\t')     # get the elments of every row
            for i in range(self._numFeat):
                lineArr.append(float(curLine[i]))  
            self._dataMat.append(lineArr)             # data
            self._labelMat.append(float(curLine[-1]))  # label
        
    def gradientDescent(self):
        self._dataMat = np.mat(self._dataMat)
        self._labelMat = np.mat(self._labelMat).T
        for i in range(self._iterations):
            diffMat = self._dataMat * self._ws - self._labelMat
            gradient = self._dataMat.T * diffMat
            self._ws = self._ws - self._learningRate * gradient
        
        
    def standRegress(self):
        self._dataMat = np.mat(self._dataMat)
        self._labelMat = np.mat(self._labelMat).T
        xTx = self._dataMat.T * self._dataMat
        if(np.linalg.det(xTx) == 0.0):
            print('This matrix is singular, cannot do inverse')
            return
        self._ws = xTx.I * (self._dataMat.T * self._labelMat)
        
        
pathOfData = './/data//ex0.txt'
LR = linearRegression(pathOfData, 10000, 0.000005)
LR.loadDataSet()
# LR.standRegress()
LR.gradientDescent()
# show the result
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(LR._dataMat[:,1].flatten().A[0], LR._labelMat[:,0].flatten().A[0])
xCopy = LR._dataMat.copy()
xCopy.sort(0)
yHat = xCopy * LR._ws
ax.plot(xCopy[:,1],yHat)
plt.show()
```
    效果如图7所示。
![](https://img-blog.csdn.net/20181009183623256?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxNDI1NTg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
                                                                                  图7 梯度下降法线性回归
~~~~未完待续~~~~~~
参考资料：
[1] Andrew Ng 的 机器学习课程
[2] 机器学习实战
