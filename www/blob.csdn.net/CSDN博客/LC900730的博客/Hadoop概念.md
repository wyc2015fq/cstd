# Hadoop概念 - LC900730的博客 - CSDN博客
2017年11月16日 13:59:27[lc900730](https://me.csdn.net/LC900730)阅读数：108
Jobconf对象指定了作业的各种参数。它授予我们对整个作业如何运行对控制权。当我们在Hadoop集群上运行这个作业时，我们将代码打包成JAR文件(Hadoop会在集群中分发这个包)。我们没有明确指定这个JAR文件对名称，而是在JobConf构造函数中传递一个类，Hadoop会找到这个包含此类对JAR文件。 
在创建JobConf对象后，我们将指定输入输出路径。通过调用FileInputFormat内静态方法addInputPath来定义输入路径，可以是单个文件／目录或者文件模式对路径。同时addInputPath()可被调用多次从而实现多路径输入。
输出路径是FileOutputFormat内静态方法addInputPath来指定。
通过setMapperClass()和setReducerClass()这2个方法来指定要使用的map和reduce类型。
通过setOutputKeyClass()和setOutputValueClass()这2个方法来指定map和reduce函数的输出类型。
Mapreduce作业(job)是客户端执行的单位，包括输入数据MapReduce程序和配置信息。Hadoop通过把作业分成若干个小任务(task)来工作，其包括两种类型的任务：map任务和reduce任务。 
2种类型的节点控制作业执行过程：jobtracker和多个tasktracker。jobtracker通过调度任务在tasktracker上运行，来协调所有运行在系统上的作业。Tasktracker运行任务的同时，把进度报告传送到jobtracker，jobtracker则记录每项任务的整体进展情况。如果其中一个任务失败，jobtracker可以重新调度任务到另一个tasktracker。Hadoop把输入数据划分为等长的小数据发送到MapReduce，称为输入分片(input split)或分片。Hadoop为每个分片(split)创建一个map任务，由它来运行用户自定义的map函数来分析每个分片中的记录。 
拥有许多分片就意味着处理每个分片的时间与处理整个输入时间相比是比较小的。如果我们并行处理每个分片,且分片是小块数据，那么
