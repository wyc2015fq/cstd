# 统计学习方法(-) - LC900730的博客 - CSDN博客
2017年12月16日 23:50:46[lc900730](https://me.csdn.net/LC900730)阅读数：149
结构风险(Structural risk minimization,SRM)是为了防止过拟合而提出来的策略，结构风险最小化等价于正则化(regularization)。在经验风险上加上表示模型复杂度的正则化项(regularizer)或罚项(penalty term)。在假设空间、损失函数以及训练集确定的情况下，结构风险的定义是 
$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+ \lambda J(f)$
其中$J(f)$是模型的复杂度，是定义在假设空间F上的泛函。模型f越复杂，复杂度J(f)就越大。反之模型越简单，复杂度J(f)就越小。复杂度表示对复杂模型的惩罚，$\lambda$$\geq$0是系数，用以权衡经验风险和模型复杂度。
贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。
结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求解最优模型就是求解最优化问题。 
$min\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+ \lambda J(f)$
监督学习问题就变成经验风险或结构风险函数的最优化问题。
### 训练误差与测试误差
假设学习到的模型是Y=$\hat f(X)$,训练误差是模型Y=$\hat f(X)$关于训练数据集的平均损失： 
$R_{emp}(\hat f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$
    其中N是训练样本容量
测试误差是模型Y=$\hat f(X)$关测试数据集的平均损失： 
$e_{test}=\frac{1}{N^\prime}\sum_{i=1}^NL(y_i,f(x_i))$
    其中$N^\prime$是测试样本容量
例如当损失函数是0-1损失时，测试误差就变成了常见测试数据集上的误差率。 
$e_{test}=\frac{1}{N^\prime}\sum_{i=1}^{N^\prime} I(y_i≠\hat f(x_i))$
    这里I是指示函数(indicator function)，即$y_i≠\hat f(x_i)$时候为1，否则为0。
相应地准确率是 
$r_{test}=\frac{1}{N^\prime}\sum_{i=1}^{N^\prime} I(y_i=\hat f(x_i))$
    显然 
$r_{test}+e_{test}=1$
### 过拟合与模型选择
当假设空间含有不同复杂度(如不同的参数个数)的模型时，就需要面临模型选择的问题。
## 正则化与交叉验证
### 正则化
模型选择的典型方法是正则化(regularization)。正则化是结构风险最小化策略的实现，是经验风险上加一个正则化项或罚项。 
正则化一般具有如下形式： 
$min \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f) $
其中第一项是经验风险，第2项是正则化项。$\lambda \ge 0是调整两者之间关系的系数 $。
### 泛化误差
$R_{emp}(\hat f)=E_p[L(y_i,\hat f(X))]=\int_{x✖️y}L(y_i,\hat f(x))P(x,y)dxdy $
泛化误差是所学习到的模型的期望风险。
