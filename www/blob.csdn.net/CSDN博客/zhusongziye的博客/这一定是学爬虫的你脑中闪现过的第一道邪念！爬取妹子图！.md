# 这一定是学爬虫的你脑中闪现过的第一道邪念！爬取妹子图！ - zhusongziye的博客 - CSDN博客





2018年11月03日 20:10:37[zhusongziye](https://me.csdn.net/zhusongziye)阅读数：130








相信很多小伙伴最初写爬虫都是从爬取图片开始的，那么我们今天就来分析一下，一个最简单的爬取图片的项目是怎么实现的。本文目标受众为新入坑爬虫的小白，大神可略过。

重点：本代码只用于网友学习，其资源所有权归网站所有，不允许恶意传播；爬取时请遵守爬虫公约，不要恶意爬取。

首先，我们来看一下爬取结果。

![](https://img-blog.csdnimg.cn/20181103200757526.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXNvbmd6aXll,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/20181103200808220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXNvbmd6aXll,size_16,color_FFFFFF,t_70)

以上两张截图是我在代码运行中的截图。如果感兴趣的话，请往下看吧。



在爬取图片之前，我们先要确定爬取的网站，本文示例中爬取的网站为优美网。网站地址：    http://www.umei.cc/tags/shaonv.htm    之所以选择这个网站，是因为这个网站基本无反扒设置，比较适合新手。



我们先打开浏览器，启用调试，然后打开网页地址，从Network标签页中可以看到访问的网页数据，分析response，可知，网页图集中的url存储在了class="TypeBigPics" 的a标签下。同时可以发现，图集的名称就是a属性的内容。

![](https://img-blog.csdnimg.cn/2018110320083638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXNvbmd6aXll,size_16,color_FFFFFF,t_70)

图片请放大查看



接下来，我们点击下一页，观察浏览器中url的变化。第一页的地址为：http://www.umei.cc/tags/shaonv.htm    ，第二页的地址为：http://www.umei.cc/tags/shaonv_2.htm，可见只是在最后的shaonv后面添加了"_2"，同理我们可以拼接出第三页，第四页。然后我们发现，如果第一页写成http://www.umei.cc/tags/shaonv_1.htm 也是可以同样访问的，所以我们的url可确定为http://www.umei.cc/tags/shaonv + “_n” + “htm”，其中n为第几页。



在分析完主网页之后，我们需要分析图集中网页和图片的关系。随意点开一个图集，同样在浏览器中调试观察可知，网页中的图片地址存储在class="ImageBody" 的 div标签中，我们只要从该标签中提取url并存储即可。点击图片，访问图集中的第二张照片，观察浏览器中的url变化。发现原url后缀添加了_n，变化方式同之前分析的主页变化方式一样。但是有一点需要注意，第一张图片不能向我们之前访问主页那样 + “_1”来访问，所以第一张图片需要单独处理。

![](https://img-blog.csdnimg.cn/20181103200856880.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXNvbmd6aXll,size_16,color_FFFFFF,t_70)

图片请放大查看



至此，我们已经分析完了网站结构，也在脑海中有了大致的思路。通过访问主页，从class="TypeBigPics" 的a标签下获取图集的url，不同的主页url前缀相同，后缀用“_n”变化得来；获取到图集的url后，访问图集url，从class="ImageBody"的a标签中获取图片url，不同的图片url前缀相同，后缀用“_n”变化得来，第一张图片需要单独处理。



有了整体架构，再加上一些必要的判断、即可得到最终的代码。代码如下。

本段代码没有用代理ip，也没有用多线程，如果想要用代理和多线程的小伙伴，不妨自己动手更改下。

链接: 接: https://pan.baidu.com/s/1Xz_S8K2yRDKAHKO9OWfdtw 密码: 密码: gecs

骚年，量力而行哦~

```
1import re
  2import requests
  3import time,os
  4from bs4 import BeautifulSoup
  5
  6def create_dir(path):
  7    '''
  8    :param path: 给定一个文件名
  9    :return: 在./image下创建文件夹并返回路径
 10    '''
 11    url = "./image/" + path
 12    #判断当前文件夹是否存在
 13    if os.path.exists(url):
 14        print("文件夹已经存在")
 15    else:
 16        os.makedirs(url)
 17        print("%s创建成功"%url)
 18    #将当前文件夹作为路径返回
 19    return url
 20
 21def get_image(url,forder):
 22    '''
 23    :param url: 图集URL
 24    :param forder: 存储的文件夹
 25    :return:
 26    '''
 27    #从图集URL中提取图片URL并存储图像
 28    #设置heander和Agent参数
 29    Agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36"
 30    headers = {"User-Agent": Agent}
 31    #分析URL得出，每个图集最多不超过50张，此处我们假设有100张
 32    for i in range(1,100):
 33        #第一张图片的url不需要改
 34        if i == 1:
 35            html_url = url
 36        #第二张开始，需要更改url
 37        else:
 38            html_url = url.split(".htm")[0] + "_" + str(i) + ".htm"
 39        r = requests.get(html_url,headers=headers)
 40
 41        if r.status_code == 200:
 42            soup = BeautifulSoup(r.content)
 43            #分析网页源码，可知class：ImageBody的div里面存储图片地址
 44            find_image_div = soup.find_all("div",attrs={'class':'ImageBody'})
 45            find_image_div = str(find_image_div)
 46            #在div中匹配图片地址
 47            pattern = re.compile(r'((http)?:\/\/)[^\s]+.jpg')
 48            #尝试用正则获取URL。有些图片中会获取不到，所以用了try。第二页第四个图集的第六张照片就匹配不到
 49            try:
 50                img_url = pattern.search(find_image_div).group(0)
 51            except:
 52                continue
 53            #从图片地址中获取图片并写入文件
 54            r = requests.get(img_url)
 55            image_name = forder + "/" +str(i) + ".jpg"
 56            if os.path.exists(image_name):
 57                time.sleep(1)
 58                print("文件已存在")
 59            else:
 60                with open(image_name,'wb') as f :
 61                    f.write(r.content)
 62                    #因为我们没有使用代理，为避免反扒，延迟1.5s
 63                    time.sleep(1.5)
 64                    print("%s已存储"%image_name)
 65        else:
 66            print("当前图集爬取完成,切换下一图集")
 67            break
 68
 69def get_image_url(url):
 70    '''
 71    :param url: 网页URL
 72    :return:
 73    '''
 74    #从网页中提取图集的URL
 75    Agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36"
 76    headers = {"User-Agent":Agent}
 77    for i in range(7,100):
 78        #从URL中分析可知，不同图集页面只是尾缀不一样
 79        get_url = url.split(".htm")[0] + "_" + str(i) + ".htm"
 80        r = requests.get(get_url,headers=headers)
 81        #如果页面可正常访问，则提取图集地址，否则跳出循环
 82        if r.status_code == 200:
 83            soup = BeautifulSoup(r.content)
 84            #分析源代码可知，图集地址存储在class：TypeBigPics的a标签中
 85            find_a = soup.find_all("a",attrs={'class':'TypeBigPics'})
 86            for n,a in enumerate(find_a):
 87                string_a = str(a)
 88                #提取图集地址的正则
 89                pattern_url = re.compile(r'((http)?:\/\/)[^\s]+.htm')
 90                #提取图集名称的正则，用来创建文件夹
 91                pattern_forder_name = re.compile(r'<div class="ListTit">(.*)</div></a>')
 92                #获取图集地址及图集名称
 93                image_url = pattern_url.search(string_a).group(0)
 94                name = pattern_forder_name.search(string_a).group(1)
 95                print("第%s页第%s个图集开始爬取"%(i,n+1))
 96                print("-------------------------------------")
 97                #用图集名称创建文件夹
 98                forder = create_dir(name)
 99                #将图集中的图片存储在文件夹中
100                get_image(image_url,forder)
101                print("-------------------------------------")
102        else:
103            print("超出最大网页范围")
104            break
105
106if __name__ == "__main__":
107    get_image_url("http://www.umei.cc/tags/shaonv.htm")
108    print("爬取结束")
```





