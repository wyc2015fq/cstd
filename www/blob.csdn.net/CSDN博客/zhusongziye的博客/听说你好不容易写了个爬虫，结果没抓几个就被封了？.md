# 听说你好不容易写了个爬虫，结果没抓几个就被封了？ - zhusongziye的博客 - CSDN博客





2018年10月18日 21:47:00[zhusongziye](https://me.csdn.net/zhusongziye)阅读数：174








![](https://img-blog.csdn.net/20181018214604985?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXNvbmd6aXll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

近来知乎上如雨后春笋般冒出了大把大把的爬虫教程。这是好事，学了 Python 基础的同学们可以很轻松地搜出许多练手的例子。不过我不是针对谁，我是说网上绝大多数的爬虫教程，其实都缺乏可操作性。

是的，也包括我自己写过的。

主要原因有两点：
- 
教程是死的，网站是活的。页面会改版，接口会更新。一个爬虫教程的案例三个月之后还能有效已经是万幸了。比如我自己教程里的查天气案例，接口改动过很多次，数据也早就不更新。但发出去的文章被转发几次后就很难再维护更新了。我也只能在自己的论坛上发布更新消息和问题答疑。有需要的同学请在论坛bbs.crossincode.com 上搜索 **查天气**，进入帖子查看。

- 
但凡数据比较有价值的网站，一定都会有反爬措施，既是对数据的保护，也是避免机器流量干扰到正常用户的体验。所以光是写个网络请求，几乎不可能成功拿到数据。反爬措施千千万，应对反爬的手段万万千，这就是个不停斗智斗勇的过程，不存在一个教程就教会的万金油方法。




反爬里面最常见的一种手段就是，判断你的请求频率。如果你短时间内发送了大量的请求，甭管你是不是人，先封你账号或 IP 一段时间再说。所以，这就成了一个矛盾的地方：爬得太快会被封，爬得太慢又很耗时间。一般教程也许会说句：想要提升抓取效率并且降低被封的风险，可以使用代理 IP。然而这话说着倒轻松，网上免费的代理 IP 也不少，但每次找来能用的却没几个。总不至于每次为了写点小练习还去花钱买很多付费代理吧。况且现如今你真要买，也还未必能顺利买到可用的。

于是我们决定自己动手，一劳永逸地解决这个老大难问题：**实现一个自动获取可用代理 IP 的接口**。

基本思路还是从网上的几大免费平台获取 IP 地址，不同的是我们定期去检测 IP 的可用性。在调用接口时，提供可用性最高的 IP 地址列表。

网页上列出了几十个最新的推荐 IP，只是临时找几个做测试，可直接访问查看。

网页地址：http://lab.crossincode.com/proxy/

（点击文章下发 阅读原文 可进入）

API 接口地址：http://lab.crossincode.com/proxy/get/

请求方法：GET

频率限制：不高于3秒1次

请求示例：

获取 5 个 IP

http://lab.crossincode.com/proxy/get/?num=5

获取 5 个 HTTPS 的 IP

http://lab.crossincode.com/proxy/get/?num=5&head=https

返回结果示例：

```
{
  "proxies": [
    {
      "http": "117.90.0.225:9000"
    },
    {
      "http": "186.154.146.26:8080"
    },
    {
      "http": "175.155.25.27:808"
    },
    {
      "http": "124.88.67.52:843"
    },
    {
      "http": "119.5.0.7:808"
    }
  ],
  "code": 1
}
```

于是，在你做爬虫练习时，只要通过接口获取几个 IP，作为你请求的 proxy，即可大大提高抓取的效率。目前我们自己的爬虫项目都在使用此接口。

不过仍然要说明，即使经过多次验证，也无法保证一个 IP 的绝对可用性。不同时间不同地域不同网络都有可能导致请求超时或失效。所以你的代码中也还是需要自己做好相应的异常处理。这是个概率问题，工具的作用只是尽可能提高概率。

我们不生产 IP，我们只是互联网的搬运工。

完整的接口参数说明，以及项目的源代码：

接口说明及代码地址： 

https://github.com/zx576/proxypool



