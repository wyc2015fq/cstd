# PVANET算法笔记 - AI之路 - CSDN博客





2018年03月09日 19:58:25[AI之路](https://me.csdn.net/u014380165)阅读数：2502








论文：PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection 

论文链接：[https://arxiv.org/abs/1608.08021](https://arxiv.org/abs/1608.08021)

代码链接：[https://github.com/sanghoon/pva-faster-rcnn](https://github.com/sanghoon/pva-faster-rcnn)
RCNN系列的object detection算法总体上分为特征提取、RPN网络和分类回归三大部分，Faster RCNN的效果虽好，但是速度较慢，这篇文章的出发点是改进Faster CNN的特征提取网络，也就是用PVANET来提取特征作为Faster RCNN网络中RPN部分和RoI Pooling部分的输入，**改进以后的Faster RCNN可以在基本不影响准确率的前提下减少运行时间**。我们知道加宽和加深网络向来是提升网络效果的两个主要方式，因为要提速，所以肯定做不到同时加宽和加深网络，因此PVANET网络的总体设计原则是：less channels with more layers，深层网络的训练问题可以通过residual结构来解决。**另外因为PVANET结构的设计不涉及网络量化等加速操作，所以如果要进一步加速的话可以再使用加速算法**。总体来说PVANET网络主要利用了以下三个思想： 

1、Concatenated rectified linear unit (**C.ReLU**) is applied to the early stage of our CNNs (i.e., first several layers from the network input) to reduce the number of computations by half without losing accuracy. 

2、**Inception** is applied to the remaining of our feature generation sub-network. An Inception module produces output activations of different sizes of receptive fields, so that increases the variety of receptive field sizes in the previous layer. We observed that stacking up Inception modules can capture widely varying-sized objects more effectively than a linear chain of convolutions. 

3、We adopted the idea of **multi-scale representation like HyperNet that** combines several intermediate outputs so that multiple levels of details and non-linearities can be considered simultaneously.
**C.ReLU**

C.ReLU（concatenated ReLU）主要是用来减少计算量。C.ReLU的提出来自于对CNN网络的前面一些卷积层输出的观察：**In the early stage, output nodes tend to be “paired” such that one node’s activation is the opposite**。也就是说输出基本上是负相关的（可以参看C.ReLU原论文），这样相当于卷积核存在冗余，有一半的计算是可以省掉的。因此**C.ReLU的定义也非常简单：C.ReLU(x)=[ReLU(x), ReLU(-x)]，这个中括号就是concatenation操作，x就是卷积层的输出，Figure1中的Negation操作就是对x取负数的操作**。原文如下：C.ReLU reduces the number of output channels by half, and doubles it by simply concatenating the same outputs with negation, which leads to 2x speed-up of the early stage without losing accuracy. 另外在本文中，和原始的C.ReLU相比，作者还额外添加了scale/shift层用来做尺度变换和平移操作，相当于一个线性变换，改变原来完全对称的数据分布，如下图Figure1所示。原文如下：**Compared to the original C.ReLU, we append scaling and shifting after concatenation to allow that each channel’s slope and activation threshold can be different from those of its opposite channel.**
![这里写图片描述](https://img-blog.csdn.net/20180309195341434?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**Inception**

对于size较大的object检测，需要用卷积核size较大（也就是receptive field较大）的卷积层去提取特征，反之亦然。这样看来，卷积核尺度多样的Inception结构就成为了作者的不二之选。原文如下：We found that Inception can be one of the most cost-effective building block for capturing both small and large objects in an input image. 
**Figure2展示了Inception结构的receptive filed多样性**。图中一共包含3个Inception结构，每个Inception结构中包含7种不同size的receptive field size，黄色矩形框的长度表示某一个receptive field size的feature map的channel数量。可以看出第一个Inception结构中feature map的receptive field size主要是1、3、5，第三个Inceptive结构中则较为均匀地分布在从1到13的receptive field size中，这样就保证了高层部分receptive field的多样性。 
![这里写图片描述](https://img-blog.csdn.net/20180309195610511?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
Figure3是作者使用的Inception结构，其中右边和左边相比多了stride=2，所以输出的feature map的size减半。 
![这里写图片描述](https://img-blog.csdn.net/20180309195628896?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**HyperNet**

多层特征融合可以尽可能利用细节和抽象特征，这种做法在object detection领域也常常用到，比如SSD，但是在融合的时候需要注意融合的特征要尽量不冗余，否则就白白增加计算量了。 
**Table1是PVANET的网路结构图**。前半部分采用常规的卷积，后半部分采用Inception结构，另外residual结构的思想也贯穿了这两部分结构。之所以引入residual结构，主要还是回到作者一开始提到的网络结构设计原则：less channels with more layers。受限于深层网络的训练瓶颈，所以引入residual结构。C.ReLU那一列包含1*1、K*K和1*1卷积，其中K*K部分就是前面Figure1的C.ReLU结构，而前后的两个1*1卷积是做通道的缩减和还原，主要还是为了减少计算量。强调下**关于特征层融合的操作：将conv3_4进downscale、将conv5_4进行upscale，这样这两层feature map的size就和conv4_4的输出size一样，然后将二者和conv4_4进行concate得到融合以后的特征。**
![这里写图片描述](https://img-blog.csdn.net/20180309195706483?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
博客一开始的时候介绍到PVANET的作用主要是用来提取特征的，提取到的特征需要作为Faster RCNN的RPN网络和RoI Pooling层的输入，具体描述如下： 

1、For computational efficiency, only the first 128 channels in convf are fed into the region proposal network (RPN).  

2、R-CNN takes all 512 channels in convf. For each RoI, 6x6x512 tensor is generated by RoI pooling, and then passed through a sequence of fully-connected layers of “4096 - 4096 -(21+84)” output nodes.4.
**实验结果：**

Table2是在VOC2007数据集上的不同配置的PVANET网络实验结果。 
![这里写图片描述](https://img-blog.csdn.net/20180309195745238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
Table3是在VOC2012数据集上的PVANET网络和Faster RCNN、RFCN网络的对比。 
![这里写图片描述](https://img-blog.csdn.net/20180309195808465?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)











