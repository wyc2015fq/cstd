# Generative Adversarial Nets（译） - 数据之美的博客 - CSDN博客
2019年02月16日 17:13:36[看穿数据之美](https://me.csdn.net/oppo62258801)阅读数：179
转自 [https://blog.csdn.net/wspba/article/details/54577236](https://blog.csdn.net/wspba/article/details/54577236)
仅供参考，如有翻译不到位的地方敬请指出。
论文地址：[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)
[论文翻译](https://www.baidu.com/s?wd=%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)：[XlyPb](http://blog.csdn.net/wspba/article/details/54577236)（[http://blog.csdn.net/wspba/article/details/54577236](http://blog.csdn.net/wspba/article/details/54577236)）
## 摘要
我们提出了一个通过对抗过程估计生成模型的新框架，在新框架中我们同时训练两个模型：一个用来捕获数据分布的生成模型G，和一个用来估计样本来自训练数据而不是G的概率的判别模型D，G的训练过程是最大化D产生错误的概率。这个框架相当于一个极小化极大的双方博弈。在任意函数G 和D 的空间中存在唯一的解，其中G恢复训练数据分布，并且D处处都等于1212。 在G和D 由多层感知器定义的情况下，整个系统可以用反向传播进行训练。在训练或生成样本期间不需要任何马尔科夫链或展开的近似推理网络。 实验通过对生成的样品进行定性和定量评估来展示这个框架的潜力。
## 1.介绍
深度学习的任务是寻找丰富的层次模型，能够在人工智能领域里用来表达各种数据的概率分布，例如自然图像，包含语音的音频波形和自然语言语料库中的符号等。到目前为止，在深度学习领域，目前为止最成功的的模型之一就是判别式模型，通常它们将高维丰富的感知器输入映射到类标签上。这些显著的成功主要是基于反向传播和丢弃算法来实现的，特别是具有特别良好梯度的分段线性单元。由于在最大似然估计和相关策略中出现的许多难以解决的概率计算的困难，以及很难利用在生成上下文中时使用分段线性单元的好处，深度生成模型的影响很小。我们提出一个新的生成模型估计程序，来分步处理这些难题。
在提到的对抗网络框架中，生成模型对抗着一个对手：一个学习去判别一个样本是来自模型分布还是数据分布的判别模型。生成模型可以被认为是一个伪造团队，试图产生假货并在不被发现的情况下使用它，而判别模型类似于警察，试图检测假币。在这个游戏中的竞争驱使两个团队改进他们的方法，直到真假难分为止。
这个框架可以针对多种模型和优化算法提供特定的训练算法。在这篇文章中，我们探讨了生成模型通过将随机噪声传输到多层感知机来生成样本的特例，同时判别模型也是通过多层感知机实现的。我们称这个特例为对抗网络。在这种情况下，我们可以仅使用非常成熟的反向传播和丢弃算法训练两个模型，生成模型在生成样本时只使用前向传播算法。并且不需要近似推理和马尔可夫链作为前题。
## 2.相关工作
含隐变量的有向图模型可以由含隐变量的无向图模型替代，例如受限制波兹曼机（RBM），深度波兹曼机（DBM）和它们很多的变种。这些模型之间的相互影响可以被表达为非标准化的势函数的乘积，再通过随机变量的所有状态的全局整合来标准化。这个数量（配分函数）和它的梯度的估算是很棘手的，尽管他们能够依靠马尔可夫链和蒙特卡罗（MCMC）算法来估计，同时依靠MCMC算法的混合也会引发一个严重的问题。
深度信念网络（DBN）是一个包含一个无向层和若干有向层的混合模型。当使用一个快速逐层训练法则时，DBNS 会引发无向模型和有向模型相关的计算难题。
不是利用似然函数的估计或约数的选择准则已经被提出来了，例如分数匹配和噪音压缩评估（NCE）。他们都需要知道先验概率密度知识用来分析指定一个规范化的常量。请注意,许多有趣的带有一些隐层变量的生成模型（如DBN和DBM），它们甚至不需要一些难以处理的非标准化的概率密度先验知识。一些模型如自动编码降噪机和压缩编码的学习准则与分数匹配在RBM上的应用非常相似。在NCE中，使用一个判别训练准则来拟合一个生成模型。然而,生成模型常常被用来判别从一个固定噪音分布中抽样生成的数据，而不是拟合一个独立的判别模型。由于NCE使用一个固定的噪音分布，仅仅是从观测变量的一个小子集中学习到一个大致正确的分布后，模型的学习便急剧减慢。 
最后，一些技术并没有用来明确定义概率分布，而是用来训练一个生成器来从期望的分布中拟合出样本。这个方法优势在于这些机器学习算法能够设计使用反向传播算法训练。这个领域最近比较突出的工作包含生成随机网络（GSN），它扩展了广义的除噪自动编码器:两者都可以看作是定义了一个参数化的马尔可夫链，即一个通过执行生成马尔科夫链的一个步骤来学习机器参数的算法。同GSNs相比，对抗网络不需要使用马尔可夫链来采样。由于对抗网络在生成阶段不需要循环反馈信息，它们能够更好的利用分段线性单元，这可以提高反向传播的效率。大部分利用反向传播算法来训练生成器的例子包括变分贝叶斯自动编码和随机反向传播.
## 3.对抗网络
当模型是多层感知器时，对抗模型框架是最直接应用的。为了学习生成器关于数据xx上的分布pgpg, 我们定义输入噪声的先验变量pz(z)pz(z),然后使用G(z;θg)G(z;θg)来代表数据空间的映射。这里GG是一个由含有参数θgθg 的多层感知机表示的可微函数。我们再定义了一个多层感知机D(x;θd)D(x;θd)用来输出一个单独的标量。D(x)D(x) 代表xx 来自于真实数据分布而不是pgpg的概率，我们训练DD来最大化分配正确标签给不管是来自于训练样例还是GG生成的样例的概率.我们同时训练GG来最小化log(1−D(G(z)))log(1−D(G(z)))。换句话说，DD和GG的训练是关于值函数V(G,D)V(G,D)的极小化极大的二人博弈问题： 
minGmaxDV(D,G)=𝔼x∼pdata(x)[logD(x)]+𝔼z∼pz(z)[log(1−D(G(z)))].minGmaxDV(D,G)=Ex∼pdata(x)[log⁡D(x)]+Ez∼pz(z)[log⁡(1−D(G(z)))].
在下一节中，我们提出了对抗网络的理论分析，基本上表明基于训练准则可以恢复数据生成分布，因为GG和DD被给予足够的容量，即在非参数极限。如图11展示了该方法的一个非正式却更加直观的解释。实际上，我们必须使用迭代数值方法来实现这个过程。在训练的内部循环中优化DD到完成的计算是禁止的，并且有限的数据集将导致过拟合。相反，我们在优化DD的kk个步骤和优化GG的一个步骤之间交替。只要GG变化足够慢，可以保证DD保持在其最佳解附近。该过程如算法11所示。
实际上，方程11可能无法为GG提供足够的梯度来学习。训练初期，当GG的生成效果很差时，DD会以高置信度来拒绝生成样本，因为它们与训练数据明显不同。因此，log(1−D(G(z)))log(1−D(G(z)))饱和。因此我们选择最大化logD(G(z))logD(G(z))而不是最小化log(1−D(G(z)))log(1−D(G(z))) 来训练GG，该目标函数使GG和DD的动力学稳定点相同，并且在训练初期，该目标函数可以提供更强大的梯度。 
![这里写图片描述](https://img-blog.csdn.net/20170116195046434?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图1.训练对抗的生成网络时，同时更新判别分布（DD，蓝色虚线）使D能区分数据生成分布pxpx（黑色虚线）中的样本和生成分布pgpg (GG，绿色实线) 中的样本。下面的水平线为均匀采样zz的区域，上面的水平线为xx的部分区域。朝上的箭头显示映射x=G(z)x=G(z)如何将非均匀分布pgpg作用在转换后的样本上。GG在pgpg高密度区域收缩，且在pgpg的低密度区域扩散。(a)考虑一个接近收敛的对抗的模型对：pgpg与pdatapdata相似，且DD是个部分准确的分类器。(b)算法的内循环中，训练DD来判别数据中的样本，收敛到：D∗(x)=pdata(x)pdata(x)+pg(x)D∗(x)=pdata(x)pdata(x)+pg(x)。(c)在GG的11次更新后，DD的梯度引导G(z)G(z)流向更可能分类为数据的区域。(d)训练若干步后，如果GG和DD性能足够，它们接近某个稳定点并都无法继续提高性能，因为此时pg=pdatapg=pdata。判别器将无法区分训练数据分布和生成数据分布，即D(x)=12D(x)=12。
**算法1.**生成对抗网络的minibatch随机梯度下降训练。判别器的训练步数，kk，是一个超参数。在我们的试验中使用k=1k=1，使消耗最小。 
![这里写图片描述](https://img-blog.csdn.net/20170116200019244?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 4.理论结果
当z∼pzz∼pz时，获得样本G(z)G(z)，产生器GG隐式的定义概率分布pgpg为G(z)G(z)获得的样本的分布。因此，如果模型容量和训练时间足够大时，我们希望算法11收敛为pdatapdata的良好估计量。本节的结果是在非参数设置下完成的，例如，我们通过研究概率密度函数空间中的收敛来表示具有无限容量的模型。
我们将在4.14.1节中显示，这个极小化极大问题的全局最优解为pg=pdatapg=pdata。我们将在4.24.2节中展示使用算法11来优化等式11，从而获得期望的结果。
**4.1全局最优：pg=pdatapg=pdata**
首先任意给生成器GG，考虑最优判别器DD。 
**命题1.**固定GG，最优判别器DD为： 
D∗G(x)=pdata(x)pdata(x)+pg(x)DG∗(x)=pdata(x)pdata(x)+pg(x)
**证明.**给定任意生成器GG，判别器DD的训练标准为最大化目标函数V(G,D)V(G,D)： 
V(G,D)=∫xpdata(x)log(D(x))dx+∫zpz(z)log(1−D(g(z)))dz=∫xpdata(x)log(D(x))+pg(x)log(1−D(x))dxV(G,D)=∫xpdata(x)log⁡(D(x))dx+∫zpz(z)log⁡(1−D(g(z)))dz=∫xpdata(x)log⁡(D(x))+pg(x)log⁡(1−D(x))dx
对于任意的(a,b)∈ℝ2∖{0,0}(a,b)∈R2∖{0,0}，函数y→alog(y)+blog(1−y)y→alog⁡(y)+blog⁡(1−y)在[0,1][0,1]中的aa+baa+b 处达到最大值。无需在Supp(pdata)∪Supp(pg)Supp(pdata)∪Supp(pg)外定义判别器，证毕。
注意到，判别器DD的训练目标可以看作为条件概率P(Y=y|x)P(Y=y|x)的最大似然估计，当y=1y=1时，xx来自于pdatapdata；当y=0y=0时，xx来自pgpg。公式11中的极小化极大问题可以变形为： 
C(G)=maxDV(G,D)=𝔼x∼pdata[logD∗G(x)]+𝔼z∼pz[log(1−D∗G(G(z)))]=𝔼x∼pdata[logD∗G(x)]+𝔼x∼pg[log(1−D∗G(x))]=𝔼x∼pdata[logpdata(x)Pdata(x)+pg(x)]+𝔼x∼pg[logpg(x)pdata(x)+pg(x)]C(G)=maxDV(G,D)=Ex∼pdata[log⁡DG∗(x)]+Ez∼pz[log⁡(1−DG∗(G(z)))]=Ex∼pdata[log⁡DG∗(x)]+Ex∼pg[log⁡(1−DG∗(x))]=Ex∼pdata[log⁡pdata(x)Pdata(x)+pg(x)]+Ex∼pg[log⁡pg(x)pdata(x)+pg(x)]
**定理1.**当且仅当pg=pdatapg=pdata时，C(G)C(G)达到全局最小。此时，C(G)C(G)的值为−log4−log⁡4。
**证明.**pg=pdatapg=pdata时，D∗G(x)=12DG∗(x)=12（公式22）。再根据公式44可得，C(G)=log12+log12=−log4C(G)=log⁡12+log⁡12=−log⁡4。为了看仅当pg=pdatapg=pdata时C(G)C(G)是否是最优的，观测： 
𝔼x∼pdata[−log2]+𝔼x∼pg[−log2]=−log4Ex∼pdata[−log⁡2]+Ex∼pg[−log⁡2]=−log⁡4
然后从C(G)=V(D∗G,G)C(G)=V(DG∗,G)减去上式，可得： 
C(G)=−log(4)+KL(pdata‖‖‖pdata+pg2)+KL(pg‖‖‖pdata+pg2)C(G)=−log⁡(4)+KL(pdata‖pdata+pg2)+KL(pg‖pdata+pg2)
其中KL为Kullback–Leibler散度。我们在表达式中识别出了模型判别和数据生成过程之间的Jensen–Shannon散度： 
C(G)=−log(4)+2⋅JSD(pdata‖‖pg)C(G)=−log⁡(4)+2⋅JSD(pdata‖pg)
由于两个分布之间的Jensen–Shannon散度总是非负的，并且当两个分布相等时，值为00。因此C∗=−log(4)C∗=−log⁡(4)为C(G)C(G)的全局极小值，并且唯一解为pg=pdatapg=pdata，即生成模型能够完美的复制数据的生成过程。
**4.2算法11的收敛性**
**命题2.**如果GG和DD有足够的性能，对于算法11中的每一步，给定GG时，判别器能够达到它的最优，并且通过更新pgpg来提高这个判别准则。 
𝔼x∼pdata[logD∗G(x)]+𝔼x∼pg[log(1−D∗G(x))]Ex∼pdata[log⁡DG∗(x)]+Ex∼pg[log⁡(1−DG∗(x))]
则pgpg收敛为pdatapdata。
**证明.**如上述准则，考虑V(G,D)=U(pg,D)V(G,D)=U(pg,D)为关于pgpg的函数。注意到U(pg,D)U(pg,D) 
为pgpg的凸函数。该凸函数上确界的一次导数包括达到最大值处的该函数的导数。换句话说，如果f(x)=supα∈fα(x)f(x)=supα∈Afα(x)且对于每一个αα，fα(x)fα(x) 是关于xx的凸函数，那么如果β=argsupα∈fα(x)β=arg⁡supα∈Afα(x)，则∂fβ(x)∈∂f∂fβ(x)∈∂f。这等价于给定对应的GG和最优的DD，计算pgpg的梯度更新。如定理11所证明，supDU(pg,D)supDU(pg,D)是关于pgpg的凸函数且有唯一的全局最优解，因此，当pgpg的更新足够小时，pgpg收敛到pxpx，证毕。
实际上，对抗的网络通过函数G(z;θg)G(z;θg)表示pgpg分布的有限簇，并且我们优化θgθg而不是pgpg本身。使用一个多层感知机来定义GG在参数空间引入了多个临界点。然而，尽管缺乏理论证明，但在实际中多层感知机的优良性能表明了这是一个合理的模型。
## 5.实验
我们在一系列数据集上，包括MNIST、多伦多面数据库（TFD）和CIFAR-1010，来训练对抗网络。生成器的激活函数包括修正线性激活（ReLU）和sigmoid 激活，而判别器使用maxout激活。Dropout被用于判别器网络的训练。虽然理论框架可以在生成器的中间层使用Dropout和其他噪声，但是这里仅在生成网络的最底层使用噪声输入。
我们通过对GG生成的样本应用高斯Parzen窗口并计算此分布下的对数似然，来估计测试集数据的概率。高斯的σσ参数通过对验证集的交叉验证获得。Breuleux等人引入该过程且用于不同的似然难解的生成模型上。结果报告在表11中。该方法估计似然的方差较大且高维空间中表现不好，但确实目前我们认为最好的方法。生成模型的优点是可采样而不直接估计似然，从而促进了该模型评估的进一步研究。 
![这里写图片描述](https://img-blog.csdn.net/20170116202338356?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
表1.基于Parzen窗口的对数似然估计。MNIST上报告的数字是测试集上的平均对数似然以及在样本上平均计算的标准误差。在TFD上，我们计算数据集的不同折之间的标准误差，在每个折的验证集上选择不同的σσ。在TFD上，在每一个折上对σσ进行交叉验证并计算平均对数似然函数。对于MNIST，我们与真实值（而不是二进制）版本的数据集的其他模型进行比较
训练后的生成样本如下图2、2、图33所示。虽然未声明该方法生成的样本由于其它方法生成的样本，但我们相信这些样本至少和文献中更好的生成模型相比依然有竞争力，也突出了对抗的框架的潜力。 
![这里写图片描述](https://img-blog.csdn.net/20170116202518033?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图2。来自模型的样本的可视化。最右边的列示出了相邻样本的最近训练示例，以便证明该模型没有记住训练集。样品是完全随机抽取，而不是精心挑选。与其他大多数深度生成模型的可视化不同，这些图像显示来自模型分布的实际样本。此外，这些样本是完全不相关的，因为，采样过程并不依赖马尔科夫链混合。a) MNIST；b) TFD；c) CIFAR-10（全连接模型）；d) CIFAR-10（卷积判别器和“解卷积”生成器）
![这里写图片描述](https://img-blog.csdn.net/20170116202608440?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图3.通过在完整模型的zz空间的坐标之间进行线性内插获得的数字。
## 6.优势和劣势
新框架相比以前的模型框架有其优缺点。缺点主要为pg(x)pg(x)的隐式表示，且训练期间，DD和GG必须很好地同步（尤其，不更新DD时GG不必[过度训练](https://www.baidu.com/s?wd=%E8%BF%87%E5%BA%A6%E8%AE%AD%E7%BB%83&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，为避免“Helvetica情景”。否则，xx值相同时GG丢失过多zz值以至于模型pdatapdata多样性不足），正如Boltzmann机在学习步间的不断更新。其优点是无需马尔科夫链，仅用反向传播来获得梯度，学习间无需推理，且模型中可融入多种函数。表22总结了生成对抗网络与其他生成模型方法的比较。 
![这里写图片描述](https://img-blog.csdn.net/20170116202834050?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
表2.生成建模中的挑战：对涉及模型的每个主要操作的深度生成建模的不同方法遇到的困难的总结。
上述优势主要在计算上。对抗的模型也可能用数据实例，仅用流过判别器的梯度，从间接更新的生成模型中获得一些统计优势。这意味输入部分未直接复制进生成器的参数。对抗的网络的另一优点是可表示很尖，甚至退化的分布，而基于马尔科夫链的方法为混合模式而要求模糊的分布。
## 7.结论和未来研究方向
该框架允许许多直接的扩展：
- 条件生成模型p(x∣c)p(x∣c)可以通过将cc作为GG和DD的输入来获得。
- 给定xx，可以通过训练一个任意的模型来学习近似推理，以预测zz。这和wake-sleep算法训练出的推理网络类似，但是它具有一个优势，就是在生成器训练完成后，这个推理网络可以针对固定的生成器进行训练。
- 能够用来近似模型所有的条件概率p(xS∣xS̸)p(xS∣xS̸)，其中SS通过训练共享参数的条件模型簇的关于xx索引的一个子集。本质上，可以使用生成对抗网络来随机拓展MP-DBM。
- 半监督学习：当标签数据有限时，判别网络或推理网络的特征不会提高分类器效果。
- 效率改善：为协调GG和DD设计更好的方法，或训练期间确定更好的分布来采样zz，能够极大的加速训练。
本文已经展示了对抗模型框架的可行性，表明这些研究方向是有用的。
