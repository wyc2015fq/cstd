# 论文笔记：Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling - 数据之美的博客 - CSDN博客
2018年10月28日 19:54:30[看穿数据之美](https://me.csdn.net/oppo62258801)阅读数：62
感想
最近深度学习面试的时候，有个面试官问了我LSTM，我一下子傻眼了，确实不怎么好懂，学LSTM已经有半年的时间了，但是对这个玩意儿却还不怎么明白，可能是没用过它的缘故吧，我找了一篇它和GRU比较的论文，这篇论文没有从理论上证明哪个模型的好坏，只是从实验，应用场景的角度发现GRU在一些场景比LSTM强，GRU是2014年提出的模型，可以说是一种LSTM的变体，使得计算和更新效率提高了，并且还取得了不错的效果
1.介绍
RNN最近在机器学习中表现了良好的效果，特别是当输入或者输出是可变长度的时候。最近，RNN在机器翻译这种具有挑战性的任务上超过了现有最好的系统的性能。有趣的是传统的RNN效果不好，但是一些有复杂循环隐藏层的单元的RNN,例如长短时记忆（long short-term memory）单元，成功的应用到了许多应用中，并且取得了不错的效果。
在这篇文章中，我们评估了两个最相近的变体。其中一个是LSTM(long short-term memory),另一个是GRU(gated recurrent unit ).LSTM因其长短时依赖的关系，在基于序列的任务中国工作得很好。GRU用于机器翻译的场景中（context of machine translation）。我们评估这两种单元和一个更传统的tanh单元在序列模型中的表现。我们用了3种有许多声音的音乐数据集和由Ubisoft提供的内部数据集，Ubisoft数据集中每个样本是初始语音的表示（raw speech representation）。
通过实验，我们可以得出以下的结论，所有的模型都使用固定数量的参数，在一些数据集上，，GRU比LSTM单元在CPU时间上的收敛速度上更快，并且在参数更新和泛化上性能更佳。
2.Recurrent Neural Network
RNN是一个卷积前馈（conventional feedforward）神经网络的一个拓展，它可以处理可变长度的输入序列（variable-length sequence input）,RNN有一个循环隐藏状态，这个状态每个时刻只依赖于前一时刻的激励。
更正式一点，给定一个序列x=(x1,x2,…,xT),RNN更新隐藏状态ht如下：
Ф是一个非线性函数，例如有仿射变换（affine transformation）的logistic sigmoid函数，输出是y=(y1,y2,…,yT),它也可以是一个可变长度。
传统上，循环隐藏状态的更新的如下式：
其中，g是一个平滑的，有边界的函数，例如logistic sigmoid函数或者一个双曲正切函数（hyperbolic tangent function）。
一个生成式的RNN输出的是一个序列下一个元素的概率分布，给定现在的状态ht, 生成模型可以捕获可变长度模型的分布，只需要用一个特别的输出标记一个序列的末尾就可以实现可变长度。序列的可能性可以被分解为
最后一个元素是一个特别的序列末尾值（special end-of-sequence value），我们对每一个条件概率建模
但是，RNN很难进行训练并捕捉到长短时依赖（long-term dependencies），因为梯度要么消失（到多数情况）了要么就爆炸（很少）了，这使得基于梯度的优化方法很难优化模型。不是因为在梯度刻度方面的变化，而是由于长时的依赖（long-term dependencies）效果被短时的依赖（short-termdependencies）隐藏了。研究者提出了两种方法解决这个问题，
1.     设计一个比简单的随机梯度下降（SGD）更好的学习算法，例如使用一个简单的裁剪的梯度（simple clipped gradient），裁剪的梯度中梯度向量的范数被裁剪；或者使用二阶方法，但如果二次导数的增长形式跟一阶导数一样，这种方法可能对这个问题不怎么敏感。
2.     这种方法是这篇文章特别关注的，即设计比通常激励函数更复杂的激励函数（activation function），包括一个简单非线性元素级别的仿射变换，这通过使用门单元（gating units）实现的。最初在激励函数或者一个循环单元的尝试提出了LSTM单元（long short-term memory），最近，研究者提出了另一种类型的循环单元，GRU单元。RNN在需要捕获长短时依赖的任务上表现出色，当然，这些任务不限于语音识别，机器翻译，还有很多其他序列任务。
3.  Gated Recurrent Neural Networks
3.1  Long Short-Term Memory Unit
LSTM是由Hochreiterand Schmidhuber提出来的，随后经历了许多小修改，我们实现的是Graves（2013年）的LSTM的实现，不像循环单元，循环单元只是简单的计算输入信号的权重和一个非线性函数。每一个在t时刻的LSTM单元j有一个记忆c^jt,LSTM的输出或者激励是
O^jt是一个输出门，它决定这该记忆单元的值是否输出。输出门的计算如下
其中，σ是一个logistic sigmoid函数，V0是一个对角矩阵（diagonal matrix）。
记忆单元c^jt通过一万已存在记忆的部分来更新的，并且增加了一个新的内容c~^jt:
记忆的遗忘是由遗忘门f^jt模块完成的，新记忆内容是由输入门i^jt模块完成的。门的计算如下
Vf和Vi是对角矩阵。
不想传统的循环层那样，传统的循环层会在每一步重写它的内容，而LSTM单元可以决定是否保留这些记忆。如果LSTM单元从每个阶段的输入序列中检测到了一个重要的特征，它可以很容易携带一个长距离的信息，捕获潜在的长距离的依赖（long-distance dependencies）。
上图为LSTM单元，我简单解释一下，首先输入是IN,输入包括input x和这层t-1时刻的值h，我们就经过双曲正切函数tanh，得到c~,紧接着我们就可以更次年c,c是c~和c在t-1时刻的加权和，得到c以后，我们就可以计算单元的输出OUT,c经过双曲正切函数tanh，然后和o相乘就得到OUT,o的计算是输入x，这个单元t-1时刻的输出值，以及c的值的加权和。
3.2  Gated Recurrent Unit
GRU是2014年提出来使的每个循环单元可以自适应的捕捉不同时间刻度下的依赖（adaptively capture dependencies of different time scales.）。与LSTM单元相似，GRU具有调节信息流动的门单元，但是，没有一个单独的记忆单元（memory cells）。
GRU在时刻t的激励h^jt是一个线性的修改，如下
其中，更新门为z^jt，它决定多少单元跟新它的激励，或者内容。更新门的计算为
这个过程是把现在的状态和新的状态求和，和LSTM的单元类似。GRU没有采取任何机制去控制那个状态暴露出来，而是每次所有状态都会暴露。
候选激励h~^jt和传统的循环单元计算相似
其中，rt是一个重置门的集合，当r^jt接近于0的时候，重置门高效的使得这个单元表现为好像是在读这个输入序列的第一个符号。Rt和ht-1是元素相乘的符号，具体解释可以参见这篇LSTM文章，这篇文章用的是小圆圈代替，不过讲的都是一个东西：
https://zybuluo.com/hanbingtao/note/581764
example:
重置门r^jt和更新门的计算相似
上图为GRU单元，简单解释一下，首先是IN,首先计算t时刻的候选激励h~,它是由t时刻的输入x，该层t-1时刻的值和重置门rt相乘的权重和；紧接着我们计算更新门z,z是由t时刻的输入x,t-1时刻的h值的权重和的激励，然后我们就可以计算h了，h是t时刻t时刻的1-z和t-1时刻的h值相乘，然后更新门z和候选激励h~相乘，两者相加即得到t时刻的h了，最后通过out输出。
3.3  Discussion
1.     它们共有的最主要的特征是从t时刻到t+1时刻的更新，这是传统的循环单元所没有的，传统的循环单元一直用一个新的值来替换激励或者一个单元的内容。这个值是由输入和先前的隐藏状态计算得来的。另外，LSTM和GRU都会保存现有的内容（existing
content）并且会增加新的内容（new content）。这样做有两个优点：
a)     在一个很长序列步（a long series of steps）的输入流中，每个单元很容易记得现有的一些特定的特征。任何重要的特征，要么由LSTM的遗忘门决定要么由GRU的更新门更新，它不会被重写，只是去维护更新
b)     可能更重要的是，这个额外的单元有效的创造了跨越多个时间步的快捷路径，这些路径很容易使错误反向传播，不至于迅速消失（门单元将近饱和为1），这是因为我们穿过了多个，有界非线性单元。结果减少了由于梯度消失带了的困难。
2.     这两个单元也有很多的区别，LSTM单元的一个特征控制记忆内容的暴露（exposure of the memory content），这个GRU没有。在LSTM单元中，记忆内容的数量是由输出门控制的，但GRU是把所有的内容都暴露出来，没有进行控制。另一个区别是输入门（input gate）的位置，或者与之对应的重置门（reset gate）的位置。LSTM单元计算新的记忆内容的时候，它没有控制从上一时间步传来的信息的数量。而是控制控制有多少新的内容被添加到记忆单元（memory cell），记忆单元和遗忘门是分开的。另一方面，当计算新的，候选的激励的时候，GRU控制着从前一个激励的信息流动，它不是控制着有多少的候选激励被添加。
从这些区别和相似点，很难得出哪个门单元更好，有研究者报告说，根据他们先前的实验，这两个单元在机器翻译上的性能不相上下。这促使我们写了这篇文章，LSTM和GRU单元的经验比较。
4 实验
4.1 任务和数据集
我们在序列模型上比较LSTM,GRU和tanh单元，序列模型目标是学习一个序列的分布，在给定一个训练序列的集合下，我们计算它的最大似然估计：
其中，θ是模型参数的集合，跟具体地，我们评估复音音乐模型（polyphonic music modeling）和语音信号模型的任务。
对于复音音乐模型，我们用了3个复音音乐数据集：Nottingham, JSB Chorales, MuseData and Piano-midi.这些数据集包含许多个序列，这些序列的每个符号分别代表93,98,108维二元向量。我们使用sigmoid函数作为输出单元。
我们使用了由Ubisoft提供的内部语音信号模型数据集，每个序列是一个一维的音频信号，在每一个时间步，我们设计一个RNN去用20个连续的样本去预测后面的10个连续的样本。我们使用两种不同版本的数据集：第一个数据集样本序列的长度是500（Ubisoft A），第二个数据集样本的序列长度是8000（Ubisoft B），Ubisoft A有7230个序列，Ubisoft B有800个序列。我们使用20个组件的混合高斯（mixture ofGaussians）作为输出层。
4.2 模型
每一个任务，我们训练三种不同的RNN,每个RNN有LSTM单元，GRU单元，tanh单元中的一种，实验最开始的目标是比较公平的比较三种单元，我们的模型的参数量大小几乎一样，我们有意的使模型足够小，使得可以避免过拟合。
上表是实验中所用模型的大小。
上图为训练集和测试解的log概率的平均值。在复音音乐数据集的案例中，GRU-RNN超过了其它结构的RNN的性能，Nottingham除外。就三个音乐数据集而言，三个模型的性能表现相近。另一方面，LSTM和GRU在Ubisoft数据集上的性能比传统的tanh-RNN强，LSTM表现最佳。
每个模型我们都用RMSProp（由hinton发明的，有兴趣的话自行百度）来训练，用固定标准差0.075的权重噪声进行训练。在每一次更新，如果把梯度大于1的话，我们把梯度规范化为1，这是用来防止梯度爆炸的问题。我们选择了一个学习率去最大化我们验证的性能，学习率是从10个随机候选的log参数，这些参数服从U(-12,-6).验证集也用与early-stop训练。
上图显示了验证的学习曲线，就音乐数据集而言，GRU训练得更快，参数更新也快。对Ubisoft而言，如下图，尽管tanh-RNN更新需要的计算量更小，但是模型没有取得更好的效果。
上图是不同类型单元的训练和验证集的学习曲线，y轴对应的是模型的负log似然值（negative log likelihood of the model）。
结果表明，新型的门单元的比传统的循环单元表现更优，首先速度经常更快，最终的解更佳。但是我们的结果不能断定LSTM和GRU哪个更好，这表明，循环单元门的选择严重取决于数据集和与之对应的任务。
参考文献
[1]. Junyoung Chung, Çaglar Gülçehre,KyungHyun Cho, Yoshua Bengio:
Empirical Evaluation of Gated RecurrentNeural Networks on Sequence Modeling. CoRR abs/1412.3555 (2014)
[2]. 零基础入门深度学习(6) -长短时记忆网络(LSTM).
https://zybuluo.com/hanbingtao/note/581764
--------------------- 
作者：农民小飞侠 
来源：CSDN 
原文：https://blog.csdn.net/w5688414/article/details/78079335 
版权声明：本文为博主原创文章，转载请附上博文链接！
