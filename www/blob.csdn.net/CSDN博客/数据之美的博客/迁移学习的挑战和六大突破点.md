# 迁移学习的挑战和六大突破点 - 数据之美的博客 - CSDN博客
2018年06月26日 17:07:26[看穿数据之美](https://me.csdn.net/oppo62258801)阅读数：97
个人分类：[迁移学习](https://blog.csdn.net/oppo62258801/article/category/7758999)
本文转载自机器之心 作者：蒋思源  原文链接：http://jiqizhixin.com/article/2956
> 
5 月 28 日，机器之心主办的为期两天的全球机器智能峰会（GMIS 2017）进入第二天，全天议程中最受关注的是多位重要嘉宾出席的领袖峰会，包括《人工智能：一种现代方法》的作者 Stuart Russell、第四范式联合创始人兼首席科学家杨强、科大讯飞执行总裁兼消费者事业群总裁胡郁、阿尔伯塔大学教授及计算机围棋顶级专家Martin Müller、Element AI 联合创始人 Jean-Sebastien Cournoyer 等。
上午，第四范式首席科学家、香港科大计算机科学与工程系主任杨强发表了主题为《迁移学习最新进展》的演讲，他探讨分享了机器学习，人工智能到底有哪些最新的进展，以下是该演讲的主要内容：
首先，杨强教授回顾而今日的围棋大战，而给杨强教授比较深印象是柯洁所说的：「AlphaGo 看上去像神一样的存在，好像是无懈可击。」而我们如果从机器学习的角度来看，其还是有弱点的，而且这个弱点还很严重。这个弱点即，AlphaGo 没有迁移学习的能力。
机器的一个能力是能使用大量数据进行学习，所以数据的质量是非常重要的。但是，AlphaGo 不能在学会围棋后，迁移到拥有下象棋的能力。因此，机器如今是没有这种推广能力的，这就是我们今天要探讨的问题——迁移学习。
但是在说这个题目之前，我们先看看能从 AlphaGo 2.0 里学到的知识：
- 
今年的数据和去年的数据大有不同，数据的质量使得AlphaGo的性能得到大幅度提升，所以数据的质量是非常重要的。
- 
计算架构也很重要，去年 AlphaGo 用了上千块 CPU和成百块GPU，但今年只用了很少量的TPU。因此，今年的计算架构有了一种飞跃性的变化。
- 
算法也是非常重要的，强化学习即让计算机自我训练、自我学习。如果我们赋予机器能够自我学习的能力，它在某些方面将拥有超越人的能力，这种自我学习的算法极其重要。
这三点对于商业活动其实也是非常重要的，我们可以了解到现有成功的人工智能应用，都有高质量的数据、优良计算架构和自我学习的闭环的能力。
人类在学会自行车后，再学摩托车就非常容易了；看了一两张图片，就可以把它扩展到许多其他不同的景象。我们能把我们过去的经验带到不同的场景，这样就有了一种能够适应新环境的能力。
机器如何才能也具有这种能力呢？杨强教授表示人类有一个诀窍，而这个诀窍可以应用到机器学习中——即迁移学习的要素——就是发现共性，发现领域之间的共性。如果一旦发现了这种关键的共性（共同特征）迁移学习就变得非常容易。
## 为什么我们需要研究迁移学习？
- 
首先，生活上我们遇到更多的是小数据，而在小数据上学习的模型，才是真正的智能。
- 
第二，我们希望构建的系统不仅在那个领域能够发挥作用，在其周边也可以发挥作用。即我们希望系统是可靠的，其可以举一反三和融会贯通，这也是我们赋予智慧的一种定义。
- 
第三，我们希望更重要的是如何能够把一个通用的系统加上个人的小数据，而迁移到个人的场景当中去，因此我们可以向个性化方向发展。迁移学习就是一个必不可少的工具。
但迁移学习为什么如此难以实现？因为即使是人类也很难发现这些共同点。
## 迁移学习的进展
杨强教授下面接着给大家介绍了一下，迁移学习最近有哪些进展。
第一个进展，即如果我们面临一个机器学习问题，那么可以通过把问题的结构和内容分离开来而发现不同问题之间的共性。虽然这种方式并不容易，但一旦能够完成地话，系统举一反三的能力就非常强了。![blob.png](http://jiqizhixin.com/data/upload/ueditor/20170528/592a6514324d1.png)
以上是2005年《Science》的一篇文章，有三个研究者在手写字体识别上将结构和手写的方式、斜体字等各种写法区分开。他们发现，在学习结构这一方面，用一个例子就可以实现，这也称之为单个例学习。
第二个进展，是因为过去我们在学习方面太注重发现共性本身，但是却没有注意在不同的层次之间发现这些共性。因此，如果我们把问题分解为不同层次，那么层次化的系统就更容易帮助我们构建机器学习的迁移。
在图像识别上，如果我们把分类改变，那么传统的机器学习就需要用大量数据重新训练，花大量时间训练一个新的模型。但是现在利用这种层次型的迁移学习，我们会发现，不同的层次具有不同的迁移能力，这样迁移能力就有了一个定量的估计。所以，当出现新的问题时，我们就可以把某些层次给固定住，而将其他的区域用小数据训练。这样就能够达到迁移学习的效果。
第三个进展，过去的迁移学习，往往是有一个领域已经做好了模型，而我们的目的是要把它迁移到一个新的领域，从旧领域迁移到新领域，从一个多数据的领域迁移到少数据的领域，这种叫单步的迁移。但是我们现在发现，很多场景是需要我们分阶段来的。比如说我们分四年去上大学，因为我们需要把我们的知识进行分段学习，以一个课程为基础转到下一个课程。![blob.png](http://jiqizhixin.com/data/upload/ueditor/20170528/592a65bb4d5e8.png)
用这个思想，我们也可以进行多步传导式的迁移，比如说我们可以建立一个机器学习的深度网络，这个网络的中间层即能够照顾目标问题领域，又能照顾原领域。
第四，学习如何迁移。这20年中，我们积累了甚至上百种迁移学习的算法，但现在有一个问题，即当遇到一个新的机器学习问题时，我们不知道到底该用哪个算法。其实，既然有了这么多的算法，有了这么多的文章，我们可以把他们这些经验总结起来，用来训练一个新的算法，这个算法它的老师就是所有这些机器学习算法和数据。所以，这种学习如何迁移，就像我们常说的学习如何学习，这个才是学习的最高境界，就是学习方法的获取。
第五个进展，即把迁移学习本身作为一个元学习（Meta Learning），然后再赋予到不同学习的方式上。也就是在当前机器学习的问题或者是模型上面加一个迁移学习的模式，而变成一个迁移学习的模型。这种加一个模式的办法就是元学习。假设我们有一个强化学习模型，我们在上面做一个“外套”，就能把它成功变成一个迁移学习模型。
最后一个的进展，就是用数据生成式的迁移学习-GAN
我们可以通过简单的类比来理解生成对抗网络：即两个人比赛，看是 A 的矛厉害，还是 B 的盾厉害。比如，我们有一些真实数据，同时也有一把乱七八糟的假数据。A 拼命地把随手拿过来的假数据模仿成真实数据，并揉进真实数据里。B 则拼命地想把真实数据和假数据区分开。
这里，A 就是一个生成模型，类似于卖假货的，一个劲儿地学习如何骗过 B。而 B 则是一个判别模型，类似于警察，一个劲儿地学习如何分辨出 A 的骗人技巧。如此这般，随着 B 的鉴别技巧的越来高超，A 的骗人技巧也就越来越纯熟。一个造假一流的 A，就是我们想要的生成模型。
最后杨强教授说，我们在深度学习上已经有了很大的成就， 而我们今天在努力进行各种尝试，在强化学习，比如说AlphaGo，但是杨强教授认为机器学习的明天是在小数据、个性化、可靠性上面，就是迁移学习的发展。
