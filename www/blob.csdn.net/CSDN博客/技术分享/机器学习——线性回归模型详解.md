# 机器学习——线性回归模型详解 - 技术分享 - CSDN博客

2018年11月02日 18:09:12[丿一叶秋丶](https://me.csdn.net/zhuqiang9607)阅读数：509标签：[机器学习																[线性回归模型																[梯度下降算法																[特征](https://so.csdn.net/so/search/s.do?q=特征&t=blog)](https://so.csdn.net/so/search/s.do?q=梯度下降算法&t=blog)](https://so.csdn.net/so/search/s.do?q=线性回归模型&t=blog)](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)
个人分类：[机器学习](https://blog.csdn.net/zhuqiang9607/article/category/8298280)


## 线性的含义

**线性的理解**：它是一条直线，这可能让我们回到初中、高中时候定义的y=ax+b，而在线性回归模型中，它是一个特征或多个特征和结果的拟合模型（好比预测房子的价格，房子的面积、房龄、房间数等等特征，而所要预测的房子的价格就是需要的结果），最终表示为线性函数关系式。

**线性函数**：可以把房子对应的特征整合为一组集合$\left ( x_{1}^{j},x_{2}^{j},x_{3}^{j},\cdot \cdot \cdot ,x_{n}^{j} \right )$，对应的结果为$y^{j}$，其中$i$表示的是样本的个数，以$j=1$为例，可得线性函数关系式

$y=a_{0}+ b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{3}+\cdot \cdot \cdot+b_{m}x_{m}$

**线性回归模型**：说白了就是一个函数，好比求解$f(x)=ax+b$，而要求解的函数往往写成

$h_{\theta}(x)=h_{0}+ \theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\cdot \cdot \cdot+\theta_{m}x_{m}$

此外，为了方便合并，都会添加$x_{0}=1$这项，于是函数为

$h_{\theta}(x)=\theta_{0}x_{0}+ \theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\cdot \cdot \cdot+\theta_{m}x_{m}$

然后，更简洁的公式出来了，如下

$h_{\theta}(x)=\sum_{i=0}^{m}\theta_{i}x_{i}=\theta^{T}X$

其中，$\theta_{i}$是$x_{i}$对应的权值（系数），$\theta^{T}、X$分别表示权值向量、特征向量，而实际工业中$X$是已知的一些样本，而所要求的是$\theta^{T}$，求出来了，那模型也就有了。

## 线性回归模型求解

为了求解$\theta^{T}$，会先定义损失函数（或成本函数）

$J(\theta)=\frac1{2m}\sum_{i=0}^{m}(h_{\theta}(x^{i})-y^{i})^{2}$

然后需要尽量减小$J(\theta)$，常用的方法有梯度下降法（Gradient Descent），牛顿法和拟牛顿法，模拟退火法（Simulated Annealing） 等等，这里以梯度下降法为例，公式为

$\theta_{j}:=\theta_{j}-\alpha\frac\partial{\partial\theta_{j}}J(\theta)$

其中$\alpha$是步长，步长太大，容易错过$arg$$min(J(\theta))$，太小效率太慢，有的情况会落入局部最小值。

![在合适的情况下才能取得全局最小值点](https://img-blog.csdnimg.cn/20181102174638261.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXFpYW5nOTYwNw==,size_12,color_FFFFFF,t_70)

然后，$J(\theta)$代入可得

$\theta_{j}:=\theta_{j}-\alpha\sum_{i=0}^{m}(h_{\theta}(x^{i})-y^{i})x_{j}^i$

得到新的$\theta_{j}$代入$J(\theta)$中，反复迭代，直到两次$\theta_{j}$相等或差值小于人为设定的阈值（超参数），这时的$h_{\theta}(x)$，即为要找的模型。

## 结语

本文对线性回归模型做了简单的分析，但由于实际中的应用，可能会有各种各样的变种，但是有必要先了解最基本模型的原理，以及处理方式，以至于不把机器学习当黑盒使用。不可避免的文中还有很多不足之处，有待进一步完善。


