# 《卷积网络》卷积神经网络基础 - 浅梦的博客 - CSDN博客





2017年11月21日 22:07:37[浅梦s](https://me.csdn.net/u012151283)阅读数：252
个人分类：[深度学习](https://blog.csdn.net/u012151283/article/category/6698461)









# 边缘检测例子

![这里写图片描述](https://img-blog.csdn.net/20171121204627248?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

# 填充Padding
- Valid Padding 
$p = 0,output = 1+\frac{n-f}{s}$
- Same Padding 

保持卷积输出和输入维度相同， 
$1+\frac{n-f+2p}{s}=n$
$p = \frac{(n-1)s-n+f}{2}$
# 卷积步长Stride

每次卷积核卷积操作后平移的维度。 
$ \lfloor\frac{n-f+2p}{s}+1\rfloor$

 当s=1,f=3,p=1或s=1,f=5,p=2时，卷积前后尺寸不变。
# 三维上的卷积

![这里写图片描述](https://img-blog.csdn.net/20171121211606764?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

设输入维度$n*n*depth$，卷积核个数为$n_c$(即为输出通道数量)，则卷积核维度$depth*f*f*n_c$， 

共有$n_c$个卷积核，每个卷积核对上一层$depth$个feature map进行卷积操作。 

则输出维度$(n-f+1) * (n-f+1) * n_c$。 

每个卷积核的参数量为$f*f*depth + 1$
# 池化层Pooling Layer

池化一般分为MaxPooling和AveragePooling。池化层没有需要学习的参数。 

与卷积层不同的是，池化操作是分别对每个切片单独进行计算的。而三维卷积是对所有切片一起计算的。 

通常使用Valid Padding，即padding size =0。 

设输入维度$n*n*depth$，池化层参数$f*f$

则输出维度$(n-f+1) * (n-f+1) * depth$。
# 使用卷积的原因
- 参数共享 

通常一个特征检测子（如边缘检测）在图像某一部位有用也在其他部位生效。 

同一个卷积核在图像的不同部位保持同一组参数。- 稀疏连接 

每一层的输出只依赖于前一层一小部分的输入。

# CNN的反向传播

![这里写图片描述](https://img-blog.csdn.net/20171215191004338?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

# 池化层的反向传播

![这里写图片描述](https://img-blog.csdn.net/20171215191341532?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

CNN是全连接特殊形式，所以局部感受野之外的神经元为0，核参数在神经元之间共享。
# 参考资料

> 
《深度学习》deeplearning.ai










