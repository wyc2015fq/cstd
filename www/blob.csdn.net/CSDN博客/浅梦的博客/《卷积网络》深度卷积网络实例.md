# 《卷积网络》深度卷积网络实例 - 浅梦的博客 - CSDN博客





2017年11月22日 18:43:50[浅梦s](https://me.csdn.net/u012151283)阅读数：517








# 经典网络

## LeNet-5

![这里写图片描述](https://img-blog.csdn.net/20171122121926329?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

共有约60k个参数， 
**特点**- 在论文中，激活函数使用的是sigmoid和tanh，那个时候还未使用ReLU.
- 当时出于节约计算力的考虑，不同的卷积核计算了不同的channel。（这里等之后看了论文补充）
- 池化层后添加了非线性激活函数(sigmoid)，而现在很少这样使用。 

> 
[LeCun et al.,1998. Gradient-based learning applied to document recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)




  论文Section2讨论了网络的架构，Section3讨论了实验结果。





## AlexNet

![这里写图片描述](https://img-blog.csdn.net/20171216140333777?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20171122123244924?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**特点**
- 与LeNet类似，但是更大，约60m参数量。
- 使用了ReLU
- 多GPU训练
- Local Response Normalization(LRN) channel-wise normalization。现在并不常用

> 
[Krizhevsky et al.,2012 ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)


## VGG-16

![这里写图片描述](https://img-blog.csdn.net/20171216140420242?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20171122124100434?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**特点**- VGG网络使用了非常简单结构，约138M参数
- 所有的卷积层均为CONV = 3*3,s=1,padding=same
- 所有的池化层均为MAX-POOL=2*2,s=2 

> [Simonyan&Zisserman 2015. Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556)


# ResNet

![这里写图片描述](https://img-blog.csdn.net/20171122181400574?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20171122223310907?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



- Residual block 
![这里写图片描述](https://img-blog.csdn.net/20171216140847675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20171122180539460?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

使用残差块可以训练更深的网络。ResNet中使用的残差块有下面两种。- The identity block 
![这里写图片描述](https://img-blog.csdn.net/20171122222624592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)- The convolutional block 
![这里写图片描述](https://img-blog.csdn.net/20171122222805072?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

当残差模块的输入和输出的形状不匹配时，使用卷积模块修改shortcut中输入的形状，使得shortcut(x)和输出形状匹配。 

shortcut上的卷积层不适用任何非线性激活函数。其主要任务是对输入施加一个学习到的线性函数来改变输入的维度。- 工作原理 

identity function is easy for residual block to learn. 

不会由于网络的加深使得效果变差。 



> 
[He et al., 2015. Deep residual networks for image recognition](https://arxiv.org/abs/1512.03385)


# Network in Network(1x1卷积)

![这里写图片描述](https://img-blog.csdn.net/20171216141151388?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20171122181912007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

可以用来缩减channel数量

> 
Lin et al., 2013. Network in network


# Inception

![这里写图片描述](https://img-blog.csdn.net/20171216140533240?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20171122183118317?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

特点



- Inception block 
![这里写图片描述](https://img-blog.csdn.net/20171122182157599?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20171122182858289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20171216141249786?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)- 同时使用多种卷积核和池化，不必选择。卷积和池化均使用same填充。 




> 
Szegedy et al.,2014,Going Deeper with Convolutions
  


# CNN使用建议
- 使用开源实现
- 迁移学习 
![这里写图片描述](https://img-blog.csdn.net/20171216141726008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)- 数据扩增 

使用CPU进行图片生成，GPU训练
- CV现状 
![这里写图片描述](https://img-blog.csdn.net/20171122184157878?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20171122184250700?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
# 参考资料

> 
《深度学习》 deeplearning.ai 

   Introduction to Deeplearning HEC













