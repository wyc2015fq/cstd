# 深度学习中激活函数的选择 - 浅梦的博客 - CSDN博客





2017年10月01日 22:38:07[浅梦s](https://me.csdn.net/u012151283)阅读数：1787








# 为什么引入非线性激活函数

如果不使用非线性的激活函数，无论叠加多少层，最终的输出依然只是输入的线性组合。 

引入非线性的激活函数，使得神经网络可以逼近任意函数。

# 常用激活函数

![这里写图片描述](https://img-blog.csdn.net/20171215144923511?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## sigmoid函数

![这里写图片描述](https://img-blog.csdn.net/20171215192119419?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
$\sigma(z)=\frac{1}{1+e^{-z}}$
$\sigma(z)'=\sigma(z)(1-\sigma(z))$

sigmoid函数是最常见的激活函数，常用于输出层。其值域为0到1。
## tanh双曲正切函数

![这里写图片描述](https://img-blog.csdn.net/20171215192223116?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$
$g(z)'=1-g(z)^2$
$tanh$函数的值域位于-1和1之间。 

可以看作是sigmoid函数向下平移和伸缩的结果。 

经验表明，在隐藏层中使用$tanh$函数的效果是优于sigmoid函数的，因为其输出更接近零均值。这会使得下一层的学习更加简单。
sigmoid函数和tanh函数共同的**缺点**是，在z特别大或者特别小的情况下，函数的梯度会变得特别小，接近0，这会使得梯度下降的速度变慢。 

一般在二分类问题的输出层使用，不在隐藏层使用。

## ReLU修正线性单元

![这里写图片描述](https://img-blog.csdn.net/20171215192251294?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
$relu(z)=max(0,z)$

当z为正的时候，导数恒为1，当z为负的时候，导数恒为0。 

在零点的导数不存在，在实际使用中，z不会恰好为0，一般为一个很小的浮点数，这时导数取0或1都可以。
ReLU易于优化。只要出与激活状态，导数都能保持较大。梯度不仅大而且一致。二阶导数几乎处处为0，并且在ReLU处于激活状态时，一阶导数处处为1。

通常将偏置b设置成一个小的正值，如0.1。使得其对训练集中大多数输入呈现激活状态，并且允许导数通过。

ReLU是最常用的激活函数。

## Leaky ReLU

![这里写图片描述](https://img-blog.csdn.net/20171215192322812?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
$leakyrelu(z)=max(0.01z,z)$

relu和leakyrelu的**优点**- 相比于sigmoid等，计算量小，
- 相比于sigmoid，不容易出现梯度消失情况，能加速网络训练速度
- 使得一些神经元输出为0，增加稀疏性，防止过拟合。

## PReLU

$g(z)=max(az,z)$

将$a$作为学习的参数。

## maxout 单元

maxout单元进一步扩展了relu。maxout将z划分为每组具有k各值的组，而不是使用作用于每个元素的函数$g(z)$。每个maxout单元则输出每组中的最大元素 
$g(z)_i=\max\limits_{j\in G^{(i)}}z_j$

这提供了一种方法来学习对输入x空间中多个方向响应的分段线性函数。 

maxout单元可以学习具有**多达k段的分段线性的凸函数**。
maxout单元因此可以视为学习*激活函数*本身而不仅仅是单元之间的关系。使用足够大的k，maxout可以以任意的精确度来近似任何凸函数。特别地，具有两块的maxout层可以学习实现和传统层相同的输入x的函数，这些传统层可以使用relu,prelu等。  

每个maxout单元现在由k个权重向量来参数化，而不仅仅是一个，所以maxout单元通常比relu需要更多的正则化。

参考资料

> 
DeepLearning deeplearning.ai 

  《深度学习》第6章 

  Introduction to Deep Learning HEC









