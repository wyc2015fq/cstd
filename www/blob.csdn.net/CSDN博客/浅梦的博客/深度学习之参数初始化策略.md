# 深度学习之参数初始化策略 - 浅梦的博客 - CSDN博客





2017年10月13日 22:09:43[浅梦s](https://me.csdn.net/u012151283)阅读数：5294








# 为什么需要参数初始化策略

## 目的

为了让神经网络在训练过程中学习到有用的信息，需要参数更新时的梯度不为0。在一般的全连接网络中，参数更新的梯度和反向传播得到的状态梯度以及输入激活值有关。那么参数初始化应该满足以下两个条件： 

1. 初始化必要条件一：各层激活值不会出现饱和现象(对于sigmoid,tanh)； 

2. 初始化必要条件二：各层激活值不为0。
## 全零初始化存在的问题

全零初始化方法在前向传播过程中会使得隐层神经元的激活值均未0，在反向过程中根据BP公式，不同维度的参数会得到相同的更新。 

需要破坏这种“对称性”。 
![这里写图片描述](https://img-blog.csdn.net/20171215204755763?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 激活函数输入值的方差

![这里写图片描述](https://img-blog.csdn.net/20171216010522283?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20171216010602569?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

从上述推导可以看出，神经元输出的方差会随着神经元数量的增大而变多。 
![这里写图片描述](https://img-blog.csdn.net/20171216010919367?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
# 标准初始化

标准初始化方法通过对方差乘以一个系数确保每层神经元的输出具有相同的方差，提高训练收敛速度。

标准均匀初始化方法保证了激活函数的输入值的均值为0，方差为常量$\frac{1}{3}$，和网络的层数和神经元的数量无关。对于sigmoid激活函数来说，可以确保自变量处于有梯度的范围内。 

但是注意对于$sigmoid$函数，其输出是大于零的，这违反了上面推导中关于$E(x_i)=0$的假设。综上，标准初始化方法更适用于$tanh$激活函数。

标准正态初始化方法保证激活函数的输入均值为，方差为1。

对于含有$n_{in}$个输入和$n_{out}$个输出的全连接层，
- standard_normal 
$W_{i,j}\sim N(0,\frac{1}{\sqrt{n_{in}}})$
- standard_uniform 
$W_{i,j}\sim U(-\frac{1}{\sqrt{n_{in}}},\frac{1}{\sqrt{n_{in}}})$

# Xavier初始化(glorot初始化)

glorot认为优秀的初始化应该使得各层的激活值和状态梯度的方差在传播过程中的方差保持一致。 

glorot假设DNN使用激活值关于0对称且在0处梯度为1的激活函数(如tanh)。 

记 
$s^i=z^iW^i+b^i$，$z^{i+1}=f(s^i)$

有 
$\frac{\partial{Cost}}{\partial{s_k^i}}=\frac{\partial{Cost}}{\partial{s^{i+1}}}W_k^{i+1}f'(s_k^i) \text{..............(2)}$
$\frac{\partial{Cost}}{\partial{w_{l,k}^i}}=\frac{\partial{Cost}}{\partial{s_k^i}}z_l^i\text{..............(3)}$

根据之前的假设 
$f'(s_k^i)\approx1\text{..............(4)}$, 
$Var[z^i]=Var[x]\prod_{i'=0}^{i-1}n_{i'}Var[W^{i'}]\text{..............(5)}$
![这里写图片描述](https://img-blog.csdn.net/20180214144832412?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

为了满足之前的假设，即 
![这里写图片描述](https://img-blog.csdn.net/20180214144923766?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

综合以上两个要求， 

Xavier初始化要求神经元的权重的方差$Var(w)=\frac{2}{n_{in}+n_{out}}$- xavier_normal 
$W_{i,j}\sim N(0,\sqrt{\frac{2}{n_{in}+n_{out}}})$
- xavier_uniform 
$W_{i,j}\sim U(-\sqrt{\frac{6}{n_{in}+n_{out}}},\sqrt{\frac{6}{n_{in}+n_{out}}})$

该方法有一定限制，其推导过程假设激活函数在零点附近接近线性函数，且激活值关于0对称。 

sigmoid函数和relu函数不满足这些假设。 

在tensorflow中，
```python
w = tf.get_variable("w",shape[100,10],initializer=tf.contrib.layers.xavier_initializer())
```

# He初始化
- he_normal 
$W_{i,j}\sim N(0,\sqrt{\frac{2}{n_{in}}})$
- he_uniform 
$W_{i,j}\sim U(-\sqrt{\frac{6}{n_{in}}},\sqrt{\frac{6}{n_{in}}})$
**ReLU建议使用**。
# 参考资料

> 
[东山客的博客](http://blog.csdn.net/victoriaw/article/details/73000632)
[码农王小呆的博客](http://blog.csdn.net/manong_wxd/article/details/78734725)
[Xavier论文](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
[Kaiming论文](https://arxiv.org/abs/1502.01852)










