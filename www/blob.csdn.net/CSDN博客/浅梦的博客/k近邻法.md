# k近邻法 - 浅梦的博客 - CSDN博客





2017年08月25日 21:14:40[浅梦s](https://me.csdn.net/u012151283)阅读数：137
个人分类：[机器学习																[k近邻法](https://blog.csdn.net/u012151283/article/category/7194655)](https://blog.csdn.net/u012151283/article/category/6653295)








k近邻法不具有显式的学习过程。k近邻法利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 
**k值的选择，距离度量及分类决策规则**是k近邻法得三个基本要素。

# k近邻算法

k近邻法

> - 根据给定的**距离度量**，在训练集T中找出与x最领进的k个点，涵盖这k个点的x的邻域记作$N_k(x)$；
- 在$N_k(x)$中根据分类决策规则（如多数表决）决定x的类别y： 
$y=\arg\max\limits_{c_j}I(y_i=c_j),i=1,...,N;j=1,...,K\text{(3.1)}$

  式(3.1)中，$I$为指示函数，即当$y_i=c_j$时$I$为1，否则$I$为0。
# 距离度量

闵科夫斯基距离 
$L_p(x_i,x_j)=(\sum\limits_{l=1}^N|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$

这里$p\ge1$。当p=2时，称为欧式距离。 

当$p=1$时，称为曼哈顿距离。 

当$p=\infty$时，它是各个坐标举例的最大值，即 
$L_\infty(x_i,x_j)=\max\limits_l{|x_i^{(l)}-x_j^{(l)}|}$
# k值的选择

k值小，整体模型复杂，容易过拟合。

# 分类决策规则

多数表决规则等价于经验风险最小化。

# k近邻法的实现：kd树

参考文献

> 
《统计学习方法》第3章








