# 朴素贝叶斯算法 - 浅梦的博客 - CSDN博客





2017年08月25日 23:10:33[浅梦s](https://me.csdn.net/u012151283)阅读数：186








朴素贝叶斯(naive Bayes)算法是基于贝叶斯定理与特征条件独立假设的分类方法。 

对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

# 朴素贝叶斯算法的学习与分类

## 基本方法

朴素贝叶斯算法对条件概率分布作了**条件独立性**的假设。 


$\begin{align*}P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\&=\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\text{(4.3)}\end{align*}$

朴素贝叶斯算法学习到生成数据的机制，所以属于生成模型。 

朴素贝叶斯法分类时，对于给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_k,|X=x)$，将后验概率最大的类作为x的类输出。 

后验概率计算根据贝叶斯定理进行： 
$P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\text(4.4)$

将式(4.3)代入式(4.4)有 
$P(Y=c_k|X=x)=\frac{\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}{\sum_k\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}\text(4.5)$

朴素贝叶斯分类器可表示为 
$y=f(x)=\arg\max\limits_{c_k}\frac{\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}{\sum_k\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}\text(4.6)$

注意到，在式(4.6)中分母对所有$c_k$都是相同的，所以， 
$y=f(x)=\arg\max\limits_{c_k}{\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}\text(4.7)$
## 后验概率最大化的含义

根据期望风险最小化准则可推得后验概率最大化。

# 朴素贝叶斯法的参数估计

## 极大似然估计

先验概率$P(Y=c_k)$的极大似然估计是 
$P(Y=c_k)=\frac{\sum\limits_{i=1}^NI(y_i=c_k)}{N},k=1,...,K$

设第j个特征$x^{(j)}$可能的取值集合为${a_{j1},...,a_{js_j}}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是 
$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum\limits_{i=1}^NI(y_i=c_k)}$
## 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况。这会影响到后验概率的计算结果，使分类的结果产生偏差。 

条件概率的贝叶斯估计是 
$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_i=c_k)+S_j\lambda}$

式中$\lambda\ge0$，$S_j$为第j个特征取值的种类。等价于在随机变量各个取值的频数上赋予一个整数$\lambda >0$。当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑(Laplace smoothing)。 

先验概率的贝叶斯估计是 
$P(Y=c_k)=\frac{\sum\limits_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda},\text{K为Y的类别数}$
朴素贝叶斯法中假设输入变量都是条件独立的，如果假设它们之间存在概率依存关系，模型就变成了贝叶斯网络。

# 常用的朴素贝叶斯分类器

根据假设不同的$P(X^{(j)}|y=c_k)$分布，

## 高斯贝叶斯分类器

假设特征的条件概率分布满足高斯分布

## 多项式贝叶斯分类器

假设特征的条件概率分布满足多项式分布 
$P(X^{(j)}=a_{s_j}|y=c_k)=\frac{N_{kj}+\alpha}{N_k+\alpha n}$

## 伯努利贝叶斯分类器

假设特征的条件概率分布满足二项分布 
$P(X^{(j)}=a_{s_j}|y=c_k)=pX^{(j)}+(1-p)(1-X^{(j)})$， 

要求特征的取值为0或1，且$P(x^{(J)}=1|Y=c_k)=p$

与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是。伯努利模型中每个特征的取值只能是1或0.（文本分类，单词出现于否。） 

参考文献
> 
统计学习方法 第4章 

  Python大战机器学习 第3章








