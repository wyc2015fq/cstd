# 最大熵模型 - 浅梦的博客 - CSDN博客





2017年08月27日 12:54:21[浅梦s](https://me.csdn.net/u012151283)阅读数：229
个人分类：[机器学习																[贝叶斯分类器](https://blog.csdn.net/u012151283/article/category/7194656)](https://blog.csdn.net/u012151283/article/category/6653295)








最大熵(maximum entropy model)由最大熵原理推导实现。这里首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式。

# 最大熵原理

**最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。**
**最大熵原理也可以表述为在满足约束条件的模型中选取熵最大的模型。**

假设离散型随机变量X的概率分步是$P(X)$，则其熵 
$H(P)=-\sum\limits_{x}P(x)\log P(x)$

熵满足下列不等式： 
$0\le H(P)\le\log |X|$

式中，$|X|$是X的取值个数，当且仅当X的分步是均匀分布时右边的等号成立。 

当X服从均匀分布时，熵最大。
# 最大熵模型的定义

最大熵原理应用到分类得到最大熵模型。 

假设分类模型是一个条件概率分布$P(Y|X)$。 

给定一个训练集T，学习的目标是用最大熵原理选择最好的分类模型。 

首先考虑模型应该满足的条件。给定训练集，可以确定联合分步$P(X,Y)$的经验分步和边缘分布$P(X)$的经验分布，分别以$\widetilde P(X,Y)$和$\widetilde P(X)$。 

用特征函数(feature function)$f(x,y)$描述输入x和输出y之间的某一个事实。其定义是 
$f(x,y)=  \begin{cases}1& \text{x与y满足某一事实}\\0& \text{否则}\end{cases}$

特征函数$f(x,y)$关于经验分布$\widetilde P(X,Y)$的期望值，用$E_{\widetilde P}(f)$表示。 
$E_{\widetilde P}(f)=\sum\limits_{x,y}\widetilde P(x,y)f(x,y)$

特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\widetilde P(x)$的期望值，用$E_{p}(f)$表示。 
$E_{ P}(f)=\sum\limits_{x,y} \widetilde P(x)P(y|x)f(x,y)$

如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即 
$E_{p}(f)=E_{\widetilde P}(f)\text{(6.10)}$

或 
$\sum\limits_{x,y}\widetilde P(x)P(y|x)f(x,y)=\sum\limits_{x,y}\widetilde P(x,y)f(x,y)\text{(6.11)}$

我们将式(6.10)或式(6.11)作为模型学习的约束条件。假如由n个特征函数$f_i(x,y),i=1,...,n,$那么就有n个约束条件。 
![这里写图片描述](https://img-blog.csdn.net/20170828215925452?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
# 最大熵模型的学习

![这里写图片描述](https://img-blog.csdn.net/20170828220048153?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

将约束最优化的原始问题转换为无约束最优化的对偶问题。通过求解对偶问题求解原始问题。 
![这里写图片描述](https://img-blog.csdn.net/20170828220446432?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20170828220557471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20170828220657785?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

# 极大似然估计

![这里写图片描述](https://img-blog.csdn.net/20170828220826090?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20170828220921755?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MTI4Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
参考资料

> 
[【机器学习】Logistic Regression 的前世今生（理论篇）](http://blog.csdn.net/cyh_24/article/details/50359055#t7)









