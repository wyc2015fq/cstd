# 《强化学习》 基本概念和交叉熵方法 - 浅梦的博客 - CSDN博客





2018年05月29日 23:44:48[浅梦s](https://me.csdn.net/u012151283)阅读数：568








# 基本概念

## 监督学习与强化学习
|监督学习|强化学习|
|----|----|
|通过学习近似参考答案|通过试验和错误来学习最优策略|
|需要正确答案|代理的动作需要反馈|
|模型不影响输入数据|代理可以影响自己的观察|

## MDP形式定义

![这里写图片描述](https://img-blog.csdn.net/20180529230854404?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxNTEyODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## RL的目标

最大化累积奖赏的期望 
![这里写图片描述](https://img-blog.csdn.net/20180529231049380?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxNTEyODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# CEM交叉熵方法

## 算法步骤
- 初始化策略
- 重复 
- 抽样N个sessions
- 选取elite sessions:选择前M个最好的session(奖励最大的)
- 更新策略使得elite sessions中的动作优先级更高


## Tabular CE

![这里写图片描述](https://img-blog.csdn.net/20180529232304698?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxNTEyODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## Smoothing
- 上述方法存在一个问题：如果某个状态只访问了一次，那么在那一次所采取的动作将会在未来一直被采取。
- 应用平滑技术： 
$\pi(a|s)=\frac{[\text{took a at s}]+\lambda}{[\text{was at s}]+\lambda \cdot N_{actions}}$
- 另一种：平滑更新 
$\pi_{i+1}(a|s)=\alpha\cdot\pi_{opt}+(1-\alpha)\pi_{i}(a|s)$

## 随机MDP
- 如果环境具有随机性，算法会更偏向“幸运”的session。
- 在(由随机性带来的)幸运session上训练并不好。
- 解决方法：在每一个状态抽样若干个动作，从那些state-action对开始运行若干模拟，将结果平均。降低随机性的影响。

# Approximate CE

## 算法步骤

![这里写图片描述](https://img-blog.csdn.net/20180529233753399?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxNTEyODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 连续动作空间

![这里写图片描述](https://img-blog.csdn.net/20180529234105555?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxNTEyODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## CE Trick
- 记住最近3-5次迭代的sessions 
- 全部用于训练（利用最近的sessions和新抽样的sessions作为候选）
- 简单环境下可能导致收敛变慢

- 使用熵来正则化 
- 防止过早收敛，防止最优动作无法被探索到

- 并行采样
- 如果是部分观测问题，使用RNN

CE方法总结：
- 容易实现
- 效果还不错
- 
黑盒优化
- 不知道环境信息
- 不知道中间奖赏信息

- 
样本效率低，采样了很多sessions，只有部分可以用于训练。在现实环境很难使用。

- 需要完整的session才能训练




