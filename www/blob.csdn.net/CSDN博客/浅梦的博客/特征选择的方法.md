# 特征选择的方法 - 浅梦的博客 - CSDN博客





2017年10月09日 14:41:56[浅梦s](https://me.csdn.net/u012151283)阅读数：3843








常见的特征选择方法大致可分为三类：过滤式(filter)，包裹式(wrapper)和嵌入式(embedding)

# 过滤式选择

**过滤式方法先对数据集进行特征选择，然后再训练学习器，**特征选择过程与后续学习器无关。 

相当于先用特征选择过程个对初始特征进行“过滤”，再用过滤后的特征来训练模型。

## 方差阈值选择

方差很小的属性，意味着该属性的识别能力很差。极端情况下，方差为0，意味着该属性在所有样本上的值都是一个常数。 
`sklearn`库提供`feature_selection.VarianceThreshold(threshold=0.0)`方法可以通过计算各个属性上的方差过滤掉小于指定阈值的属性。

## 单变量特征提取

通过计算每个特征的某个统计指标，然后根据该指标来选取特征。 
`sklearn`库提供`feature_selection.SelectKBest`和`feature_selection.SelectPercentile`方法可保留在指定统计指标上得分最高的k个特征或百分之k个特征。 

sklearn提供的统计指标函数有- sklearn.feature_selection.f_regression:基于线性回归分析来计算统计指标。适用于回归问题
- sklearn.feature_selection.chi2:计算卡方统计量，适合分类问题
- sklearn.feature_selection.f_classif:根据方差分析（Analysis of variance,ANOVA）的原理，依靠F-分布为概率分布的依据，利用平方和与自由度计算的组间与组内均方估计出F值，适用于分类问题

# 包裹式选择

**包裹式特征选择直接把最终要使用的学习器的性能作为特征子集的评价准则。**

包裹式特征选择的目的就是为给定学习器选择最有利于其性能，“量身定做”的特征子集。 

由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能看，包裹式特征选择比过滤式特征选择更好。 

另一方面，特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多。
## 递归特征消除RFE
- 首先：学习器在初始特征集合以及初始的权重上训练。
- 然后：学习器学得每个特征的权重，剔除当前权重最小的那个特征，构成新的训练集。
- 再将学习器在新的训练集上训练，直到剩下的特征数量满足条件为止。 

sklearn提供的RFE类原型为 
`class sklearn.feature_selection.RFE(estimator,n_features_to_select=None,step=1,estimator_params=None,verbose=0)`
## RFECV

sklearn还提供了RFECV类，它是RFE的一个变体，它执行一个交叉验证来寻找最优的剩余特征数量，因此不需要指定保留多少个特征。原型为 
`class sklearn.feature_selection.RFECV(estimator,step=1,cv=None,scoring=None,estimator_params=None,verbose=0)`

# 嵌入式特征选择

在过滤式特征选择中，特征选择过程与学习器训练过程有明显分别； 
**嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，**即在学习器训练过程中自动地进行了特征选择。 

sklearn提供了`SelectFromModel`来实现嵌入式特征提取。SelectFromModel使用外部提供的estimator来工作。estimator必须有coef_或feature_importances属性。 

原型为 
`class sklearn.feature_selection.SelectFromModel(estimator,threshold=None,prefit=False)`

参考资料
> 
《机器学习》第11章 

  《Python大战机器学习》 11章








