# 集成学习 - KinboSong的博客 - CSDN博客
2017年03月12日 22:16:33[songjinbo3](https://me.csdn.net/KinboSong)阅读数：617
个人分类：[机器学习算法](https://blog.csdn.net/KinboSong/article/category/6791887)
参考文献：[http://blog.csdn.net/chenhongc/article/details/9404583](http://blog.csdn.net/chenhongc/article/details/9404583)
集成学习通过构建并结合多个学习器来完成学习任务。先产生个体学习器，再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据中产生，例如决策树、BP神经网络。
**弱学习**：准确率仅比随机猜测略高的学习算法称为弱学习算法。
**强学习**：准确率很高并能在多项式时间内完成的学习算法称为强学习算法。
集成学习方法大致可分为两大类：
1、个体学习器件存在强依赖关系、必须串行生成的序列化方法，如boosting算法；
2、个体学习器间不存在强依赖关系、可同时生成的并行化方法，如bagging和随机森林（random forest，RF）。
**一、决策树：**
**1、决策树分类**
**回归树：RMSE（root mean square error，均方根误差）**
**分类树：信息熵、信息增益、基尼系数**
**2、ID3算法**
**参考：[www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html](www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)**
![](https://img-blog.csdn.net/20170313091831450?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**3、C4.5**
**参考：[www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html](www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)**
![](https://img-blog.csdn.net/20170313091839622?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**4、CART**
**参考：[http://blog.csdn.net/acdreamers/article/details/44664481](http://blog.csdn.net/acdreamers/article/details/44664481)（包括Gini指数）**
**注意:Gini指数越低，越有利于划分**
**5、剪枝**
**![](https://img-blog.csdn.net/20170313091844294?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)**
**二、随机森林**
     随机森林是一种多功能的机器学习算法，能够执行回归和分类、降维的任务。
优点：
![](https://img-blog.csdn.net/20170313092544188?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
综述：
![](https://img-blog.csdn.net/20170313092530475?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**三、bootstrap**
![](https://img-blog.csdn.net/20170313112916552?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**四、Bagging**
用bootstrap抽样方法训练基分类器，然后集成在一起
**五、boosting（下面讲adaboost）**
参考文献：[http://blog.csdn.net/dark_scope/article/details/14103983](http://blog.csdn.net/dark_scope/article/details/14103983)
《机器学习》（周志华）8.2小节：
 boosting
adaboost算法本身是通过**改变数据分布**实现的，它根据每次训练集之中的每个样本分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改权值的新数据传送给下层分类器进行训练，然后将每次训练得到的分类器融合起来，作为最后的决策分类器。
![](https://img-blog.csdn.net/20170313172415822?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**六、Bagging、RF和boosting（adaboost）的区别**
![](https://img-blog.csdn.net/20170313173759404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)![](https://img-blog.csdn.net/20170313173802404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2luYm9Tb25n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
