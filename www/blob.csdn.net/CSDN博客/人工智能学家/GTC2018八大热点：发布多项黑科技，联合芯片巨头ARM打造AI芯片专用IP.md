# GTC2018八大热点：发布多项黑科技，联合芯片巨头ARM打造AI芯片专用IP - 人工智能学家 - CSDN博客
2018年03月29日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：291
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBWsKBAloeRecZuR2rfZSbr18wy4Eg4HosqgStxmRoIW4ENWEupciaIdLm87biam5rxT8qQwN8TNqYwQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
来源：人工智能和大数据丨公众号
一年一度的GTC至今已经迎来了第十个年头，虽然它每年或多或少的给网友一些“失望”，但它也终究是代表着GPU行业的高水平技术大会。那么，今年的GTC大会，英伟达又给业界带来了哪些惊喜呢？
北京时间3月28日凌晨，英伟达公司创始人兼首席执行官黄仁勋宣布了该公司在芯片、AI 平台、自动驾驶上的一系列新动作。
> 
**一、光线追踪（ray-tracing）**
开场介绍的第一个项目就是英伟达在前不久GDC上发布的NVIDIA RTX光线追踪技术（ray-tracing），这项技术是英伟达耗时10年打造的，能够提供电影级画质的实时渲染，渲染出逼真的反射、折射和阴影画面，几乎与真实世界的照片或视频很难区分开来。现场展示的视频片段就是用光线追踪技术实时渲染的，而且并不需要一个超级强大的超算电脑，只需要一台DGX-Station。
在真实世界中，我们看到的3D物体被光源照亮，且光子可以在到达观看者的眼睛以前从一个物体反弹到另一个物体。光线追踪技术则是反过来，通过从我们的眼睛（观景式照相机）反向追踪光线捕捉这些效果，通过追踪2D视表面上每个像素的光线的路径，并应用到场景的3D模型中。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/90RBB8jjBpldk8dSj4azKvT7cBDF7iaN8DLnVKUITSdxwAibE8aBRyHURF8cmy58cOCImibZ1QdTTnxCb5EkHrIAw/640?wx_fmt=png)
可想而知，这种技术的计算量非常大，一般渲染复杂的特殊效果可能需要花上几天甚至几周的时间，所以此前该项技术一直仅限于高成本的电影制作中。
不过，目前，随着GPU性能日益强悍，能够支持光线追踪的电脑也越来越多，通过Volta架构的GPU配合英伟达的RTX技术，产品设计师、游戏设计师、建筑师们能够在几秒内即可生成逼真的产品模型。
> 
**二、新版Quadro GV100，首次采用Volta架构**
Quadro GV100 具有 32GB 内存，且可借助 NVIDIA NVLink 2 互联技术，通过并联两块 Quadro GPU 扩展至 64GB，在所有适用于此类应用的平台中其性能最高。
在性能方面，GV100 基于 NVIDIA Volta GPU 架构，可提供每秒 7.4 万亿次浮点运算的双精度性能、每秒 14.8 万亿次浮点运算的单精度性能、以及每秒 118.5 万亿次浮点运算的深度学习性能。NVIDIA RTX 内置的 NVIDIA OptiX AI-denoiser 可实现实时的 AI 去噪，英伟达表示且其性能相当于采用 CPU 时的 100 倍。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/90RBB8jjBpldk8dSj4azKvT7cBDF7iaN8TicP3Cds607FE60PoFibcD6hibibpmJPicua0M5q43Zr0cLQpA3K3lsVXFA/640?wx_fmt=png)
> 
**三、医疗图像处理超级电脑Clara**
黄仁勋在现场推出了第一款专用于医疗图像处理的超级电脑Clara，它能够支持CUDA、CUDNN、TensorRT、OGL、RTX技术。
在现场，黄仁勋展示了一个医疗图像实时处理的影像片段。这个段影像是用十几年的超声波老设备拍摄而成，本来只能看到2D的黑白图像。然而当数据传进Clara后，配合人工智能软件，可以在2D图像中分析出3D的腔膛形状（图中红色部分）。因此，医院可以在现有医疗设备上直接接入这台电脑。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/90RBB8jjBpldk8dSj4azKvT7cBDF7iaN8sdmfw2UwhrRBhqyibuLpiat1lcXWQ3KcYFxicRYibOSNxFsQ9jDe0NSoWQ/640?wx_fmt=png)
目前，英伟达正在和众多医疗厂商合作，除了GE通用电气、三星电子等大厂外，还有像图玛深维、推想科技等AI医疗创业公司。
> 
**四、新版 TensorRT 推理软件 TensorRT 4，并将 TensorRT 集成至谷歌的 TensorFlow 框架。**
这是一款可编程应用平台（Programmable Inference Platform），当你将一个神经网络训练好了之后，可以通过TensorRT可编程平台，简便快捷地将这个训练好了的神经网络部署（Deploy）到英伟达的GPU上。
新版TensorRT 4能够支持INT8和FP16精度运算，能够将数据中心的功耗降低70%。
而且，英伟达还与谷歌进行了深度合作，将TensorRT整合进如今最广泛应用的AI开源框架谷歌TensorFlow 1.7中。而且现在还能够加速图像、视频、语言、NLP等AI应用。
英伟达表示，TensorRT 4 可用于快速优化、验证及部署在超大规模数据中心、嵌入式与汽车 GPU 平台中经过 训练的神经网络。相比 CPU，针对计算机视觉、神经网络机器翻译、自动语音识别、语音合成 与推荐系统等常见应用，该软件最高可将深度学习推理的速度加快 190 倍。而且为了进一步精简开发，英伟达与谷歌的工程师已将 TensorRT 集成至 TensorFlow 1.7，使得在 GPU 上运行深度学习推理应用更加容易。
> 
**五、AI平台新进展**
如同往届，黄仁勋对英伟达 AI 平台做了介绍，公布了其中的一系列重要进展，包括全新 Tesla V100 32GB GPU 的 2 倍内存、革命性的 NVSwitch 结构、以及全面的软件堆栈推动性能提升、深度学习工作站 DGX-2 成为首款性能高达每秒 2 千万亿次浮点运算的深度学习系统、发布深度学习引擎 TensorRT 4 等。英伟达表示，相较于六个月前发布的上一代产品 DGX-1，其深度学习工作负载性能实现了 10 倍提升。
在大会上，黄仁勋宣布，新版的 Tesla V100 内存扩容了一倍。「5 年前 AlexNet 在 ImageNet 上展示了突破性的能力，」黄仁勋说道，「它有 8 层，数百个参数。而今天我们能够看到数百层的神经网络，内含数十亿参数，深度学习模型经过五年的发展，体量扩大了 500 倍。」
而这样的计算需求可由「世界上最大的 GPU」DGX-2 进行处理，它是由 16 块 32GB 内存的 Tesla V100 计算卡通过 NVSwitch 进行连接（显卡间的通信速度是 PCI 的 20 倍，每秒 300Gbyte）所组成的，共拥有 2000TFPLOS 的 Tensor Core 算力，售价 39.9 万美元。NVSwitch 是今天黄仁勋宣布的全新的 GPU 互联结构。
DGX-2 是首款能够提供每秒两千万亿次浮点运算能力的单点服务器，具有 300 台服务器的深度学习处理能力，占用 15 个数据中心机架空间，而体积则缩小 60 倍，能效提升 18 倍。
而后，黄仁勋宣布了英伟达在 AI 推理上的一系列动作。黄仁勋表示，基于在数据中心、汽车应 用、以及包括机器人和无人机等嵌入式设备领域中，诸如语音识别、自然语言处理、推荐系统、 以及图像识别等新功能的支持，面向深度学习推理的 GPU 加速正在获得越来越多的关注。
「我们需要超级计算机来帮助自己寻找更高效的能源存储方法，探索地球的内部，预测未来的自然灾害，以及模拟微观世界的变化。」黄仁勋说道。
> 
**六、下一代DRIVE Orin自动驾驶芯片**
黄仁勋在现场还展示了英伟达感知基础（Perception Infrastructure）项目，这是一个大型的深度学习模型，能够收集并分析不同传感器（如摄像机、雷达等等）得出的距离、天气、雷达感知、高精地图等等不同数据。
在接下来2-3年间，英伟达还将技术研发这一技术，直到最后能够搭载在所有新车上。黄仁勋说，这是我们至今遇到的最为复杂的问题之一。
> 
**七、推出 DRIVE Constellation 仿真系统**
自动驾驶一直是 GTC 大会的重要部分，今天，英伟达展示了一套用于使用照片级真实感模拟，基于云的自动驾驶汽车测试系统。
该系统被称为 NVIDIA DRIVE Constellation，是一种基于两种不同服务器的计算平台。第一台服务器运行 NVIDIA DRIVE Sim 软件，用以模拟自动驾驶汽车的传感器，如摄像头、激光雷达和雷达。第二台服务器搭载了 NVIDIA DRIVE Pegasus AI 汽车计算平台，可运行完整的自动驾驶汽车软件堆栈，并能够处理模拟数据，这些模拟数据如同来自路面行驶汽车上的传感器。
要实现自动驾驶汽车的量产部署，我们需要一种能够在数十亿英里的行驶中进行测试和验证的解决方案，以实现足够安全性和可靠性。黄仁勋介绍说，DRIVE Constellation 可以将视觉计算和数据中心方面的专业知识相结合以实现这一目标。借助虚拟现实技术，测试者可通过对数十亿英里的自定义场景和极端情况进行测试，从而提高算法的稳定性，而花费的时间和成本仅为实际道路测试的一小部分。
> 
**八、联合芯片巨头ARM打造AI芯片专用IP**
英伟达重磅宣布，将联合芯片巨头ARM打造AI芯片专用IP，这款IP属于ARM几年2月公布的Trillium项目的一部分，其技术源于英伟达Xavier芯片以及去年开源的DLA深度学习加速器项目。
英伟达本次宣布同ARM合作，将在数十亿物联网设备上实现深度学习。NVIDIA深度学习加速器IP将集成到Arm的Project Trillium平台中，以便于构建深度学习IoT芯片。
去年，英伟达也正式免费开源了完整版DLA（Deep Learning Accelerator，深度学习加速器），让厂商可以免费下载使用，打造属于自己的低功耗AI芯片（比如IoT芯片）。
今年2月，芯片巨头ARM公布了其人工智能项目Trillium，同时推出两款专用IP，分别为物体检测OD处理器和机器学习ML处理器。
ARM是全球智能设备第一大主流芯片架构提供商，全球超过90%的智能设备采用了ARM的芯片架构，包括手机、平板、手表、电视、无人机等等。而英伟达，作为全球AI浪潮的引领者，能够为人工智能提供强大的计算力，二者一拍即合。
本次ARM牵手英伟达推出专用的IOT设备人工智能IP，将会有助于人工智能在终端设备广泛铺开，使得上亿、甚至数十亿台IOT设备都能够用上低功耗、低成本的AI芯片，使物联网芯片公司能够轻松地将AI集成到它们的设计中，并帮助它们将智能且价格实惠的产品带给全球数十亿的消费者。
NVIDIA副总裁兼自主机器事业部总经理Deepu Talla表示：“推理将成为每个物联网设备的核心能力。我们将与ARM一同推进这一趋势的发展，帮助数百家芯片公司轻松采用深度学习技术。”
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
