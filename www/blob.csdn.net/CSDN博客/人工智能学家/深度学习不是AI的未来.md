# 深度学习不是AI的未来 - 人工智能学家 - CSDN博客
2017年10月01日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：199
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUXZeZqflr1kXB1B6a8MCtR3BdfMlv2zKTElLcFwDiaI5ldWGsobdOJQ5vsdv0Picv6bKOGQIUWRviag/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
*来源：中国机器人*
*概要：深度学习并不是人工智能的同义词!由于谷歌、Facebook等巨头公司宣传人工智能工具时主要谈的就是深度学习，甚至只谈深度学习，因此大众误以为所有的人工智能新的篇章都(将)由深度学习书写。*
　　现在每一个人都正在学习，或者正打算学习深度学习，它是目前人工智能诸多流派中唯一兴起的一个。各个年龄阶段的数十万人都在学习着免费和收费的深度学习课程。太多的创业公司和产品的命名以“深度”开头，深度学习已然成了一个流行语，但其真正的落地应用实际上却很少。绝大多数人忽略了一个事实：深度学习只占机器学习领域的1%，而机器学习又只占人工智能领域的1%。而实际中的绝大多数任务则是用余下99%的知识技术来处理的。一个“只会深度学习的专家”并不是“人工智能专家”。
　　深度学习并不是人工智能的同义词!由于谷歌、Facebook等巨头公司宣传人工智能工具时主要谈的就是深度学习，甚至只谈深度学习，因此大众误以为所有的人工智能新的篇章都(将)由深度学习书写。然而，真实情况并非如此。决策树算法，比如 XGBoost没有成为头条，却在很多Kaggle表格数据竞赛中默默地击败了深度学习。媒体暗示AlphaGo的成功全部归于深度学习，但实际上它是蒙特卡洛树搜索+深度学习，这表明深度学习单枪匹马很难取胜。很多强化学习的任务是通过神经进化的 NEAT 算法(通过增强拓扑的进化神经网络)得到解决的，而不是反向传播算法。人工智能领域存在着“深度误传”。
**　　与100+国内外技术专家探索2017前瞻热点技术**
　　我并不是说深度学习没有解决问题：它令人印象深刻。树和其他算法并没有完胜深度学习，并且在某些任务上深度学习无法被取代，但是我希望未来一些非深度学习系统可被(重新)发现以击败深度学习。或许能解释目前深度学习决策的黑箱问题。同样我也希望能读到探讨“灾难性遗忘”问题的深度学习文章，它是指在学习新知识时快速遗忘先前已学习知识的倾向，并且需要每天对抗“过拟合”。关于“智能”：深度学习会简单相信所给的训练数据，而不去理解什么是真或假、现实或想象、公平或不公。人类也会误信假新闻，但只是在某种程度上，甚至孩童都知道电影是虚构的，不是真实的。想了解更多内容可以阅读这篇文章：《人工智能(深度学习)的简单解释》。
　　20 年前，每个人都在学习 HTML，这个手动编写网页的标记语言当时被认为足以成就一个互联网亿万富翁。和其他人一样，我学习了每一项看起来有用的技术，如 HTML、移动app和深度学习，并且我希望大家在今后的人生都一直学习新事物。事实上，你一生中不能只学习一项技术。即使你学习了深度学习，你也不会一辈子了解人工智能。1995 年 HTML 开始过时，无法满足需求，取而代之的是 CSS、Java 和服务器语言。同样地，深度学习有一天也会过时，并且无法满足需求。现在大多数流行的手机 APP 根本用不到 HTML，那么谁又会知道未来的人工智能APP是否用得到深度学习呢?
　　不过实际上，深度学习是 1980 年代的技术，比HTML还老：由于有了更多的训练数据，1970 年代的“带有隐藏层的神经网络”得到了更好的结果，被重新命名为深度学习，之后被大肆炒作。1992年我简要地查看了一些神经网络的源代码，以及分形算法和细胞自动机。正如绝大多数人一样，当时我并没有选择深度学习，只是把它当作毫无实际价值的学术数学难题。相反，我重点学习视频游戏的 3D 技术、互联网技术等等，因为它们可以即刻获得结果。但是我们都错了，深度学习借助大数据可以大有作为!2015 年的 Deep Dream 简直令我着迷，接着是GAN等等。不过，深度学习并不是人类可以创造的完美人工智能科技的终点。
　　数十年来，“古老”的深度学习技术已被广泛研究和更新以更准确地解决更多任务，但是没有一个深度学习网络结构(卷积、RNN、RNN + LSTM、GAN 等)可以解释其自身的决策。无疑深度学习在未来还会解决更多的问题，取代更多的工作，但不太可能解决所有的问题，或者保持惊人的进步以对其所作决定的公正性进行合理的解释。
![0?wx_fmt=png](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUXZeZqflr1kXB1B6a8MCtRApG9PnrnnuLo0LkoTibgOOynD3fxoMmFYBnyV2NF9vAxnJGdBk6qYSA/0?wx_fmt=png)
*　深度学习无法理解哲学家柏拉图与亚里士多德。*
　　未来人工智能应探索其他的新方法，或者被忽视的旧方法，而不仅仅是深度学习。深度学习的一个局限是把数据中最常见的内容作为真理，把统计学上较稀少、或与较常出现的内容相反的东西看作谬论。深度学习的公正性并非来自其自身，而是人类筛选和准备的深度学习数据。深度学习可以阅读并翻译文本，但不是以“人类的方式”。如果使用超过100本书训练深度学习模型：40本书告诉它仇恨、战争、死亡和摧毁如何是坏的，60本书告诉它希特勒的纳粹思想是好的，那么该模型最终会成为100%的纳粹!
　　如果在训练数据集中纳粹主义是最流行的观点，深度学习靠自己永远无法明白为什么杀害犹太人、同性恋以及残疾人是错误的。难怪深度学习无法解释其自身决策，除了最简单的：“我(深度学习)读到最多的是‘纳粹主义是正确的’，因此它应该是正确的”。深度学习将会学习并模仿最具缺陷的逻辑而不去思考它的缺陷，包括恐怖主义。甚至孩童都可以自己明白电影中哪个家伙是坏人，但是深度学习做不到，除非人类首先明确教导它。深度学习中有些东西很酷，比如带有反向传播的梯度下降，以及自定义的深度学习硬件，但这大多来自统计学和几何学，很可能不会出现在 2037 年的人工智能时代。
　　对很多任务来说，深度学习人工智能正在或者将会变成违法的、不被社会所兼容的。**2018年5月25日起，公司或个人在收集 28 个欧洲国家公民数据时应遵循《一般数据保护条例》(GDPR)。届时欧洲的一些APP将被禁止使用深度学习，这导致AI初创公司拼命寻找深度学习的替代方案，否则将面临罚款的风险。罚款金额为全球营收的4%，包括美国部分。关于自动化决策的GDPR要求深度学习具有解释其决策的能力，以及防止基于种族、观点、健康等歧视现象的发生。**类似于GDPR的法律正在全球范围内制定，这只是时间问题。《美国公平信用报告法》要求披露所有对消费者信用评分产生不利影响的因素，最多只允许有4个。深度学习的因素可谓海量，而不仅仅是4个，如何将其简化为4个呢?人工智能，正如比特币ICO，开始忽视法规，但是法律与惩罚总会降临。
　　如果深度学习系统的工作是采取更多相关决策，而不是简单区分一张图像是否是猫，或者在自拍的哪部分添加兔耳，它们将会被非深度学习系统取代。人工智能必须是负责任的，可以使用简单、合法有效的语言向法官和用户解释其输出结果，这与深度学习大不相同。深度学习的复杂性，对法官和用户来说就像是“魔术”，这是一种法律风险，而不是一个很酷的未来。深度学习将会建议或警示人类，比如从医疗图像中检测疾病，并获得医生的验证，但这只是缺乏细节的部分自动化。面对被人工智能拒绝、并寻求解释的人们(例如工作、贷款被拒绝等)，我们能说什么呢?
　　法律包含“解释权”，比如为什么工作或贷款被拒绝。深度学习给出的是非自然(合法)语言解释的结果。深度学习的代码容易获得，却不为法官或用户所接受，因为即使最好的数学家或其他算法也无法搞明白它，或者将模型简化成可以理解的语言。即使最后的决策是由人类做出的，人工智能工具也应给出详细的理由，人们可以认为它是错的(然后无视、推翻人工智能的决定)，或者通过简单的复制、粘贴并且签署人工智能给出的解释来快速接受。没有人知道如何修改深度学习以给出简单到人类可理解的解释，因此深度学习不可能做到完全与我们的要求相符!这一问题同样影响到了若干其他人工智能和机器学习算法，但不像深度学习受影响那么严重。比如，如果决策树算法变成提升树或集成树，它也会变得不可解释。但是未来，可能会出现或发现能为自己的决策辩护的新的人工智能，它们将会在常规决策中取代深度学习和人类的位置。
　　在GDPR规定的情况中，只有人类职员可以拒绝申请：人工智能可自动化完成积极的结果，否则，如果它拒绝了一项贷款、工作等，就应该将这项任务交给人，让人工来处理这些会让用户感到生气或者要追究的消极结果。但是在拒绝的情况下，人类将不会从基于深度学习的人工智能中获得帮助或解释，他们不能获知深度学习的逻辑是否正确。他们需要自己从头检查数据，以决定是否最终拒绝，并且要为这个决定写一份合理的解释。此举风险在于为了节约时间和成本，人类员工会为人工智能的拒绝决定编造假的解释，并盲目接受人工智能的认可。但是决定人工智能给出拒绝决策的公平性的法官会询问为什么别人的被接受了，并且进行对比。安全起见，无论类似GDPR的法规是如何规定的，对于接受和拒绝，你都要有充足的理由。非深度学习的人工智能系统将最终成为唯一被人类所采用的系统，它们会把所有决策的解释提供给用户、法官和支持人员，不论是完全或部分自动化的决策。
　　在法律和深度学习之前，解释性已经是一个大问题。在反垄断案例中，谷歌等公司被质问为什么是这个产品而不是其他产品出现在搜索结果的第一个。下面也是深度学习出现之前的事：很多其他的算法同样以疯狂的方式混合数据以得到结果，因此没有哪个人可以轻易地重构出决策原因。法官被告知：工程师并不了解详情，以及被当作证据的几页线性代数。这样不会有好结果：甚至在特定的法律存在之前，多个案例承担着数十亿美元的罚款，收到要更变系统的警告。被自动拒绝了工作、贷款、退款的用户集体起诉商店、银行或保险公司的自动决策单元，将会变成常态，所以无法解释便意味着“无以辩护”、被罚款以及一场品牌公关灾难。
　　对大部分人来说，“人工智能”意味着科幻电影中能够给出聪明解释的人工智能，电影中人类可以快速决定自己是否同意，这样易于进行法律验证。大多数听到公司是“人工智能第一位”或“增加人工智能”的人，包括法官和撰写《一般数据保护条例》(GDPR)等法律的人，都会期待和电影中一样的“人工智能”，即如果被法院传召，它能够捍卫自己的决定，这令用户和法官都印象深刻。然而我们得到的是无法解释的“深度学习人工智能”，仅仅因为其缺乏可解释性，这些人工智能即使在它们能够解决的问题上也不会经常得到使用。深度学习无法节省成本，也不会取代那些需要敏锐的自动决策的工作。即使在人类必须作出最终决策的情况下，人工智能工具能够解释自己的建议，也比它不给出理由就做出回应更加可取。可解释的人工智能一旦被(重新)发现，将会更加安全、合法、廉价、快速，并且取代深度学习和人类。深度学习在 20 世纪60到80年代发明，而到2010年才重新被发现，或许未来可解释的人工智能基础也已经被某些研究者描述出来，但是由于不是深度学习，所以可能在许多年内都没人关心和开发该类型的人工智能，直到它们被重新发现和炒热。
　　关于自动决策的GDPR也需要防范基于种族、意见、健康状况等产生的歧视。但是使用用户生成的数据，如社交媒体和新闻(而不是真实的数据，例如医疗或财务记录)训练的深度学习模型通常暗含邪恶的偏见。如前所述，深度学习可以读取大量文本和数据，并模仿其内容，但无法批判性地理解内容。深度学习只相信它在数据中频繁看到的事物、底层模式和趋势，因此它会放大人类社会的偏见和问题。如果数据显示被逮捕的黑人比白人多，那么一旦有人犯罪，深度学习将首先怀疑黑人。数据显示公司董事会董事中男性比例高于女性，深度学习将在招聘中更倾向于男性应聘者。
　　深度学习决策会比训练数据的平均样本包含更深刻的偏见，如种族歧视、性别歧视。这个问题在所有的机器学习算法中都会发生，但是深度学习模型是其中最难测试、检测、控制和调整的。例如聊天机器人变得纳粹化、充满仇恨，或者美图软件中给黑人照片美白。这个问题很难解决，与其试图解决它，很多深度学习实验直接因为它而突然取消。
![0?wx_fmt=png](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUXZeZqflr1kXB1B6a8MCtRm4h1Cg08tKFAuuicy1lGZME1QBYHCXlTluhI3Liaa9ibj9B8pbI6zPoicw/0?wx_fmt=png)
　　你无法通过在训练之后添加补丁，来修复一个带有偏见、种族和性别歧视的深度学习模型。深度学习是一个神经网络，与其他人工智能方法不同，你无法通过局部补救来修改某个答案，而是必须使用不同的、完全平衡以及公正的、稀有的真实世界数据对该网络进行重新训练。深度学习可以在不理解数据的情况下模仿数据中的内容：它不会否定任何数据，不会发现社会上的偏见，而只是“学习数据”。你应该雇佣一个人类员工，专门创建来自理想社会的假的、公正的数据，在这里白人与黑人被逮捕的频率相同，董事会中50%的董事都是女性，等等。但是，由人类专家创建海量无偏见数据的成本如果仅是为了训练深度学习模型，用人工智能取代人类的价值又在哪里呢!此外，即使你已经训练出真正公正的深度学习模型，你也没有证据可以向法官或用户证明其决策的公正性，因为它无法提供解释。
　　深度学习用于没有法律风险的非商业应用或者游戏，其重要性将会降低。当可解释的人工智能流行起来时，深度学习将会像磁带或阴极电视一样被抛弃。在游戏中输给机器人的人类不太可能说服法官因为人工智能公司无法解释人工智能是怎么赢的而对其罚款。不满FaceApp把自己的自拍照修的更老、更年轻，或者换了性别的人也不太可能说服法官因为FaceApp无法解释人工智能是如何决定新面孔的而对其罚款(除了一个“种族变化”滤镜，遭到大规模抗议后被移除，完全不需要法官参与)。在医疗图像中进行疾病检测是一项安全的深度学习应用，前提是用户在服药之前先向人类医生寻求确认。
　　合法的深度学习市场非常有限：在决策结果造成财政、健康上的区别，或者存在歧视，而深度学习无法帮助人们理解决策是否公正以及为什么公正的时候，法官都可以进行处罚。那么自动驾驶呢?似乎在艺术、游戏或高级幽默以外的领域使用深度学习都有法律风险。有需要时，现有的非深度学习方法可以取代深度学习，新方法也会被(重新)发现，因此人工智能的发展将会继续顺利进行。尤其是如果每个人都将研究(并投资)人工智能和机器学习科学领域中的所有新旧算法，而不只是深度学习：这也是成为“人工智能全方位专家”的唯一路径。
　　深度学习除了正在“非法”用于很多可解任务以外，它也不能被用于解决以下一系列问题：那些需要抽象推理来找出数据中哪些是公平，哪些是不公平的任务，以及那些需要解释其自行作出的决定的任务。即使对于那些不需要解释的任务，例如图像识别，深度学习看起来是最好的系统，但是也不如人类自己的眼睛保险。你可以轻而易举地使用“对抗样本”来让深度学习系统出错：为一张猫的图片加入一些不可见的噪点，机器就会把它误认为其他不相关的东西，比如一只狗。人类看到这样的图片仍然看得出它是一只猫，而深度学习会将其理解成一只狗或者其它黑客秘密嵌入的东西。这一点可以用到路牌上，来黑掉目前的自动驾驶汽车。新一代的人工智能系统必须克服这个问题才能取代深度学习。
　　著名深度学习库Keras作者François Chollet曾在一篇名为《深度学习的限制》的文章中说到：“深度学习唯一真正能成功做到的是使用连续几何变换，在给定大量人为标注数据的情况下将空间X映射到空间Y的能力。”这些空间拥有多重维度，不仅仅是三维的，这就是深度学习可以模仿毕加索的作画风格、在德州扑克中“Bluff”，以及在其他任务中模仿人类创造力的原因。但是对于外行人来说，这也许意味着：深度学习模型经过训练可以拥有识别猫的能力，而它本身不知道什么是猫;它可以是一个种族主义者，但不知道什么是种族主义。深度学习可以识别猫、具有种族主义、或赢得很多游戏，这一点是令人瞩目的，有时也有实际用途，但是深度学习无法解释为什么图中的动物是猫，或者一个决策是否带有种族歧视。
　　在《深度学习的未来》一文中，Keras的作者描述了一种只有“几何模块”的全新深度学习系统，它应该与尚未出现的“算法模块”和“元学习器”相关。这种系统可以显著增加能够解决的问题类型与数量，但因为深度学习模块的存在，这种方式仍然无法解释它的决策。这就像我们不能用言语解释大脑中计算出的某些感觉或图像。人类会解释一切事物，但大多数都是编造的、过于简单的理由，而每个人似乎都认为是准确的。机器的算法却总是被要求做到完全准确。有一些专家正在开发完全不包含深度学习的全新人工智能系统，但他们缺乏资金支持：现在所有人都只投资深度学习，而且这个风潮还将持续一段时间。没有人知道下一个人工智能大事件将会是关于什么的，但不太可能是深度学习2.0。
　　深度学习正被大肆炒作，因为尽管涉及利益冲突，目前也只有出售深度学习软、硬件的人在关于人工智能的访谈中发声。你可曾见过任何“自然智能”专家，如心理学家和哲学家支持过深度学习?
　　如果你并不急于学习人工智能，或者还没有时间，我认为你可以等待下一代人工智能系统的兴起然后直接学习它，跳过深度学习 1.0 时代。如果你急需学习人工智能，或者自己也有足够的时间，我建议你深入了解整个人工智能及机器学习领域的知识，而不仅仅是深度学习。
