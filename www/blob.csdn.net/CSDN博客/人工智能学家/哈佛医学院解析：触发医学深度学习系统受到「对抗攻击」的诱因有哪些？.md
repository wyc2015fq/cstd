# 哈佛医学院解析：触发医学深度学习系统受到「对抗攻击」的诱因有哪些？ - 人工智能学家 - CSDN博客
2018年04月23日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：182
![640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/waLJGrhJM0fGJDibT169HYNKib1iaof3w7SLu4USWtctyaQs0aPoIiclp9ISzbvyt0qOCf2EPJic4nJnicyzskYb1qwA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
原文来源：arXiv
作者：Samuel G. Finlayson、Isaac S. Kohane、Andrew L. Beam
「雷克世界」编译：EVA
对抗样本的发现引起了人们对深度学习系统的实际部署的关注。在本文中，我们认为，就货币激励和技术脆弱性（monetary incentives and technical vulnerability）而言，医学领域可能特别容易受到对抗攻击的影响。为此，我们概述了医疗保健经济及其针对欺诈所创立的激励措施，我们将对抗攻击扩展到三个流行的医学成像任务，我们还提供了具体的例子以说明这些攻击可以被实际执行的方法和原因。
对于我们每一个具有代表性的医学深度学习分类器，白盒和黑盒攻击都是有效的，而且是人类难以察觉的。我们强烈建议在临床环境中使用深度学习系统，并鼓励对特定领域的防御策略进行研究。
在过去六年里，深度学习已经改变了计算机视觉，并且已经在大量面向消费者的产品中得到了应用。令许多人感到兴奋的是，这些方法将继续扩大范围，而且新的工具和产品将通过深度学习的使用得到改善。深度学习的一个特别令人兴奋的应用领域是在临床应用中。近期有许多备受瞩目的深度学习例子在放射学、病理学和眼科学任务中取得了与人类医生同等的地位。
在某些情况下，这些算法的性能超过了大多数个体医生在进行面对面比较时的能力。这导致一些人推测，医学成像的整个专业，如放射学和病理学，可能会被彻底改造或完全消失。此外，在2018年4月11日，人们朝着这个未来迈出了重要的一步：美国食品和药物管理局（the U.S. Food and Drug Administration， FDA）宣布批准了第一个计算机视觉算法，该算法可以在没有人类临床医生介入的情况下被用于医学诊断。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fGJDibT169HYNKib1iaof3w7Siay4bTKVSSzDSpETZSTEibOkucbJy92zSdIrrObz0b9cibnXrlZxpAgSA/640?wx_fmt=png)
图1：对抗样本的概述：通过添加一个精心设计的干扰，能够将一张模型正确归类为良性的图像转换为网络100％确信为恶性的图像
与医学深度学习的进步并行的是，所谓“对抗样本”的发现揭示了即使是最先进的学习系统也存在漏洞。对抗样本——被设计为会导致错误分类的输入——已经迅速成为机器学习社区中最受欢迎的研究领域之一。虽然人们对对抗样本的许多兴趣来源于它们能够阐明当前深度学习方法中可能存在的局限性的能力，但对抗样本还是受到了关注，因为它们可能对在虚拟和物理环境中部署这些算法造成网络安全威胁。然而，在以往的研究中，科学家们尚未彻底解决在医学背景下发生对抗攻击的可能性。
考虑到美国医疗保健的巨大成本，将昂贵的人类“赶出圈子”并用一种极其廉价和高度精确的深度学习算法取代他或她，似乎是一种明智的做法。这种做法看起来尤其诱人，鉴于最近的一项研究发现，医生和护理人员的薪酬是美国医疗保健成本相对于其他发达国家而言较高的主要驱动因素之一。
然而，考虑到目前这些算法的脆弱性，医学成像任务广泛自动化的实现过程中存在一个未被重视的缺点：如果我们认真地考虑将人类医生完全“赶出圈子”(现在在一个设置中至少有通过美国食品和药物管理局的法律制裁,后续可能会有更多的行动),我们也不得不考虑对抗攻击可能会如何为欺诈和危害带来新的机会。事实上，即使在循环过程中有人类存在，任何利用机器学习算法进行诊断、决策或赔偿的临床系统都可能受到对抗样本的操纵。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fGJDibT169HYNKib1iaof3w7SkoDH0XrsAElJwRLMkZoYD9NDOhuSEetYYZiaf79BRYUmibuoA4XDJ4oA/640?wx_fmt=png)
图2：对抗样本生成的特征结果。显示在每幅图像左下角的百分比表示模型指定该图像患病的概率。绿色=模型在该图像上是正确的。红色=模型不正确。可以看出，在每种情况下，人类不可感知的变化足以使分类器对错误的分类100％自信
在本文中，我们认为，医疗保健尤其容易受到对抗攻击的影响，并且存在巨大的诱因去激励潜在的不良行为者实施这些攻击。我们将以往的对抗样本研究结果扩展到三种以最先进的医学分类器为模型的医学深度学习系统中，并且我们提供了关于医学中对抗攻击可能性范围的观点。由于医疗保健系统很复杂，而且管理过程可能显得错综复杂，人们可能难以想象这些攻击是如何实施的。
因此，为了在实际案例中对危害的抽象可能性进行具象化，我们描述了一个展望，在其中，许多任务已经通过深度学习实现完全自动化，并给出可能由对抗攻击所导致的欺诈行为的具体例子。我们的目标是提供使对抗攻击成为威胁的医疗管道的不同特征的背景，同时证明这些攻击在真实的医学深度学习系统中的实际可行性。我们希望，对医学中对抗样本潜在危害认识的增强能够鼓励机器学习社区致力于解决方案的研究，使这些技术能够安全地部署在医疗保健领域。
可以说，使用深度学习改善医疗保健和医学的发展前景是非常令人兴奋的。我们之所以这么乐观是因为，如果这些技术能够得以明智地实施，就可以改善结果并降低成本。有鉴于此，不出意外地，数十家私营公司和大型医疗中心已经开始努力在临床实践环境中部署深度学习分类器。随着这种发展趋势的推进，医学深度学习算法将不可避免地在已经拥有数十亿美元的医疗信息技术行业中占据优势。然而，医疗保健经济的大规模发展也为欺诈行为带来了重大发展机遇和诱因。
在这项研究中，我们概述了对抗样本之所以能够在医疗领域造成不成比例的巨大威胁的系统性和技术性原因。我们也证明了我们所认为的第一个在医疗系统上执行对抗性攻击的例子。我们希望我们的研究结果有助于促进计算机科学家和医疗专业人士之间对于对抗样本威胁的讨论研究。对于机器学习研究人员，我们建议对那些设计用以确保攻击不可行或至少可以追溯识别的基础设施和算法解决方案进行研究。对于医疗服务提供者、付款人和政策制定者，我们希望这些实际案例可以激发人们有意义的讨论，以探讨这些算法究竟应该如何融入临床生态系统，尽管他们目前很容易受到此类攻击。
原文链接：https://arxiv.org/pdf/1804.05296.pdf
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
