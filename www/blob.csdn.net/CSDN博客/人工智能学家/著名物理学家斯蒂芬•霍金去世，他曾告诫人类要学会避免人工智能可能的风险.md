# 著名物理学家斯蒂芬•霍金去世，他曾告诫人类要学会避免人工智能可能的风险 - 人工智能学家 - CSDN博客
2018年03月14日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：336
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBXvkp4oFoGLr7gJgqH92ViauAZMB2ffglD9rzjo0N7CDSj7yl2yLaNQV4ENCPpeqyTMnJOiczfhHG3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
据多家媒体报道，著名的英国物理学家斯蒂芬·霍金于3 月 14 日去世，享年 76 岁。霍金教授的孩子露西，罗伯特和蒂姆发表了声明确认了这一消息。
斯蒂芬·威廉·霍金(Stephen William Hawking)，1942年1月8日出生于英国牛津，英国剑桥大学著名物理学家，现代最伟大的物理学家之一、20世纪享有国际盛誉的伟人之一。其主要研究领域是宇宙论和黑洞，证明了广义相对论的奇性定理和黑洞面积定理，提出了黑洞蒸发理论和无边界的霍金宇宙模型，在统一20世纪物理学的两大基础理论——爱因斯坦创立的相对论和普朗克创立的量子力学方面走出了重要一步。获得CH(英国荣誉勋爵)、CBE(大英帝国司令勋章)、FRS(英国皇家学会会员)、FRSA(英国皇家艺术协会会员)等荣誉。
霍金在生前曾一再警示我们谨慎发展人工智能，他关于人工智能可能存在的威胁方面的言论引导了人类关于人工智能伦理的思考。这对人工智能的健康发展无疑具有深远影响。
2017年11月，霍金在接受《连线》杂志采访时表示，人工智能有可能会取代人类，最终会演变成一种超越人类的新生命形式。他认为，即使人们会设计出计算机病毒，但也有人会相应的改进和完善自己的AI技术，到那时就会出现一种超越人类的新生命形式。
霍金认为，为了防止那一天的到来，人类的对策应该是寻找新的行星。因为到时候追求效率的机器会想“甩脱”人类，而人类已经到达了一个不能后退的临界点。地球对人们来说太小了，全球人口正在以惊人的速度增长，人们正处于自我毁灭的危险境地之中。因此他倡导应该有更多人投身于科学事业。霍金称，制定新的太空计划是当前科学界的迫切工作，一个新的“太空时代”可以吸引年轻人参与科学，进而找到宇宙中合适的行星作为人类另一个可以居住的地方。
但霍金也强调，当今人工智能正在飞速发展，获得了巨大的投资，也存在巨大的优势。技术革命可以帮助消除工业化对自然界造成的一些损害。
霍金说，我们的目标是最终根除疾病和贫穷，在这一过程中我们生活的方方面面也会转变。而“成功创造了人工智能，很可能是人类文明史上最伟大的事件。但如果我们不能学会如何避免风险，那么我们会把自己置于绝境”。
霍金已不是第一次对人工智能的崛起表示担心。2016年10月，霍金也曾警告称，人工智能可以发展自己的意志，并与人类的意志抗衡。他在剑桥大学智能未来研究中心发表演讲探讨人工智能快速发展的意义时表示：人工智能的崛起对于人类来说既是好事也是坏事。生物大脑能够实现的目标，和计算机能够实现的目标之间没有太大区别。因此从理论上来说，计算机可以模拟人类的智慧并超越它。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
