# 揭秘|多伦多大学反人脸识别，身份欺骗成功率达99.5% - 人工智能学家 - CSDN博客
2018年06月10日 18:40:16[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：84
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb2NhLuZhPNNThl4DqlYjOia6w3xNlE2tG9qyvF8fHSYdOy9QdrdpFNIEvXphic0Wib5K2HuyvlHbL8JA/640?wx_fmt=png)
来源 ：机器人大讲堂
摘要：在一些社交媒体平台，每次你上传照片或视频时，它的人脸识别系统会试图从这些照片和视频中得到更多信息。比如，这些算法会提取关于你是谁、你的位置以及你认识的其他人的数据，并且，这些算法在不断改进。
**现在，人脸识别的克星——“反人脸识别”问世了。**
多伦多大学Parham Aarabi教授和研究生Avishek Bose的团队开发了一种算法，可以动态地破坏人脸识别系统。
**他们的解决方案利用了一种叫做对抗训练（adversarial training）的深度学习技术，这种技术让两种人工智能算法相互对抗。**
**现在，深度神经网络已经被应用于各种各样问题，如自动驾驶车辆、癌症检测等，但是我们迫切需要更好地理解这些模型容易受到攻击的方式。**在图像识别领域，在图像中添加小的、往往不可察觉的干扰就可以欺骗一个典型的分类网络，使其将图像错误地分类。
这种被干扰的图像被称为对抗样本（ adversarial examples），它们可以被用来对网络进行对抗攻击（adversarial attacks）。在制造对抗样本方面已经有几种方法，它们在复杂性、计算成本和被攻击模型所需的访问级别等方面差异很大。
一般来说，对抗攻击可以根据攻击模型的访问级别和对抗目标进行分类。白盒攻击（white-box attacks）可以完全访问它们正在攻击的模型的结构和参数；黑盒攻击（black-box attacks）只能访问被攻击模型的输出。
一种基线方法是快速梯度符号法（FGSM），它基于输入图像的梯度对分类器的损失进行攻击。FGSM是一种白盒方法，因为它需要访问被攻击分类器的内部。攻击图像分类的深度神经网络有几种强烈的对抗攻击方法，如L-BFGS、acobian-based Saliency Map Attack(JSMA)、DeepFool和carlin - wagner等。然而，这些方法都涉及到对可能的干扰空间进行复杂的优化，这使得它们速度慢，计算成本高。
**与攻击分类模型相比，攻击目标检测的pipeline要困难得多。**最先进的检测器，例如Faster R-CNN，使用不同尺度和位置的对象方案，然后对它们进行分类；其目标的数量比分类模型大几个数量级。
此外，如果受到攻击的方案只是总数的一小部分，那么仍然可以通过不同的方案子集正确地检测出受干扰的图像。因此，成功的攻击需要同时欺骗所有对象方案。
在这个案例中，研究人员证明了对最先进的人脸检测器进行快速对抗攻击是可能的。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/UicQ7HgWiaUb2NhLuZhPNNThl4DqlYjOia69lzRtmIweWjUibDicUFz9Eo0eicAA06wUFJcdY7IAPdDvbAANvcHIEIQA/640?wx_fmt=jpeg)
**研究人员开发了一种“隐私滤镜”，可以干扰人脸识别算法。**该系统依赖于2种AI算法：一种执行连续的人脸检测，另一种设计来破坏前者。
研究人员提出一种针对基于Faster R-CNN的人脸探测器的新攻击方法。该方法通过产生微小的干扰（perturbation），当将这些干扰添加到输入的人脸图像中时，会导致预训练过的人脸探测器失效。
为了产生对抗干扰，研究人员提出针对基于预训练Faster R-CNN人脸检测器训练一个生成器。给定一个图像，生成器将产生一个小的干扰，可以添加到图像中以欺骗人脸检测器。人脸检测器只在未受干扰的图像上进行脱机训练，因此对生成器的存在浑然不觉。
随着时间的推移，生成器学会了产生干扰，这种干扰可以有效地欺骗它所训练的人脸探测器。生成一个对抗样本相当快速而且成本低，甚至比FGSM的成本更低，因为为输入创建一个干扰只需要在生成器经过充分的训练后进行前向传递（ forward pass）。
**两个神经网络相互对抗，形成“隐私”滤镜**
**研究人员设计了两个神经网络：第一个用于识别人脸，第二个用于干扰第一个神经网络的识别人脸任务。**这两个神经网络不断地相互对抗，并相互学习。
其结果是一个类似instagram的“隐私”滤镜，可以应用于照片，以保护隐私。其中的秘诀是他们的算法改变了照片中的一些特定像素，但人眼几乎察觉不到这些变化。
“干扰性的AI算法不能‘攻击’用于检测人脸的神经网络正在寻找的东西。” 该项目的主要作者Bose说：“例如，如果检测网络正在寻找眼角，干扰算法就会调整眼角，使得眼角的像素不那么显眼。算法在照片中造成了非常微小的干扰，但对于检测器来说，这些干扰足以欺骗系统。”
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/LJiau2qPWAcUwBf77uC9c7Jdj6fLl9DEDICgOsIR6Y5clkQlcFeB1o7SMKrz4FJ3G5ETlzVwjMGdchcV1nwUUlw/640?wx_fmt=jpeg)
算法1：对抗生成器训练
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb2NhLuZhPNNThl4DqlYjOia66cicXU1bp0BlkbFTRu0s8hwRq69Lupm74pyQYn2CWGdKQeKVicuKDTqw/640?wx_fmt=png)
给定人脸检测置信度的对抗成功率。α值是边界框区域被分类为人脸之前的confidence threshold，右边两列表示600张照片中检测到脸部的数量。
研究人员在300-W人脸数据集上测试了他们的系统，该数据集包含多种族，不同照明条件和背景环境的超过600张人脸照片，是一个业界的标准库。结果表明，他们的系统可以将原本可检测到的人脸比例从接近100%降低到0.5％。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb2NhLuZhPNNThl4DqlYjOia6dmyAyYRYWSNn4hL7UnpOXFH09BPJYNH9dB59cia1lrhKSTlfwMbbydA/640?wx_fmt=png)
**所提出的对抗攻击的pineline，其中生成器网络G创建图像条件干扰，以欺骗人脸检测器。**
Bose说：“这里的关键是训练两个神经网络相互对抗——一个创建越来越强大的面部检测系统，另一个创建更强大的工具来禁用面部检测。”该团队的研究将于即将举行的2018年IEEE国际多媒体信号处理研讨会上发表和展示。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb2NhLuZhPNNThl4DqlYjOia6w3xNlE2tG9qyvF8fHSYdOy9QdrdpFNIEvXphic0Wib5K2HuyvlHbL8JA/640?wx_fmt=png)
将300-W数据集的人脸检测和相应的对抗样本进行对比，这些样本具有生成的干扰，没有被Faster R-CNN人脸检测器检测到。被检测到的人脸被包围在具有相应置信度值的边界框中。 为了可视化，干扰被放大了10倍。
除了禁用面部识别之外，这项新技术还会干扰基于图像的搜索、特征识别、情感和种族判断以及其他可以自动提取面部属性。
**接下来，该团队希望通过app或网站公开这个隐私滤镜。**
“十年前，这些算法必须要由人类定义，但现在是神经网络自己学习——你不需要向它们提供任何东西，除了训练数据，”Aarabi说。“最终，它们可以做出一些非常了不起的事情，有巨大的潜力。”
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
