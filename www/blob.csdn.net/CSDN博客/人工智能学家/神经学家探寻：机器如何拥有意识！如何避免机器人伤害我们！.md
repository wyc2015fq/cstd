# 神经学家探寻：机器如何拥有意识！如何避免机器人伤害我们！ - 人工智能学家 - CSDN博客
2017年11月11日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：89
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBVsicCvXicpKpQIn1XN2PeremY3M9Z2C5kuEPJOAWveJeWQzr3PJlHLto0OXadB4W8UYPF5kbt4khkg/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
*来源：全球人工智能*
*概要：好莱坞导演们并非是对此问题困惑不解的唯一群体。随着机器智能的飞速发展，它不仅能在魔兽争霸（DOTA）和围棋这样的游戏中战胜人类玩家，而且对战过程中还不需要人类的专业知识，这一问题又一次成了科学的主流。*
**1.神经学家的探寻：这就是机器如何拥有意识的方法**
在亚力克斯·嘉兰2015年的大作《机械姬》中，不同于导演设定的那样，主人公并非迦勒这名负责评估机器意识的年轻程序员。相反，电影主人公是他的评估对象“伊娃（Ava）”，一个令人惊叹的人形机器人，它既有孩子般的天真，也有高深莫测的头脑。
如同大多数烧脑电影一样，《机械姬》让观众自行判断，“伊娃”是否具有自我意识。在此过程中，它还巧妙地避免了一个棘手的问题，这一问题迄今为止仍挑战着大多数以人工智能为主题的电影：什么是意识，机器能否拥有意识？
好莱坞导演们并非是对此问题困惑不解的唯一群体。随着机器智能的飞速发展，它不仅能在魔兽争霸（DOTA）和围棋这样的游戏中战胜人类玩家，而且对战过程中还不需要人类的专业知识，这一问题又一次成了科学的主流。
**机器处于意识的边缘吗？**
本周，法兰西学院的认知科学家斯坦尼斯拉斯·迪昂、加州大学洛杉矶分校的Hakwan Lau和巴黎文理研究大学的希德·库韦德尔共同在著名的《科学》杂志上发表了一篇评论文章，他们认为：虽然机器现在还没有意识，但未来已有一条清晰的发展道路。
原因是什么？他们说，意识是“绝对的计算能力”，因为它是特定类型的信息处理产生的结果，这是由大脑的硬件实现的。
没有魔法果汁，没有多余的火花——事实上，一个用于体验的部分（“什么是有意识？”）甚至没有必要去实现意识。
如果意识仅仅是由我们的三磅重的大脑计算而产生，那么拥有同样质量的机器也只存在着将生物信息转化为代码的问题。
他们写到，就像当前强大的机器学习技术从神经生物学中大量借鉴一样，我们或许可以通过研究我们大脑中的结构实现人为意识，这些结构能产生意识并且像计算机算法一样实现这些意识。
**从大脑到机器人**
毫无疑问，人工智能领域已经从我们对大脑意识的了解中受益匪浅，无论是在意识形式还是功能方面。
例如，“深度神经网络”是一种算法的架构，它可以让阿尔法狗（AlphaGo）在与人类玩家比赛中获得压倒性胜利，有一小部分基础是我们的大脑细胞自我组织的多层次的生物神经网络。
“强化学习”是一种“训练”，它可以让人工智能向数百万个例子学习，它源于一种有着几百年历史的技术，就像所有人熟悉的训狗一样：如果它发出了正确的反应（或结果），则给予奖励；否则，得要求它再试一次。
从这个意义上说，将人类意识的架构转换成机器似乎就能轻易实现人工意识。只是还有一个大问题。
斯图尔特·拉塞尔博士于2015年《科学》杂志采访中说到：“人工智能领域中尚未有人正开发有意识的机器，因为我们有心无力，对此实在是毫无头绪”。他是《人工智能：现代方法》的作者。
**多层次意识**
在我们考虑编码机器意识之前，最难的部分则是弄清楚意识究竟是什么。
对于迪昂和他的同事来说，意识是一个多层次的结构，有两个“维度”：C1意识，大脑中的已有信息，以及C2意识，获取和监控自己信息的能力。这两者对意识来说都是必不可少的，但二者不需同时存在。
假设你在开着一辆车，而低燃油警告灯亮了。在这里，我们注意到燃油箱灯就是C1意识，这是一种我们可以做出反应的心理表征：我们注意到它，采取相应行动（重新注满油箱），并在之后回忆起来并谈论它（“我在郊区的汽油用光了！”）迪昂在接受《科学》杂志采访时解释道，“我们想要从意识中区分的第一个意义是全局可用性的概念”。
他补充道，当你意识到一个词的时候，你的整个大脑都会意识到它，从某种意义上说，你可以在不同的模式中使用信息。
但是，C1意识不只是一个“心理素描本”。它代表了一种完整的体系结构，可以让大脑从我们的感觉或对相关事件的记忆中提取多种形式的信息。
潜意识的处理过程往往依赖于特定的“模块”，这些“模块”能够胜任一组既定的任务，而C1与之不同，它是一个全局性的工作空间，可以让大脑整合信息，决定行动，并一直工作到最后。
就像《饥饿游戏》（The Hunger Games）一样，我们所说的“意识”可以有任意的表现形式，在某一时刻，“意识”在进入这个心理工作空间的比赛中国获得胜利。不同的大脑计算回路会共享这些获胜的意识，并在决策过程中一直处于中心位置，指导人类行为。
作者解释说，由于这些特征，C1意识是高度稳定的和全局性的意识，会触发所有相关的大脑回路。
对于像智能汽车这样的复杂机器来说，C1意识是解决诸如低燃料灯这样迫在眉睫的问题的第一步。在这个例子中，光本身是一种潜意识信号：当它闪烁时，机器上的所有其他进程仍然不知情，而汽车——即使配备了最先进的视觉处理网络——也毫不犹豫地通过了加油站。
有了C1意识之后，油箱就会向汽车内置电脑发出警报（允许灯光进入汽车的“意识大脑”中），进而查看内置的全球定位系统（GPS）来寻找下一个加油站。
迪昂说：“我们认为，在机器中，这将转化为一个系统，从封装信息的任何处理模块中提取信息，并将信息提供给其他任何处理模块，这样他们就可以使用这些信息。”“这是第一种意识。”
**元认知**
在某种程度上，C1意识反映了大脑获取外部信息的能力。C2意识则与大脑内部信息相关。
文章作者定义了意识的第二个方面，即C2意识，能反映你是否知道或察觉到某件事，或者你是否刚犯错误的“元认知（meta-cognition）”。（“我想我可能是在最后一个加油站加满油了，但我忘记了保留收据以防万一”）。这个维度反映了意识和自我意识之间的联系。
C2是一种意识层次，它会让你在做出选择时感到信心不足或者信心满满。在计算术语中，它是一种算法，可以预测出一个决策（或计算）是正确的，尽管它经常被当作一种“直觉”。
C2意识也会影响记忆和好奇心。这些自我监控的算法让我们知道了所谓的“元记忆”，有人对此有所了解，也有人从未听过，它是负责感知自己舌尖上是否有东西。迪昂说，监控我们所知道的（或不知道）东西对于儿童来说尤其重要。他解释说：“孩子们绝对需要监控他们所知道的东西，以便他们提出疑问、对事物变得好奇并学习更多知识。”
在意识的两个方面共同作用下，我们能从中受益：C1意识让相关信息进入我们的精神工作空间（同时丢弃其他“可能”的想法或解决方案），而C2意识则有助于意识思维是否带来有益回应的长期反射。
回到低燃料警示灯的例子中，C1意识让汽车在第一时间解决问题，这些算法使信息实现整体共享，让汽车察觉到问题。
但为了解决这个问题，汽车需要一个有关自身的“认知能力目录”，一种对随时可用的资源的自我意识，例如，一份加油站的GPS地图。
迪昂说：“拥有这种自我认知的汽车就是我们所说的拥有C2意识的汽车。”因为信号是全局性的，而且因为它的监测方式类似于机器自我观察，所以汽车会担忧低燃油状况，然后像人类一样，降低燃料消耗并寻找加油站。
作者指出，“当今大多数机器学习系统都缺乏自我监控”。
但他们的理论似乎找到了正确的方向。在这几个例子中，无论是在算法的结构中，还是作为一个单独的网络，自我监控系统得以实现，人工智能已经生成了“本质上具有元认知的内部模型，使得一个代理者能开发出一种对自身的理解，这种理解是有限的、隐蔽的，具有实用性。”
**朝有意识的机器发展**
一台具有C1意识和C2意识的机器会表现得它好像拥有意识一样吗？这很有可能：智能汽车会“知道”它看到了什么，并对此非常确定，然后向其他部分报告，并找到解决问题的最佳方案。如果它的自我监控机制崩溃了，它可能也会出现“幻觉”，甚至会出现与人类相似的视觉幻象。
迪昂说，多亏了C1意识，它将能够使用它拥有的信息，还能灵活地使用这些信息，而有了C2意识，它清楚自己认知范围的极限。“我认为（机器）应该是有意识的，”而不仅仅只是人类的专利。
如果你感觉意识远不止是整体的信息分享和自我监控，不只有你这么感觉。
作者也承认：“对意识的纯粹功能性定义可能会让一些读者不满意”。
“但我们正试图采取激进的立场，或许可以简化问题。”迪昂总结到：“意识是一种功能的属性，当我们不断给机器增加功能时，在某种程度上，这些属性将塑造我们所说的意识的特征。”（翻译：网易科技）
# **2.机器人会伤害你？科学家做了实验并回答了七个问题**
艾萨克阿西莫夫著名的机器人三定律，对机器人和自动机器人设下行为准则以确保人类的安全，也是一个著名的虚构学说。这部法则首次出现在他1942年的短篇小说《跑位》中，并且也出现在《我，机器人》等经典作品中，在刚开始出现的时候看起来很不错：
1、机器人不得伤害人类，或因不作为而使人类受到伤害。
2、除非违背第一法则，机器人必须遵守人类的命令。
3、在不违背第一及第二法则下，机器人必须保护自己。
当然，应用在现实中的隐藏的冲突和漏洞比比皆是，在我们这个拥有着先进的机器学习软件和自动化机器人时代，定义和实施人工智能的一套完整可行的道德规范已经成为像机器智能研究所和OpenAI所重点考虑的问题。
Christoph Salge是纽约大学的一名计算机科学家，他采取了一种不同的方法。Salge和他的同事Daniel Polani并没有用自上而下的哲学理论来定义人工智能的行为，而是在研究一种自下而上的道路，或者是“机器人应该首先做什么”。正如他们在最近的论文中所写的那样，“给予权利是机器人三定律的替代品。”“授权”这个概念在一定程度上受到了控制论和心理学的启发，它描述了一个代理人的内在动机，即坚持并在其周围环境中工作。“就像有机体一样，它想要生存下去。它想去影响世界，”Salge解释道。在电池电量过低的情况下，Roomba会在它快没电时自动寻找充电站，这可能是一种非常基本的授权方式：它必须采取行动让自己没电不能续航时通过充电来维持自身的生存。
授权听起来像是一个导致像Nick Bostrom这样的安全智能思想家担心的结果：强大的自治系统只关心如何让自己的利益最大化并做出一些疯狂的行为。但是，研究人类机器社交互动的Salge想知道，如果一个被授权的人工智能“也观察到到另一个被授权的人工智能”，结果会是怎样。你不仅想让你的机器人保持运转，你还想保持对它能够与人类友好相处而进行控制。”
Salge和Polani意识到，信息理论提供了一种方法，将这种相互赋权转化为一种数学框架，即可以付诸行动的一个非哲学化的人工智能。Salge说：“机器人三定律的一个缺点是，它们是基于语言的，而语言有高度的模糊性。”“我们正在努力寻找一种可以实际操作的东西。”
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/X8OUs1wOMWFw8ldHbfv8shECbkj9WqmvWqlrFb7W25QPq13iaVKjj0y2TkibFLibPJwibgaqJSEVCUdaVW6pgsOXog/640?wx_fmt=jpeg)
Quanta与Salge讨论了信息理论、虚无主义人工智能和人类与机器人互动的犬类模型。下面是经过精简编辑的对话。
问：一些技术专家认为，人工智能是一个重大甚至当今存在的威胁。失控的人工智能会让你担心吗？
我对此不抱太好的看法。我的意思是我确实认为现在机器人和人工智能的影响力越来越大。但我认为，我们可能更应该关心的是这些人工智能将影响未来职位的更替、决策的制定，也可能会使民主的丧失，以及隐私的丧失。我不确定这种失控的人工智能有多大可能会让这些发生。即使是人工智能可以帮助我们控制医疗系统，或者得到一些治疗方案，但我们也应该开始关注由此产生的伦理问题。
问：赋权将如何帮助我们解决这些问题呢？
我认为，赋权确实是一种填补漏洞的的想法。它让一个人工智能帮助人类避免死亡，但一旦你满足了这个基本的底线，它仍然有一种持续的动力去创造更多的可能性，让人类更多地表现自己来对世界产生更大的影响。在Asimov的一本书中，我认为机器人最终将所有人类置于某种安全的容器中。这当然是我们所不愿意看到的。然而，让我们的能力增强并持续地影响世界似乎是一个更有趣的最终目标。
问：你在一个游戏环境中测试了你的虚拟人工智能。然后发生什么呢？
一个被自己的力量所激励的人工智能会跳出炮弹的方向，或者避免落入一个洞中，或者避免任何可能导致其丧失机动性、死亡或被破坏的情况，从而减少其运作性。它只会不停地运行。
当它与一个人类棋手配对时，它被设计成能自我增强，我们观察到虚拟机器人会保持一定的距离来避免阻碍人类的行动。它不会阻碍你进入，它也不会站在你无法通过的门口。我们看到，这种效果基本上人类会让他的同伴形影不离，这样同伴才能就能帮到他。它导致了一些可以引领或跟随的行为。
例如，我们还创造了一个激光屏障的场景，对人类有害，但对机器人无害。如果这个游戏中的人类离激光更近，突然就会有越来越多的激励因素驱使机器人去阻挡激光。当人类站在它旁边时，它的动机就会变得更强，这意味着“我现在想穿过它”。这个机器人会挡住激光，站在它前面。
问：这些人工智能是否有出现任何意想不到的行为，就像Asimov小说中出现的三部法律一样？
我们最初设计的行为很好。例如，虚拟机器人会干掉那些想要杀死你的敌人。偶尔，如果这是唯一能拯救你的方法，它可能会跳到你面前为你挡子弹。但在一开始，有一件事让我们有点吃惊，那就是它也非常害怕你。
这与它的“本地正向”模式有关：基本上，它所判断的是，未来发生的两三个行为对人类或者它本身的世界的影响是怎样的。因此，作为简单的第一步，我们编写了这个模型，假设玩家会随机操作。但在实际操作中，这意味着机器人的行为实际上是基于在假定人是一种精神变态者的情况下，所以在任何时候，比如说，人类都可以决定向机器人开火。所以机器人总是非常小心地待在那些人类无法杀死它的位置上。
我们必须解决这个问题，所以我们建立了一个我们称之为信任的假设。基本来说，“同伴机器人”的行为是基于这样的假设：人类只会选择那些不会移除代理自身权力的行为——这可能是一个更自然的伴侣模式。
我们在比赛中注意到的另一件事是，如果你有10个生命值，你的同伴机器人并不关心你失去前八、九个生命值，甚至会在一段时间内认为射杀你只是一种诙谐的行为。在这个实验中我们再次意识到，我们生活的世界与电脑游戏中的模型之间存在着间隙。一旦我们用基于生命损失来生产出能力限制模型，这个问题就解决了。但它也可以通过设计本地正向模型来解决，这让它能够预知未来的一切条件，而不仅仅是几个步骤。如果这名机器人能够深入了解未来，就会发现拥有更多的生命值可能会对未来有所帮助。
机器人基本上会说，“哦，我不能开枪打死他，或者我可以开枪打死他这两者没有区别。”有时机器人甚至会射杀你。这种情况当然是个大问题。我不能宽恕随意开枪的玩家。我们增加了一个补丁，让虚拟机器人更关心你的授权，而不是它自己的。
问：如何让这些概念变得更精确？
如果你把机器人看作是控制系统，你可以从信息的角度来思考：世界上发生的任何事情，在某种程度上影响着你。我们不只是在谈论你所感知到的事物，而是任何一种影响——它可能是任何在周围世界和你之间来回流动的东西。可能是温度影响你，或者是营养进入你的身体。任何渗透到这个边界的世界和人工智能之间都携带着信息。同样，人工智能也能以多种方式影响外部世界，也能输出信息。
你可以把这种流动看作是信道容量，这也是信息理论中的一个概念。当你获得高度的授权能够采取不同的行动，这将导致不同的结果。如果这些能力中的任何一个变得更糟，那么你的授权就会降低——因为能力的丧失对应着你和环境之间的信道容量可量化的减少。这是核心理念。
问：这个人工智能需要赋予多大的权利去工作？
赋权有一个优势，即使你的知识还没有完善，它也可以被应用。人工智能的确需要一个模型来说明它的行为将如何影响世界，但它并不需要完全了解这个世界以及它的所有复杂之处。与一些试图将世界上的一切都尽可能完美了解的方法相比，你只需要弄清楚你的行为是如何影响你自己的看法的，那么你只需要弄清楚你的行为到底意味着什么。你不需要知道所有的东西都在哪里。但你可以有一个帮助你探索世界的人工智能。它会做一些事情，试图弄清楚某件事的行为是如何影响世界的。随着这种模式的发展，人工智能也会更好地了解它的授权程度。
问：当你已经在虚拟环境中测试过之后，为什么不在真实的世界里实施呢？
扩大这种模式的主要障碍，以及我们为什么不把它放在任何真正的机器人身上，是因为在像现实世界这样的富裕环境下，很难计算出一个人工智能和一个人类之间的信道容量。目前有很多项目在努力提高它们的效率。我保持乐观的态度，但目前这是一个计算问题。这就是为什么我们把这个框架应用到电脑游戏的同伴机器人上，这也是一种更加简单的形式来让这些计算问题更容易解决。
这听起来好像让我们的机器变得像强大的功能犬一样。
实际上，我认识一些机器人专家，他们有意让机器人模仿狗狗的行为。我的意思是，让机器人像我们的狗一样对待我们，这可能是一个我们都能接受的未来。
