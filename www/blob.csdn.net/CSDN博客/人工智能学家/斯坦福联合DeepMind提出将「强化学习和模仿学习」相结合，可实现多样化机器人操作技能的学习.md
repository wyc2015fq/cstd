# 斯坦福联合DeepMind提出将「强化学习和模仿学习」相结合，可实现多样化机器人操作技能的学习 - 人工智能学家 - CSDN博客
2018年03月02日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：1099
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fLg1X5X5XZpXPS5Irib9zb2TI9y73e9KnbtLdn4uRK5Dicicsn9kRwcU3A3S5VEtic6brzwPBU62kGfg/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
*原文来源：arXiv*
*作者：Yuke Zhu、Ziyu Wang、Josh Merel、Andrei Rusu、Tom Erez、Serkan Cabi、Saran Tunyasuvunakool、Janos Kram ´ ar、Raia Hadsell、Nando de Freitas、Nicolas Heess*
*「雷克世界」编译：嗯~阿童木呀*
我们提出了一种无模型的深度强化学习方法，利用少量演示数据以辅助一个强化学习智能体。
我们将这种方法应用于机器人操控任务中，并对能够直接从RGB摄像机输入映射到关节速度的端到端视觉运动策略进行训练。
我们的实验结果证明，我们的方法可以解决各种各样的视觉运动任务，对于这些视觉运动来说设计一个脚本控制器会很费力。
我们的实验结果表明，我们的强化和模仿智能体的性能表现要远远比单独使用强化学习或模仿学习进行训练的智能体好得多。
我们还举例说明，这些以大量视觉和动态变化进行训练的策略可以在零次学习模拟—真实（sim2real）的迁移方面取得初步成功。有关此研究的简短视觉描述可以点击链接查看。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fLg1X5X5XZpXPS5Irib9zb25DM8xSCKC1dAeNwAKXbXuxeQ1Gks7r7rLWvNjk8kyGDmVV9uVJzsYA/640?wx_fmt=png)
*我们提出了一个原则性的机器人学习管道。我们使用3D运动控制器来收集人类在一个任务上的演示。我们的强化和模仿学习模型利用这些演示以促进在模拟物理引擎中的学习。然后，我们执行sim2real迁移，将学习的视觉运动策略部署到真正的机器人上。*
最近，在深度强化学习（RL）领域取得了很大的进展，在诸如视频游戏和围棋等若干个具有挑战性的领域表现非常出色。
对于机器人技术而言，强化学习与诸如神经网络等强大的函数近似相结合，为设计复杂的控制器提供了一个通用框架，否则将难以对其进行手工操作。
可以这样说，在机器人控制方面，基于强化学习的方法有着悠久的历史，但通常用于低维运动的表示中。
在过去的几年中，深度强化学习使用基于模型（例如Levine等人、Yahya等人、Levine等人所提出的）和无模型（例如Chebotar等人、Gu等人和Popov等人所提出的）技术，在机器人操控领域获得了日益增长的成功，在模拟和实际硬件中都是如此。
然而，使用无模型强化学习技术，让视觉运动控制器的端到端学习实现远程和多阶段操控任务仍然是一个具有挑战性的问题。
为机器人开发强化学习智能体需要克服几个重大挑战。
机器人技术策略必须能够将来自噪声传感器（例如摄像机）的多模式和局部观测转化为具有许多自由度的协调活动。
与此同时，实际的任务往往伴随着丰富的接触动态（contactrich dynamics），并沿着多个维度（视觉外观、位置、形状等）变化，从而构成了显著的泛化挑战。基于模型的方法可能在处理如此复杂的动态和大的变化方面难以着手。
由于样本复杂度非常高，直接在真实机器人硬件上对无模型方法进行训练可能令人望而生畏。实际上，真实强化学习训练的难度往往因为安全考虑，以及访问关于环境状态信息（例如一个目标的位置，定义一个奖励函数）的难度而加剧。
最后，即使在模拟中，完善的状态信息和大量的训练数据都可用时，探索可能仍然是一个重大挑战，尤其是对于on-policy方法来说更是如此。
这部分往往是由于频繁的高维和连续行动空间，但也是由于设计一个合适的奖励函数所存在的困难造成的。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fLg1X5X5XZpXPS5Irib9zb2n4yRQPGFib6YzzeZCCP57ezKudibwNRpiaIia4XSH9SGE4h2BiaW0ZaevyA/640?wx_fmt=png)
*模型的概述。我们模型的核心是深度视觉运动策略，它将摄像机观察和本体感受特性作为输入，并产生下一个关节速度。*
在本文中，我们提出了一种无模型深度强化学习方法，可以直接从像素输入中解决各种机器人操控任务。我们的主要见解是：
（1）通过利用少量的人为演示来减少连续空间探索的难度。
（2）利用若干新技术，在训练期间利用特权和特定任务的信息，以加速和稳定多阶段任务中的视觉运动策略的学习。
（3）通过增加训练条件的多样性改进泛化能力。因此，这些策略在系统动力学、目标外观、任务长度等具有显著变化的情况下，运行良好。此外，我们展示了两个任务上非常有发展前景的初步结果，其中，在模拟中进行训练的策略能够实现零次学习迁移到一个真实机器人中。
我们对六种操作任务的方法进行了评估，包括提升、堆叠、浇注等。这组任务包括多阶段和长时间任务，并且它们需要直接从像素中进行完整的9-DoF关节速度控制。控制器需要能够处理显著的形状和外观变化。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fLg1X5X5XZpXPS5Irib9zb2TI9y73e9KnbtLdn4uRK5Dicicsn9kRwcU3A3S5VEtic6brzwPBU62kGfg/640?wx_fmt=png)
*在我们的实验中对六个操作任务的可视化*
为了应对这些挑战，我们的方法将模仿学习与强化学习融合到一个统一的训练框架中。
我们的方法以两种方式对演示数据加以利用：首先，它使用一种混合奖励，将任务奖励与基于生成式对抗模仿学习的模仿奖励相结合。这有助于探索，同时仍然使得最终的控制器能够在任务上优于人类演示者。
其次，它使用演示轨迹来构建一个状态课程（a curriculum of states），以便在训练期间对事件进行初始化。这使得智能体能够在早期的训练阶段了解任务的后期阶段，从而有助于解决长期任务。
因此，我们的方法解决了所有六项任务，而对于这些任务而言，单独使用强化学习和模拟学习基线都不能得以解决。
为了避开实际硬件训练中所存在的局限性，我们采用了最近显示出非常有发展前景的sim2real模式。
通过使用物理引擎和高吞吐量的强化学习算法，我们可以仿真机器人手臂的并行副本，以在一个富含接触点的环境中执行数百万次复杂的物理交互，同时消除机器人的安全性和系统重置的实际问题。
此外，在训练期间，我们可以使用一些新技术以利用关于真实系统状态的特权和特定任的务信息，包括学习单一模式中策略和值、以目标为中心的GAIL鉴别器，以及视觉模块中的辅助任务。
这些技术可以稳定和加速策略学习，而不会在测试时对系统施加任何约束。
最后，我们将诸如视觉外观、目标几何形状和系统动力学等训练条件进行多样化。这改善了不同任务条件下的泛化能力以及从模拟到现实的迁移。
我们使用相同的模型和相同的算法，只对训练设置进行小规模的特定于任务的修改，以学习六个不同机器人手臂操作任务的视觉控制器。
如图1所示，从收集人类演示到在模拟中进行学习，并通过sim2real策略迁移返回到实际世界中的部署，这实例化了一个视觉运动学习管道。
我们的研究结果已经证明，将强化和模仿学习结合起来能够在相当大的程度上提高我们训练系统的能力，这些系统能够从像素上解决具有挑战性的灵活操控任务。
我们的方法实现了机器人技能学习的完整三个阶段：首先，我们收集了少量演示数据以简化勘探问题；
其次，我们依靠物理模拟来进行大规模的分布式机器人训练；
第三，我们执行了实际的部署的sim2real迁移。
在今后的研究工作中，我们将试图提高学习方法的样本效率，并利用现实际问题中的经验弥补策略转移的现实差距。
原文链接：https://arxiv.org/pdf/1802.09564.pdf
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
