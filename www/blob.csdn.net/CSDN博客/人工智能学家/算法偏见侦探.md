# 算法偏见侦探 - 人工智能学家 - CSDN博客
2018年12月24日 20:12:18[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：93
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBWt9HLwVNicgMibLlrwj5RFWPB6uWgaWByPYzQLibQIBGd5yFR4qHxGSLe3JRyVic6zk0WcVOcdIDLRlQ/640?wx_fmt=jpeg)
来源：AI 科技评论
摘要：随着越来越多的算法不断渗透入社会的层层面面，如医疗机构、政府部门，对算法偏见的讨论越来越多。这个月，Nature 杂志评选出 2018 年最受欢迎的十大科学长篇专题报道，其中，Rachel Courtland 一篇讨论算法偏见的文章成功当选。
2015 年，一位忧心忡忡的父亲问了卫生经济学家 Rhema Vaithianathan 一个至今仍在她脑海中挥之不去的问题。
那天，一小群人聚集在宾夕法尼亚州匹兹堡市的一间地下室里，听 Rhema Vaithianathan 解释软件如何能够解决虐待儿童的问题。每天，该地区的热线电话都会接到几十个怀疑附近有孩子处于危险之中的来电；其中一些来电会被呼叫中心的工作人员标记以便调查。但是这个系统并不能掌握所有的虐童案件。Vaithianathan 和她的同事们刚刚签订了一份价值 50 万美元的合同，该合同要求他们开发出能够帮助解决该问题的算法。 
卫生经济学家 Vaithianathan 是新西兰奥克兰理工大学社会数据分析中心的联合主任，他用下面的例子告诉人们这个算法是如何工作的：例如，一个使用大量数据（包括家庭背景和犯罪记录）训练得到的工具，可以在接到电话时生成风险评分。 这可能有助于通知审核员标记出需要调查的家庭。
当 Vaithianathan 邀请听众提问后，那位（前面提到的）忧心忡忡的父亲站起来发言。 他说，他曾经染上了毒瘾，并与毒瘾做过艰难的斗争。社工曾因此将他的孩子从家中带走。 但目前，他已经戒毒成功一段时间了。在电脑评估他的记录的时候，他为改变自己的生活所做的这些努力难道就毫无意义吗？换句话说：算法对他的评价是否不公平？
## **我们能打开人工智能的黑盒吗？**
Vaithianathan 向这位父亲保证，人们总是会改过自新的，他的努力不会被忽视。但是时至今日，即使这种自动化评估工具已经部署完毕，Vaithianathan 仍然在思考这位父亲的问题。计算机的计算结果正越来越多地被用于控制那些可能改变人一生的决定，包括应该拘留哪些被指控犯罪的被告、调查哪些可能存在虐待儿童现象的家庭，以及近来的「预测性警务」的趋势（社区警察应该关注哪些问题）。这些工具有望使决策更加一致、准确和严谨。 但是对这一系统的监管是有限的：没人知道有多少这样的系统正在被使用。这些算法的不公平性正在引起警惕。例如，2016 年，美国记者辩称，用于评估未来的犯罪活动风险的系统会歧视黑人被告。
纽约大学研究人工智能的社会影响的研究中心「AI Now」研究院的联合创始人 Kate Crawford 表示：「我最担心的是，我们提出的系统本应改善问题，但最终却可能使问题恶化」。
在 Crawford 和其他人提出警告时，政府正试图使软件更具公信力。去年 12 月，纽约市议会通过了一项法案，他们成立了一个特别工作组，用于提出公开分享关于算法信息的方案的建议，并调查它们是否存在偏见。今年，法国总统 Emmanuel Macron 表示，法国将公开政府使用的所有算法。在本月发布的指导意见中，英国政府呼吁那些在公共部门从事数据工作的人要公开透明，并负起责任。于五月底生效的欧洲通用数据保护条例（GDPR），也将促进算法问责制。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmj8icmY3CwZXXJZfibCRBFIkBcGCdibicyp705FicDSkStUD9OOQkn73RwAx05jGicGP0ia18W91OURzktQQ/640?wx_fmt=jpeg)
Rhema Vaithianathan 构建算法来帮助标记出潜在的儿童虐待案件
在这样的活动中，科学家们面临着一个复杂的问题：使算法公平究竟指的是什么？ Vaithianathan 等为公共机构工作的研究人员，试图开发出负责任的、有效的软件。他们必须努力解决自动化工具可能引入偏见或加深现有的不平等现象的问题，尤其是如果这些工具正被嵌入到一个本已具有一定歧视性的社会体系中时。
「有一个相当活跃的研究团体，他们正试图开发从外部审核评估这类系统的方法」。
盐湖城犹他大学的理论计算机科学家 Suresh Venkatasubramanian 指出，自动化决策工具所引出的公平性问题并不是一个全新的问题，人们使用评估犯罪或信用风险的精算工具已经有几十年的历史。随着大型数据集和更复杂模型的普及，人们越来越难以忽视它们在伦理方面的影响。「计算机科学家别无选择，我们必须开始进行这方面的研究。我们再也不能忽视算法的公平性，看看这样会发生什么」。
## **公平性的折中**
2014 年，匹兹堡所在的 Allegheny 郡人类服务部门的官员打电话征求关于自动化工具的建议时，他们还没有决定如何使用它。但是他们知道自己应该对新系统采取开放的态度。该部门数据分析、研究和评估办公室副主任 Erin Dalton 表示：「我极其反对把政府资金用于不能向社会说明我们在做什么的黑箱解决方案上」。该部门拥有一个建于 1999 年的中央数据仓库，其中包含大量个人信息，包括住房、精神健康状态和犯罪记录。Dalton 说，Vaithianathan 的团队在关注儿童福利方面做出了巨大努力。
2016 年 8 月，Allegheny 家庭筛查工具（AFST）被推出。对于每个打进热线的电话，呼叫中心的员工都会看到由自动风险评估系统生成的得分（1 至 20 分），其中 20 分对应于被认定为最高风险的个案。AFST 预计这些高得分家庭的孩子最有可能在两年内被从家中带走，或者因为打电话者怀疑这些孩子受到了虐待而再次被送到郡里（郡县正在放弃第二种评价指标，该指标似乎并不能准确反映出需要进一步调查的案件）。
位于加利福尼亚州的斯坦福大学的独立研究员 Jeremy Goldhaber-Fiebert 仍然在评估这个工具。但 Dalton 说，初步的结果表明，该工具是有帮助的。她表示，采用该工具后，呼叫中心工作人员提交给调查人员的案件中似乎包含了更多有着合理的担忧的实例。电话审核员似乎也会对类似的案件做出更加一致的决定。尽管如此，他们的决定并不一定与算法的风险评分相符；郡政府希望使两者的结果更接近一致。
## **改革预测性警务**
随着 AFST 被部署，Dalton 希望得到更多帮助，以确定该系统是否可能存在偏见。2016 年，她找来匹兹堡卡内基梅隆大学的统计学家 Alexandra Chouldchova，帮助她分析该软件是否会歧视特定群体。Chouldchova 此前已经在考虑算法中的偏见问题，而且将参与到一个已经引发了关于这个问题的广泛辩论的案件。
同年 5 月，新闻网站 ProPublica 的记者报道了 Broward County 法官使用的商业软件，这些软件有助于判定一个被指控犯罪的被告是否应该在审判前被从监狱中释放出来。记者们说这个软件对黑人被告有偏见。这个被称为 COMPAS 的工具可以生成一个得分，它被用来衡量一个人在两年内再次犯罪的可能性。
Propublica 团队调查了数千名被告的 COMPAS 得分，这些分数是该团队通过公共记录请求获得的。通过比较黑人和白人被告，记者们发现，「假正例」（被判断为有罪，实际无罪）的黑人被告与白人被告的比例是严重失调的：黑人被 COMPAS 列为高风险人群，但实际上他们随后却没有被指控罪行。
该算法的开发者是一家总部位于密歇根州的名为 Northpointe （现在是俄亥俄州坎顿市的 Equivant）的公司，该公司认为这个工具没有偏见。他们说，COMPAS 还能够很好地预测被归类为高犯罪风险人群的白人或黑人被告是否会再次犯罪（这是一个「预测性平价」的例子）。Chouldechova 很快发现，Northpointe 和 ProPublica 的公平度量是对立的。预测性平价、相等的假正例错误率和相等的假负例错误率都可以作为体现「公平」的方式，但是如果两个群体之间存在差异——例如白人和黑人被再次逮捕的概率（参见后文「如何定义『公平』」章节） ，那么在统计学上，就不可能实现完全的公平。伦敦大学学院研究可靠性机器学习的研究员 Michael Veale 表示：「鱼和熊掌不可兼得！如果你想在某一方面做到公平，那么在另一个听起来也很合理的情况下，你可能必然做不到公平」。
## **如何定义「公平」？**
研究算法中的偏见的研究人员说，定义公平的方法有很多，但这些方法有时候是矛盾的。
我们不妨想象一下，在刑事司法系统中使用一种算法为两组嫌疑人（用蓝色和紫色表示）打分，从而衡量他们再次被捕的风险。历史数据表明，紫色组被捕的概率更高，因此模型会将更多的紫色组的人归类为高危人群（见下图顶部）。即使模型开发人员试图不直接告诉模型一个人应该被归为蓝色还是紫色，以避免产生偏见，但这种情况也会发生。这是因为用作训练输入的其他数据可能与蓝色或紫色相关。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmj8icmY3CwZXXJZfibCRBFIkBzjc72sIreseNxibmibKic4DV6u33P2LvoaenHGA2mG24bRu9ROZdmBT7Q/640?wx_fmt=jpeg)
尽管高风险状态不能完美地预测该嫌疑人是否会再次被捕，但该算法的开发者试图使预测结果公平：对于这两组人来说，「高风险」指的是有 2/3 的几率在两年内再次被捕。（这种公平称为预测性平价。）未来的逮捕率可能不会遵循过去的模式，但是在这个简单的例子中，假设它们确实如预期的那样：蓝色组的 3/10 和紫色组的 6/10（以及每组中 2/3 被标记为高风险的人）确实被再次逮捕了（见下图中底部的灰条）。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmj8icmY3CwZXXJZfibCRBFIkBAibmxPWB0fU2Sa2ia4VOaHuG3Gjns8gkyD2Ju6ibUYOh1ibc2nj7y1XKcQ/640?wx_fmt=jpeg)
该算法满足预测性平价（无论黑人和白人被告是否有相同的风险评分总体准确率），但是仍然存在一个问题。在蓝色组中，7 人中有 1 人（14%）被误认为是高危人群，而在紫色组中，4 人中有 2 人（50%）被误认为高危人群。因此，紫色个体更有可能成为「假正例」——被误认为高风险。
只要蓝色组和紫色组的成员再次被捕的概率不同，那么就很难实现预测性平价和相等的假正例率。从数学上来说，要做到这一点同时满足第三项公平标准（除了预测性平价和相等的假正例率）是不可能的：相等的假负例率（被认定为低风险但随后又再次被捕的个体；在上面的例子中，紫色和蓝色组的假负例率恰好相等，同为 33%）。
一些人认为紫色组的假正例率更高体现出了算法的歧视性。但其他研究人员认为，这并不一定是算法存在偏见的确凿证据。这种不平衡还可能有一个更深层次的原因：紫色组可能一开始就不公平地成为了逮捕的目标。根据过去的数据，该算法能够准确地预测更多的紫色组成员将被再次逮捕。因此，我们可以认为该算法（甚至可以确定）有事先存在的社会偏见。
AI 科技评论注：更多关于统计悖论的信息，可以参见这个著名的统计学悖论，第一次听说的人很可能怀疑人生一文。
事实上，从数学角度来说，还有更多的方式来定义公平：在今年 2 月的一次会议上，计算机科学家 Arvind Narayanan 发表了题为「21 个公平性的定义及其策略」的演讲，他指出还有其它的定义方式。一些调查过 ProPublica 的案例的研究人员，包括 Chouldchova，指出「不相等的错误率是否表明算法存在偏见」尚不清楚。斯坦福大学的计算机科学家 Sharad Goel 说，他们反而反映了这样一个事实：即算法对一个群体比对另一个群体更难做出预测。「事实证明，这或多或少是一种统计学的假象」。
对于某些人来说，ProPublica 的案例凸显了这样一个事实，即许多机构缺乏资源来寻求并正确评估算法工具。芝加哥大学的数据科学与公共政策中心的主任 Rayid Ghani 表示：「如果有的话，这样的情况告诉我们的是：雇佣 Northpointe 的政府机构没有给出明确的衡量算法公平性的定义。我认为，各国政府需要学习并接受培训，学习如何寻求这些系统，如何定义算法应该被衡量的指标，以及如何确保供应商、咨询师和研究人员提供的系统实际上是公平的」。
Allegheny 郡的经验表明要解决这些问题是多么困难。Chouldchova 受邀在 2017 年初开始研究 Allegheny 的数据，她发现这个工具也存在类似统计上的失衡现象。她说，该模型有一些「非常不理想的特性」。在不同的种族之间的错误率的差异远远高于预期。而且，由于尚不清楚的原因，被认为受虐待风险最高的白人儿童被从家中带走的可能性小于被认为受虐待风险最高的黑人儿童。Allegheny 和 Vaithianathan 的团队目前正在考虑转而使用另一种模型。「这可能有助于减少不公正的现象」，Chouldchova 说。
尽管统计失衡是一个有待解决的问题，但算法中潜藏着更深层次的不公平性（它们可能会加剧社会的不公正现象）。例如，像 COMPAS 这样的算法可能原本是旨在预测未来犯罪活动的可能性，但它只能依赖于可测量的模式：例如被逮捕。警务实践的差异可能意味着一些社会团体成为被逮捕几率更高的目标，他们可能因为会在其他社会团体中被忽视的罪行而被捕。David Robinson是 Upturn 的执行董事（Upturn 是一个位于华盛顿特区的非营利性社会司法组织），他说：「即使我们准确地预测了一些案件，但我们在准确地预测案件的同时可能也对一些人群采取了不公正的对待」。这在很大程度上将取决于法官在多大程度上依赖此类算法来做出裁决，而我们对此知之甚少。 
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmj8icmY3CwZXXJZfibCRBFIkB9tSutsc49zTiayq19GwGQOalexgK5SYSGYr7sUuJSXZhEy0zE3HdFhA/640?wx_fmt=jpeg)
新泽西州卡姆登市的警察使用自动化工具来帮助确定哪些地区需要巡逻。
Allegheny 的工具也受到了类似的批评。作家、政治学家 Virginia Eubanks 认为，不管这个算法是否准确，它都是基于有偏见的输入工作的，因为黑人和混血家庭更有可能被热线电话所提到。此外，由于该模型依赖于 Allegheny 体系中的公共服务信息，而且使用此类服务的家庭普遍贫穷，该算法会对较贫穷家庭进行更严格的审查，从而对这些家庭更加不公平。Dalton承认，现有的数据是一个我们不得不面对的限制，但她认为人们仍然需要这个工具。 Allegheny 郡今年早些时候在 AFST 网站上回应 Eubanks 时表示：「贫困这一不幸的社会问题并不能否认我们具有『为那些需要我们关怀的儿童提高我们的决策能力』的责任！」
## **透明度及其限制**
尽管一些机构建立了自己的工具或商业软件，但学者们发现自己在公共部门算法方面的工作有很大的市场需求。在芝加哥大学，Ghani 一直在与一系列机构合作，包括芝加哥公共卫生部门，他们一起研究一种预测哪些家庭可能藏有对健康有危害的铅的工具。在英国，剑桥大学的研究人员与 Durhan 郡的警方合作，建立了一个模型，帮助他们确定可以对哪些人采取干预方案，作为起诉的替代办法。Goel 和他的同事今年建立了斯坦福计算政策实验室，该实验室正在与包括旧金山地区检察官办公室在内的政府机构进行合作。地区检察官办公室的分析师 Maria McKee 认为，与外界的研究人员的合作关系至关重要。他说：「我们都知道什么是对的，什么是公平的，但我们往往没有工具，也没有进行研究，来准确、条理清晰地告诉我们如何实现这一目标」。
人们非常希望提高案件的透明度，这与 Allegheny 采取的方针一致。Allegheny 郡与利益相关方进行了接触，并向记者敞开大门。AI Now 研究所的 Crawford 说，当算法是「不能接受算法审核、审查或公开辩论的封闭循环」时，这样通常会激化问题。但是现在还不清楚如何使算法更加开放。Ghani 认为，简单地公布一个模型的所有参数并不能提供对其工作机制的解释。透明度也可能与隐私保护相冲突。在某些情况下，透露太多关于算法工作原理的信息可能会让不怀好意的人攻击这个系统。
Goel 说，问责制的一大障碍是，这些机构往往不会收集它们如何使用这些工具或这些工具的性能的数据。「很多时候并不存在所谓的透明度，因为没有什么信息是可以分享的」。例如，加利福尼亚州的立法机构起草了一份法案，寻求能够帮助人们减小被告必须支付保释金的几率的风险评估工具，然而这种做法因为会惩罚低收入被告而受到诟病。Goel 希望该法案强制要求收集法官之所以不同意使用该工具的支撑案例的数据，以及包括判决结果在内的每个案件的具体细节。他说，「我们的目标是从根本上减少监禁，同时维护公共安全，所以我们必须知道这样做是否有效」。
Crawford 说，我们将需要一系列「正当程序」基础设施来确保算法的可靠性。今年 4 月，AI Now 研究所为希望可靠地采用算法决策工具的公共机构制定了一个框架；除此之外，该研究所呼吁征求社区的意见，并让人们能够对与他们的决议提出上诉。
## **人工智能研究存在盲点**
许多人希望法律能够强制执行这些目标。Solon Barocas 是一名康奈尔大学的研究人工智能伦理和政策问题的研究员，他说，实际上曾经有过一些这样的先例。在美国，一些消费者保护法规在对作出不利于公民信用评价的决定时，会给予公民解释的权利。而 Veale 说，早在 20 世纪 70 年代，法国就立法赋予公民解释权和对自动裁决提出异议的权利。
最大的考验将是 5 月 25 日生效的欧洲 GDPR。某些规定（例如获得有关自动决策案件所涉逻辑的有意义信息的权利）似乎促进了算法问责制。但英国牛津互联网研究所的数据伦理学家 Brent Mittelstadt 表示，对于那些希望评估算法公平性的人来说，GDPR 实际上可能会制造一个「法律雷区」，从而实际上妨碍算法公平。要检验一个算法是否在某些方面存在偏见（例如，它是否会偏袒一个种族，而歧视另一个种族），最好的方法就是了解系统涉及到的人的相关属性。但是，Mittelstadt 说，GDPR 对使用这些敏感数据的限制十分严格，处罚也非常高，以至于那些有能力评估算法公平性的公司可能没有什么动力去处理这些信息。 他说：「这似乎会限制我们评估算法公平性的能力」。
那些让公众对算法有一定了解、并吸引公众关注的 GDPR 法案的作用范围也存在一些问题。如前所述，一些 GDPR 规则仅适用于完全自动化的系统，这可以排除「算法对决策有一定影响，但应该由人做出最后决定」的情况。Mittelstadt 说，这些细节最终应该在法庭上澄清。
## **审核算法**
与此同时，研究人员正在推进检测算法中的偏见的策略，这些算法尚未对公众开放审核。Barocas 说，公司可能不愿意讨论他们将如何解决公平问题，因为这将意味着首先要承认他们的公平性存在问题。他说，即使他们这样做了，他们根据算法采取的行为中的偏见可能会有所改善，但不会从根本上消除偏见。「因此，任何有关这个问题的公开声明，都不可避免地承认这个问题依然存在」。但最近几个月，微软和 Facebook 都宣布将开发工具来检测算法偏见。
一些包括波士顿东北大学的计算机科学家 Christo Wilson 在内的研究人员，试图从外部揭示商业算法的偏见。 比如，Wilson 创造了一些虚拟的乘客，他们声称自己在寻找 Uber 出租车，他还在一个求职网站上上传了虚拟履历，以测试性别偏见。还有人正在开发一些软件，他们希望这些软件能够被广泛用于自我评估。今年 5 月，Ghani 和他的同事发布了名为 Aequitas 的开源软件，帮助工程师、政策制定者和分析师审核机器学习模型中的偏见。数学家 Cathy O’Neil 一直在强调用算法做出决策的危险性，她成立了一家公司，私下与一些公司进行了合作，审核他们的算法公平性。
一些研究人员已经开始呼吁，在刑事司法应用和其他领域，人们应该从狭隘地专注于建立预测算法的狂热中退一步。例如，一个工具可能擅长预测谁将不会出现在法庭上，但是最好问问为什么他们不会出现。或许，他们应该设计一些干预措施，比如短信提醒或交通援助，这可能会提高他们出现在法庭上的概率。纽约大学法学院的民权律师、种族正义倡导者 Vincent Southerland 说：「这些工具通常帮助我们做出一些小的修正，但我们需要的是全面的改变」。 他表示，围绕算法鲁棒性展开的激烈辩论「迫使我们所有人询问和回答这些真正棘手的基本问题，这些问题涉及我们正在使用的系统以及它们的运作方式」。
Vaithianathan 目前正在将她的儿童虐待预测模型扩展到科罗拉多州的 Douglas 和 Larimer 郡，她认为，建立更好的算法是十分有价值的（即使它们所嵌入的总体系统是有缺陷的），也就是说，「算法不能被硬生生地嵌入这些复杂的系统里」。她说到，它们必须在理解更广泛的具体应用背景的专家的帮助下被实现。但是即使是最好的工作也会面临挑战。她表示，在缺乏直接的答案和完美的解决方案的情况下，提高算法的透明度是最好的选择。「我总是说: 如果你不能做到完全正确，那就让自己变得更诚实」。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
