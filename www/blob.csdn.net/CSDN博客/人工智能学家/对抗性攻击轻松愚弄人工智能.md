# 对抗性攻击轻松愚弄人工智能 - 人工智能学家 - CSDN博客
2018年07月24日 22:56:19[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：251
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXTIF8y8d3LRLXd0qF5WeEricziagIzjc7dzBuWBYuxLM4SCKa6BokBBq9UeqwHe912VoyueNYqPlhg/640?wx_fmt=jpeg)
对抗性攻击欺骗图像识别算法将3D打印乌龟认作步枪。
来源：中国科学报
摘要：在日前于瑞典斯德哥尔摩举行的国际机器学习会议上，一群研究人员描述了他们的3D打印乌龟。
在日前于瑞典斯德哥尔摩举行的国际机器学习会议上，一群研究人员描述了他们的3D打印乌龟。大多数人会说，它看上去就像一只乌龟，但人工智能（AI）算法有不同的看法。在大多数时候，AI认为这只乌龟看上去像一支步枪。类似地，它将一个3D打印的棒球视为浓缩咖啡。这是“对抗性攻击”的例子——稍作改变的图片、物体或者声音可愚弄AI。
AI领域一些引人注目的进步，尤其是在消化了训练数据集后能识别声音或物体的机器学习算法，刺激了语音助手和自动驾驶汽车的发展。但这些AI应用很容易被欺骗。
在此次会议上，对抗性攻击是一个热门主题。研究人员报告了欺骗AI以及保卫它们的各种新方法。令人担忧的是，会议的两个最佳论文奖之一给了这样的结论：其发现受到保护的AI并非像其开发者可能认为的那样安全。“我们这些机器学习领域的专家并不习惯于从安全性的角度看待问题。”共同主导上述3D打印研究的麻省理工学院（MIT）计算机科学家Anish Athalye表示。
致力于研究对抗性攻击的计算机科学家表示，他们正在提供一种服务，就像指出软件安全缺陷的黑客一样。“我们需要重新思考所有的机器学习途径，以便使其更强劲。”MIT计算机科学家Aleksander Madry认为。
研究人员表示，这些攻击在科学上也有用处，为了解被称为神经网络的AI应用提供了罕见的窗口。神经网络的内在逻辑无法得到清晰明了的解释。加州大学伯克利分校计算机科学家Dawn Song表示，对抗性攻击是强大的镜头，“透过它，我们能理解获得的机器学习知识”。
去年，Song和同事将一些贴纸放到停车标志上，从而欺骗一种常见的图像识别AI认为其是每小时不超过45英里的限速标志。这一结果令自动驾驶汽车公司不寒而栗。
研究人员正在设计更复杂的攻击。在另一场会议上，Song还将报告一种让图像识别AI不只误判物体，还对其产生幻觉的“花招”。在一项测试中，凯蒂猫隐约出现在机器的街景视图中，而路上行驶的汽车却消失了。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
