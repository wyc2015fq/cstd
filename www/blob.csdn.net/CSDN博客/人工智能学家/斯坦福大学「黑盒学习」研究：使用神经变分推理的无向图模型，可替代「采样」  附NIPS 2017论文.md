# 斯坦福大学「黑盒学习」研究：使用神经变分推理的无向图模型，可替代「采样」 | 附NIPS 2017论文 - 人工智能学家 - CSDN博客
2017年11月11日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：175
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBVsicCvXicpKpQIn1XN2PeremMSFGQAibxtStKXDib83o2ok6Mjn6pIA3B9amsMeRiaa03bKZ5xU8lbcgQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
*来源：雷克世界*
*编译：嗯~阿童木呀、多啦A亮*
*概要：机器学习中的许多问题可以自然地用无向图模型的语言表达。在这里，我们提出了无向模型的黑箱学习和推理算法，优化了模型的对数似然的变分近似。我们的方法的核心是我们表示为灵活神经网络的函数q参数化配分函数的上限。*
**摘要：**
机器学习中的许多问题可以自然地用无向图模型的语言表达。在这里，我们提出了无向模型的黑箱学习和推理算法，优化了模型的对数似然的变分近似。我们的方法的核心是我们表示为灵活神经网络的函数q参数化配分函数的上限。它在学习过程中使得配分函数成为可能，加速采样，并通过统一的变分推理框架来训练各种混合有向/无向模型。我们经验性地证明了我们的方法在几个流行的生成建模数据集上的有效性。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0eE9f305ibvibHhO0hicTCsmm45qx0rd5Bb5YwMFU1iaEPVqzF4piaMGo0Zwobgt7jxdgSicbONjyFQ7xVQ/640?wx_fmt=png)
**介绍**
机器学习中的许多问题可以自然地用无向图模型的语言表达。无向图模型用于计算机视觉、语音识别、社会科学、深度学习等其他领域。许多基本的机器学习问题都以无向模型为中心，然而，这类分布的推理和学习会带来很大的计算挑战。
在这里，我们尝试通过针对无向概率图模型P的新变分推理和学习技术来解决这些挑战。我们方法的核心是，无向概率模型P的对数划分函数的上限是由一个近似分布q来表示的，我们表示为一个灵活的神经网络。当q = p时，我们的界是紧密的，对q感兴趣的类在q的参数中是凸的。最有趣的是，它导致了对数似然函数log p的下限，这使得我们能够在一个类似于黑盒变分推理的变分框架中拟合无向模型。
我们的方法相比以前的方法有了许多优点。首先，它能够以黑箱方式训练无向模型，即我们不需要知道模型的结构来计算梯度估计（例如，如在吉布斯采样中那样）。相反，我们的估计只需要评估模型的非标准化概率。在q和p联合优化时，我们的界也提供了一种在学习过程中跟踪配分函数的方法。在推理时，从我初始化的MCMC链（或者它本身可以提供样本）的无向模型中，学习的近似分布q可以用来加速采样。此外，我们的方法自然地集成了最近的有向图模型的变分推理方法。我们预计我们的方法将在自动化概率推理系统中最为有用。
作为我们如何使用这些方法的一个实例，我们研究了不同类别的混合有向/无向模型，并展示了如何在一个统一的黑箱神经变分推理框架中对它们进行训练。那些混合模型已经在早期的深度学习文献中很流行，并从神经科学的原理中获得灵感。它们对相同数量的变量也具有较高的建模能力。相当有趣的是，我们确定了设置使模型也更容易训练。
**实验**
对近似分布可视化
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0eE9f305ibvibHhO0hicTCsmm4ptxrFeVWHWlj0b20Noro7yHFytHm5W5KYj4ib2rGNuAPlhmNZibfQYXA/640?wx_fmt=png)
我们训练了限玻尔兹曼机（RBM）模型，对q的每一步执行两个梯度步骤。上图显示了混合伯努利q的每个组分的平均分布; 人们可以在其中区分各种数字的形状。这证实q确实接近于p。
加速无向模型的采样
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0eE9f305ibvibHhO0hicTCsmm40Y6HibwYqntmjVO81ETiaJx3y10Ir0ib4KsAiaLMHvhwjicicB48gqXvrKTA/640?wx_fmt=png)
模型完成训练后，我们可以用近似q来初始化一个MCMC采样链。 由于q是p的粗略近似，因此产生的链应该混合得更快。为了证实这个想法，我们在随机初始化的吉布斯（Gibbs）抽样链（顶部）以及从q（底部）样本初始化的链中绘制相邻图形样本。后一种方法在几个步骤中看起来的数字似乎是可信的，而前者产生样本是比较模糊的。
**相关研究**
其实，我们的研究主要启发于对变分自编码器和相关模型的黑盒变分推理，其中，涉及对由神经网络参数化的近似后验概率进行拟合。而我们的研究为无向模型提供了类似的方法。广泛应用的无向模型包括受限深度玻尔兹曼机（Restricted and Deep Boltzmann Machines）以及深度信念网络（Deep Belief Networks）。而与我们的研究最为接近的就是离散的VAE模型。然而，Rolfe试图对p(x|z)进行有效的优化，而RBM的先验p(z)是使用PCD进行优化的。在我们的研究中是使用标准技术来对p(x|z)进行优化的，并且将着重关注p(z)。
更普遍地来讲，我们的研究提出了一种替代性方法，可以替代基于采样的学习方法，而大多数无向模型的变分方法都是以推理为中心的。我们的方法可以扩展到中小型数据集，并且在混合有向/无向生成式模型（hybrid directed-undirected generative models）中最为有用。它的速度可与PCD方法媲美，并能够提供附加的益处，如分区函数追踪（partition
 function tracking）和加速采样。最重要的是，我们的算法是黑盒式的，并且在不需要知道模型结构的情况下就可以推导出梯度或进行分区函数评估。我们预期我们的方法将在诸如Edward这样的自动推理系统中最为有用。
另外，我们方法的可扩展性局限性主要在于，当q与P不能进行很好的拟合时，对梯度和分区函数进行蒙特卡罗评估中的高方差。在实际中，我们发现诸如拟似然（pseudo-likelihood）这样的简单度量法在诊断这个问题上是非常有效的。当用RBM先验对深度生成模型进行训练时，我们注意到弱q的引入模型崩溃了（但是训练仍然收敛）。然后我们通过增加q的复杂性并使用更多的样本解决了这些问题。最后，我们还发现q梯度的得分函数评估器不能很好地向较高的维度扩展。而更好的梯度评估器可能会进一步改进我们的方法。
**结论**
综上所述，本文提出了新的无向模型的变分学习和推理算法，从重要抽样和χ2散度最小化的角度出发，优化了配分函数的上限。我们的方法通过黑匣子的方式训练无向模型，并将在自动推理系统中产生作用。我们的框架在速度方面与采样方法相比具有竞争性，并且提供了额外的优点，如配分函数跟踪和加速采样。我们的方法也可以用来训练使用统一变分框架的混合有向/无向模型。最有趣的是，它使具有离散潜变量的生成模型更具表现力和更容易训练。
