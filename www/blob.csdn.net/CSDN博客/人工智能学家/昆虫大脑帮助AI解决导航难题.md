# 昆虫大脑帮助AI解决导航难题 - 人工智能学家 - CSDN博客
2017年09月22日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：191
![640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBUmuOfHW3q0Mv0GwQmXlx5IfIUGFVTbeeeD8L1qrW6oLpkBicoMTQNXAibwVR0EWL9osE83RJ3SjyCA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
*原作者：SAKYASINGHADASGUPTA, LEAPMIND INC*
*译者：彭婷*
*概要：**无人机和其他自主机器人需要通过移动或其他有效的方案来解决现实生活中的问题，而这些问题有的小到平常的包裹运输，有的大到紧急搜索和救援任务。*
无人机和其他自主机器人需要通过移动或其他有效的方案来解决现实生活中的问题，而这些问题有的小到平常的包裹运输，有的大到紧急搜索和救援任务。通过机器学习和基于昆虫的矢量导航系统，Agent可以在不用GPS的情况下导航找到关键位置，从而达到真正的自主。
在摄像机和其他传感器信息的帮助下，机器人能够学会根据环境感官线索独立导航找到火灾发生地点。而由于用来表示位置的向量是以地心为参考测量的，所以多个Agent间可以相互通信，这样就可以加快执行救援和灭火任务。
在Agent间相互通信之后，执行任务时的灵活性和协调速度将大大提高，从而在自然灾害救援任务，即拯救生命时的成功率和效率也会增加。向大自然学习将有助于未来通过复杂的现实世界环境进行自主远程导航。
**我们可以从蚂蚁身上学到什么？**
蚂蚁和蜜蜂是优秀的导航员。例如，撒哈拉沙漠的蚂蚁可以在高于60°C（140°F）的恶劣条件下觅食，从而生存下去。在这个极端的环境下，他们无法像其他蚂蚁一样使用信息素进行长途追踪，回到巢穴。相反，他们会进行称为路径集成的生物计算。这个路径的计算涉及到天空颜色导航（它们在空中看到的颜色与我们人类所看到的不同）和用来估计它们当前的位置的统计学刺激信息。
路径集成不仅可以让它们安全地返回巢穴，同时还有助于它们的向量记忆。这些记忆足以证明在蚂蚁和蜜蜂中可以产生目标导航。而这些能力使昆虫能够在几百米（例如蚂蚁）至几公里（例如蜜蜂）的路途中进行导航，所以这种控制系统在人造物质的应用中极具潜能。
受到这一想法的启发，我开始与Dennis Goldschmid（来自葡萄牙的Champalimaud Centre for the Unknown）和Poramate Manoonpong博士（来自丹麦南部大学）进行合作。 在最近发表在Neurorobotics前沿的论文中，我们解释了如何开发针对自动Agent目标导航的神经计算模型。我们的模拟机器人能够学习并存储基于路径集成的矢量记忆。
![0?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUmuOfHW3q0Mv0GwQmXlx5I1Qhyic1LicHLV0cCNB4J9dWp00WICX0JoZWCM6rgY0fHHBN6JxAibe78w/0?wx_fmt=png)
**从蜜蜂到机器人**
Agent使用路径集成来存储矢量记忆，而凭借这些记忆，它可以在没有全球位置信息的情况下穿过很长的距离达到目的地或是导航绕过障碍物，安全地返回到家里。
神经模型的所有组件都在标准的笔记本电脑上运行，其中有两种是在仿真中创建的人工代理。初步的模拟包括模拟二维环境中昆虫行为的位点试剂。然后，我们在一个基于三维物理的模拟器中，使用该模型，对具有19度自由度肢体运动范围的模拟复合六足机器人进行了路径集成和导航。这个证明了数学模型的有效性及其在复杂步行机上的可行性。在南部丹麦大学Manoonpong博士团队的合作下，该导航系统将在昆虫仿生机器人AMOS-II上执行。
![0?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBUmuOfHW3q0Mv0GwQmXlx5Iz0Dcbjz96zdOWqibDwiaGIvb6uNOIHj1Ewl741hiciaOR4ABibc8DFlPsLA/0?wx_fmt=jpeg)
开源多传感器电机机器人平台AMOS II（高级移动传感器驱动步行设备第二版）蒙Polamate Manoonpong博士（SDU）以及Bernstein Center forComputational Neuroscience的 Göttingen提供。
**奖励式学习**
我们的Agent还会使用有奖学习规则来加强从路径积分获得的向量记忆。对昆虫而言，奖励就是有食物的场所。我们研究的模型不仅在代理中再现了目标导向和路线形成，而且还可以预测昆虫的导航行为。更重要的是，它为现实世界中导航代理的决策应用提供了一个简单的计算框架。
在模拟中，提供奖励是一种积极信号，它表示代理学会了关联感官线索。在现实世界中，通过加强某些位置的感官线索，这种导航可以应用于移动机器人。（例如，根据视觉或其他感官线索让移动机器人派送包裹）一直以来，即使没有全球定位系统，机器人也可以使用我们的路径集成机制来保持其本地位置的连续跟踪。
**闭合环路**
在融合了各种Agent决策学习机制的生物系统的启发下，为了进一步研究，我们正着手建立一个新颖的闭环学习框架。另外，受人类大脑学习的启发，在这个框架中，强化学习与其他学习机制（例如监督学习）可以在闭路循环中工作。
允许不同学习机制（即强化学习，模仿学习和无监督学习）之间进行相互反馈的闭环网络取得了更为有效的进展，从而让自动化的Agent更快地学会完成新任务。 目前大部分深度学习或深层强化学习的工作都注重使用并优化一种学习机制。然而，受大脑启发的闭环方法可能会提供一个更为有效并可扩展的学习框架。
Sakyasingha Dasgupta博士是东京创业公司LeapMind研究团队的首席科学家。
本文系网易新闻· 网易号“各有态度”特色内容。
