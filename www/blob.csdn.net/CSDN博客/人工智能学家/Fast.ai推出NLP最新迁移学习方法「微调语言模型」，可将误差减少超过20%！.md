# Fast.ai推出NLP最新迁移学习方法「微调语言模型」，可将误差减少超过20%！ - 人工智能学家 - CSDN博客
2018年01月21日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：227
![?wx_fmt=jpeg&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/waLJGrhJM0fgHeqrAChDd3mZPocUPM5kvHibo0PiaSZyV15ZlQzrQbU0tTLHVwCVDgz6u8YDIVTAJYFcGBZxHLnA/?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
原文来源：arxiv
作者：Jeremy Howard、Sebastian Ruder
「雷克世界」编译：嗯~是阿童木呀
可以这样说，迁移学习已经促使计算机视觉领域发生了具有革命性的变化，但对于自然语言处理（NLP）中的现有方法来说，仍然需要从零开始对其进行针对于特定任务的修改和训练。我们提出了微调语言模型（Fine-tuned Language Models，FitLaM），这是一种有效的迁移学习方法，可以应用于NLP中的任何任务，并且引入一些关键技术以对现有最先进的语言模型进行微调。我们的方法在五个文本分类任务上的性能表现要明显优于现有最先进的技术，在大多数数据集上的实施中能够将误差减少18-24％。我们对我们的预训练模型和代码进行了开源设置，以便社区采用，使该方法具有更为广泛的应用。
![?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fgHeqrAChDd3mZPocUPM5kqeZuRem2GF6QmpiaDdJp4gBfB2mUeQs9LfpjuS6jcfGv3AQNyz0RIlQ/?wx_fmt=png)
迁移学习对于计算机视觉（CV）的发展起着很大的作用。应用型CV模型（包括目标检测、分类和分割）很少是从零开始进行训练的，而是对已经在ImageNet、MS-COCO和其他数据集上进行预训练的模型进行细微调整得到的（Sharif Razavian等人于2014年、Long等人于2015年、He等人于2016年、Huang等人于2017年提出）。
文本分类是一类常见的自然语言处理（NLP）任务，它涉及许多诸如垃圾邮件、欺诈和机器人检测、应急响应和商业文件分类（如法律发现）等重要的实际应用。
![?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fgHeqrAChDd3mZPocUPM5klicp8QqtmGOicpyc84mlTlztdze4jv2Z9icH4CCX3kAEV4ptNC5Gxaic4g/?wx_fmt=png)
*文本分类中的数据集和任务*
尽管深度学习模型已经在诸多自然语言处理任务上达到了当前最先进的技术水平，但这些模型都是从零开始进行训练的，这需要大量的数据集，而且需要若干天的时间才能达到收敛状态。对于利用迁移学习的自然语言处理任务来说，这已经处于算机视觉领域中较为落后的状态了。对预训练的词嵌入进行微调，是一个简单的迁移学习技术，它只针对模型的第一层，已经在实践中产生了超乎想象的影响力，并在大多数当前最为先进的模型中进行使用。考虑到对模型进行预训练的好处，我们应该能够比随机初始化模型其余参数这一举措做得更好。
当前常用的方法是将来自诸如语言建模或机器翻译等其他任务的嵌入与不同层的输入连接在一起。然而，这些方法存在着这样一个问题，它们仍然是从零开始对主要的任务模型进行训练，并将预训练的嵌入作为固定参数进行处理，从而限制了它们的实用性。
![?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fgHeqrAChDd3mZPocUPM5kfqibXeTr3cpZrWHGDBfEkJnHPKKb1FqSosSFU9hjaL8s9rP1YQc1XxA/?wx_fmt=png)
*在两种文本分类数据集上进行测试，所取得的精确度得分情况*
可以这样说，一个成功的NLP迁移学习技术应该能够达到与其计算机视觉技术相对应的类似标准：a）该方法应该能够充分利用大量的可用数据；b）它应该利用一个可以独立进行优化的任务，从而进一步实现下游的改进；c）它应该依赖于一个可以应用于大多数NLP任务的单一模型；d）在实践中应该很容易进行使用。
![?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0fgHeqrAChDd3mZPocUPM5kaQABdRN3icHpAWLbJKBE2gP2pnJkibsEaeZib2UUFdhUNibicmkutVyJwhQ/?wx_fmt=png)
*在三个文本分类数据集上进行测试，所达到的误差率（%）情况*
我们提出将微调语言模型（FitLaM）作为NLP的一种有效的迁移学习形式，它完全满足上述标准。我们方法使用的是语言建模，这是一项几乎具有无限数据量的任务，并且能够推动当前最先进技术取得最新进展。它通过预训练，能够将大量的外部数据以及域内数据进行集成。
FitLaM依赖于一个简单的循环神经网络（RNN），而不需要对其进行任何的修改。我们只需要使用一个或多个针对于特定任务的线性层对模型进行扩充，相较于现有的方法来说，它只需要考虑少量的参数。我们提出了一种新的微调技术，即判别式微调（discriminative fine-tuning），它对较低层进行微调以调到一个相较于较高层较低的程度，从而保留通过语言建模所获得的知识。我们还介绍了一些技术，这些技术示微调能够取得较好性能和进行更快训练的关键所在。
我们在五个经过广泛研究，具有不同大小和类型的文本分类任务中对我们的迁移学习方法进行了评估，实验结果表明，相较于以往高度针对于特定任务的研究和当前最先进的方法来说，我们方法的性能表现具有显著的优越性。
**我们所取得的成就大致如下所示：**
1.我们归纳总结了CV和NLP中迁移学习的相似之处，并为NLP中有效的迁移学习方法提供了相关的依据。
2.我们提出了微调语言模型（FitLaM），这种方法对于NLP的任何任务来说，可以用以实现类似于CV中那样的迁移学习方法。
3.我们提出使用判别式微调以保留以往的知识，并避免在微调过程中产生严重的遗忘。
4.我们引入了一种用于文本分类的基于时间的反向传播（Back-Propagation Through Time，BPT3C），这是一种新的方法，通过线性层将分类器的损失反向传播到任何序列大小的RNN输出中。
5.我们引入了一些技术，它们是对预训练语言模型进行微调的关键所在。
6.我们在五个代表性的文本分类数据集上的性能表现要明显优于现有的文本分类方法，其中，在大多数数据集的误差减少了18-24％。
7.我们开源了我们的预训练模型以及相关代码，从而希望能够实现更为广泛的应用。
我们提出了一种适用于NLP任务的有效迁移学习方法——FitLaM，以及一种称之为判别式微调的方法，这种有效的微调方法可以对不同层进行不同程度的调整，以避免过程中的灾难性遗忘。我们已经引入了于文本分类的基于时间的反向传播（BPT3C），这种方法能够将分类器的损失反向传播到任何序列大小的RNN输出中，除此之外，我们还引入了若干起着关键作用的好方法，从而能够实现较好的微调性能表现和更快速地进行训练。实验结果表明，我们的方法要明显优于现有的迁移学习技术，以及用于五个具有代表性的文本分类任务的最新技术。总的来说，我们已经证明了用于NLP的迁移学习的优势所在，并希望我们的研究结果将能够促进用于NLP的迁移学习能够取得更好的新进展。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。由互联网进化论作者，计算机博士刘锋与中国科学院虚拟经济与数据科学研究中心石勇、刘颖教授创建。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
