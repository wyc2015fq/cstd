# 洪小文：以科学的方式赤裸裸地剖析AI（二）｜从寒冬到复兴 - 人工智能学家 - CSDN博客
2017年11月08日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：102
![640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVWP4sBk12DmXtwwf8X5rs1ITr4vicXaOEtMeoZcLN20wrcpHhDfxiasY1UdwpLSKblPLQMTUjyW0Vw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
*来源：微软研究院AI头条*
*概要：1956年，在达特茅斯会议之后，包括很多国家政府，美国国家科学基金会、军方，大家满怀希望投了很多钱。但是到1975年以后发生了几件事情，让AI进入了寒冬。*
谈AI的历史，需要谈谈很有名的AI寒冬。
第一次AI寒冬是在1975年左右。1956年，在达特茅斯会议之后，包括很多国家政府，美国国家科学基金会、军方，大家满怀希望投了很多钱。但是到1975年以后发生了几件事情，让AI进入了寒冬。
第一件事是，因为AI只能解决Toy domain（摆弄玩具一样的简单任务）。那个时候做语音，只有10个词汇；下象棋，大概是20个词汇；做视觉的人，都不能辨认出一个椅子。第二件事情，1956年美国打越战，还有石油危机，所以经济也不是那么好；还有一个很有名的英国学者Lighthill，说AI就是在浪费钱，AI的研究经费也因此遭到大幅削减。
到1980年开始，有些公司如IBM开始做一些专家系统，可以说也是有限的应用。尽管有一些缺点，但还是可以做一些事情，据说有十个亿的产出。因此，AI也就开始回春。我也是这个时候开始进入AI，所以也蛮幸运的。
我是80年代去美国CMU（卡内基梅隆大学）的。我记得当时日本很有钱，到处在美国买楼、建实验室，所以当时日本提出了一个第五代电脑系统计划（5th generation computer systems，FGCS）。当时还有公司专门做 Lisp Machines（通过硬件支持为了有效运行Lisp程序语言而设计的通用电脑）。就有点像今天DNN红，大家都在做DNN芯片，那时候大家都在做Lisp
 Machines，Thinking ( Connection ) Machines，然后神经网络也刚开始发芽。
不过，到1990年中，AI又第二次遇冷，为什么会这样？因为第五代计划失败，Lisp Machines和Thinking ( Connection ) Machines都做不出来；而神经网络，虽然有意思，但并没有比其他一些统计的方法做得好，反而用的资源还更多，所以大家觉得也没什么希望了，于是AI又进入第二个冬天。
**1990年代统计路径的显现**
差不多在冬天这个时刻，统计的方法，使用数据的方法出现了。
AI在1990年以前都是用所谓的研究人脑的方式来做；而我们有太多理由来相信人脑不是靠大数据的。比如，给一个小孩子看狗和猫，看几只他就可以辨认了。可用今天的方法，要给计算机看几十万、几百万只狗跟猫的图片，它才能辨认是狗还是猫。用大数据这种方法，就在第一次AI寒冬和第二次AI寒冬之间开始萌芽。虽然AI是一批计算机科学家搞出来的，但事实上有跟AI极其相关的一门知识叫模式识别。模式识别一直以来都由工程师在做，从1940年代统计学家就在做模式识别。
我们这代人学计算机就知道两个人，一个人叫傅京孙（K. S. Fu），另外一个人叫窦祖烈（Julius T. Tou）。如果AI选出60个人的名人堂，里面会有一个叫傅京孙，那是大牛。傅京孙严格上来讲他不算做AI的，但是可以包括进来，因为他也做模式识别。模式识别里面也有两派，一派叫统计模式识别（Statistical Pattern Recognition），一派叫做句法模式识别（Syntactic
 Pattern Recognition）。80年代的时候，句法是很红的，统计的无人问津，后来1990年以后大家都用统计。
我们做语音的人很清楚，后来引入了隐马尔可夫模型（Hidden Markov Model），都是统计的方法，到今天还是很有用。尤其是在华尔街，做金融投资，做股票，很多都是做时间序列（time series data），而隐马尔可夫模型这个东西是很强大的。甚至可以说，统计的方法是我们做语音的人（发展起来的）。而且早在1980年，我们做语音的人就讲出这句话“There
 is no data like more data（没有什么样的数据比得上更多的数据）”。从现在的角度来看，这是非常前瞻性的，而且就是大数据的概念。我们那个时代的数据量无法和现在相比，但我们已经看出来了数据的重要。而且IBM在这方面是了不起的，他们一个做语音的经理有次说，每次我们加一倍的数据，准确率就往上升。
决策树也是第一个被语音研究者所使用。然后就是贝叶斯网络（Bayesian Network），几年前红得不得了，当然现在都是用深度学习网络（deep neural network, DNN，在输入和输出之间有多个隐含层的人工神经网络）了。我为什么要提这些东西？今天我觉得很多人上AI的课，可能75%、80%都会讲DNN，其实AI还是有其它东西的。
今天要教AI也是非常困难的。我还特别看了一下最近的AI教科书。学术界教AI，还会教这些东西，但是如果去一般或者大多数公司，全部都是在讲DNN。我觉得现在找不到一本好的AI教科书，因为早期的书统计没有讲，或者没有讲DNN。我也看了下加州大学伯克利分校的Stuart J. Russell 跟Peter Norvig写的教科书《Artificial Intelligence:
 A Modern Approach》，里面DNN提了一点。可能现在也不好写AI，因为AI提了这么多东西，人家说根本没用，不像DNN的确很有用。
我稍微解释一下DNN和一般统计方法的差别。统计的方法一定要有一个模型，但是模型一定是要有假设。而你的假设多半都是错的，只能逼近这个模型。数据不够的时候，一定要有一定的分布。当数据够了，DNN的好处是完全靠数据（就可以），当然也需要很大的计算量。所以DNN的确有它的优点。以前我们用统计的方法做，还要做特征提取，用很多方法相当于做了一个简易的知识表示；现在用DNN连特征提取都不用做了，只用原初数据进去就解决了。所以现在讲AI不好讲的原因是，DNN讲少了也不对，讲多了的话，说实在的，全是DNN也有问题。
**神经网络的起伏**
最早的神经网络叫感知器（Perceptron），跟第一个寒冬有关。因为一开始的感知器没有隐含层（hidden layer），也没有激活函数（activation function），结果Marvin Minsky和Seymour Papert这两位就写了一本书《感知器》说，感知器连异或（XOR）都做不出来。那么，做感知器还有什么用？所以基本上就把整个神经网络第一代的进展扼杀了。
![?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/zUBN52ncQtTRJ9voQOMYbZWxmzSpkliaLe80FKMy5nCj95eLDwy7Ric5dKk5v9GPYooJaMlGIexec3XMyibr6ZWAw/?)
*感知器连最简单的逻辑运算“异或”都无法做到，某种程度上导致了AI的寒冬*
其实后来人们发现误会了，其实书并没有说的那么强，不过的确造成了很大的影响。一直到1980年，做认知心理学的人，代表性的如Rumelhart和Hinton才复兴了AI。
Hinton早期是做认知心理学的。Hinton先在UCSB（加利福尼亚大学圣巴巴拉分校），后来到了CMU。Rumelhart, Hinton and McClelland 复兴了多层的感知器，加了隐含层以及back-propagation 算法，这个时候神经网络就复兴了。而且神经网络只要加上隐含层，事实上，只要加一层，再加上激活函数，就可以模拟，甚至还有人证明可以模拟任意的函数，所以神经网络一下子就变的红了。卷积神经网络（Convolutional neural
 network，CNN）那时候就开始出来了，然后是递归神经网络（Recurrent neural network，RNN）。因为如果要处理过往的历史，有存储, 就需要回溯。用于语音和自然语言处理的时间延迟的神经网络（Time-Delayed NN，TDNN） 也都有了。
不过，那时候数据不够多。数据不够多就很容易以偏概全。第二个因素是，计算的资源不够，所以隐含层也加不了太多。这样，神经网络虽然大家都很有兴趣，也能够解决问题，但是却有更简单的统计方法，如支持向量机（Support vector machine，SVM），能够做到一样或者略好。所以在1990年代就有了AI的第二次冬天，直到DNN的出现才又复苏。
**AI的复苏**
AI的复苏，可能要从1997年开始说起。1997年，深蓝打败了国际象棋冠军Garry Kasparov。这里我要提一下一个人叫许峰雄。他当时在CMU做一个当时叫做深思（deep thought）的项目，基本上架构都有了。结果，IBM非常聪明。他们到CMU参观，看到许峰雄这个组。然后也没花多少钱，就买下了这个组，让这些人到IBM做事。IBM当时就看到，在五年之内就可以打败世界冠军，其实真正的贡献都是在CMU做的。许峰雄后来也离开了IBM，加入了我们，一直做到退休。AI的复苏实际上才刚开始。有人说这个也没有帮助到AI复苏，因为深蓝可以打败国际象棋的冠军，也不是算法特别了不起，而是因为他们做了一个特殊芯片可以算得很快。当然，AlphGo也算得很快，算得很快永远是非常重要的。
到了2011年，IBM做了一个问题回答机器叫沃森（Watson），打败了Jeopardy游戏的冠军。Jeopardy这个游戏有一点像记忆的游戏：问一个常识的问题，给四个选项。其实沃森打败人也没什么了不起的。
到2012年，AI的复苏就已非常明显。机器学习和大数据挖掘变成了主流，几乎所有的研究都要用，虽然还不叫AI。事实上很长一段时间，包括我们做语音和图像，对外都不讲AI。因为AI这个名字那时变得有点名声不好。人们一说起AI，就是不起作用。第二次AI寒冬的时候，只要听说某个人是做AI，那就认为他做不成。其实机器学习也是AI的一支。
现在回到深度学习，有三个人物对深度学习做出了很大贡献。第一位，Hinton。这个人非常了不起。了不起之处在于当没有人在乎神经网络的时候，他还在孜孜不倦的做这个东西。第二个做CNN的人物是Yann LeCun。他也是做CNN一辈子，在AI冬天的时候继续做，所以今天很多CNN该怎么用来自于Yann LeCun。另外一个叫做Yoshua Bengio。
所以今天讲到DNN、讲到AI，没有前人的种树，就没有后人的乘凉。这61年的发展，这些辛苦耕耘的人，大家需要记住这些人。今天在台面上讲AI的人都是收成果实的人，讲自己对AI有什么贡献，我觉得就太过了。
还有一个跟AI有关的，大家记得Xbox几年前有一个叫Kinect，可以在玩游戏的时候用这个东西，我觉得这是第一个发布的主流的动作和语音感知设备。当然之后就有2011年苹果的Siri，2012年Google语音识别的产品，以及微软2013年产品，这些都是AI的复苏。直到2016年，AlphaGo打败了李世石，打败了柯杰，AI就彻底复苏了。
**今天的AI**
DNN、DNN还是DNN。我不是有意要贬低DNN的重要性，但如果说DNN代表了所有的智慧也言过其实。DNN绝对非常有用，比如计算机视觉，会有CNN；自然语言或者语音的，就有RNN，长短时记忆（Long Short-Term Memory，LSTM）。计算机视觉里面有一个图片集ImageNet。我们很荣幸在几乎两年前，微软在该图片集上辨认物体可以跟人做得一样好，甚至超过人。
语音也是一样，微软在差不多一年前，在Switchboard，任意的一个任务里面也超过了人类。机器翻译我相信大家都常用，可能是每天用。甚至看起来好像有创造性的东西也出现了，比如小冰可以写诗。我也看到很多电脑画出来的画，电脑做出来的音乐，都表现的好像也有创造力一样。
不过，虽然AI很红，机器学习，大数据大家都听过，特别是做学问的人还听过大数据挖掘，那么这三者有多大的差别？我常说这三个东西不完全一样，但是今天这三个的重复性可能超过90%。所以到底是AI红，还是大数据红呢？还是机器学习红呢？我觉得有那么重要吗？
