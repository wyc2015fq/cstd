# 谷歌大脑最新研究：不用「训练」！高斯过程「超越」随机梯度下降 - 人工智能学家 - CSDN博客
2017年11月04日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：218
![640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/waLJGrhJM0cy4ZsaxcUn3rAzAlnQr90Oibot2peLBCmfVLXqibht6QP17tJibGP9LediapVfy4Gc1ibqHW1vfEwCLVg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
*来源：雷克世界*
*编译：嗯~阿童木呀、多啦A亮*
*概要：近年来，深度神经网络作为一种灵活的参数模型，以能够适应复杂的数据模式而著称。*
可以这样说，一个具有独立同分布（independent identically distributed，i.i.d）先验参数的深度完全连接神经网络，就等同于在无限网络宽度限制下的高斯过程（GP）。这种对应关系使得仅通过简单的矩阵计算，便能够为回归任务上的神经网络提供精确的贝叶斯推理。而对于单隐层网络来说，这个GP的协方差函数早已为人所知。
最近，多层随机神经网络的核函数已经被开发出来，但只适用于贝叶斯框架之外。因此，以前的研究并没有明确使用这些内核作为一个GP的协方差函数和使用一个深度神经网络进行完全贝叶斯预测之间的对应关系。在本项研究中，我们推导出了这种对应关系，并开发出一个在计算上颇为高效的管道来计算协方差函数。然后，我们使用生成的GP对MNIST和CIFAR-10上的深度神经网络进行贝叶斯推理。我们发现基于GP的预测结果非常好，并且可以超越使用随机梯度下降（SGD）进行训练的神经网络。我们观察到，随着层宽度的增加，经过训练的神经网络的精确度接近于相应的基于GP的计算，并且GP的不确定性与预测误差密切相关。然后，我们就想到，将观察结果与随机神经网络中信号传播的最新发展联系起来。
近年来，深度神经网络作为一种灵活的参数模型，以能够适应复杂的数据模式而著称。作为一种对比，长期以来，高斯过程一直是传统的非参数化建模工具。实际上，Radford Neal提出的一个对应关系（于1994年提出）在无限宽度的限制条件下将这两个模型对等起来了。
![0?wx_fmt=png](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0cy4ZsaxcUn3rAzAlnQr90OSQX2DrPor43ia2bpU7IhyuRlOntPnq7VFcySmFrCCENvkqSTkribvYVQ/0?wx_fmt=png)
*Radford Neal*
我们可以考虑使用具有独立同分布随机参数的深度完全连接神经网络，网络的每一个标量输出，即最终隐藏层的仿射变换（affine transformation），将是独立同分布的总和。在无限宽度的限制条件下，中心极限定理（Central Limit Theorem）意味着经由神经网络（NN）计算的函数即是一个从高斯过程（GP）提取的函数。而在单隐层网络的情况下，这个GP内核的形式是广为人知的（Neal于1994年、Williams于1997年提出）。
这个对应意味着，如果我们选择假设空间为无限宽的神经网络，那么，权重和偏差的独立同分布先验就可以被函数相对应的GP先验所代替。正如Williams（于1997年）所指出的那样，这种替代使得我们可以使用神经网络对回归任务进行精确的贝叶斯推理。该计算需要在训练和测试集上建立必要的协方差矩阵以及简单的线性代数计算。
鉴于神经网络的普及和发展，重新审视这一领域是很有必要的。我们描述了深度神经网络和GP之间的对应关系，并利用它对回归任务的神经网络进行贝叶斯训练。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0cy4ZsaxcUn3rAzAlnQr90Od4ibzCpKamryPQXkGpJjrOSwqsoydSBfW8Iz1noWAoBNLpHicwtmwS7w/640?wx_fmt=png)
**相关研究**
一般来说，我们的研究涉及GP、贝叶斯学习和组合内核的各个方面。无限神经网络与GP之间的这种对应关系，最初是由Radford Neal发现的。Williams（于1997年）计算出了这种用于具有误差函数或高斯非线性的单隐层神经网络的解析GP核函数，并强调在回归任务中使用GP先验以进行精确的贝叶斯推理。Duvenaud等人（于2014年）探讨了构建深度GP的几条路径，并观察了被组成了无限多次的内核的退化形式，但他们并没有像我们那样得到GP内核的形式。Hazan和Jaakkola（于2015年）所探讨的内核依赖于辅助GP。
我们在GP上下文之外也进行了相关的研究，但这是在组合内核结构进行的。Cho和Saul（于2009年）推导出了一个多项式级的非线性的组成核，其中包括Sign和ReLU非线性，并可用于GP中。可以说，尽管上下文是不同的，但我们的核心构成方式与他们的是相符的。Daniely等人（于2016年）将组合内核的构造扩展到了神经网络，其潜在的有向无环图（他们称之为“计算框架”）是通用的。他们还证明，利用双重激活形式化情况下，由完全连接的拓扑构成的具有相同非线性的组合内核在无限多次组合的情况下将变得退化。在与组成内核不同的背景下，Poole等人（于2016年）、
 Schoenholz等人（于2017年）针对完全连接网络和有界非线性的具体情况研究了相同的潜在递归关系。它们区分了超参数空间中具有不同的固定点和收敛行为的区域。进行这些研究的重点是更好地理解深度网络的表达性和可训练性。
从一个递归的、确定性的内核函数计算方面考虑的话，我们首先指定一个GP的形式，它对应于一个深度的、无限宽度的神经网络——（以下称为神经网络GP（NNGP））。该方法对于通用的逐点非线性是非常有效的。我们开发了一个在计算上非常有效的方法，用以计算与具有固定超参数的深度神经网络相对应的协方差函数。
在这项研究中，作为我们NNGP构造概念的第一个证明，我们关注的是回归任务中的精确贝叶斯推理，并将分类视为类标签上的回归。虽然缺乏原则性，但最小二乘分类表现良好（Rifkin等人于2003年提出），使我们能够将通过GP得到的精确推理，与通过在复杂任务（MNIST和CIFAR-10分类）上得以训练的神经网络得到预测相比较。需要注意的是，我们有可能将GP扩展到具有交叉熵损失的softmax分类中（Williams和Barber于1998年、
 Rasmussen和Williams于2006年提出），而这是我们打算在接下来的工作中着手研究的。我们在MNIST和CIFAR-10上进行了贝叶斯预测的实验，并与基于梯度的标准方法进行训练的神经网络进行比较。实验探讨了贝叶斯训练的不同的超参数设置，包括网络深度、非线性、训练集大小(包括由成千上万的图像组成的完整数据集)，以及权重和偏差方差。我们的实验表明，NNGP的最佳性能始终优于用基于梯度的技术训练的神经网络的NNS，并且从超参数中选择的最佳NNGP设置也常常超过传统训练的结果。我们进一步观察随着网络宽度的增加，基于梯度训练的神经网络的性能接近于NNGP计算。此外，NNGP的性能取决于内核的结构，它可以与在具有随机参数网络中的信号传播的近期研究联系起来（Schoenholz等人于2017年提出）。
**结论和未来的方向**
通过利用无限宽度的限制，我们已经指定了深度神经网络的先验和高斯过程之间的对应关系，其核函数是组合的，但是以全确定和可微的方式构建。在函数使用GP先验可以实现对矩阵计算回归的精确贝叶斯推断，因此我们能够在没有基于随机梯度的训练的情况下从深度神经网络获得预测和不确定性估计。在类似的超参数设置下，性能与在同一回归任务上训练的最好的神经网络相当。虽然我们能够对有些较大的数据集（大小为50k）进行实验，但是我们打算利用最新的可扩展GP研究更大规模的学习任务的可扩展性。
在已经提到的那些方面，我们建议另外一些有趣的方向。在我们的实验中，我们观察到优化的神经网络的性能接近于宽度增加的GP计算。基于梯度的随机优化是否实现近似贝叶斯计算是一个有待进一步研究的有趣问题。最近的研究认为，SGD可以从贝叶斯后验近似采样，需要进一步的研究来确定SGD是否在实践中通常采用的条件下大致实现了贝叶斯推断。此外，神经网络GP（NNGP）提供了明确的不确定性评估。这对于预测深度学习的关键应用中的模型失效，或者用于主动学习任务（其可以用于识别手工标签的最佳数据点）可能是有用的。
论文下载：https://arxiv.org/pdf/1711.00165.pdf
