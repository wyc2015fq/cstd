# 从算法到硬件，一文读懂2019年 AI如何演进 - 人工智能学家 - CSDN博客
2019年03月05日 22:18:44[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：107
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBV6pBFs9FfbY1nJsFGSfQ4IyKFkG7AJRz3XUib4icOribLtgvnDsaptILKdA9yLsWLNnYwsic1ohYibJ2g/640?wx_fmt=jpeg)
来源：图灵TOPIA
翻译：黄姗，杨鹏岳
> 
在刚刚过去的2018年，人工智能领域的大事件、新发现和新进展层出不穷。
BERT重磅发布，刷新了很多NLP的任务的最好性能；GAN相关研究论文持续增长，生成的照片达到了以假乱真的程度；Deepfakes发展神速，让许多政客和明星供大众娱乐了一番；强化学习也在与人类的对战游戏中独领风骚......
硬件方面，Nvidia一骑绝尘，Intel努力求变，定制硬件市场繁荣；
除此之外，自动驾驶、AI伦理等也是过去一年的讨论重点。
回顾2018展望2019，人工智能和机器学习将走向何方？
Medium的一位专栏作者为此撰文概括了过去一年中人工智能领域的一些模式，并试图勾勒出其中的某些趋势。注意，这篇总结是以美国的发展为中心展开，以下是文章全文：
**算法**
毫无疑问，算法话语权由深度神经网络（DNN）主导。
当然，你可能会听说有人在这里或那里部署了一个“经典的”机器学习模型(比如梯度提升树或多臂老虎机)，并声称这是每个人唯一需要的东西。有人宣称，深度学习正处于垂死挣扎中。甚至连顶级的研究人员也在质疑一些深度神经网络（ DNN） 架构的效率和抗变换性。
但是，不管你喜欢与否，DNN无处不在: 自动驾驶汽车、自然语言系统、机器人——所有你能想到的皆是如此。
在自然语言处理、生成式对抗网络和深度增强学习中，DNN取得的飞跃尤为明显。
**Deep NLP: BERT以及其他**
尽管在2018年之前，文本使用DNN（比如word2vec、GLOVE和基于LSTM的模型）已经取得了一些突破，但缺少一个关键的概念元素：迁移学习。
也就是说，使用大量公开可用的数据对模型进行训练，然后根据你正在处理的特定数据集对其进行“微调”。在计算机视觉中，利用在著名的 ImageNet 数据集上发现的模式来解决特定的问题，通常是一种解决方案。
问题是，用于迁移学习的技巧并不能很好地应用于自然语言处理（NLP）问题。在某种意义上，像 word2vec 这样的预先训练的嵌入正在弥补这一点，但它们只能应用于单个单词级别，无法捕捉到语言的高级结构。
然而，在2018年，这种情况发生了变化。 ELMo，情境化嵌入成为提高 NLP 迁移学习的第一个重要步骤。ULMFiT 甚至更进一步: 由于不满意嵌入式的语义捕捉能力，作者找到了一种为整个模型进行迁移学习的方法。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHA0xXueIzcjwaxsnYoiarknD3lqJ3YDOOVibSKtJiaOMh6qSW9VfEfBW9Og/640?wx_fmt=jpeg)
但最有趣的进步无疑是BERT的引入。通过让语言模型从英文维基百科的全部文章中进行学习，这个团队能够在11个 NLP 任务中取得最高水准的结果——这是一个壮举！更妙的是，它开源了。所以，你可以把这一突破应用到自己的研究问题上。
**生成式对抗网络（GAN）的多面性**
CPU的速度不会再呈现指数级的增长，但是生成式对抗网络（GAN）的学术论文数量肯定会继续增长。GAN多年来一直是学术界的宠儿。然而，其在现实生活中的应用似乎很少，而且这一点在2018年几乎没有改变。但是GAN仍然有着惊人的潜力等待着我们去实现。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHA9ibRTRCDiaV1g4ar6YNlsGiauacKvBjicktGzfibfTBPzM3UkdOSbQ1VTsQ/640?wx_fmt=png)
目前出现了一种新的方法，即逐步增加生成式对抗网络: 使生成器在整个训练过程中逐步提高其输出的分辨率。很多令人印象深刻的论文都使用了这种方法，其中有一篇采用了风格转移技术来生成逼真的照片。有多逼真？你来告诉我：
这些照片中哪一张是真人？
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHAINGHkJosnRqMbkujiagLRTbl3H5OqUJPFA3KTHLZHyCOVObrh9fqusg/640?wx_fmt=png)
这个问题有陷阱：以上皆不是。
然而，GAN是如何工作的，以及它为什么会起效呢？我们对此还缺乏深入的了解，但是我们正在采取一些重要的措施: 麻省理工学院的一个团队已经对这个问题进行了高质量的研究。
另一个有趣的进展是“对抗补丁“，从技术上来说它并非是一个生成式对抗网络。 它同时使用黑盒(基本上不考虑神经网络的内部状态)和白盒方法来制作一个“补丁”，可以骗过一个基于 CNN的分类器。 从而得出一个重要的结果：它引导我们更好地了解深度神经网络如何工作，以及我们距离获得人类级别的概念认知还有多远。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHAUFl7XG0XXoO1HjSt1NqTI8ystr3Jg9yicdIBMtkpjSnfMTXCW3RUOvQ/640?wx_fmt=jpeg)
你能区分香蕉和烤面包机吗？人工智能仍然不能。
**强化学习（RL）**
自从2016年 AlphaGo 击败李世石后，强化学习就一直是公众关注的焦点。
在训练中，强化学习依赖于“奖励”信号，即对其在最后一次尝试中表现的评分。电脑游戏提供了一个与现实生活相反的自然环境，让这种信号随时可用。因此，RL研究的所有注意力都放在了教 AI玩雅达利游戏上。
谈到它们的新发明 DeepMind，AlphaStar又成了新闻。这种新模式击败了星际争霸 II的顶级职业选手之一。星际争霸比国际象棋和围棋复杂得多，与大多数棋类游戏不同，星际争霸有巨大的动作空间和隐藏在玩家身上的重要信息。这次胜利对整个领域来说，都是一次非常重要的飞跃。
在RL这个领域，另一个重要角色OpenAI也没有闲着。让它们声名鹊起的是OpenAI Five，这个系统在2018年8月击败了Dota 2这个极其复杂的电子竞技游戏中99.95%的玩家。
尽管 OpenAI 已经对电脑游戏给予了很多关注，但是他们并没有忽视 RL 真正的潜在应用领域: 机器人。
在现实世界中，一个人能够给予机器人的反馈是非常稀少且昂贵的：在你的 R2-D2（电影中的虚拟机器人）尝试走出第一“步”时，你基本上需要一个人类保姆来照看它。你需要数以百万计的数据点。
为了弥合这一差距，最新的趋势是学会模拟一个环境，同时并行地运行大量场景以教授机器人基本技能，然后再转向现实世界。OpenAI和Google都在研究这种方法。
**荣誉奖：Deepfakes**
Deepfakes指一些伪造的图像或视频，（通常）展示某个公众人物正在做或说一些他们从未做过或说过的事情。在“目标”人物大量镜头的基础上训练一个生成式对抗网络，然后在其中生成包含所需动作的新媒体——deepfakes就是这样创建的。
2018年1月发布的名为FakeApp的桌面应用程序，可以让所有拥有计算机科学知识的人和对此一无所知的人都能创建deepfakes。虽然它制作的视频很容易被人看出来是假的，但这项技术已经取得了长足的进步。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHA8ARw4RNwWqxSbeqSVnYwvsJobn9K0qZFSLPUYEBQTfKpsaHicCLIZkA/640?wx_fmt=jpeg)
**基础设施**
**TensorFlow与PyTorch**
目前，我们拥有很多深度学习框架。这个领域是广阔的，这种多样性表面上看是有意义的。但实际上，最近大多数人都在使用Tensorflow或PyTorch。如果你关心可靠性、易于部署性和模型重载等SREs 通常关心的问题，那么你可能会选择 Tensorflow。如果你正在写一篇研究论文，而且不在谷歌工作，那么你很可能使用PyTorch。
**ML作为一种服务随处可见**
今年，我们看到了更多的人工智能解决方案，它们被一个未获得斯坦福大学机器学习博士学位的软件工程师打包成一个供消费的 API。Google Cloud和Azure都改进了旧服务，并且增加了新服务。AWS机器学习服务列表开始看起来十分令人生畏。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHA9bFaicPiaKEFv6dTlntpbTWo2YhEfMr4u0keHoRticwN0GNEcib81YEztw/640?wx_fmt=png)
天啊，AWS的服务很快就会多到需要两级目录层次结构来展示了。
尽管这种狂热现象已经冷却了一些，但还是有很多创业公司发出了挑战。每个人都承诺了模型训练的速度、推理过程中的易用性和惊人的模型性能。
只要输入你的信用卡信息，上传你的数据，给模型一些时间去训练或者微调，调用 REST (或者，给更有前瞻性的创业公司GraphQL)的 API，就可以成为人工智能方面的大师，甚至不需要搞清楚“随机失活(dropout)”是什么。
有了这么多的选择，为什么还有人会费心自己建造模型和基础设施呢？实际上，现成的 MLaaS 产品在80% 的实用案例中表现得非常好。如果你希望剩下的20% 也能正常工作，那就没那么幸运了: 你不仅不能真正地选择模型，甚至不能控制超参数。或者，如果你需要在云的舒适区之外的某个地方进行推断——一般情况下都做不到。这就是代价。
**荣誉奖：AutoML和AI Hub**
今年推出的两项特别有趣的服务均由谷歌发布。
首先，Google Cloud AutoML是一套定制的 NLP 和计算机视觉模型培训产品。 这是什么意思？汽车设计师通过自动微调几个预先训练的模型，并选择其中最好的那个，从而解决了模型定制问题。这意味着你很可能不需要自己去定制模型。
当然，如果你想做一些真正新鲜或不同的东西，那么这个服务并不适合你。但是，谷歌在大量专有数据的基础上预先训练其模型，这是一个附带的好处。想想所有关于猫的照片，它们一定比 Imagenet 更具推广性！
第二，AI Hub 和 TensorFlow Hub。在这两者出现之前，重复使用某人的模型确实是件苦差事。基于 GitHub 的随机代码很少能用，通常记录得很差，而且一般来说，处理起来并不愉快。还有预先训练的迁移学习权重……这么说吧，你甚至不想尝试把它们用于工作中。
这正是TF Hub想要解决的问题: 它是一个可靠的、有组织的模型存储库，你可以对其进行微调或构建。只要加入几行代码——TF Hub 客户端就可以从谷歌的服务器上获取代码和相应的权重——然后，哇哦，它就可以正常工作了！
Ai Hub 更进一步：它允许你共享整个ML管道，而不仅仅是模型！它仍然处于 alpha 测试阶段，但如果你明白我的意思的话，它已经比一个连最新的文件也是“3年前才修改”的随机存储库要好得多。
**硬件**
**Nvidia（英伟达）**
如果你在2018年认真研究过ML，尤其是DNN，那么你就曾用过一个（或多个）GPU。因此，GPU的领头羊在这一年里都非常忙碌。
随着加密狂潮的冷却和随后的股价暴跌，Nvidia发布了基于图灵架构的全新一代消费级卡。新卡仅使用了2017年发布的基于Volta芯片的专业卡，且包含了被称为Tensor Cores的新的高速矩阵乘法硬件。矩阵乘法是DNN运行方式的核心，因此加快这些运算将大大提高新GPU上神经网络训练的速度。
对于那些对“小”和“慢”的游戏GPU不满意的人来说，Nvidia更新了他们的“超级计算平台”。 DGX-2具有多达16块Tesla V，用于FP16操作的480 TFLOP（480万亿次浮点运算），真可谓是一款“怪物”盒子。而其价格也更新了，高达40万美元。
此外，自动硬件也得到了更新。Jetson AGX Xavier是Nvidia希望能为下一代自动驾驶汽车提供动力的一个模块。八核CPU、视觉加速器以及深度学习加速器，这些都是日益增长的自动驾驶行业所需的。
在一个有趣的开发项目中，Nvidia为他们的游戏卡推出了基于DNN的一种功能：深度学习超级取样（Deep Learning Super Sampling）。其想法是去替换抗锯齿，目前主要通过先渲染分辨率高于所需（例如4倍）的图片然后再将其缩放到本机监视器分辨率来完成。
现在，Nvidia允许开发人员在发布游戏之前以极高的质量去训练图像转换模型。然后，使用预先训练的模型将游戏发送给最终用户。在游戏过程中，图形通过该模型来运作以提高图像质量，而不会产生旧式抗锯齿的成本。
**Intel英特尔**
英特尔在2018年绝对不是人工智能硬件领域的开拓者，但似乎他们希望改变这一点。
令人惊讶的是，英特尔的大多数动作都发生在软件领域。英特尔正在努力使其现有和即将推出的硬件更加适合开发人员。考虑到这一点，他们发布了一对（既令人惊讶又有竞争力的）工具包：OpenVINO和nGraph。
他们更新了自己的神经计算棒：一个小型USB设备，可以加速任何带USB端口的DNN，甚至是Raspberry Pi。
有关英特尔独立GPU的传闻变得越来越错综复杂。虽然这一传闻持续流传，但新设备对DNN训练的适用性仍有待观察。绝对适用于深度学习的是传闻中的专业深度学习卡，它们的代号为Spring Hill和Spring Crest。而后者基于初创公司Nervana（英特尔几年前已将其收购）的技术。
**寻常（和不常见）的定制硬件**
谷歌推出了他们的第三代TPU：基于ASIC的DNN专用加速器，具有惊人的128Gb HMB内存。256个这样的设备组装成一个具有超过每秒100千兆次性能的集合体。谷歌今年不再仅凭这些设备来挑逗世界的其他玩家了，而是通过Google Cloud向公众提供TPU。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/XsnGnibCUOT6FkRm9WpQNXdegVJmLfxHAnmJ1UqGlvNVZRiccSHvaTq4uVf644DpPxByibKbgzVkFfC39uOAibryXg/640?wx_fmt=jpeg)
在类似的、但主要针对推理应用程序的项目中，亚马逊已经部署了AWS Inferentia：一种更便宜、更有效的在生产中运行模型的方式。
谷歌还宣布了Edge TPU项目：这个芯片很小：10个芯片加起来才有一美分硬币的大小。与此同时，它能做到在实时视频上运行DNN，并且几乎不消耗任何能量，这就足够了。
一个有趣的潜在新玩家是Graphcore。这家英国公司已经筹集了3.1亿美元，并在2018年推出了他们的第一款产品GC2芯片。根据基准测试，GC2在进行推理时碾压了顶级Nvidia服务器GPU卡，同时消耗的功率显着降低。
**荣誉奖：AWS Deep Racer**
亚马逊推出了一款小型自动驾驶汽车DeepRacer，以及一个赛车联盟。这完全出人意料，但也有点像他们之前推出DeepLens时的情况。这款400美元的汽车配备了Atom处理器，400万像素摄像头，wifi，几个USB端口，以及可运行数小时的充足电量。
自动驾驶模型可以使用完全在云端的3D模拟环境进行训练，然后直接部署到这款车上。如果你一直梦想着建造自己的自动驾驶汽车，那么亚马逊的这款车就能让你如愿，而不必再去自己创立受到VC支持的公司了。
**接下来还有什么？重点会转向决策智能**
既然算法、基础设施和硬件等让AI变得有用的因素都比以往任何时候要更好，企业于是意识到开始应用AI的最大绊脚石在于其实际性层面：你如何将AI从想法阶段落实到有效、安全又可靠的生产系统中？
应用AI或应用机器学习（ML），也称为决策智能，是为现实世界问题创建AI解决方案的科学。虽然过去我们把重点放在算法背后的科学上，但未来我们应该对该领域的端到端应用给予更加平等的关注。
**人工智能在促进就业方面功大于过**
 “人工智能会拿走我们所有的工作”是媒体一直反复宣扬的主题，也是蓝领和白领共同的恐惧。而且从表面上看，这似乎是一个合理的预测。但到目前为止，情况恰恰相反。例如，很多人都通过创建标签数据集的工作拿到了薪酬。
像LevelApp这样的应用程序可以让难民只需用手机标记自己的数据就可以赚到钱。Harmon则更进一步：他们甚至为难民营中的移民提供设备，以便这些人可以做出贡献并以此谋生。
除了数据标签之外，整个行业都是通过新的AI技术创建的。我们能够做到几年前无法想象的事情，比如自动驾驶汽车或新药研发。
**更多与ML相关的计算将在边缘领域进行**
Pipeline的后期阶段通常通过降采样或其他方式降低信号的保真度。另一方面，随着AI模型变得越来越复杂，它们在数据更多的情况下表现得更好。将AI组件移近数据、靠近边缘，是否会有意义吗？
举一个简单的例子：想象一个高分辨率的摄像机，可以每秒30千兆次的速度生成高质量的视频。处理该视频的计算机视觉模型在服务器上运行。摄像机将视频流式传输到服务器，但上行带宽有限，因此视频会缩小并被高度压缩。为何不将视觉模型移动到相机并使用原始视频流呢？
与此同时，多个障碍总是存在，它们主要是：边缘设备上可用的计算能力的数量和管理的复杂性（例如将更新的模型推向边缘）。专用硬件（如Google的Edge TPU、Apple的神经引擎等）、更高效的模型和优化软件的出现，让计算的局限性逐渐消失。通过改进ML框架和工具，管理复杂性问题不断得到解决。
**整合AI基础架构空间**
前几年人工智能基础设施相关活动层出不穷：盛大的公告、巨额的多轮融资和厚重的承诺。2018年，这个领域似乎降温了。虽然仍然有很多新的进步，但大部分贡献都是由现有大型玩家做出的。
一个可能的解释也许是我们对AI系统的理想基础设施的理解还不够成熟。由于问题很复杂，需要长期、持久、专注而且财力雄厚的努力，才能产生可行的解决方案——这是初创公司和小公司所不擅长的。如果一家初创公司“解决”了AI的问题，那绝对会让人惊奇不已。
另一方面，ML基础设施工程师却很少见。对于大公司来说，一个仅有几名员工、挣扎求生的创业公司显然是很有价值的并购目标。这个行业中至少有几个玩家是为了胜利在不断奋斗的，它们同时建立了内部和外部工具。例如，对于AWS和Google Cloud而言，AI基础设施服务是一个主要卖点。
综上可以预测，未来在这个领域会出现一个整合多个玩家的垄断者。
**更多定制硬件**
至少对于CPU而言，摩尔定律已经失效了，并且这一事实已经存在很多年了。GPU很快就会遭受类似的命运。虽然我们的模型变得越来越高效，但为了解决一些更高级的问题，我们需要用到更多的计算能力。这可以通过分布式训练来解决，但它自身也有局限。
此外，如果你想在资源受限的设备上运行一些较大的模型，分布式训练会变得毫无用处。进入自定义AI加速器。根据你想要的或可以实现的自定义方式，可以节省一个数量级的功耗、成本或潜在消耗。
在某种程度上，即使是Nvidia的Tensor Cores也已经投身于这一趋势。如果没有通用硬件的话，我们会看到更多的案例。
**减少对训练数据的依赖**
标记数据通常很昂贵，或者不可用，也可能二者兼有。这一规则几乎没有例外。开放的高质量数据集，如MNIST、ImageNet、COCO、Netflix奖和IMDB评论，都是令人难以置信的创新源泉。但是许多问题并没有可供使用的相应数据集。研究人员不可能自己去建立数据集，而可提供赞助或发布数据集的大公司却并不着急：他们正在构建庞大的数据集，但不让外人靠近。
那么，一个小型独立实体，如创业公司或大学研究小组，如何为那些困难的问题提供有趣的解决方案呢？构建对监督信号依赖性越来越小，但对未标记和非结构化数据（廉价传感器的互联和增多使得这类数据变得很丰富）依赖性越来越大的系统就可以实现这一点。这在一定程度上解释了人们对GAN、转移和强化学习的兴趣激增的原因：所有这些技术都需要较少（或根本不需要）对训练数据集的投资。
所以这一切仅仅是个泡沫？
这一行业已进入热门人工智能“盛夏”的第七年。这段时间内，大量的研究项目、学术资助、风险投资、媒体关注和代码行都涌入了这个领域。
但人们有理由指出，人工智能所做出的大部分承诺仍然还未兑现：他们最近优步打车的行程依然是人类驾驶员在开车；目前依然没有出现早上能做煎蛋的实用机器人。我甚至不得不自己绑鞋带，真是可悲至极！
然而，无数研究生和软件工程师的努力并非徒劳。似乎每家大公司都已经十分依赖人工智能，或者在未来实施此类计划。AI的艺术大行其道。自动驾驶汽车虽然尚未出现，但它们很快就会诞生了。
2018年，美国在人工智能领域发展迅速，中国也不遑多让。这个趋势从近期百度和BOSS直聘联合发布的《2018年中国人工智能ABC人才发展报告》中就可窥一斑。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
