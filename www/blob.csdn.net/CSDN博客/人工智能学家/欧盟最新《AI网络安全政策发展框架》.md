# 欧盟最新《AI网络安全政策发展框架》 - 人工智能学家 - CSDN博客
2019年04月16日 23:15:30[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：39
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBWiaVxib15fYlTD7Vln7dq5SoL5NCdVoIW7BiaXFibTibWWE7hCz51KoRkzo3l7xspsaKvdpX5L0HAJicfg/640?wx_fmt=png)
来源：学术plus
**报告主要内容编译如下**
**下载全文报告请参见文末链接**
**一、 概要**
AI进入主流商用，数据收集是关键
在过去几年，人工智能（AI）迅速超越研究和学术范畴，进入主流商用，创新的自主代理利用人工智能并改变信息的访问和使用方式。自主代理多种多样，从智能手机中的数字助理到支持供应链的自动机器人。因自主程度和操作环境的差异，安全性和隐私考虑可能会有所不同。自主系统的一个关键方面是数据收集，主要用于以定性和及时的方式支持要求苛刻的功能。然而，由于处理的数据丰富，可能还涉及个人数据，除了依赖第三方提供商之外，还需要考虑安全和隐私等诸多因素。
欧盟倡议：安全与隐私
本报告强调了一些需要考虑的因素，例如未经授权的自主系统，劫持和滥用透明度和问责制，普遍性，存储和处理的不透明性，并给出了一系列影响未来欧盟政策倡议的建议，包括：
●欧盟委员会、欧盟相关机构以及公私营部门利益相关方应进一步促进和支持采用安全和隐私设计原则，作为自主代理和系统的启动、设计和实施的先决条件。
●欧盟委员会、欧盟相关机构、公私部门利益相关方应促进识别和交流最佳实践的合作方法，逐步提出一系列基线安全要求，然后转变为可广泛接受的技术规范和标准。
●欧盟委员会、欧盟相关机构、公私部门利益攸关方应通过建立适当的道德准则，进一步保障和支持关于促进和保护人权的现有举措。
**二、 安全和隐私**
(一) 安全问题
### 1. 劫持和误用
自主无人机、无人驾驶汽车、机器人和其他设备都是通过软件系统来控制的。软件开发过程中使用的方法、以及软件测试的范围和广度都会影响漏洞数量和严重程度。开发者应该证明设计过程中采用了可管理安全（managed security）方法，包括记录安全软件开发、质量管理和信息安全管理过程。
### 2. 干扰
自主系统非常依赖传感器来感知环境，因此可能会遭遇来自传感器的安全风险。安全研究人员在2016年就证明了自动驾驶汽车使用的传感器易受到非接触式攻击的影响。供应商应该确保交付的产品的设计和预配置是基于良好的安全实践，并严格遵守最小权限等安全设计和开发原则。
### 3. 透明性，可审计性的局限
自主代理的行为并不全都在软件代码中有详细的规定，而是根据软件、训练和感知的环境的不同，其行为具有一定不确定性。在许多高级自主代理中，训练和操作阶段是没有区别的，因为训练过程是持续贯穿代理的整个生命周期的。因此，训练的过程并不是完全由厂商控制的。那么厂商就应当提供关于代理设计的综合性和可理解的文档，描述自主代理的架构、功能、使用的协议、软件组件或硬件的实现，与其他组件和内外部服务的交互和接口，以确保代理是以一种最安全的方式实现和应用的。
### 4. 坚持安全原则
与信息系统设计、开发和应用的基本安全规则类似，应当识别、定义和应用自主代理全生命周期应当坚持的基本安全原则。因此，自主代理的组件、使用和提供的服务应当是安全的：   
● 安全设计。代理或服务的设计和应用应该是安全的，以确保可用性、机密性、完整性和可审计性等关键安全特性；
● 默认安全。代理或服务在初始实现时就应该提供能支持这些安全特性的能力；
● 全生命周期安全。安全应该贯穿初始应用、维护到不再使用的全生命周期中；
● 可验证安全。上述安全特征和原则都应当是可验证的。
(二) 隐私问题
1. 普适性与最小化原则
自主代理的学习能力是基于机器学习算法的，而机器学习算法本身就需要大量的数据集作为输入。也就是说代理会去不断地收集尽可能多的数据，以自动驾驶汽车为例，自动驾驶汽车装备的传感器会收集半径300m范围的数据信息，通过还装配了摄像头和麦克风、雷达和激光传感器，可以创建环境中物体的高分辨率3D表示。
从目前来看，自主系统还不具备区分有用数据和无用数据的能力。自主系统的能力高度依赖环境感知的能力，而环境是复杂和动态的，因此很难区分哪些信息是有用的，哪些信息是无用的。
### 2. 数据保护：删除数据仍能利用
自主代理处理的实时数据可以在使用后不影响性能的情况下删除。出于不同的原因这些数据一般都会长时间保存，包括事故和意外的调查取证。以自动驾驶汽车为例，训练数据也可能会用于事故和意外调查取证，因此也要保存。
即使数据被删除了，也会留下可以被利用的痕迹。研究人员在2017年就证明了在不访问机器学习模型内部结构的情况下确定特定记录是否是机器学习算法训练集的一部分的可能性。同时研究人员证明了机器学习易受到推断攻击。因此，对机器学习来说，删除训练集数据并不是隐私保护的有效手段。而且工程师、开发者、研究人员、调查人员和其他利益相关方都可能会访问到数据集，因此数据机密性保护也是一个复杂的问题。
### 3. 数据聚集和再利用
数据的聚集和再利用已经非常普遍。比如，自动驾驶汽车和无人机都连接着所属公司的控制中心，收集的数据会发送到控制中心做进一步分析。同样地，软件代理也会与厂商共享数据，这样厂商才能提供更好的服务给用户。开发者和服务提供商将从不同用户处收集的数据与其他源的数据融合在一起，就可以创建详细的用户画像、预测用户行为、猜测用户的需求。
从活动A收集的数据可能会被进一步利用和分析，用作活动B，这就叫做再利用。比如自主代理训练所收集的数据也可能会用于市场营销，事故调查所收集的数据可能会被用于用户画像等等。
### 4. 黑盒：数据处理的不透明性
机器学习是以一种黑盒的方式处理和展现给用户的。机器学习算法并不对结果进行解释，也无法确定某个特定的数据实例是否会影响最终的决策。对基于机器学习算法模块的自主代理来说，无法证明个人数据处理的合法性、公平性和透明性。因此需要监管和执行机构参与来分配和确定相关的责任。
**三、 网络安全政策发展框架**
本文为欧盟成员国提供自主代理和相关应用领域在安全和隐私相关的政策发展提供一个框架。基于对人工智能技术的分析，研究人员提出网络安全领域的政策发展框架应该坚持的2个原则：
- 
包容性原则（Inclusiveness）：应当反映和考虑更广泛的利益相关方的兴趣和优先事项；
- 
开放性原则（openness）：应当支持未来数字革新技术在不同领域的应用。
具体来说，该框架应该：促进技术标准和规范的开发和采用；鼓励最佳安全实践和经验；促进利益相关方的协同和合作关系。
**四、 总结与建议**
## (一) 应用安全和隐私设计原则
自主系统的关键就是能够处理大量数据，但自主系统必须以一种可以确保安全关键特性可用性、机密性、完整性和可审计性的设计方法。通过在从安装开始的全生命周期拥有保证其安全特性的能力。
欧盟委员会、欧盟相关机构以及公私营部门利益相关方应进一步促进和支持采用安全和隐私设计原则，作为自主代理和系统的启动、设计和实施的先决条件。
## (二) 开发基线安全需求
人工智能和自主代理的快速发展催生了系统和产品设计、开发等过程对应的方法和指南的需求。开发者也可以从解决安全和隐私挑战的指南中获益，这种指南可以通过最佳实践、基线安全需求标准的开发来得出。
欧盟委员会、欧盟相关机构部门利益相关方应促进识别和交流最佳实践的合作方法，逐步提出一系列基线安全要求，然后转变为可广泛接受的技术规范和标准。
## (三) 解决伦理问题
自主代理和系统常常需要做出一些复杂的决策，因此引入了与人权、尊严和非歧视等相关的伦理问题。解决这一困境需要一种跨学科和协调的方法。
欧盟委员会、欧盟相关机构部门利益攸关方应通过建立适当的道德准则，进一步保障和支持关于促进和保护人权的现有举措。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
