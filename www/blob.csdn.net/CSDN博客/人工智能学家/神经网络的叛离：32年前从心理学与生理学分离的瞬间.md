# 神经网络的叛离：32年前从心理学与生理学分离的瞬间 - 人工智能学家 - CSDN博客
2018年10月30日 23:11:24[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：120
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWIbMMyNMGrsjdQIlMo3AOoicKdOBVbDnibQJv1ibmicWErBVDicxibGaLuibCGvicYIfPBvUZXI7GgibSatKA/640?wx_fmt=jpeg)
来源：大数据文摘
编译：Andy
反向传播算法隶属于深度学习，它在解决模型优化问题的方面有着重要的地位。
这一算法由被誉为深度学习之父的Geoffrey Hinton提出。1986年，他发表了名为Learning representations by back-propagating errors" (Rumelhart, Hinton & Williams, Nature, 1986) 的论文，到目前为止已有将近16000次的引用，可以说是一篇位于神经网络研究金字塔顶端的论文。
但是，学界对这篇代表反向传播算法的论文却态度褒贬不一。反对的声音称，这是一篇有点无聊的论文，。
本文作者，谷歌Business Insight团队的数据科学家，Takashi J OZAKI认为，1986年的那篇论文的意义，并不单单只是提出了反向传播，更是“神经网络从心理学和生理学分离，转向机器学习领域的一个重大转折。” 
他将从这篇论文谈起，详细的介绍他对反向传播算法的看法。
下文是作者以第一人称论述反向传播算法的意义，Enjoy！
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxWIbMMyNMGrsjdQIlMo3AOoiap3oWibwW3fQzHiakB1I86PZRKLJbGSs1vxsxdABDoukuvY2Dvos28cg/640?wx_fmt=png)
反向传播简单结构
前不久，@tmaehara 桑在 Twitter 上这样说。
> 
完全搞不懂 back propagation 的原论文有什么创新点… - 原论文感觉就不过用链式法则先微分然后加上梯度法而已...
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWIbMMyNMGrsjdQIlMo3AOovc2CHD85RT7JQEicsTDOv1GjnAVCoKMhicyaiaPkGPCic9M1ODq5TiayltA/640?wx_fmt=jpeg)
原论文：
http://elderlab.yorku.ca/~elder/teaching/cosc6390psyc6225/readings/hinton%201986.pdf 
Learning representations by back-propagating errors" (Rumelhart, Hinton & Williams, Nature, 1986) 这篇论文，在 Google Scholar Citation到目前为止已有将近 16000 次的引用，可以说是神经网络研究金字塔顶端的论文。
它在过去无法进行非线性分割的Rosenblatt的感知机（perceptron）中，导入了看似有些奇怪的back propagation（反向传播） 方法，于是让其变得可进行非线性分割。可以说是一次范式转移（根本性突破）的研究。
现在被称为深度学习的神经网络已完全被反向传播给占据了。因此或许可以说，如果没有这篇论文也就不会有现在的深度学习热潮了。
换句话说，现在的人工智能热潮，正是从30多年前这篇小论文开始的吧。
但是，如果你实际去读读这篇论文就会发现，正如 @tmaehara 桑指出来的一样，这边论文本身非常无聊。就如很多神经网络教科书里常有的一样，轻描淡写般地，用链式法则先微分然后用梯度法优化，就完了。
读到这里估计会非常疑惑，**“为什么这篇无聊的论文会占据金字塔的顶端？”**
**“Who Invented Backpropagation? (谁发明了反向传播？)” **
Schmidhuber 在批评文中写到，早在上世纪50年代到60年代，在其他领域就有类似想法被提出，甚至70年代还有FORTRAN语言的算法实现。而对于这些研究，Rumelhart和Hinton的团队却连引用都没有，没有给予最基本的敬意。这个指责在一定程度上确实有说服力。
但是，对于从最早的感知机开始到反向传播，再到深度学习的轨迹，我们必须要意识到的是，那是一个**“和我们现在所考虑的神经网络完全不同的世界”。**
从这个角度看的话，就会发现1986年的那篇论文的意义，并不单单只是提出了反向传播，更是**“神经网络从心理学和生理学分离，转向机器学习领域的一个重大转折。”** 下面，我就基于自己的读到的一些知识，简单解释一下。
- 
过去的神经网络属于心理学与生理学
- 
1986 年 Nature 的论文意味着神经网络从心理学和生理学的分离
- 
于是向着**“模式识别” “机器学习”**转移，后进一步成为深度学习
- 
现在神经网络还继续沿用**“Neural（神经）”**的理由
**过去的神经网络属于心理学与生理学**
关于联结主义，很多年轻人或许并不太知道，过去一提神经网络就会想到心理学家和生理学家的研究，以及**“联结主义”**的基本理论。读读维基百科上联结主义的词条也会明白，本来神经网络就是以“模拟人脑的信号处理”为大前提而建立起来的体系。
它的方法论，有基于认知科学特有的**“人体中（某种程度上的）黑箱化”**，还有用符合心理学的模型来试图解释人类大脑功能的性质。简单来说，这是支撑神经网络的心理学部分。
另一方面，还有以研究与**“人脑”**连接相关的生理方面为基础的，被称为**“计算神经科学”**的东西。这个读读维基百科也知道，其实就是用符合生理学的模型来试图解释人类大脑功能。如果说联结主义还有神经网络是一种抽象概念性模型的话，那么可以说计算神经科学是论证人类大脑在多大程度上与神经网络相似（或者不相似）。
本来神经计算学就不仅仅只考虑像神经网络这样大脑的神经模型，它还研究诸如神经元放电模式代表什么信息这样的**“神经活动解释”**类的课题，有必要特别指出计算神经科学其实是个很大的领域。简单来说，这是支撑神经网络的生理学部分。
其实不管是上面哪种情况，都在某种程度上假设了**“神经网络=人脑的模仿”**。在我看来，至少到 2000 年左右，都还有人认为神经网络和人脑的关系是密不可分的。然而神经网络给我的印象是，其源自生理学或心理学，而后再从工程学和计算机科学的视角进行了二次解读。
于是乎，比如说神经网络原型的感知机，最初就被认为是基于形式神经元的人脑（特别是小脑）学习模型。而且它的构造也和实际的小脑的生理学构造确实挺像，现在都还作为联结主义和计算神经科学的重要范例之一，被记载在比如维基百科的许多文献上。
顺便一提，现在作为机器学习年度峰会而变得有名的顶级会议 NIPS ，也如它全名 Conference on Neural Information Processing Systems （神经信息处理系统大会）一般，过去主要是计算神经科学的会议。
比如说看看1990年该会的记录，绝对是现在完全想象不到的，基本大多都是关于真正人脑的研究。其实我在理研 BSI（理化学研究所—脑科学综合研究中心）还是菜鸟研究员的时候，实验室就是以**“神经元放电活动的同步**”为其中一个研究主题，所以我也就对这方面也有些了解，于是看 NIPS 当时的记录发现，刚才说的神经元放电活动的建模论文也都有些。所以可见，过去的神经网络就是如此带有**“浓浓心理学（认知科学）和生理学味”**的东西。
于是不可避免的，过去在研究神经网络时，总会想**“不管是什么样的新方法，一定也要遵循心理学或生理学”**，而这无形中形成了一种制约。
当然，这只是我个人的一些推测。但实际上，我还是神经科学领域菜鸟研究员的时候也有这种心理，在读各种关于人认知机能的计算理论模型论文时，会边读边脑补上** “嗯嗯，这篇论文有生理学的依据，所以很可靠。” **就连视觉神经科学研究的大佬 Zeki 也在其写的经典大作《A Vsion of the Brain》中严厉批评，**“David Marr 的视觉信息处理模型没有什么生理学依据，所以没啥用。” **由此也可见当时对生理学根据有无的重视。
此外，如果读读我年轻时这方面的经典著作《脳の計算理論 （脑的计算理论）》，就能深刻体会到当时的氛围。不过这本书已经绝版了，很难入手。
**1986 年 Nature 的论文意味着神经网络从心理学和生理学的分离**
在这样一个神经网络和心理学还有生理学不可分的时候，正是开头提到的那篇1986年的Nature论文横空出现，将神经网络从中拉了出来。
如果对神经科学有些了解的人，可能知道就连神经科学中，关于对等反向传播的“**大脑皮质内从高阶区域向低阶区域的反馈信号”**意义的讨论本身，其实也是2000年前后没那么久之前的事情了。从2000年再往前推14年的话，说不定议论反向传播人们只会把你当成怪人。
于是这样，在**“这东西不管怎么看也找不到什么生理学的依据”**的情况下提出反向传播，对生理学根据基本无视，提出具有范式转移的反向传播机制，还有配套的梯度法进行最优化，并向大家展示了其强大的实用性。这，才是我个人认为1986年 Nature 论文最大的意义。
在最后结论部分，Nature 论文给出了这样的总结：
> 
这样的学习方法，似乎并不是大脑学习的一种合理模型。然而，将这个方法应用在各种各样的任务上后显示，通过权重空间的梯度下降，可以构建出很有趣的内部表征。这表明，很值得在神经网络中找出更有生理可行性的方法来进行梯度下降。
这里直接坦言“虽然反向传播没有生理学依据，但很有用”。从另一个角度来看的话，说不定这就是神经网络从不得不有心理学或生理学证据的枷锁中，挣脱的第一步呢。
而且提出这样重要观点的，还不是像应用数学或计算机工程学这样的“**外人”，**而是联结主义中支柱般存在的 Rumelhart 和 Hinton 二人，这样看来无疑意义更加伟大。并且，虽然Hinton老师现在被认为是深度学习的三始祖之一，还是神经网络和机器学习的大牛，但看看维基百科的话就会发现，他本来其实是认知心理学家（Rumelhart 也一样）。
当然，并不是说这之后神经网络就完全和心理学生理学分离了，至少看文献的话大概在2000年之前，仍被纳入联结主义中去。但是，根据已明确表示出来的，神经网络不一定要依赖心理学和生理学的趋势，未来将会更彻底从心理学生理学还有联结主义中分离出来吧。
**向着“模式识别” “机器学习”转移，之后进一步成为深度学习**
我本科虽然是计算机科学的学生，但在1999年时却记得**“机器学习”**这个词并不常见。用的更多的是**“模式识别”**这个词。作为参考，看了看维基百科“Pattern recognition”的词条，发现引用里面果然大部分都是在2000年前后，和我当时的印象一致。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxWIbMMyNMGrsjdQIlMo3AOoBUfeFbLnvCib9ibCzKgnZJgOfpRGlbImp1MtIH1HcKgwsRiaMq6b0AAibA/640?wx_fmt=png)
顺便一提，这是我本科时上课用到的经典教科书（《好懂的模式识别》）。现在看来似乎这本书的续篇更加有名，但这本书还是将模式识别基本的东西都提到了，是本挺好的入门书。
这本书当然也提到了神经网络和反向传播。此外，过去神经科学等教科书上提到神经网络时，一定会提到从小脑与感知机的异同到神经网络的各种历史，而这本书却没对这方面花太大篇幅。
此外，就如其他各种有名教科书一样，反而是以如何基于链式法则导出反向传播，还有如何实现它为主来说明。顺便一提，我最初开始数据科学家的工作时，就是从书架上抽出这本书，一边看着梯度法的公式，一边从零实现反向传播算法。
正如2000年左右，神经网络已被看做脱离了心理学和生理学范畴的“**模式识别”**的一种。之后和**“机器学习”**的成功一起被看做是统计学习模型的一种，再然后是2006年Hinton老师的Science论文，还有就是通过2012年的 ImageNet Challenge，神经网络和它的终极形态深度学习，慢慢以机器学习王者的姿态稳居宝座之上。
现在的神经网络研究，有着各种奇思妙想的网络模型层出不穷**“我想出来的网络才是最好的”**乱况，还有不断引入最先端的最优化解法，和利用过去完全想象不到的超大规模并行处理运行的进步，此外也有单作为数学理论分析对象和作为发现新物体性质的应用研究手段等等。神经网络简直慢慢成了将工程学和计算机科学各流派精粹集中在一起的天下第一武道大会。
顺带一提，维基百科日文版的**“神经网络”**词条中，也有类似的描述。
> 
Neural Network（神经网络，缩写：NN）是一种数学模型，旨在通过计算机模拟表达脑功能中的一些特征。虽然研究起源于对生物体大脑的建模，但由于对神经科学看法的改变，逐渐与大脑模型的差异变得显着。为了区分于生物学和神经科学上的神经网络，所以也称为人工神经网络。
人工神经网络（ANN）或联结主义系统，是大致由构成动物大脑的生物神经网络启发的计算系统，人工神经网络最初的目的是想像人脑一样解决问题。但随着时间的推移，注意力重点开始转移到了执行特定任务上去，导致了从生物学的偏离。
这里明确表达了类似**“神经网络已经脱离了人脑本身”**的看法。距离1986年的Nature论文过去32年，神经网络现在已慢慢远离联结主义的起源，开始成为了大家公认的机器学习王者。
虽然开头那句话已说了很多遍了，但是还是要重复一遍：“产生了让神经网络从心理学和生理学分离，并向着机器学习方向转移的契机”，才是1986年那篇 Nature 论文的最大意义。这便是我翻阅神经网络历史后得出的感想。
**现在神经网络还继续沿用“神经”的理由**
顺带一提，过去作为讨论神经信息处理顶级会议的 NIPS，维基百科英文版最近的描述也已发生巨大的转变。
该会议在2016年有5,000名注册参与者，2017年有8,000名，使其成为人工智能领域最大的会议。除了机器学习和神经科学，NIPS 涉及的其他领域包括认知科学，心理学，计算机视觉，统计语言学和信息论。
虽然NIPS首字母缩写中的“**神经**”已成为一种历史残留物，但自2012年后深度学习的复兴，加上高速计算机和大数据的推动，在语音识别，图像物体识别，语言翻译等领域取得惊人的成果。
上面毫不客气地直接写到“NIPS 首字母缩写中的“神经”已成了一种历史残留物”。其实2013年去太浩湖参加那年的 NIPS 时，虽然其中还有部分关于神经活动信号数据分析的研究，好像“神经”尚还健在，但更多还是与深度学习相关的研究。
此后就变得更加多了。
在围棋中的世界冠军级表现，是基于灵感来自视觉皮层（ConvNet）的层次结构的神经网络架构，和受基础神经节启发（时间差分学习）的强化学习。
还有“尽管如此，受大脑启发的神经网络（和其综合运用）还是有很大的应用可能性”，我同意这一点。 而且我还认为正因为这点，即使在已不把模仿人脑当做目标的现在，神经网络还继续沿用**“神经”**这个词。
**最后**
关于2000年以前的情况，大都是从我碰巧同时学过的神经科学和计算机科学的经验，还有各种文献中参考而得出来的。当然我自身并没有在那个时代去体验这些。如果在文章中关于事件关系还有解释中发现错误的话，非常高兴你能指出来m(_ _)m。
**引用**
* 1：我认为这是当然后续ImageNet挑战的必要的科学论文在2006年和2012年韩丁老师
* 2：但是出汗这么多，有些不再手头放手是在书本
* 3：但是，这篇文章略有恶意描述，所以要小心处理
* 4：小脑感知理论也有过早死亡的天才戴维·马尔在35岁提出的事实，似乎经常谈到今天著名的成功故事
* 5：例如https://papers.nips.cc/paper/421-analog-computation-at-a-critical-point-a-novel-function-for-neuronal-oscillations.pdf如该侧
* 6：肯定有2和1/2维草图模型
* 7：在已知为具有分层结构，诸如各种类型的感官字段的位点的特定功能的神经解剖学
* 8：（神经科学的趋势，2000年）Lamme和Roelfsema是我反复读毕业论文的学生当时http://www.kylemathewson.com/wp-content/uploads/2010/03/LammeRoelfsema-2000-TiN-Reentrant- Vision.pdf
* 9：例如，在“神经元的活动，并在皮层区域的皮层之间的层次之间的反馈投影的关系”的一个评论性论文将保持，直到2000年左右，多少回https://www.ncbi.nlm.nih.gov/考研/？长期=审查+％5Bptyp％5D +反馈+％5Bti％5D +神经元+皮质
* 10：因为这是某次研究会议的主要内容
* 11：没有比标题更容易理解的印象。
* 12：最后的程度，英国石油公司在情节提出感知批评和鲁梅哈特由明斯基辩驳的历史故事是休息时间出来
* 13：对于首次提出在SVM是1963年，因为扩展是由是92年到非线性SVM Vapnik等例子老板，居然之嫌“机器学习”作为工程领域很久以前就是我正在发芽
相关报道：https://tjo.hatenablog.com/entry/2018/10/23/080000
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
