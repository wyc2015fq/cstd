# 人工智能的算法黑箱与数据正义 - 人工智能学家 - CSDN博客
2018年03月12日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：210
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHVXxG6cjyHBbvnoKfwPVUx2Q1V1mXic4ldGqXMfkicCMrJAS5glrXS5icQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
来源：FT中文网
一个月前，《终极算法》作者、人工智能著名学者、华盛顿大学教授 Pedro Domingos 在社交网络中写道：“自 5 月 25 日起，欧盟将会要求所有算法解释其输出原理，这意味着深度学习成为非法的方式。”一石激起千层浪。人们不禁要问：欧盟为何出台这个法规？以深度学习为核心的人工智能真的会遭遇重大挫折？中国应当借鉴并仿效吗？
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHspmV2PRcnwWcS7VZCafvn66COnCWbFJg4Yb4cplBAP4OlicfpibLtL1w/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
**利用人工智能的自动化决定**
尽管真正拥有知觉和自我意识的“强人工智能”仍属幻想，但专注于特定功能的“弱人工智能”早如雨后春笋般涌现。在万物互联的背景下，以云计算为用，以个人数据为体，以机器学习为魂的智能应用已经“润物细无声”。
从今日头条的个性化推送到蚂蚁金服的芝麻信用评分，从京东的“奶爸当家指数”到某旅游网站用大数据“杀熟”，个人信息自动化分析深嵌入到我们日常生活之中。在法律上，我们称之为“基于个人信息的自动化决定”。简单来说，就是通过自动化的数据处理，评估、分析及预测个人的工作表现、经济状况、位置、健康状况、个人偏好、可信赖度或者行为表现，进而利用这种“数据画像”（profiling），在不同的业务场景中作出有关数据主体的各项决定。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHoq2APPIVkK3vPbbuklLnryesZ19LueYDXWZInqQb1ZFECoeFPIvxXw/640?wx_fmt=png)
人工智能的自动化决定一方面可以给我们带来便利，比如智能投顾或智能医疗，但另一方面，它绝非完美无缺，它可能出错，甚至还可能存在“恶意”。美国马萨诸塞州的居民John Gass便深受其害。联邦调查局的反恐识别系统将他误认为是另一位司机，并吊销了他的驾驶执照，于是，他不得不费时费力，让当局相信他不是那名司机。
其实，John Cass已经非常幸运。在美国，每周超过1000人被机场使用的算法错误地标记为恐怖分子。一名美国航空公司的飞行员在一年中被拘留了80次，因为他的名字与爱尔兰共和军领导人的名字相似。这还不算是最糟糕的。人工智能的算法依赖于大数据，而大数据并非中立。
它们从真实社会中抽取，必然带有社会固有的不平等、排斥性和歧视的痕迹。例如，为了在Twitter上与千禧一代进行对话，微软开发了Tay聊天机器人，它旨在学习如何通过复制网民的语音来模仿他人。可仅仅在试用24小时后，它就被引入歧途，成为支持种族灭绝的反女权主义纳粹分子，以至于发出了“希特勒无罪”的消息。更有甚者，美国法院用以评估犯罪风险的算法COMPAS，亦被证明对黑人造成了系统性歧视。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHH2aicetPtkgKvdkk7bYTYHnJUS1yFBIzrvHVVFrNA7nhA9rQu6Rlsdg/640?wx_fmt=png)
无论是程序错误，还是算法歧视，在人工智能的前沿领域——深度学习中，都变得难以识别。华盛顿特区的Sarah Wysocki是一位被普遍认可的老师，但当2009年政府用一个自动化决定程序来评价教师表现时，她和其他205人因得分不佳被解雇。据称，该决定以少数学生的成绩为依据，可学校始终无法解释为何优秀教师会落得如此下场。华盛顿学校的难题有着深层次原因。
与传统机器学习不同，深度学习并不遵循数据输入、特征提取、特征选择、逻辑推理、预测的过程，而是由计算机直接从事物原始特征出发，自动学习和生成高级的认知结果。在人工智能输入的数据和其输出的答案之间，存在着我们无法洞悉的“隐层”，它被称为“黑箱”（black box）。
这里的“黑箱”并不只意味着不能观察，还意味着即使计算机试图向我们解释，我们也无法理解。哥伦比亚大学的机器人学家 Hod Lipson把这一困境形象地描述为“这就像是向一条狗解释莎士比亚是谁。”
**《统一数据保护条例》的应对**
正是因为人工智能的自动化决定对个人权利的重大影响，将于2018年5月25日生效的欧盟《统一数据保护条例》（GDRR）在1995年《数据保护指令》（Directive 95/46/EC）的基础上，进一步强化了对自然人数据的保护。
首先，尊重个人的选择权。当自动化决定将对个人产生法律上的后果或类似效果时，除非当事人明确同意，或者对于当事人间合同的达成和履行来说必不可少，否则，个人均有权不受相关决定的限制。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHKgHmwFuibt4ybbl5r2c7cb1LKerqe1lak9yKRbIsEUx0m7icq3o8S4Bg/640?wx_fmt=png)
其次，将个人敏感数据排除在人工智能的自动化决定之外。根据《统一数据保护条例》第9（1）条，“敏感数据”即有关种族、政治倾向、宗教信仰、健康、性生活、性取向的数据，或者可唯一性识别自然人的基因数据、生物数据。
由于这些数据一旦遭到泄露、修改或不当利用，就会对个人造成不良影响，因此，欧盟一律禁止自动化处理，即使当事人同意亦是如是，只有在明确的法律规定时才存在例外。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHZ30DmKf8MQKdia664Nice0Aqwge8MvwoBoczcqDFLQLia2DibUkFKVbl5g/640?wx_fmt=png)
再次，增加数据使用者在个人数据收集时的透明度。根据《统一数据保护条例》第13条（f）和第14条（g），如果个人数据将用于自动化决定，那么至少应当向个人提供相关决定的重要性、对个人预期的影响以及有关运算逻辑的“有用信息”。
比如，在银行收集个人数据时，应当告知其可能使用人工智能对贷款人资质进行审核，而审核的最坏结果（如不批贷）也应一并披露。此外，由于我们都不是技术专家，因此，这里的“有用信息”不但应浅显易懂，为每个人理解，而且要有助于每个人主张自己在《统一数据保护条例》或其他法律下的权利。还是以贷款审核为例，当我们觉得被不公正对待时，银行提供的信息就应当成为法院审理的重要依据。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHwlEoCIWuaBiaUficZEEHRbvFBd8EQWFbw52H3dwO0iaxuiaUBpL4GNNmLA/640?wx_fmt=png)
最后，如果个人对自动化决定不满，则有权主张人工介入，以表达自己的观点并提出质疑。这一规定和上述透明度要求相结合，产生了针对人工智能的所谓“解释权”，而这正是Pedro Domingos的担忧所在。考虑到算法黑箱，深度学习的合法化似乎是个无解的问题。但事实上，这可能是个误解。
一方面，“有用信息”的提供是在收集数据之时，而非作出自动化决定之后，其意味着个人仅仅概括地了解系统的一般原则即可，并不需要彻底把握某项具体决定的逻辑。另一方面，法律所看重的是“可理解”（explainable），而不是“可阐释（interpretable）。换言之，它不关注人工智能内部究竟如何运作，而只关心输入数据和输出结果的关联关系。在加州大学伯克利分校发布的《人工智能的系统挑战：一个伯克利的观点》（A Berkeley View of Systems Challenges for AI）中，这种关联性被称“反事实问题”测试。
在个人被拒绝贷款的例子中，人工智能系统必须能否回答如果诸如“我不是女性，是不是就能批贷？”“如果我不是小企业主，是不是就能批贷”这样的问题。因而数据使用者有义务建构出一套具有交互诊断分析能力的系统，通过检视输入数据和重现执行过程，来化解人们的质疑。这才是“人工介入”的真实含义。
**将数据正义引入中国**
数据是数字经济的关键生产要素，人工智能是数字经济的关键产业支柱。如何在发掘数据的经济价值、发展人工智能的同时，保障个人的权利和自由，依然是数字社会的未解难题。
当前，我国尚无《个人信息保护法》，在不久前出台的《个人信息安全规范》中，第7.10条“约束信息系统自动决策”也只是赋予了个人提出申请的程序性权利，并不涉及实质约束。
无独有偶，中国电子技术标准化研究院发布的《人工智能标准化白皮书》虽然已关注到人工智能的伦理和隐私问题，但着墨不多，因过于原则而难以实施。就此而言，《统一数据保护条例》可成为我国可资借鉴的他山之石。它不仅仅提供了一系列具象的法律规则，更重要的是它在“数据效率”之外，传递出“数据正义”（data justice）的理念。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBUZZ8xYKuc9TUu3wlMskfgHl1n1HKPhlNibXXeDOiccqorW4AIy7UUC6ySchiag42jelhrgT4ZVx996w/640?wx_fmt=png)
尽管作为一个发展中的理念，数据正义的含义远未定型，但“反数据歧视”和“数据透明”必然是题中之意。在数字化生存的今天，不管是“社会人”还是“经济人”，都首先是“数字人”。
现实空间的我们被数据所记载、所表达、所模拟、所处理、所预测，现实空间的歧视也是如此。从求职歧视到消费歧视和司法歧视，数据歧视前所未有地制度化和系统化。基于此，法律首先要做的就是规定更加小心和负责地收集、使用、共享可能导致歧视的任何敏感数据。可这显然不够。从大数据的相关性原理出发，只是将敏感数据简单排除并不能保证它们不被考虑。
例如，若特定区域的人有着大量的低收入群体或少数族裔，那么区域的地理数据就可以代替收入或种族数据，用作歧视工具。所以，要识别和挑战数据应用中的歧视和偏见，“数据透明”就不可或缺。换言之，它要求在数据生产和处理日趋复杂的形势下，增强个人的知情权，从而修复信息的对称性。
关于这一点，凯文·凯利所讲的老婆婆故事是一个绝佳的例子。在故事里，你住在一个小城镇，你的邻居老婆婆知道你的一切动向和行踪，但你可能不会觉得被冒犯，因为你不在家的时候，老婆婆会帮你看家；更重要的是，你了解关于老婆婆的一切。从信息窥视的角度，数字时代的政府和企业就像邻居老婆婆，不过，他们只是部分地做到了第一点，就第二点而言，我们却还有很长的路要走。
作者：许可，法学博士、中国人民大学金融科技与互联网安全研究中心副主任
说明：本文转载自公众号：45Society
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
