# 究竟深度学习在干什么？ - 人工智能学家 - CSDN博客
2017年11月15日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：280
![640?wx_fmt=png&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/f84kJBXzrBVLqqjW2mbXicqVh1QqBibCol58naWegZrKdyzWlPc03SIhPPHqRibU9NMYS0q2VSFl0DibCIAYnXXXvQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
*来源：人机与认知实验室*
*概要： 深度学习取得了巨大的成功，这是无容置疑的。对此，我们不必再多说什么。但是，其理论基础仍然有很大的空白。*
 深度学习取得了巨大的成功，这是无容置疑的。对此，我们不必再多说什么。但是，其理论基础仍然有很大的空白。例如：究竟深度学习在做什么？为什么可以学习很多东西？这些问题极为重要。但是直到现在，仍然众说纷纭，没有定论。
我们对待深度学习的基本思路是从梳理最基本的概念入手，一步一步推进，来逐步搞清楚里面的问题。
先从最简单也是最基本的说起，那就是IPU，即信息处理单元（Information Processing Unit）。其实这个概念真的是信息处理中最基本的。一个信息进来，即是一个N维的布尔向量进来，经过处理，一个信息出去，即一个M维的布尔向量出去，这就形成了一个IPU。也就是说，我们有了一个从N维布尔空间到M维布尔空间的映射（函数，变换，或者其他术语）。可以很容易想到，IPU是很宽泛的，有太多的东西都是IPU。如通常的CPU就是一个IPU，GPU也是，一个数学函数也是一个IPU，一个程序也是，等等。当然，我们最关心的是机器学习。一个机器学习的模型，如果我们停止它的学习，它也是一个IPU。
仅仅是IPU，就没有什么重要性。机器学习的模型，最重要的特征是可以学习。讲得更精确一些，就是可以在输入数据的驱动下，改变自己处理信息的能力。因此我们定义这样一个概念：即在输入数据的驱动下，可以改变自己处理信息能力的IPU，我们称为学习机。而这个学习过程，就是机械式学习。之所以强调机械式学习，是想把人为因素排除，使得学习仅是通过输入数据，而没有人工干预。在排除了各种人工干预的因素后，我们可以仔细考虑各种情况，进而梳理清楚这里面的问题。
这样，我们就进一步有了学习机的概念：学习机是一个IPU，但是，这个IPU可以在其输入数据的驱动下而改变自己处理信息的能力。很清楚，任何机器学习模型，如果排除人工干预，就是一个学习机（深度学习模型也是如此，概率学习模型也是如此）。这当然是我们做这些梳理的目的。
让我们从学习机倒回去看看IPU。前面说了，IPU就是一个处理信息的东西。我们问，这个东西里面有什么？其实就是有一个处理信息的机制。我们这里抽象地看这个处理信息的机制，我们把它称为Processing，中文可以叫做处理。我们可以不用管这个处理具体是怎么实现的，我们可以仅定义一个处理就是能够处理信息，一个N维的布尔向量进来，经过处理，一个M维的布尔向量出去。
一个最基本问题来了：一个N-M的IPU有多少可能的处理呢？这很容易回答，有2的（M乘2的N次方）次方。这是一个极端惊人的数目。如果仅看文字没有感觉的话，可以试试M=1，N=10，在代入之后，可以看到这个数远大于2的1000次方，或者10的300次方。这还仅是N=10的情况！如果N=100？N=1000？要知道，对于信息处理来说，N=1000都非常小的，N可能很大。那样的话，IPU的处理的数目就非常非常巨大了。
这个超乎寻常的巨大的量其实是反映学习的本质的（乃至于智能的本质的）。不过只能在别的地方细谈，此处只能放过。
所谓的学习，精确定义到IPU上面，那就是IPU里面的处理改变了。处理改变了，处理信息的能力就改变了，这就是机械式学习的精确定义。因此学习机就是：它是一个IPU，在输入数据的驱动下，它的处理会改变。
我们再仔细追问，一个IPU的处理是什么？其实可以有非常多种的实现方式。例如，可以是硬件来实现的，也可以是软件来实现的，可以是通过生物机制来实现的，也可以是纯数学的方式来实现的，可以是神经网络来实现的，也可以是通过某种统计模型来实现的。总之，只要能做相应的信息处理，就好。但是经过一些仔细的推论，我们可以看到，无论IPU内部的处理是通过什么方式实现的，都必须遵循一定的数学原则，因此原则上讲，这个处理必然等价于一组逻辑门的组合。我们因此定义一个数学对象，它是用与或非三个逻辑门反复组合形成的，称之为X-形式。之所以叫X-形式，是因为它的确是一个数学形式，而我们目前对这个形式还相当未知，所以冠以X。我们证明了，任何一个处理，都可以用一个X-形式表达。这样，我们就知道IPU中间其实就是一个X-形式。而学习就是改变IPU的X-形式。那么，学习机就是在输入数据的驱动下，它的X-形式从一个变成另一个。
我们更进一步证明了，一个处理，其实可以对应多个X-形式。也就是说，对一个处理来说，有多个X-形式可以产生这同一个处理。因此，问题自然就来了，这多个X-形式中间，是否有好坏的差别？当然有，而且这个好坏差别还非常重要。因此，学习机的学习就是在改进其内在的X-形式。因此，我们知道了机械式学习，就是在输入数据的驱动下，学习机内部的X-形式在改变。
前面说了，如果一个机器学习模型没有人工干预，就是在做机械式学习，因此，我们也就可以说一个机器学习模型是在输入数据的驱动下，改变自己内在的X-形式。
如果一个学习机可以学会任何处理，那么这个学习机就是通用学习机。
这就是对通用学习机和X形式的简介
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/qX2AoDIKJ2UeGOtrS1Mr93sicKQKYEdxXtFknRCmEglZhjjMLrvwAeT73thjgCQMFEwzJdlWjCZu0AbDCgIRnfw/640?wx_fmt=jpeg)
现在我们从机械式学习的角度来看深度学习。
首先说明，深度学习经过这几年的大发展，已经发展成了一个非常庞大的集合体，已经有了很多模型很多方法。为了清楚和容易入手起见，我们选取Hinton的原初的深度学习模型来展开讨论，Hinton的原初模型，非常具有代表性，可以使得我们的讨论相对容易一些。
可以这样描述Hinton的原初深度学习模型：它是一系列RBM的叠加，而RBM是单层人工神经网络，深度学习就是多层神经网络。我们正好可以比较深入地看RBM。我们从2-1 RBM看起，这是最简单的神经网络，有两个实参数。这样的模型，当然是一个学习机。因为仅有两个实参数，学习就是调整这两个实参数，学习也就相当于在参数在一个二维平面R上的运动。这样，我们从前面讲的机械式学习的角度看，就很清楚，2-1
 RBM是这样一回事：这个二维平面R被切成6个区域，每一个区域对于一个X-形式，在学习中，如果参数从一个区域跨到另一个区域，就相当于X-形式换成了另一个X-形式。更具体的讨论，请参考英文原文。虽然2-1 RBM是一个很简单很特殊的例子，但是这里揭示的情况：把参数空间切成若干区域，而每一个区域对应一个X-形式，却是普遍的。
我们然后对3-1 RBM，N-1 RBM，N-M RBM都做了讨论，也都清楚地看到这个情况：把参数空间切成若干区域，而每一个区域对应一个X-形式，却是普遍的。
然后，我们把RBM层叠起来，就组成了Hinton的原初的深度学习模型。同样，对这个模型，我们也看到了：把参数空间切成若干区域，而每一个区域对应一个X-形式。
这样，我们就清楚了深度学习模型究竟在干什么！总结一下：
1. 当模型在形成时，就相当于把参数空间切成了若干区域。
2. 在任何一个这样的区域中，虽然参数可以很不同，但是，其实都代表同一个X-形式，而相应的处理也是相同的。仅有在参数跨过边界，进入不同区域时，才会发生不同的X-形式，才会有不同的处理。
3. 深度学习就是在数据的驱动下，从一个X-形式变到另一个X-形式。
**更具体的讨论，请参考我的英文原文：**
**https://arxiv.org/pdf/1711.03577.pdf**
当然Hinton的原初深度学习模型仅是深度学习模型的一种，而且是最简单的一种。现在深度学习已经加入了更多的东西和方法，如卷积，池化，剪枝，等等。而且其中的非线性切断函数也是多样的，不仅仅是Sign函数，如ReLU。这些都给深度学习模型带来很多变化。但是这个基本情况仍然如此：把参数空间切成有限块若干区域，而每一个区域对应一个X-形式。当然切出来的区域就更复杂，不是简单的超平面和直积切成的区域。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/qX2AoDIKJ2UeGOtrS1Mr93sicKQKYEdxXnkfHhLJIe571al4P7kSJLpTHMhJKsZibs0saNvHicKQaeg1bwSicGhtlQ/640?wx_fmt=jpeg)
前面讲的这个深度学习的基本情况非常重要，它决定了深度学习的基本能力和特征。当我们清楚了这个基本情况后，我们就可以对深度学习的一些方面做一些评论：
**为什么深度学习能很有效**
为什么深度学习很有效？我认为，有两个基本的原因：
其一：一个深度学习模型建立之时，其实就决定了这个模型是否有效，因为在这时，这个模型能够触及的全部X-形式已经到位，不可能再有新的X-形式来参与了。如果我们期望的X-形式已经在里面，我们就有可能找到它。如果期望的X-形式不在里面，那么无论我们付出多大的努力和具备多大的数据，都不可能学习到期望的东西。因此，深度学习模型的建立是决定性的。
其二：已经发展出了一套比较有效的方法来寻找期望的X-形式（当然如果这个X-形式不在，那么这些方法也就无效）。需要指出，这套方法基本上建立在线性代数运算的基础上，可以做到高度平行化。高度平行化是在现有计算结构下充分利用计算能力的必需。但是如果要加入动态调整（如最新的“胶囊”方法），这种高度平行化，就可能失去。
***逻辑或者概率**
通过我们的这个角度，可以很清楚看清楚，一个深度学习模型在处理输入信息到输出信息时，是遵循一个逻辑陈述（即其内部的X-形式），是完全逻辑的，并无随机因素。当然，它是如何获得这个逻辑陈述（即X-形式）的，就不排除随机因素。而且在实践中，随机因素的成分可以相当大。
***数据和深度学习**
深度学习中，最重要的是数据驱动。但是，有一个根本的问题就是，需要多少数据才能够有效学习？目前没有理论来说明。如果是一个完全概率的学习模型，有足够的统计学理论来说明需要多少数据。但是，如前所说，深度学习的核心是X-形式，而且这个X-形式可能相当复杂（这就是深度嘛），不可能凭空得到这个X-形式。只有在足够的数据支持下，才有希望获得这个X-形式。那么需要多少数据，什么数据？我们已经有了一些初步理论来说明要什么数据，多少数据。
***深度学习的重大不足**
其1：深度学习的基本特性，决定了深度学习的能力极限，那就是，深度学习模型建立的时候，就已经确定了所有可能的X-形式，就是说，当模型一旦确定，它能够学习什么，不能学习什么，已经定下了极限，再不能超越。这个特性使得很多事情成为不可能，例如，转移学习成为很困难的事情。尤其，不可能做到通用学习机。
其2：深度学习是在一个非常巨大的欧式空间中活动的，这个欧式空间可能有上千万维。但是，真正的学习却不在这个巨大的欧式空间中，而是在那些切成的区域中，这样的间接性就使得深度学习的很多性质很模糊，例如很难清晰获得学习的动力学。
其3：这样的间接性极大局限了深度学习的效能。非常可能大量的计算是浪费在不必要的运算上面。
其4：这样把X-形式间接嵌入到区域中的方式，使得深度学习很难做得可以解释。而可解释是非常需要的。
其5：需要大量的数据。而无监督学习有重大困难。
***展望通用学习机**
我们现在比较清楚了深度学习的各种问题。那么，要克服这些问题，走向更高效能的机器学习，我们认为就需要展望通用学习机。
通用学习机就是可以学习任何东西的机器，我们可以用一句话来讲：通用学习机不需要人工的干预，只要有数据，就可以高效能学习，学会数据里面隐含的东西。尤其，不需要像深度学习那样，需要预先人工搭好一个模型，才能学习。我们对此已经对通用学习机做了一些理论工作，也发明了一些具体实现的方法，并且申请了专利。欢迎有更多的朋友一起来探索理论，发明方法，促进应用。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/qX2AoDIKJ2UeGOtrS1Mr93sicKQKYEdxXph5O4PGrN5KR45hLsR8jnVAN4ibygslaNo9RXczTLSPFAQMT7ljib1zw/640?wx_fmt=jpeg)
*（史元春  摄于北京）*
最后，要对看到这里的朋友表示深深的谢意。穿过了很多艰涩的概念，一直读到这里，一定是对这些问题有极大的兴趣，而且对这些问题已经有深入思考，才能读到这里。谢谢大家。对读到这里的朋友们，我就多说两句务虚的话。这个时代是激动人心的时代，这是我们终于可以开始用数学和科学对人类最关键的知识领域——认识论——做探讨的时代。在这之前，我们用数学和科学对我们的外部世界（也包括人自己的身体）做了许多伟大的探索，形成了伟大的学问。但是，对这个根本问题：为什么人脑可以反射大千世界怎样反射大千世界，还是基本停留在哲学层面，没有也不可能深入到科学原理和数学细节上面。请注意，现在伟大的帷幕已经在开启，我们很快就可以在这个伟大的领域有所作为了。而这个领域的突破，就意味整个科学和数学的巨大升华。我们碰到了这样的大时代，是我们的幸运，不是很多人有这种幸运的。我们要珍惜这个幸运。现有的深度学习仅是伟大帷幕开启的先声号角，而通用学习机也仅是在这个方向做探索的一个小努力。帷幕正在开启，大时代正在到来。让我们一起来迎接这个大时代。
两年前在中国，因时差而反侧困顿，梦境依稀中得一打油诗，愿意和大家分享：
记  梦
机械学习创新知，
学习机器待奇思，
反侧犹觉洞天地，
一行一步渐趋之。
这里的洞天地，指的是认识史上的高峰：获取用机器来反射大千世界的基本规律，并且建造机器来帮助我们获得更高层次的认识。这个高峰，我恐怕是看不到的。但是，当感觉（或者幻觉）到可以一行一步趋近的时候，那种愉悦，的确非常好！以此和大家交流。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/qX2AoDIKJ2UeGOtrS1Mr93sicKQKYEdxXBuJiaaF5l96ib7b8liaDBQnS6KyJhhTGlLY0dUFD9SaL2iaYyvVPSXvgFg/640?wx_fmt=jpeg)
*（刘伟 摄于剑桥）*
