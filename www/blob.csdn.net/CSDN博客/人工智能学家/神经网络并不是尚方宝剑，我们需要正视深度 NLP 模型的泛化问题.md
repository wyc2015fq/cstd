# 神经网络并不是尚方宝剑，我们需要正视深度 NLP 模型的泛化问题 - 人工智能学家 - CSDN博客
2018年09月08日 21:01:47[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：56
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9eu5sGtPeWeHmULdwqthm17d4zSgeGtl0wPve6x6l7qBge5Sq57aZXQ/640?wx_fmt=jpeg)
来源：AI 科技评论
前段时间的文章《顶会见闻系列：ACL 2018，在更具挑战的环境下理解数据表征及方法评价》中，我们介绍了 ACL 大会上展现出的 NLP 领域的最新研究风向和值得关注的新进展。从这些新动向上我们似乎应该对深度学习 NLP 解决方案的表现充满信心，但是当我们真的仔细讨论 NLP 模型的泛化能力时候，状况其实并不乐观。
The Gradient 博客近期的一篇文章就仔细讨论了 NLP 领域的深度学习模型的泛化性问题，展现了对学习、语言、深度学习方法等方面的诸多深入思考。不得不泼一盆冷水，即便端到端的深度学习方法相比以往的方法在测试任务、测试数据集上的表现有了长足的改进，我们距离「解决 NLP 问题」仍然有遥远的距离。AI 科技评论全文编译如下。
「泛化」是一个NLP 领域中正在被深入讨论和研究的课题。
最近，我们经常可以看到一些新闻媒体报道机器能够在一些自然语言处理任务中取得与人相当的表现，甚至超过人类。例如，阅读一份文档并回答关于该文档的问题（阿里、微软、讯飞与哈工大等等轮番刷榜 SQuAD）、确定某个给定的文本在语义上是否蕴含另一个文本（http://www.aclweb.org/anthology/N18-1132）以及机器翻译。「如果机器能够完成所有这些任务，那么它们当然拥有真正的语言理解和推理能力」这种说法听起来似乎是很合理的。
然而，事实并非如此。最近许多的研究表名，事实上最先进的自然语言处理系统既「脆弱」（鲁棒性差）又「虚假」（并未学到真正的语言规律）。
### **最先进的自然语言模型是「脆弱」的**
当文本被修改时，即使它的意义被保留了下来，自然语言处理模型也会失效，例如：
Jia 和 Liang 等人攻破了阅读理解模型 BiDAF（https://arxiv.org/abs/1611.01603）。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9o47l2MFzDILmYOP1BiaCS5icIia7V8VcIYYhcQAxKqR0icHTgz2EyherSg/640?wx_fmt=jpeg)
Jia 和 Liang 等人论文中给出的例子。
Belinkov 和 Bisk 等人（https://arxiv.org/abs/1711.02173）攻破了基于字符的神经网络翻译模型。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9HGYMOAQkwMUHZoBFm2TLX7S4qlibEAWcuxprDZMjIia2R2d2FWqvSIgg/640?wx_fmt=jpeg)
Belinkov 和 Bisk 等人论文中给出的例子。BLEU是一个常用的将候选的文本翻译结果和一个或多个参考译文对比的评测算法。
Iyyer 与其合作者攻破了树结构双向 LSTM（ http://www.aclweb.org/anthology/P15-1150）的情感分类模型。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9iaXYpPASn1vCycdibn6AIkrOk6GaKW0oB7E9FX1cU34xDLibE5LXHLd7w/640?wx_fmt=jpeg)
Iyyer 与其合作者论文中给出的例子。
### **最先进的自然语言处理模型是「虚假」的**
这些模型经常会记住的是人为影响和偏置，而不是真正学到语言规律，例如：
Gururangan 与其合作者（http://aclweb.org/anthology/N18-2017）提出了一个对比基线，它能够将对比基准数据集中 50 %以上的自然语言推理样本正确分类，而不需要事先观察前提文本（premise）。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9wFncAJSYWDMkL4Y3N5loOf4Lc9o0oIiaSOaSt9rcZbRZcCG7Y1HLQiaA/640?wx_fmt=jpeg)
Gururangan 等人论文中给出的例子。这些样本都是从论文的海报展示中截取的。
Moosavi 和 Strube（http://aclweb.org/anthology/P17-2003）表明，为共指解析任务构建的深度学习模型（http://www.aclweb.org/anthology/P16-1061）总是将以包含「country」的专有名词或普通名词与训练数据中出现的某个国家联系在一起。因此，该模型在有关训练数据中未提及的国家的文本上的表现很差。同时，Levy 与其合作者研究用用于识别两个单词之间的词汇推理关系（例如，上位词，概括性较强的单词叫做特定性较强的单词的上位词）的模型。他们发现，这些模型并没有学习到单词之间关系的特征，而是仅仅学习到了一对单词中某一单词的独立属性：某个单词是否是一个「典型上位词」（例如，「动物」一词）。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9vq70EjjmdqpEQDseT1mBtqk7JxfrzuWqV4ickcOMPwGeSqibGmj5WRAQ/640?wx_fmt=jpeg)
左图：Moosavi 和Strube 论文中的例子。右图：Levy 与其合作者论文中的例子。
Agrawal 与其合作者指出，卷积神经网络（CNN）+长短期记忆网络（LSTM）的可视化问答模型通常在「听」了一半问题后，就会收敛到预测出的答案上。也就是说，该模型在很大程度上受到训练数据中浅层相关性的驱动并且缺乏组合性（回答关于可见概念的不可见的组合问题的能力）。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9HGN5vjk6ON08icwRCt0eWGOBwJXC58uuun6DcJicNsuAfIOEiakRF5Y3g/640?wx_fmt=jpeg)
Agrawal 等人论文中给出的例子。
### **一个改进最先进的自然语言处理模型的 workshop**
因此，尽管在对比基准数据集上表现良好，现代的自然语言处理技术在面对新颖的自然语言输入时，在语言理解和推理方面还远远达不到人类的水平。这些思考促使 Yonatan Bisk、Omer Levy、Mark Yatskar 组织了一个 NAACL workshop，深度学习和自然语言处理新泛化方法 workshop
（https://newgeneralization.github.io/）
来讨论泛化问题，这是机器学习领域面临的最核心的挑战。该 workshop 针对两个问题展开了讨论：
- 
**我们如何才能充分评估我们的系统在新的、从前没有遇见过的输入上运行的性能？**或者换句话说，我们如何充分评估我们系统的泛化能力？
- 
**我们应该如何修改我们的模型，使它们的泛化能力更好？**
这两个问题都很困难，为期一天的 workshop 显然不足以解决它们。然而，自然语言处理领域最聪明的研究者们在这个工作坊上对许多方法和构想进行了概述，它们值得引起你的注意。特别是，当我们对这些讨论进行总结，它们是围绕着三个主题展开的：使用更多的归纳偏置（但需要技巧），致力于赋予自然语言处理模型人类的常识、处理从未见过的分布和任务。
### **方向 1：使用更多的归纳偏置（但需要技巧）**
目前，人们正在讨论是否应该减少或增加归纳偏置（即用于学习从输入到输出的映射函数的一些假设）。
例如，就在去年，Yann LeCun 和 Christopher Manning 进行了一场引人注意的辩论（详见雷锋网 AI 科技评论文章 AI领域的蝙蝠侠大战超人：LeCun 与 Manning 如何看待神经网络中的结构设计），讨论我们在深度学习框架中应该引入怎样的固有先验知识。Manning 认为，对于高阶推理，以及利用较少的数据进行学习的任务，结构化偏置是十分必要的。相反，LeCun 将这种结构描述成「必要的恶」，这迫使我们作出某些可能限制神经网络的假设。
LeCun 的观点（减少归纳偏置）之所以令人信服的一个论据是，事实上使用基于语言学的偏置的现代模型最终并不能在许多对比基准测试中获得最佳性能（甚至有一个段子说，「每当我从团队里开除一个语言学家，我的语言模型的准确率就会提升一些」）。尽管如此，NLP 社区还是广泛支持 Manning 的观点。在神经网络架构汇中引入语言结构是ACL 2017 的一个显著趋势。然而，由于这种引入的结构似乎在实践中并没有达到预期的效果，我们也许可以得出如下结论：探索引入归纳偏置的新方法应该是一个好的工作方式，或者用 Manning 的话来说：
> 
我们应该使用更多的归纳偏置。我们对如何添加归纳偏置一无所知，所以我们会通过数据增强、创建伪训练数据来对这些偏置进行编码。这看起来是一种很奇怪的实现方法。
事实上，Yejin Choi 已经在自然语言生成（NLG）的课题下对这个问题提出了自己的解决方法。她给出了一个通过能够最大化下一个单词的概率的通用语言模型（一个带有集束搜索（beam search）的门控循环神经网络（gated RNN），https://guillaumegenthial.github.io/sequence-to-sequence.html）生成的评论的示例。
自然的语言输入为：
> 
总而言之，我会将这个酒店强烈推荐给想要住在中心地区的人。
而不自然的、重负性的、矛盾的、乏味的输出是：
> 
总而言之，我会将这个酒店推荐给想要住在中心地区的人，并且想要居住在中心地区。如果你想要住在中心地区，这里不是适合你的地方。然而，如果你想要住在这个地区的正中心，这里就是你应该去的地方。
在她看来，当前的语言模型生成的语言之所以如此不自然，这是因为它们是：
- 
**被动的学习器。**尽管它们会阅读输入然后生成输出，但是它们并不能像人类学习者一样工作，它们不会根据诸如有意义、风格、重复和蕴含这样的合适的语言标准来反思自己生成的结果。换句话说，它们并不会「练习」写作。
- 
**肤浅的学习器。**它们并没有捕获到事实、实体、事件或者活动之间的高阶关系，而这些元素对于人类来说都可能是理解语言的关键线索。也就是说，这些模型并不了解我们人类的世界。
如果我们鼓励语言模型以一种使用特定的损失函数的数据驱动的方式学习诸如有意义、风格、重复和蕴含等语言学特征，那么语言模型就可以「练习」写作了。这种做法优于依赖于显式使用自然语言理解（NLU）工具输出的方法。这是因为，传统上的 NLU 只处理自然的语言，因此无法理解可能并不自然的机器语言。比如上面的例子中那样重复的、矛盾的、乏味的文本。由于NLU 并不理解机器语言，所以将NLU 工具应用到生成的文本上、从而指导自然语言生成（NLG）模型理解生成的模型为什么如此不自然并由此采取相应的行动是毫无意义的。总而言之，我们不应该开发引入了结构化偏置的新神经网络架构，而应该改进学习这些偏置的数据驱动的方法。
自然语言生成（NLG）并不是唯一的我们应该寻找更好的学习器优化方法的 NLP 任务。在机器翻译中，我们的优化方法存在的一个严重的问题是，我们正通过像交叉熵或语句级别 BLEU 的期望这样的损失函数来训练机器翻译模型，这种模型已经被证实是有偏的，并且与人类理解的相关性不足。事实上，只要我们使用如此简单的指标来训练我们的模型，它们就可能和人类对于文本的理解不匹配。由于目标过于复杂，使用强化学习对于 NLP 来说似乎是一个完美的选项，因为它允许模型在仿真环境下通过试错学习一个与人类理解类似的信号（即强化学习的「奖励」）。
Wang 与其合作者（http://www.aclweb.org/anthology/P18-1083）为「看图说话」（描述一幅图片或一段视频的内容）提出一种训练方法。首先，他们研究了目前使用强化学习直接在我们在测试时使用的「METEOR」、「BLEU」、「CIDEr」等不可微的指标上训练图像字幕系统的训练方法。Wang 与其合作者指出，如果我们使用 METEOR 分数作为强化决策的奖励，METEOR分数会显著提高，但是其它的得分将显著降低。他们举出了一个平均的 METEOR 得分高达40.2 的例子：
> 
We had a great time to have a lot of the. They were to be a of the. They were to be in the. The and it were to be the. The, and it were to be the.（该文本并不自然，缺乏必要的语言成分，不连贯）
相反，当使用其它的指标时（BLEU 或CIDEr）来评估生成的故事时，相反的情况发生了：许多有意义的、连贯的故事得分很低（几乎为零）。这样看来，机器似乎并不能根据这些指标正常工作。
因此，作者提出了一种新的训练方法，旨在从人类标注过的故事和抽样得到的预测结果中得到与人类的理解类似的奖励。尽管如此，深度强化学习仍然是「脆弱」的，并且比有监督的深度学习有更高的抽样复杂度。一个真正的解决方案可能是让人类参与到学习过程中的「人机循环」机器学习算法（主动学习）。
### **方向 2：引入人类的常识**
尽管「常识」对于人类来说可能能够被普遍地理解，但是它却很难被教授给机器。那么，为什么像对话、回复邮件、或者总结一个文件这样的任务很困难呢？
这些任务都缺乏输入和输出之间的「一对一映射」，需要关于人类世界的抽象、认知、推理和最广泛的知识。换句话说，只要模式匹配（现在大多数自然语言处理模型采取的方法）不能由于某些与人类理解类似的「常识」而得到提升，那么我们就不可能解决这些问题。
Choi 通过一个简单而有效的例子说明了这一点：一个新闻标题上写着「芝士汉堡对人有害」（cheeseburger stabbing）
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9qLCokvlbPMxPPwSWWpVIB48dC3QyeqlOEq2WSibJF6Zo1Qkibaic35VQQ/640?wx_fmt=jpeg)
【 图片来源：https://newgeneralization.github.io 所有者：Yejin Choi 】
仅仅知道在定语修饰关系中「stabbing」被依赖的名词「cheeseburger」修饰，还不足以理解「cheeseburger stabbing」究竟是什么意思。上图来自 Choi 的演讲。
对于这个标题，一台机器可能提出从下面这些问题：
> 
有人因为一个芝士汉堡刺伤了别人？
有人刺伤了一个芝士汉堡？
一个芝士汉堡刺伤了人？
一个芝士汉堡刺伤了另一个芝士汉堡？
如果机器拥有社会和物理常识的话，它们就可以排除掉那些你永远不会问的荒谬问题。社会常识（http://aclweb.org/anthology/P18-1043）可以提醒机器，第一种选项似乎是合理的，因为伤害人是不好的，并且因此具有新闻价值。而伤害一个芝士汉堡则没有新闻价值。物理常识（http://aclweb.org/anthology/P17-1025）则说明第三和第四个选项是不可能的，因为芝士汉堡不能被用来伤害任何东西。
除了引入常识知识，Choi 还推崇「通过语义标注进行理解」，这里的重点是应该把「说了什么」改为「通过仿真进行理解」。这模拟了文本所暗示的因果效应，不仅侧重于「文本说了什么」，还侧重于「文本没有说什么，但暗示了什么」。Bosselut 与其同事（https://arxiv.org/abs/1711.05313）展示了一个例子，用以说明为什么预测对于文本中的实体采取的动作所隐含的因果效应是十分重要的：
> 
如果我们给出「在松饼混合物中加入蓝莓，然后烘焙一个半小时」这样的说明，一个智能体必须要能够预测一些蕴含的事实，例如：蓝莓现在正在烤箱里，它们的「温度」会升高。
Mihaylov 和 Frank（http://aclweb.org/anthology/P18-1076）也认识到我们必须通过仿真来进行理解。与其他更复杂的阅读理解模型不同，他们的完形填空式的阅读理解模型可以处理「大部分用来推理答案的信息在一个故事中被给出」的情况，但是也需要一些额外的常识来预测答案：马（horse）是一种动物，动物（animal）是用来骑的，而乘骑（mount）与动物有关。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9zs0kzcxheLtOJYx6FZ6pUUZrbPiccZoELaQ7c5lRtzY9T7oIXeUHSGw/640?wx_fmt=jpeg)
一个需要常识的完形填空式的阅读理解案例。该例子来自 Mihaylov 和Frank 的论文。
很不幸，我们必须承认，现代的 NLP 就像「只有嘴巴没有脑子」一样地运行，为了改变这种现状，我们必须向它们提供常识知识，教它们推测出有什么东西是没有直接说，但是暗示出来了。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9cDEctRZuTNnCVnXl4H6TUfAAzQficSeV60aQB6kk6URQH7OqH1zPccA/640?wx_fmt=jpeg)
「循环神经网络（RNN）是无脑的嘴巴吗？」幻灯片取自 Choi 的演讲。
### **方向 3：评估从未见到过的分布和任务**
使用监督学习解决问题的标准方法包含以下步骤：
- 
确定如何标注数据
- 
手动给数据打标签
- 
将标注过的数据分割成训练集、测试集和验证集。通常，如果可能的话，我们建议确保训练集、开发集（验证集）和测试集的数据拥有同样的概率分布。
- 
确定如何表征输入
- 
学习从输入到输出的映射函数
- 
使用一种恰当的方式在测试集上评估提出的学习方法
按照这种方法解出下面的谜题，需要对数据进行标注从而训练一个识别各单位的模型，还要考虑多种表征和解释（图片、文本、布局、拼写、发音），并且将它们放在一起考虑。该模型确定了「最佳」的全局解释，并且与人类对这一谜题的解释相符。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9oXZ1X9Jtk6Wmn4NMGnibUt0GjHVqjdJvzHfuO0JTkpcmfumHukrZk5g/640?wx_fmt=jpeg)
一个难以标注的输入的示例。图片由Dan Roth 提供。
在 Dan Roth 看来：
- 
这种标准方法不具有可扩展性。我们将永远不可能拥有足够的标注数据为我们需要的所有任务训练所有的模型。为了解出上面的谜题，我们需要标注过的训练数据去解决至少五个不同的任务，或者大量的数据来训练一个端到端的模型。虽然可以利用 ImageNet 这样现有的资源来完成「单位识别」这样的组建，但是 ImageNet 并不足以领悟到「世界（world）」一词在这种语境下比「地球（globe）」要好。即使有人做出了巨大的努力进行标注，这些数据还是需要不断地被更新，因为每天都需要考虑新的流行文化。
Roth 提醒我们注意一个事实，即大量的数据独立于特定的任务存在，并且有足够多的暗示信息来为一系列任务推断出监督信号。这就是「伴随监督（incidental supervision）」这一想法的由来。用他自己的话说
（http://cogcomp.org/papers/Roth-AAAI17-incidental-supervision.pdf）：
> 
「伴随」信号指的是在数据和环境中存在的一系列若信号，它们独立于有待解决的任务。这些信号与目标任务是相互关联的，可以通过适当的算法支持加以利用，用来提供足够的监督信号、有利于机器进行学习。例如，我们不妨想一想命名实体（NE）音译任务，基于各个实体间发音的相似性，将命名实体从源语言改写成目标语言的过程（例如，确定如何用希伯来语写奥巴马的名字）。我们拥有现成的时序信号，它独立于有待解决的音译任务存在。这种时序信号是与我们面对的任务相互关联的，它和其他的信号和一些推理结果可以被用来为任务提供监督信息，而不需要任何繁琐的标注工作。
Percy Liang 则认为，如果训练数据和测试数据的分布是相似的，「任何一个有表示能力的模型，只要给了足够数据都能够完成这个任务。」然而，对于外推任务（当训练数据和测试数据的分布不同时），我们必须真正设计一个更加「正确」的模型。
在训练和测试时对同样的任务进行外推的做法被称为领域自适应。近年来，这一课题引起了广泛的关注。
但是「伴随监督」，或者对训练时任务和测试时任务不同的外推并不是常见的做法。Li 与其合作者（http://aclweb.org/anthology/N18-1169）训练了一个用于文本定语迁移的模型，它仅有对与给定的句子的定语标签，而不需要一个平行的语料库把具有相同内容、但是定语不同的句子对应起来。换句话说，他们训练了一个模型用来预测一个给定的句子的定语，它只需要被作为一个分类器进行训练。类似地，Selsam 与其合作者（https://arxiv.org/abs/1802.03685）训练了一个学着解决SAT（可满足性）问题的模型，它只需要被作为一个预测可满足性的分类器进行训练。值得注意的是，这两种模型都有很强的归纳偏置。前者使用的假设是，定语往往在局部的判别短语中较为明显。后者则捕获了调查传播算法（Survey propagation）的归纳偏置。
Percy 对研究社区提出了挑战，他呼吁道：
> 
每篇论文，以及它们对所使用的数据集的评估，都应该在一个新的分布或一个新的任务上进行评估，因为我们的目标是解决任务，而不是解决数据集。
当我们使用机器学习技术时，我们需要像机器学习一样思考，至少在评估的时候是这样的。这是因为，机器学习就像一场龙卷风，它把一切东西都吸收进去了，而不在乎常识、逻辑推理、语言现象或物理直觉。
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBVPAtQXK4XZp1iaPXQSOn2X9JNGEHzd0Zo8mxxgxxYkrql2ssgtfSAdg8LJrUeeNfePdib9K8G6Cc8w/640?wx_fmt=jpeg)
幻灯片取自 Liang 的报告。
参加 workshop 的研究人员们想知道，我们是否想要构建用于压力测试的数据集，为了观测我们的模型真正的泛化能力，该测试超出了正常操作的能力，达到了一个临界点（条件十分苛刻）。
我们有理由相信，只有在解决了较为简单的问题后，一个模型才能有可能解决更困难的案例。为了知道较为简单的问题是否得到了解决，Liang 建议我们可以根据案例的难度对它们进行分类。Devi Parikh 强调，对于解决了简单的问题就能够确定更难的问题有没有可能解决的这样的设想，只有一小部分任务和数据集能满足。而那些不包括在这一小部分中的任务，例如可视化问答系统，则不适合这个框架。目前还不清楚模型能够处理哪些「图像-问题」对，从而处理其它可能更困难的「图像=问题」对。因此，如果我们把模型无法给出答案的例子定义为「更困难」的案例，那么情况可能会变的很糟。
参加 workshop 的研究人员们担心，压力测试可能会放缓这一领域的进步。什么样的压力能让我们对真正的泛化能力有更好的理解？能够促使研究人员构建泛化能力更强的系统？但是不会导致资金的削减以及研究人员由于产出较少而倍感压力？workshop 没有就此问题给出答案。
### **结论**
「NAACL 深度学习和自然语言处理新泛化方法 workshop」是人们开始认真重新思考现代自然语言处理技术的语言理解和推理能力的契机。这个重要的讨论在 ACL 大会上继续进行，Denis Newman-Griffis 报告说，ACL 参会者多次建议我们需要重新思考更广泛的泛化和测试的情景，这些情景并不能反映训练数据的分布。Sebastian Ruder 说，这个 NAACL workshop 的主题在 RepLNLP（最受欢迎的关于自然语言处理的表征学习的 ACL workshop）上也被提及。
以上的事实表明，我们并不是完全不知道如何修改我们的模型来提高他们的泛化能力。但是，仍然有很大的空间提出新的更好的解决方案。
我们应该使用更多的归纳偏置，但是需要找出最恰当的方法将它们整合到神经网络架构中，这样它们才能够为网络架构带来我们期望得到的提升。
我们必须通过一些与人类理解类似的常识概念来提升最先进的模式匹配模型，从而使它们能够捕获到事实、实体、事件和活动之间的高阶关系。但是挖掘出常识通常是极具挑战性的，因此我们需要新的、有创造性的方法来抽取出常识。
最后，我们应该处理从未见过的分布和任务。否则，「任何具有足够足够数据的表示模型都能够完成这个任务」。显然，训练这样的模型更加困难，并且不会马上取得很好的结果。作为研究人员，我们必须勇于开发这种模型；而作为审稿人，我们不应该批评试图这样做的工作。
这些讨论虽然都是 NLP 领域的话题，但这也反映了整个 AI 研究领域内的更大的趋势：从深度学习的缺点和优势中反思学习。Yuille and Liu 写了一篇观点文章《深度神经网络到底对计算机视觉做了什么》，
（https://arxiv.org/abs/1805.04025）
Gary Marcus 更是一直宣扬，对于整个 AI 领域的问题来说，我们需要多多考虑深度学习之外的方法。这是一个很健康的信号，AI 研究人员们越来越清楚深度学习的局限性在哪里，并且在努力改善这些局限。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
