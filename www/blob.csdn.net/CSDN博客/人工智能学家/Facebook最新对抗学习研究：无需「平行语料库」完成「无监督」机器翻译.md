# Facebook最新对抗学习研究：无需「平行语料库」完成「无监督」机器翻译 - 人工智能学家 - CSDN博客
2017年11月05日 00:00:00[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：136
![640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/waLJGrhJM0cy4ZsaxcUn3rAzAlnQr90Oxo0oPA9hy9LibibFVUXZNR0G8Q7lJ8qrWialgezfEaDicqlkviaeoG8pBEw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
*来源：雷克世界*
*编译：嗯~阿童木呀、多啦A亮*
*概要：得益于最近在深度学习方面取得的进展以及大规模平行语料库的可用性，现如今，机器翻译已经在多个语言对上取得了令人印象深刻的表现。*
相信大家都知道，最近在机器翻译领域取得了令人印象深刻的成果，而这主要归功于最近在深度学习方面所取得巨大进步，以及大规模平行语料库（large-scale parallel corpora）的可用性。我们已经进行过无数次尝试，试图将这些成功扩展到低资源语言对，但这需要数以万计的并行句子。在这项研究中，我们把这个研究方向做到了极致，并研究了在没有任何平行数据的情况下，是否有可能学习翻译？我们提出构建一种模型，它用两种不同的语言从单语语料库（monolingual
 corpora）中提取句子，然后将它们映射到相同的潜在空间中。通过学习从这个共享的特征空间中以两种语言进行重构，该模型有效地学习了在不使用任何标记数据的情况下进行翻译。我们在两个广泛使用的数据集和两个语言对上对模型进行演示，结果显示，BLEU分数高达32.8，而在在训练期间甚至没有使用一个平行句。
得益于最近在深度学习方面取得的进展以及大规模平行语料库的可用性，现如今，机器翻译已经在多个语言对上取得了令人印象深刻的表现。然而，这些模型只有在提供大量的并行数据，即大约数百万个并行句子的情况下，才能很好地运行。不幸的是，并行语料库的构建成本是非常高的，因为这需要专门的专业知识，而且通常对于低资源语言来说是不可能的。相反，单语数据更容易找得到，而且许多具有有限并行数据的语言仍然拥有大量的单语数据。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0cy4ZsaxcUn3rAzAlnQr90OW9pSlU4a4RAMlanIkS2vQIV22rj7QsYAm1fezpDQCczWibGXHZsIdvw/640?wx_fmt=png)
在半监督环境中，我们已经进行了多次尝试，试图利用单语数据来提高机器翻译系统的质量。最值得注意的是，Sennrich 等人于2015年提出了一个非常高效的数据增强方案，我们称之为“回译（back-translation）”，即从目标语言到源语言的辅助翻译系统首先在可用的并行数据上进行训练，然后用于从大的目标端的单语语料库中生成翻译。然后将这些翻译的组成对以及与其相对应的参考目标（ground
 truth targets）用作原始翻译系统的附加训练数据。
另一种在目标端使用单语数据的方式是用语言模型来增强解码器（Gulcehre等人于2015年提出）。最后，Cheng等人（于2016年）、He等人（于2016年）提出在单语数据上增加一个辅助自动编码任务，这样就可以保证翻译后的句子可以再次被回翻成原文。但是，所有这些研究依然依赖于数万个平行的句子。
之前关于零资源（zero-resource）机器翻译的研究也依赖于标记信息，它们不是来自于那些有用的语言对，而是其他相关的语言对（Firat等人于2016年、Johnson等人于2016年、Chen等人于2017年提出）或其他形式的语言对（Nakayama 和 Nishida于2017年、Lee等人于2017年提出）。唯一的例外就是Ravi和Knight（于2011年）、
 Pourdamghani和Knight（于2017年）所进行的研究，他们将机器翻译问题简化为解密问题。不幸的是，他们的方法局限性在于只适用于相当短的句子，而且它只是在一个非常简单的环境中得以证明的，包括那些最常见的短句子或者是非常接近的语言中。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/waLJGrhJM0cy4ZsaxcUn3rAzAlnQr90ObGqZb9m0xYkLBkForicibdIe6lrTVSXD17ic0kmO8tzl9Uc1T6FsgOXkw/640?wx_fmt=png)
*图1：原理简笔图，用以指导我们设计目标函数。左（自动编码）：模型被训练，以便基于其噪声版本重建一个句子。其中，x是目标，C(x)是噪声输入，x^是重建。右（翻译）：模型被训练用以翻译另一个领域的句子。其中输入是在前一次迭代(t)，y = M(t）(x)处由模型本身M产生的噪声翻译（在这种情况下，翻译顺序为从源到目标（from source-to-target））。该模型是对称的，我们在其他语言中将重复相同的过程。*
在本文中，我们研究是否可以训练一个通用的机器翻译系统，而不需要任何形式的监督。我们所做的唯一假设是每种语言都存在一个单语语料库。这个假设有两个有趣的原因。 首先，当我们遇到一个我们没有注释的新语言对，就可以使用它。其次，它对任何好的半监督方法预期会产生一个强大的下界表现。
关键点是在两种语言（或领域）之间建立一个共同的潜在空间，并根据两个原则通过在两个领域进行重构来学习翻译：（1）模型必须能够从噪声版本中以给定的语言重构句子，如在标准去噪自动编码器中。（2）该模型还学习了在目标域中对同一句子进行有噪的翻译时重构任何源语句，反之亦然。对于（2），翻译的句子通过使用回译程序获得，即通过使用学习模型将源句子翻译成目标域来获得翻译后的句子。除了这些重构目标之外，我们还使用对抗正则化术语将源句子和目标句子的潜在表示限制为相同的分布，由此模型试图欺骗鉴别器，该鉴别器被同时训练以识别给定的潜在句子表示的语言。然后迭代地重复这个过程，从而产生高质量的翻译模型。为了保持我们的方法完全无监督，我们初始化我们的算法，通过使用一个基于从同一单语数据衍生出双语词汇的句子逐字翻译的无监督翻译模型。
虽然无法与使用大量并行资源的有监督方法进行竞争，但我们在第4部分中展示了我们的模型能够实现卓越的性能。例如，在WMT数据集上，在对100000对句子进行完全监督的机器翻译系统中，我们可以达到同样的翻译质量。在Multi30K-Task1数据集中，我们在所有语言对上实现了22以上的BLEU值，英翻法BLEU值达到32.76。
接下来，在第2部分中，我们将描述模型和训练算法。然后我们在第四部分给出实验结果。最后，我们在第五部分进一步讨论相关工作，并在第六部分总结我们的发现。
**结论**
我们提出了一种新的神经机器翻译方法，其中翻译模型仅使用单语言数据集学习，句子或文档之间没有任何对齐。这个方法的原理是从一个简单的无监督逐字翻译模型开始，并基于重构损失迭代地改进这个模型，并且使用鉴别器来对齐源语言和目标语言的潜在分布。我们的实验表明，我们的方法能够在没有任何监督的情况下学习有效的翻译模型。
