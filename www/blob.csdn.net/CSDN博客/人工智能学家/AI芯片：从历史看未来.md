# AI芯片：从历史看未来 - 人工智能学家 - CSDN博客
2018年07月22日 20:26:20[人工智能学家](https://me.csdn.net/cf2SudS8x8F0v)阅读数：182
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBUO1J2pUUxuiavCX4kONwfg16RM3iaxFIdGVDJnibNHQKwz1yq8CkTzLxAZWOoXlbZjYUcD169vNGH9w/640?wx_fmt=jpeg)
来源：36氪
摘要：从芯片发展的大趋势来看，目前尚处于AI芯片发展的初级阶段，无论是科研还是产业应用都有巨大的创新空间。我们相信，未来十年将是AI芯片发展的重要时期，有望在架构和设计理念取得巨大的突破。
目前，人工智能领域正不断取得突破性进展。作为实现人工智能技术的重要基石，AI芯片拥有巨大的产业价值和战略地位。自2018年伊始，整个人工智能产业对于AI芯片的热情仿佛一瞬间被彻底点燃，无论是巨头公司还是初创企业，无论是芯片厂商还是互联网公司，纷纷积极布局这一领域。放眼整个人工智能行业，一时之间可谓人声鼎沸、热闹非凡。
然而，越是繁荣的表象，整个产业界越需要保持客观与冷静。众所周知，作为人工智能产业链的关键环节和硬件基础，AI芯片有着极高的技术研发和创新的壁垒。从芯片发展的趋势来看，我们现在仍处于AI芯片发展的初级阶段。未来将是AI芯片发展的重要阶段，无论是架构还是设计理念都存在着巨大的创新空间。那么，当前AI芯片的发展现状究竟如何？下一轮的爆发点又将在哪里出现呢？
## **AI芯片的前生今世**
经过长期的发展和探索，人工智能在近几年取得了突破性进展。人工智能系统在语音识别、图像识别、围棋、德州扑克等诸多领域取得了超越人的能力的成果。究其原因，业界普遍认为，深度学习算法、海量的数据和充足的计算力这三大要素合力促成了这次突破。
其中，得益于摩尔定律在最近二十年的发展，充足的算力使得在可以接受的价格、功耗和时间内提供人工智能算法所需的计算性能。根据英特尔的处理器芯片能力和零售价格对比测算，单位价格可以购买到的计算力提升了1.5万倍，从而使“通用中央处理器”（CPU）可以支持各种人工智能任务。可以说，通过芯片技术来大幅增强人工智能研发的时机已经非常成熟。然而，由于CPU要面对成百上千种工作任务来进行设计和优化，因此不可能牺牲灵活性来专门为某一类应用做优化，因此未必针对所有AI算法都是最优的选择。为此，出现了多种CPU加专用芯片的异构计算方案，以解决计算资源和内存访问瓶颈的研究。此外，与“脑启发式”（brain-inspired）的深度神经网络不同的“类脑”（brain-like）计算研究也推出了先进的神经拟态芯片来支持超高能效比的自然学习方式。
综合来看，如果以设计理念进行划分的话，AI芯片大致可分为两大类别。
第一类是“AI加速芯片”，它是确定性地加速某类特定的算法或任务，从而达到目标应用领域对速度、功耗、内存占用和部署成本等方面的要求。目前，AI加速芯片的研发有两种主要的方式：一种是利用已有的GPU、众核处理器、DSP、FPGA芯片来做软硬件优化；另一种是设计专用的芯片，也就是ASIC。第二类是“智能芯片”，它让芯片像人一样能使用不同的AI算法进行学习和推导，处理包含感知、理解、分析、决策和行动的一系列任务，并且具有适应场景变化的能力。目前，面向综合、自适应能力的智能芯片研究有两类设计方法，一种是基于类脑计算的“神经拟态芯片”；另一种是基于可重构计算的“软件定义芯片”。
围绕这两大方向，全球各大芯片公司都积极在人工智能领域进行布局，英特尔也是如此。而英特尔的独特之处，在于能够提供全面的、多元化的解决方案。我们既提供多种芯片类型的产品，又覆盖了从终端到数据中心的使用场景。在终端领域，可以使用Movidius、Mobileye的ASIC芯片。在边缘计算中，可以使用ASIC芯片和FPGA芯片；在数据中心领域，可以灵活选择至强可扩展处理器、众核处理器和NNP等芯片方案。此外，英特尔还通过神经拟态芯片Loihi积极探索新的计算模式。
## **AI芯片的未来之路**
目前，AI芯片虽然在某些具体任务上可以大幅超越人的能力，但在通用性、适应性上相较于人类智能还有很大差距，大多数仍处于对特定算法的加速阶段。从短期来看，以异构计算（多种组合方式）为主来加速各类应用算法的落地（看重能效比、性价比、可靠性）；从中期来看，要发展自重构、自学习、自适应的芯片来支持算法的演进和类人的自然智能；从长期来看，则是朝着通用AI芯片的方面发展。
在我看来，“通用AI芯片”是AI芯片皇冠上的明珠。它最理想化的方式是淡化人工干预（如限定领域、设计模型、挑选训练样本、人工标注等）的通用智能芯片，必须具备可编程性、架构的动态可变性、高效的架构变换能力或自学习能力、高计算效率、高能量效率、应用开发简洁、低成本和体积小等特点。就目前而言，实现通用AI的主要直面两大挑战：一是通用性（算法和架构），二是实现的复杂度。通用AI芯片的复杂度来自于任务的多样性和对自学习、自适应能力的支持。因此，我们认为通用AI芯片的发展方向不会是一蹴而就地采用某一种芯片来解决问题，因为理论模型和算法尚未完善。最有效的方式是先用一个多种芯片设计思路组合的灵活的异构系统来支持，各取所长，取长补短。一旦架构成熟，就可以考虑设计SoC来在一个芯片上支持通用AI。
从短期来看，我们很难期待出现像CPU那样的AI通用算法芯片，AI杀手级应用还没出现，未来还有很长一段路要走。但必须承认的是，AI芯片是人工智能技术发展过程中不可逾越的关键阶段。无论哪种AI算法，最终的应用必然通过芯片来实现。目前，AI算法都有各自长处和短板，必须给它们设定一个合适的应用边界，才能最好地发挥它们的作用。因此，确定应用领域就成为了发展AI芯片的重要前提。
在应用方面，“无行业不AI”似乎正在成为主旋律，无论是人脸识别、语音识别、机器翻译、视频监控，还是交通规划、无人驾驶、智能陪伴、舆情监控、智慧农业等，人工智能似乎涵盖了人类生产生活的方方面面。然而，是所有的应用都需要人工智能吗？我们希望人工智能解决哪些实际的问题？什么才是AI的“杀手级”应用？这些问题目前依然等待答案。但对于芯片从业者而言，我们的当务之急是研究芯片架构问题。从感知、传输到处理，再到传输、执行，这是AI芯片的一个基本逻辑。研究者需要利用软件系统、处理器等去模仿。软件是实现智能的核心，芯片是支撑智能的基础。
从芯片发展的大趋势来看，目前尚处于AI芯片发展的初级阶段，无论是科研还是产业应用都有巨大的创新空间。从确定算法、领域的AI加速芯片向具备更高灵活性、适应性的智能芯片发展是科研发展的必然方向。神经拟态芯片技术和可重构计算芯片技术允许硬件架构和功能随软件变化而变化，实现以高能效比支持多种智能任务，在实现AI功能时具有独到的优势，具备广阔的前景。我们相信，未来十年将是AI芯片发展的重要时期，有望在架构和设计理念取得巨大的突破。
未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。
未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。
*如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”*
![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/f84kJBXzrBXtjwXLOH13nsYuQKfVHbapnHFO9iacHnzft3Q7mqEeqVf6phSiam3I17pVBMLp18riaEpPOlp4xIxzA/640?wx_fmt=jpeg)
