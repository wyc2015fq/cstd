# 《统计学习方法》学习笔记（3）--感知机 - 蓁蓁尔的博客 - CSDN博客





2017年04月24日 10:49:52[蓁蓁尔](https://me.csdn.net/u013527419)阅读数：393








补点小常识：**1958年，（李航老师《统计学习方法中》说到的是1957年）计算科学家Rosenblatt提出了由两层神经元组成的神经网络–“感知器”（Perceptron）–单层神经网络。**感知器是当时首个可以学习的ANN。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助并认为神经网络比“原子弹工程”更重要。**直到1969年，这个时期可以看作神经网络的第一次高潮。**

推荐一篇文章，写的深入浅出、很浅显易懂。详见：[神经网络浅讲：从神经元到深度学习](http://blog.csdn.net/u013527419/article/details/70198499)

## 感知机

1.感知机（Perceptron），线性二分类模型，属于判别模型： 
**输入：**各个sample的特征向量，**输出：**+1或者是-1

（1）. 感知机的**模型：**
![这里写图片描述](https://img-blog.csdn.net/20170424111556448?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

解释：线性方程wx+b=0对应于**特征空间**中的一个分离超平面（separating hyperplane），其中w为超平面的法向量，b是超平面的截距。**该平面将数据点分为正，负两类**
![这里写图片描述](https://img-blog.csdn.net/20170424112245817?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**感知机的目标就是找到最好的线性超平面**
（2）. 感知机的**策略：**

**感知器的前提认为数据集线性可分**（即存在超平面将所有样本点分成两类）。 

这里一个自然的选择是，用误分点的总数作为损失函数，但问题是这个损失函数和w，b没关系，不易优化 ，所以这里选择误分点到超平面的总距离作为损失函数，这样损失函数对于w，b是连续可导的，这样就可以使用梯度下降来找到最优解 。

![这里写图片描述](https://img-blog.csdn.net/20170424114035140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

损失函数是非负，如果没有误分点，为0，误分点越少，离超平面越近，损失函数值越小。
（3）.感知机的**算法：**

**随机梯度下降法，有原始形式和对偶形式两种。**
![这里写图片描述](https://img-blog.csdn.net/20170424203602611?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
SGD只是用一个样本点的损失函数的偏导值来修正w和b，效率会高。但问题是，这次修正只是减小对该样本点的损失值，而非所有样本点的整体的损失值，也就是所这次修正是对于该样本点的局部最优，而非对整个样本集的全局最优。所以随机梯度下降，会导致下降过程的震荡，但往往可以逼近全局最优 。

![这里写图片描述](https://img-blog.csdn.net/20170424203839218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)






