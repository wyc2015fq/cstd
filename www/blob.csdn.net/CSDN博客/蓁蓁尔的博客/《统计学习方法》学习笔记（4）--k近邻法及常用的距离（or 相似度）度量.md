# 《统计学习方法》学习笔记（4）--k近邻法及常用的距离（or 相似度）度量 - 蓁蓁尔的博客 - CSDN博客





2016年07月20日 11:20:20[蓁蓁尔](https://me.csdn.net/u013527419)阅读数：3702








一、k近邻法基础知识 

    1. 特征空间中两个实例点的距离反应了两个实例点的相似程度。 

   2. k近邻模型三要素 = 距离度量（有不同的距离度量所确定的最邻近点不同）+k值的选择（应用中，k值一般取一个比较小的值，通常采用交叉验证法来确定最优k值）+分类决策规则（往往是多数表决规则（majority voting rule），此规则等价于经验风险最小化） 

    3. 在训练数据量太大或者是维数很高时，显然线性扫描（linear scan）耗时太大，不可取。其中一个办法就是构建kd树（空间划分树，实际上是一种平衡二叉树）实现对训练数据的快速k近邻搜索。 

二、距离度量相关
> 
Note：根据数据特性的不同，可以采用不同的度量方法。一般而言，定义一个距离函数 d(x,y), 需要满足下面几个准则： 

  1) d(x,x) = 0                    // 到自己的距离为0 

          2) d(x,y) >= 0                  // 距离非负 

          3) d(x,y) = d(y,x)                   // 对称性: 如果 A 到 B 距离是 a，那么 B 到 A 的距离也应该是 a 

          4) d(x,k)+ d(k,y) >= d(x,y)    // 三角形法则: (两边之和大于第三边)- 闵可夫斯基距离(Minkowski Distance)又称Lp距离，闵氏距离不是一种距离，而是一组距离的定义。 

两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：  
![这里写图片描述](https://img-blog.csdn.net/20160720212226295)

    其中p是一个变参数，根据变参数的不同，闵氏距离可以表示一类的距离。  

    当p=1时，就是曼哈顿距离，又称L1距离或者是程式区块距离（city block distance）等。 

    当p=2时，就是欧氏距离，又称L2距离，是直线距离。 

    当p→∞时，就是切比雪夫距离 
![这里写图片描述](https://img-blog.csdn.net/20160720212332358)
**闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。**

　　举个例子：二维样本(身高,体重)，其中身高范围是150~190，体重范围是50~60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。 

   简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。- 欧式距离 

两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离： 
![这里写图片描述](https://img-blog.csdn.net/20160720212506327)

　　也可以用表示成向量运算的形式： 
![这里写图片描述](https://img-blog.csdn.net/20160720212541671)- 标准化欧氏距离(Standardized Euclidean distance ) 

    标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。 

    假设样本集X的数学期望或均值(mean)为m，标准差(standard deviation，方差开根)为s，那么X的“标准化变量”X*表示为：(X-m）/s，而且标准化变量的数学期望为0，方差为1。 

即，样本集的标准化过程(standardization)用公式描述就是： 
![这里写图片描述](https://img-blog.csdn.net/20160720212603265)

    经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：　　 
![这里写图片描述](https://img-blog.csdn.net/20160720212620859)

    如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。 - 夹角余弦(Cosine) ，几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。 

(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式： 
![这里写图片描述](https://img-blog.csdn.net/20160720212736080)

(2) 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦 

类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。　　 
![这里写图片描述](https://img-blog.csdn.net/20160720212749657)

即： 
![这里写图片描述](https://img-blog.csdn.net/20160720212801987)

夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。 

举一个具体的例子，假如新闻X和新闻Y对应向量分别是： x1, x2, …, x6400 和 y1, y2, …, y6400。则，它们之间的余弦距离可以用它们之间夹角的余弦值来表示： 
![这里写图片描述](https://img-blog.csdn.net/20160720212820019)

当两条新闻向量夹角余弦等于1时，这两条新闻完全重复（用这个办法可以删除爬虫所收集网页中的重复网页）；当夹角的余弦值接近于1时，两条新闻相似（可以用作文本分类）；夹角的余弦越小，两条新闻越不相关。- 
余弦距离与欧式距离对比

> 
余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小。 

  相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。


借助三维坐标系来看下欧氏距离和余弦距离的区别： 
![这里写图片描述](https://img-blog.csdn.net/20160720213105679)

从上图可以看出，欧氏距离衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关；而余弦距离衡量的是空间向量的夹角，更加体现在方向上的差异，而不是位置。
> 
如果保持A点位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦距离cosθ是保持不变的（因为夹角没有发生变化），而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦距离之间的不同之处。 
**欧氏距离和余弦距离各自有不同的计算方式和衡量特征，因此它们适用于不同的数据分析模型：**

  欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异。 

  余弦距离更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）。- 
调整余弦相似度算法（Adjusted Cosine Similarity） 

余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，因此没法衡量每个维度上数值的差异，会导致这样一种情况： 

用户对内容评分，按5分制，X和Y两个用户对两个内容的评分分别为（1,2）和（4,5），使用余弦相似度得到的结果是0.98，两者极为相似。但从评分上看X似乎不喜欢2这个 内容，而Y则比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3，那么调整后为（-2，-1）和（1,2），再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。- 杰卡德相似系数(Jaccard similarity coefficient) 

广义Jaccard相似度可参考，此处略：[http://blog.csdn.net/xceman1997/article/details/8600277](http://blog.csdn.net/xceman1997/article/details/8600277)

1） 杰卡德相似系数 

两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。　 
![这里写图片描述](https://img-blog.csdn.net/20160720213126883)

杰卡德相似系数是衡量两个集合的相似度一种指标。 

2）杰卡德距离 

与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。 

杰卡德距离可用如下公式表示：　　 
![这里写图片描述](https://img-blog.csdn.net/20160720213140633)

杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。 

3） 杰卡德相似系数与杰卡德距离的应用 

可将杰卡德相似系数用在衡量样本的相似度上。 

样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。 

例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。 

p ：样本A与B都是1的维度的个数 

q ：样本A是1，样本B是0的维度的个数 

r ：样本A是0，样本B是1的维度的个数 

s ：样本A与B都是0的维度的个数 

那么样本A与B的杰卡德相似系数可以表示为：这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。而样本A与B的杰卡德距离表示为： 
![这里写图片描述](https://img-blog.csdn.net/20160720213158743)

这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。 

4）杰卡德相似度算法分析 

杰卡德相似度算法没有考虑向量中潜在数值的大小，而是简单的处理为0和1，不过，做了这样的处理之后，杰卡德方法的计算效率肯定是比较高的，毕竟只需要做集合操作。

参考： 

余弦距离、欧氏距离和杰卡德相似性度量的对比分[](http://www.cnblogs.com/chaosimple/archive/2013/06/28/3160839.html)http://www.cnblogs.com/chaosimple/archive/2013/06/28/3160839.html

不同相关性度量方法的线上效果对比与分析 [](http://blog.sina.com.cn/s/blog_4b59de07010166z9.html)http://blog.sina.com.cn/s/blog_4b59de07010166z9.html

从K近邻算法、距离度量谈到KD树、SIFT+BBF算法[](http://blog.csdn.net/v_july_v/article/details/8203674)http://blog.csdn.net/v_july_v/article/details/8203674

机器学习中的相似性度量[](http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html)http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html

漫谈：机器学习中距离和相似性度量方法[](http://www.cnblogs.com/daniel-D/p/3244718.html)http://www.cnblogs.com/daniel-D/p/3244718.html
















