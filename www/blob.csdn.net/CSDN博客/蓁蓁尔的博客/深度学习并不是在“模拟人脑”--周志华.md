# 深度学习并不是在“模拟人脑”--周志华 - 蓁蓁尔的博客 - CSDN博客





2017年01月01日 14:39:17[蓁蓁尔](https://me.csdn.net/u013527419)阅读数：2977








2016年12月18日，KDD China技术峰会在深圳举行，周志华教授当天做报告《关于机器学习研究的讨论》。详细见：[http://mp.weixin.qq.com/s/5YZi2NONhLT5F5Hhbk5psg](http://mp.weixin.qq.com/s/5YZi2NONhLT5F5Hhbk5psg) 下面是自己理解了的部分，如果文章中有错误，please have no hesitate to tell me， 感谢！ 
![这里写图片描述](https://img-blog.csdn.net/20170101142159380?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 1. 机器学习不是万能的

（1）特征信息不充分：例如重要特征数据没有 

（2）数据信息不充分：例如样本数据很少 

以上两种情况下，机器学习无能为力
## 2. 机器学习算法哪个好？

具体问题、具体分析。在一个问题上A算法优于B算法，一定存在另一个问题B算法优于A算法。机器学习中“问题”的理解：一个“问题”，一定是说输入描述的属性确定了，这个数据的分布是怎么样，这时候才定义出来一个“问题”。

## 3. 机器学习做的事情，是你给我数据之后，希望能够以很高的概率给出一个好模型。

![这里写图片描述](https://img-blog.csdn.net/20170101142818904?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 4. 深度学习并不是在“模拟人脑”

深度学习就是一个多层神经网络。这个现象其实在1943年的时候，就有芝加哥大学的两位学者创立了M-P 模型，把它形式化出来。我们可以看到神经网络本质上是一个简单函数通过多层嵌套叠加形成的一个数学模型，**背后其实是数学和工程在做支撑**。而神经生理学起的作用，可以说是给了一点点启发，但是远远不像现在很多人说的神经网络研究受到神经生理学的“指导”，或者是“模拟脑”。再比如说BP（反向传播）算法，这个是1986年被重新发明出来的，完全是从数学上推导出来的，它和神经生理学基本没有联系。 

今天我们再看使用最多的深度学习模型之一：卷积神经网络，它也是这样很多层。除了原来的基本操作之后，还引入了一些操作，比如说这个信号处理里面的卷积，卷积其实是起到了一定的时间、空间的平移不变性。还有采样，把一个区域的值用一个值代替，这是数据挖掘里对噪声进行平滑的基本技术，也是缩减计算量的基本技术。所以这些都是常见的操作。 

我们可以说最早的神经网络受到一点点启发，但完全不能说是“模拟人脑”之类的。那么深度学习的层数很深了，是不是就模拟了呢？我在此引用一下Yann LeCun的说法，大家都知道LeCun是国际上深度学习领域非常著名的3位学者之一，他对“深度学习造成人工智能威胁”的说法不赞成。他尤其这样说：
> 
对深度神经网络，“我最不喜欢的描述是‘它像大脑一样工作’。我不喜欢人们这样说的原因是，虽然深度学习从生命的生物机理中获得灵感，但它与大脑的实际工作原理差别非常非常巨大。将它与大脑进行类比给它赋予了一些神奇的光环，这种描述是很危险的，这将导致天花乱坠的宣传，大家在要求一些不切实际的事情。”


## 5. 深度学习火起来的3个因素

（1）有了大量的训练数据。 

（2）有了很强的计算设备。 

（3）里面要使用大量的“窍门”（Trick）。
> 
深度学习里面有大量的Trick，所以今天来看就有点像老中医在治病一样，虽然能治病，但是什么东西是有用的，什么是没有用的，什么是起副作用的，都不太清楚，笼统地混到一起，有些浑水摸鱼的味道。这里面理论研究远远没有跟上，因为应用尝试比较容易。现在有很多深度学习架构，让大家很方便，新手学习个十天半个月就可以上手调试不同的模型做应用了，性能有提高就很快发表文章。但是理论研究的门槛很高，先要训练四五年可能才能开始做事情。这就造成很多应用方面的尝试，报道说这样好、那样好，但是能做理论的人很少，来不及去研究，而且因为很少有共性的东西。不同的人哪怕用的都是CNN，其实模型完全不同，做理论的也不知道从哪里去下手才不是浪费时间。这些问题要解决，需要有更多的人沉下心来研究基础问题，基础问题弄明白了，反过来可以更大地促进应用。








