# 神经网络浅讲：从神经元到深度学习 - 蓁蓁尔的博客 - CSDN博客





2017年04月16日 21:45:35[蓁蓁尔](https://me.csdn.net/u013527419)阅读数：1756








原文：[神经网络浅讲：从神经元到深度学习](http://www.cnblogs.com/subconscious/p/5058741.html)

这篇文章介绍很好，深入浅出、很容易懂但是该有的东西都讲到了，一口气看下来会觉得很爽。因为文章很长所以下面主要是上文中的一些摘抄。

## 1.

神经网络的“三起三落” 
![这里写图片描述](https://img-blog.csdn.net/20170416211440494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。**

神经元： 
![这里写图片描述](https://img-blog.csdn.net/20170416211950538?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20170416212024143?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**影响：**

1943年发布的MP模型，简单却建立了神经网络大厦的地基。但MP模型中，权重的值都是预先设置的，因此不能学习。1949年心理学家**Hebb提出了Hebb学习率**，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。
## 2.

**1958年，**（李航老师《统计学习方法中》说到的是**1957**年）计算科学家Rosenblatt提出了由两层神经元组成的神经网络–**“感知器”（Perceptron）**–单层神经网络。感知器是当时首个可以学习的ANN。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助并认为神经网络比“原子弹工程”更重要。直到1969年，这个时期可以看作神经网络的**第一次高潮。**

Perceptron在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。 
![这里写图片描述](https://img-blog.csdn.net/20170416213011486?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**问题：**

感知器只能做简单的线性分类任务。当人工智能领域的巨擘Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。  　　 

Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。 

由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为“AI winter”。 接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。
## 3.

Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。**1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法**，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。 
**需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active function）。**

**为什么两个线性分类任务结合就可以做非线性分类任务？**

关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。  　　 

这样就导出了两层神经网络可以做非线性分类的关键–隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。  　　 

两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。
**影响：**

两层神经网络已经可以发力于语音识别，图像识别，自动驾驶等多个领域。神经网络的学者们再次登上了《纽约时报》的专访。但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。  　　 

90年代中期，由Vapnik等人发明的SVM算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。SVM迅速打败了神经网络算法成为主流。神经网络的研究再次陷入了冰河期。
## 4

多层神经网络（深度学习），**2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。**与传统的训练方式不同，“深度信念网络”有一个**“预训练”（pre-training）**的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。 

目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。**神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。**

**现在深度学习为什么这么火？**

简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。 
![这里写图片描述](https://img-blog.csdn.net/20170416214434051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)







