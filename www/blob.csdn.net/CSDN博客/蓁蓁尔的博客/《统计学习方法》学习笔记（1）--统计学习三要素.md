# 《统计学习方法》学习笔记（1）--统计学习三要素 - 蓁蓁尔的博客 - CSDN博客





2016年07月25日 11:20:22[蓁蓁尔](https://me.csdn.net/u013527419)阅读数：1659








**统计学习方法 = 模型、策略、算法。**

统计学习方法之间的不同，主要来自于其模型、策略、算法的不同。确定了模型、策略和算法，统计学习方法也就确定了。

**简析：**

**模型**：假设空间。 
**策略**：从假设空间中选择模型的准则，也就是要最大/最小化的loss function。 
**算法**：求解上loss function得到模型的参数的方法。
**Note:** 以下以监督学习为基础来进行论述。非监督学习和强化学习同样也拥有这三要素。

**1 模型**

在监督学习当中，我们的目的是学习一个由输入到输出的映射，这个映射就是模型。一般来说，模型有两种形式，一种是概率模型（由条件概率分布表示的模型），另一种形式是非概率模型（由决策函数表示的模型）。我们根据实际情况和具体的学习方法来决定是用概率模型还是用非概率模型。 

（模型的假设空间（hypothesis space）是一集合：由输入空间到输出空间所有映射的集合，包含所有可能的条件概率分布或决策函数。

**2.策略**

统计学习的目标在于从假设空间中选取最优模型。策略即为按照什么样的准则或方法来找到这个最优模型。首先介绍损失函数和风险函数。 
![这里写图片描述](https://img-blog.csdn.net/20160725111234270)
**2）风险函数**
> 
风险函数（期望损失）可以度量平均意义下模型预测的好坏。也就是对于整个输入输出空间的损失函数的期望。 
![这里写图片描述](https://img-blog.csdn.net/20160725111410273) 这是理论上模型f(x)关于联合分布P(X, Y)的平均意义下的损失，称为风险函数（risk function）或期望损失（expected 

  loss）。需要特别说明一下。在监督学习当中，我们假设输入和输出的随机变量和服从联合概率分布P(X, 

  Y)。但是这个分布对于我们来说，是不可知的，如果可知，我们就可以直接通过P(X, Y)来求解P(Y/X)，而不需要使用统计学习方法了。 

  显然而然，对应我们来说，学习的目标就是选择期望风险最小的模型（即最好的策略就是找到让风险函数最小的模型）。可是呢，风险函数的计算需要用到P(X, 

  Y)，我们对它又不可知，所以监督学习就成了一个病态问题（ill-formed problem）。 

  ———在统计学中有一个大数定律，如果我在输入输出空间中取一个足够大的样本，用这个样本来近似的计算风险函数R_{exp}(f)。基于这样的想法，我们对于含有N组数据的训练集，定义经验损失函数： 

  模型f(x)关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记作Remp。 
![这里写图片描述](https://img-blog.csdn.net/20160725111432874)

      期望风险Rexp(f)是模型关于联合分布的期望损失，经验风险Remp(f)是模型关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险Rempf(x)趋于期望风险Rexpf(x)，所以一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目有限甚至很小，所以用经验风险估计期望风险常常并不理想，常常会导致过拟合。为了防止过拟合现象，结构风险最小化这个策略被提了出来。
**3）经验风险最小化**

> 
在假设空间，损失函数以及训练数据集确定的情况下，经验风险函数式就可以确定，经验风险最小化（empirical risk 

  minimizatiion, ERM）的策略认为，经验风险最小的模型是最优模型。 
![这里写图片描述](https://img-blog.csdn.net/20160725111551109) 当样本容量是够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛应用，比如，极大似然估计（maximum likelihood 

  estimation）就是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合（over-fitting）”现象。
**4）结构化风险最小化**

> 
结构化风险最小化（structural risk minimization, 

  SRM）是为了防止过拟合而提出来的策略。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty 

  term）。在假设空间，损失函数以及训练数据集确定的情况下，结构风险的定义是： 
![这里写图片描述](https://img-blog.csdn.net/20160725111735667)

      其中J(f)为模型的复杂度，是定义在假设空间 F 上的泛函，模型 f 越复杂，复杂度J(f)就越大；反之，模型 f 越简单，复杂度J(f)就越小，也就是说，复杂度表示了对复杂模型的惩罚，λ≥0是系数，用以权衡经验风险和模型复杂度，结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。 

      结构风险最小化的策略认为结构风险最小的模型是最优的模型： 
![这里写图片描述](https://img-blog.csdn.net/20160725111755532)
**5）统计学习中常用策略**

> 
综上，我们在统计学习中的策略一般有两种——经验风险最小化，结构风险最小化。此时，我们就把统计学习问题转为了求解下面目标函数的优化问题： 
![这里写图片描述](https://img-blog.csdn.net/20160725111922419) 或者： 
![这里写图片描述](https://img-blog.csdn.net/20160725111940248)
**3.算法**

        算法是指学习模型的具体计算方法，统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方式求解最优模型。这时，统计学习问题归结为最优化问题，统计学习的方法成为求解最优化问题的算法。













