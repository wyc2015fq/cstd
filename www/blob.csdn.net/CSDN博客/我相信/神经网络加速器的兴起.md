# 神经网络加速器的兴起 - 我相信...... - CSDN博客





2018年02月07日 00:00:00[半吊子全栈工匠](https://me.csdn.net/wireless_com)阅读数：2834









自从投身智能硬件以来，又开始重新关注嵌入式领域的相关技术。这是“2018嵌入式处理器报告: 神经网络加速器的兴起”(http://www.embedded-computing.com/processing/2018-embedded-processor-report-rise-of-the-neural-network-accelerator，作者 BRANDON LEWIS) 的短译文。









人工智能和机器学习(ML)的应用是嵌入式处理器的下一个巨大市场机遇。 然而, 传统的处理解决方案并不是为了计算神经网络的工作负载, 而这些工作负荷是许多应用动力基础, 因此需要新的架构来满足不断增长的智能需求。




随着数以十亿计的连接传感器节点被部署在物联网上, 有一件事已经变得清晰起来: 自动化无处不在。 考虑到物联网系统的天然特点, 许多物联网系统具有重要的经济、生产力和安全意义, 这种需求超越了简单规则引擎或程序设计阈值的使用。 从而, 业界已经转向人工智能和机器学习。




如今的许多人工智能和机器学习应用都依赖于人工神经网络, 人工神经网络是一系列算法，这些算法对不同层面的数据集来定义特性，进而组织成一系列结构化层次。 这些网络最初是建立在高性能计算平台上的, 这些平台使算法根据特定的参数做出决策或预测。 然后对该算法进行优化, 并将其移植到嵌入的目标中, 并根据在该领域收到的输入数据进行推理。





不同的嵌入式处理解决方案根据具体应用程序来执行神经网络算法, 这给 AI 和 ML 开发者留下了一系列可供选择项。 但是, 正如Linely Group的高级分析师、"A Guide to Processors for Deep Learning"一书的合著者 Mike Demler 所说的那样, 这些方案在权力、性能和成本方面都有权衡。






他说，"根本就没有一种嵌入式的 AI 处理器。神经网络引擎可能使用 cpu, dsp, gpu, 或者专门的深度学习加速器, 或者组合。"



*"趋势肯定是引入 cpu、 gpu 和 dsp 的加速器， 原因是它们比其他通用核更具有多领域和低功耗应用能力。 随着标准的开放式深度学习框架的使用, 如 Caffe 和 TensorFlow 的增加, 以及像 GoogleNet 和 ResNet 这样随时可用的开源网络, IP 供应商更容易设计专门用于运行各种神经网络层的硬件。 这就是为什么许多加速器不断增加越来越大的乘数累加器阵列, 因为神经网络中的大部分计算都是 MAC。"*




**人工智能工作负载的新型架构**





对于神经网络工作负载的 IP 供应商而言，一个主要侧重点是灵活性, 这样才能针对不断变化的人工智能市场的需求。 这方面的一个例子可以在 CEVA 最近发布的 NeuPro AI 处理器架构中找到, 该架构包括一个完全可编程的矢量处理单元(VPU) , 以及矩阵乘法和计算激活、池、卷积和完全连接的神经网络层(图1)。



![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/DE2dk1Gjczr7UgF585bCLabP7IIonW0gnyMwokUbXYsYVOgDgQqwLNxRsJJcnsZD2sMrIxTZ7h1Qj0ibF1ic97Cg/640?wx_fmt=png)

图1

处理神经网络工作负载的一个共同挑战是对内存进行大型数据集交换。 为了克服这个问题, NeuPro 体系结构采用了直接存储控制器(DMA) , 以提高 DDR 带宽的利用率。




其中一个更有趣的特性是能够动态地调整解析精度, 以适应各个网络层的精确要求。 根据 CEVA 成像和计算机视觉产品营销总监 Liran Bar 的说法, 这有助于最大限度地提高神经网络推理的准确性。




"并不是所有的图层都需要同样的精度。 事实上, 许多商业神经网络需要16位分辨率来保持高精度, 但同时8位对于某些层来说已经足够了,"巴尔说。 "NeuPro 提前确定每一个8位或16位层的精度, 以便充分灵活性。 例如, 当使用 NP4000产品时, 允许在运行时动态选择40008x8,204816x8或102416x16 MACs。"




在Imagination Technologies的 PowerVR Series2NX 中也有类似的功能, 这是一个神经网络加速器(NNA) , 其本地支持深度为4位。 Powervr Series2NX 采用动态缩放到极端, 但是, 支持4-, 5-, 6-, 7-, 8-, 10-, 12-, 和16位分辨率的相同核心的更高精度(图2)。






![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/DE2dk1Gjczr7UgF585bCLabP7IIonW0gVYsjvia2BmWq9jGiaCNfIUib6W3PcicFUnL1dbXHW2DyxsmZYxTF0viaOoA/640?wx_fmt=png)

图2




"人们可以把 NNA 架构看作是张量处理管道,"Imagination Technologies 公司 Vision 和 AI 副总裁罗素 · 詹姆斯说。 "它有一个针对大张量(输入数据和权重)快速卷积优化了的神经网络计算引擎, 辅之以各种其他单元的表现元素和张量操作, 如激活、汇集和规范化。 该体系结构还使用了一个优化的数据库, 可以将操作分组成传递, 从而最小化外部内存访问。"




换格式的能力, 这使得异构系统在神经网络处理中占据了先机。 Imagination Technologies提供了一个网络开发工具包(NDK) , 用于评估核, 其中包含将神经网络映射到 NNA 的工具, 调整网络模型, 以及转换在诸如 Caffe 和 TensorFlow 等框架中开发的网络。




**神经网络处理: 全员参与**




除了IP供应商, 大型芯片制造商也继续充分利用人工智能的工作负载。 NVIDIA Tegra 和 Xavier SoCs 将 CPU、 GPU 和自主驾驶系统的自定义深度学习加速器结合在一起, 而高通公司则继续在 Hexagon DSP 中构建机器学习功能。 就英特尔而言, 他们已经进行了长达18个月的疯狂采购, 收购了 Mobileye、 Movidius 和神经网络技术, 为各个市场开发神经网络技术。 甚至谷歌(Google)也创建了一个 TPU 。






这些公司都采用了不同的方法来处理神经网络工作负载, 每个架构处理的使用场景略有不同。 当开发者涉及到人工智能的时候, 当然是选择越多越好了。







![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/DE2dk1GjczoZGKwk1f08kMXcVbwM3mVauyFumwIWKe4fdVRib6UqUDqticBkkCV0xHcMoMIK4A67d9SA5TDwODOA/640?wx_fmt=jpeg)










