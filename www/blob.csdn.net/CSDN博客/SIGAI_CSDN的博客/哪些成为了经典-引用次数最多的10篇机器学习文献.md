
# 哪些成为了经典-引用次数最多的10篇机器学习文献 - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

2019年01月23日 16:03:09[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：137


近40年来机器学习领域产生了数以万计的论文，并以每年上万篇的速度增长。但真正能够称为经典、经受住历史检验、能投入实际应用的并不多。本文整理了机器学习历史上出现的经典论文，按照被引用次数对它们进行了排序，分为top10，被引用次数超过2万，被引用次数超过1万，未来有潜力的文章4部分。它们已经或者在未来具有资格被写入机器学习、深度学习、人工智能的教科书，是一代又一代研究人员为我们留下的宝贵财富。需要说明的是，引用次数对近几年新出现的文章是不公平的，它们还处于高速增长期，但好酒就是好酒，随着时间的沉淀会越来越香。

引用次数最高的10篇文献
第1名-EM算法
Arthur P Dempster, Nan M Laird, Donald B Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the royal statistical society series b-methodological, 1976.
**被引用次数：55989**
令笔者惊讶的是排名第一的居然不是支持向量机，集成学习，深度学习，决策树等历史上赫赫有名的算法，而是EM。这是EM算法的原文，引用次数高达5万多！EM算法在很多版本的排名中都被称为机器学习的10大算法之一。它在数学上优美，实现起来也很简单，是求解含有隐变量的最大似然估计、最大后验概率估计的有力工具，在高斯混合模型，隐马尔可夫模型等问题上得到了成功的应用。在SIGAI之前的公众号文章“理解EM算法”中对其原理进行了详细的介绍。
第2名-logistic回归
David W Hosmer, Stanley Lemeshow. Applied logistic regression. Technometrics. 2000.
**被引用次数：55234**
代表了线性模型这一山头。这不是logistic回归的原文，logistic回归在这之前几十年就已经被提出，但这篇文献的引用次数却达到了，虽然它不是论文而是书的形式，但其引用次数比著名的PRML还要高。这也符合我们的直观认识，logistic回归虽然简单，但却实用，在工程上，往往是越简单的东西越有用。
第3名-随机森林
Breiman, Leo. Random Forests. Machine Learning 45 (1), 5-32, 2001.
**被引用次数：42608**
代表了集成学习这一大山头。Breiman的随机森林，分类与回归树分列第3/4名。而随机森林的排名比AdaBoost算法要高。同样的，随机森林也很简单，但却好用。在SIGAI之前的公众号文章“随机森林概述”中对集成学习，bagging，随机森林进行了详细的介绍。
第4名-分类与回归树
Breiman, L., Friedman, J. Olshen, R. and Stone C. Classification and Regression Trees, Wadsworth, 1984.
**被引用次数：39580**
这是分类与回归树的原文，代表了决策树这一山头。在各种决策树中，分类与回归树（CART）应当是用的最广的，现在还被用于充当随机森林，AdaBoost，梯度提升算法的弱学习器。Breiman老爷子在2005年已经逝去，但他留给我们大片的树和森林。在SIGAI之前的公众号文章“理解决策树”中对这一算法进行了详细的介绍。
第5名-支持向量机开源库libsvm
C.-C. Chang and C.-J. Lin. LIBSVM: a Library for Support Vector Machines. ACM TIST, 2:27:1-27:27, 2011.
**被引用次数：38386**
这篇文章介绍了libsvm开源库。引用次数超过了支持向量机的原文，应该算是公开的最经典的支持向量机实现，其作者是台湾大学林智仁教授及其学生。相信很多做机器学习研究和产品的同学都用过它。在SIGAI之前的公众号文章“用一张理解SVM”，“理解SVM核函数和参数的作用”中对SVM进行了详细的介绍。
第6名-统计学习理论
An overview of statistical learning theory. VN Vapnik - IEEE transactions on neural networks
**被引用次数：36117**
Top10中唯一一篇理论层面的文章，出自Vapnik之手。他最有影响力的成果是支持向量机，VC维。但机器学习理论文章，整体来说引用次数相对较少，应该与做这些方向的研究者更少，文章更少有关，大部分人还是在做某些具体的算法。
第7名-主成分分析
Ian T. Jolliffe. Principal Component Analysis. Springer Verlag, New York, 1986.
**被引用次数：35849**
代表了降维算法这一山头。这篇文献不是主成分分析的原文，其原文发表于1个多世纪以前。这个排名对得起主成分分析的江湖地位，在各种科学和工程数据分析中，PCA被广为应用。在SIGAI之前的公众号文章“理解主成分分析（PCA）”中对PCA进行了介绍。
第8名-决策树树-C4.5
J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco, CA, 1993.
**被引用次数：34703**
又是决策树。决策树简单实用，可解释性强，是机器学习早期的重要成果。
第9名-深度卷积神经网络
Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton. ImageNet Classification with Deep Convolutional Neural Networks. 2012.
**被引用次数：34574**
代表了深度学习这一山头。深度卷积神经网络的开山之作，将严乐村的卷积神经网络发扬光大。这篇2012年才发表的文章能有如此的引用次数实属不易，刚酿造出来的酒就是名酒，出自Hinton之手这也不奇怪。如此的被引用次数是当前炙手可热的深度学习造就的。同样的，没有复杂的公式和理论，但却出奇的好用。
第10名-支持向量机
Cortes, C. and Vapnik, V. Support vector networks. Machine Learning, 20, 273-297, 1995.
**被引用次数：33540**
代表了线性模型、核技巧的山头，这是SVM正式的原文。支持向量机才排到第10位让人有些奇怪，它可是在机器学习的江湖中风光了近20年的算法，当年言必称SVM。
总结这top10的文献可以看出，简单才是美。这些文献提出的算法没有复杂的数学公式和晦涩难解的理论，但确实最经典的，因为有用！它们体现的是更深层次的哲学思想。其实在其他科学领域也是如此，数学领域中最经典的一些定理和公式也是非常的优美而简洁，类似的还有物理。在top10中，Breiman和Vapnik两次上榜。

引用次数超过2万的文献
除了top10之外，还有一些被引用次数超过2万的文章 ，也堪称经典。
Lawrence R. Rabiner. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE. 77 (2): 257–286. 1989.
**被引用次数：26466**
代表了概率图模型这一山头。终于见到了概率图模型，过去几十年中，引用最广的概率图模型当属隐马尔可夫模型（HMM）。这篇文章不是HMM的原文，但却写成了经典，对HMM的原理，在语音识别中的建模方法讲解得清晰透彻。
MacQueen, J. B. Some Methods for classification and Analysis of Multivariate Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. 1. University of California Press. pp. 281–297, 1967
**被引用次数：24035**
代表了聚类算法的山头。k均值算法的开山之作，它也在各种排名中都被称为机器学习10大经典算法，同样是简单而易于理解，我相信中学生都能看懂它！
J. Ross Quinlan. Induction of decision trees. Machine Learnin, 1(1): 81-106, 1986.
**被引用次数：20359**
介绍决策树的文献，不过多解释，地位摆在这里。Quinlan也是决策树的一大山头。

引用次数超过1万的文献
Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500). 2000: 2323-2326.
**被引用次数：12941**
Tenenbaum, Joshua B and De Silva, Vin and Langford, John C. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500). 2000: 2319-2323.
**被引用次数：11927**
流形学习的双雄，两篇代表作，开这一领域之先河。流形学习是当年非常热门的方向。这两篇文章都发在Science上，要知道，计算机科学的论文发Science和Nature是非常难的。在SIGAI之前的公众号文章“流形学习概述”中对这类算法进行了介绍。
Ronald A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7 Part 2: 179-188, 1936.
**被引用次数：15379**
线性判别分析的原文，1936年就已经发表了，那时候二战还没有爆发。
Burges JC. A tutorial on support vector machines for pattern recognition. Bell Laboratories, Lucent Technologies, 1997.
**被引用次数：19885**
介绍支持向量机在模式中应用的文章，SVM当年真是灌水的好方向！
Yoav Freund, Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. computational learning theory. 1995.
**被引用次数：16431**
AdaBoost算法的经典之作，与SVM并列为当年的机器学习双雄。这是集成学习第一个有广泛影响力的算法，在人脸检测等问题上取得了成功。在SIGAI之前的公众号文章“大话AdaBoost算法”，“理解AdaBoost算法”中对它进行了详细的介绍。
Lafferty, J., McCallum, A., Pereira, F. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proc. 18th International Conf. on Machine Learning. Morgan Kaufmann. pp. 282–289. 2001.
**被引用次数：11978**
条件随机场的经典之作，这种方法在自然语言处理，图像分割等问题上得到了成功的应用，如今还被与循环神经网络整合在一起，解决自然语言处理等领域中的一些重点问题。
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by back-propagating errors. Nature, 323(99): 533-536, 1986.
**被引用次数：16610**
严格意义上的反向传播算法的原文，发在Nature上，重要性就不解释了。现在的深度学习还是使用它。Hinton的名字再一次出现。在SIGAI之前的公众号文章“反向传播算法推导-全连接神经网络”，“反向传播算法推导-卷积神经网络”中进行了详细的讲解。
Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators. Neural Networks, 2, 359-366, 1989.
**被引用次数：16394**
神经网络的理论文章，著名的万能逼近定理，从理论上证明了至少有1个隐含层的神经网络尅逼近闭区间上任意连续函数到任意指定精度，为神经网络和深度学习提供了强有力的理论保障。
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, november 1998.
**被引用次数：16339**
LeNet网络的原文，被引用次数比严乐村同志在1989年，1990年提出卷积神经网络的论文还多。也让严乐村得到了卷积神经网络之父的称号。
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Going Deeper with Convolutions, Arxiv Link: http://arxiv.org/abs/1409.4842.
**被引用次数：11268**
GoogLeNet网络的原文，做深度学习的同学都知道。发表于2015年的文章能有如此的引用次数，当然得利于深度学习的火爆。
K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. international conference on learning representations. 2015.
**被引用次数：18980**
VGG网络的原文，经典的卷积网络结构，被用在各个地方，引用次数比GoogLeNet多不少。
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. computer vision and pattern recognition, 2015.
**被引用次数：17285**
残差网络的原文，做深度学习的同学都知道，终于用中国人名字上榜，加油！
S. Hochreiter, J. Schmidhuber. Long short-term memory. Neural  computation, 9(8): 1735-1780, 1997.
**被引用次数：15448**
LSTM的原文，让循环神经网络真正走向了实用。作者在深度学习领域做出了重要的贡献，但却非常低调，以至于很多人都不知道。
Martin Ester, Hanspeter Kriegel, Jorg Sander, Xu Xiaowei. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pp. 226–231, 1996.
**被引用次数：13817**
又是聚类算法，著名的DBSCAN-基于密度的聚类算法的典型代表。这算法也非常简单，但也非常强大，没有一个超过中学数学范围之外的公式。在SIGAI之前的公众号文章“聚类算法概述”中对它进行了介绍。
Dorin Comaniciu, Peter Meer. Mean shift: a robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002.
**被引用次数：12146**
大名鼎鼎的mean shift算法，同样是及其简洁优美，但非常好用。做机器学习，机器视觉的同学肯定都知道，尤其是做视觉领域中目标跟踪算法的。

未来可能有潜力的文献
下面这些文章的被引用次数目前还没有超过1万，但它们都还很年轻，未来很有前途，因此单独列出。需要强调的是有几篇强化学习的文章虽然是1990年代发表的，但我们也列出来了，它们会随着深度学习研究的进展而逐渐体现出更重要的价值。
Goodfellow Ian, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets. Advances in Neural Information Processing Systems, 2672-2680, 2014.
**被引用次数：6902**
生成对抗网络的开山之作，代表了深度生成模型这一山头。生成对抗网络的思想简单而优美，而且有效，出现了大量改进算法和各种应用。变分自动编码器（VAE）是仅次于GAN的深度生成模型，但其原文的被引用次数远不及GAN。
Richard Sutton. Learning to predict by the methods of temporal differences. Machine Learning. 3 (1): 9-44.1988.
**被引用次数：5108**
时序差分算法的开山之作，地位就不多解释了。
Mnih, Volodymyr, et al. Human-level control through deep reinforcement learning. Nature. 518 (7540): 529-533, 2015.
**被引用次数：4570**
深度强化学习的重量级作品，出自DeepMind公司之手。第一篇文章发表于2013年，引用次数远不及这篇，这篇可是发在Nature上，开创了DQN算法。
David Silver, et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature, 2016.
**被引用次数：4123**
AlphaGo的原文，就不解释了，地球人都知道。
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
**被引用次数：8308**
Q学习的原文，奠定了这一算法的基础，也是DQN的基础。

