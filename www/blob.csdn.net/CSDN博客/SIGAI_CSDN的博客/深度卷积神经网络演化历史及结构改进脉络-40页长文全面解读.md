
# 深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读 - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

2018年06月18日 11:43:46[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：3393


![](https://img-blog.csdn.net/20180618114551228?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
[书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
[书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)
导言从1989年LeCun提出第一个真正意义上的卷积神经网络到今天为止，它已经走过了29个年头。自2012年AlexNet网络出现之后，最近6年以来，卷积神经网络得到了急速发展，在很多问题上取得了当前最好的结果，是各种深度学习技术中用途最广泛的一种。在本文中SIGAI将为大家回顾和总结卷积神经网络的整个发展过程。
早期成果
卷积神经网络是各种深度神经网络中应用最广泛的一种，在机器视觉的很多问题上都取得了当前最好的效果，另外它在自然语言处理，计算机图形学等领域也有成功的应用。
第一个真正意义上的卷积神经网络由LeCun在1989年提出[1]，后来进行了改进，它被用于手写字符的识别，是当前各种深度卷积神经网络的鼻祖。接下来我们介绍LeCun在早期提出的3种卷积网络结构。
文献[1]的网络由卷积层和全连接层构成，网络的输入是16x16的归一化图像，输出为0-9这10个类，中间是3个隐含层。这个网络的结构如下图所示：
![](https://img-blog.csdn.net/20180618114616618?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这篇文章提出了权重共享（weight sharing）和特征图像（feature map）的概念，这些概念被沿用至今，就是卷积层的原型。网络有1个输入层，1个输出层，3个隐含层构成，其中隐含层H1和H2是卷积层，H3是全连接层。网络的激活函数选用了tanh（双曲正切）函数，损失函数选用了均方误差（mean squared error）函数，即欧氏距离的均值。网络的权重用均匀分布的随机数进行初始化，训练时参数梯度值的计算采用了反向传播算法，梯度值的更新采用了在线（online）的随机梯度下降法。
文献[2]的网络结构和文献[1]类似，用于邮政编码的识别，在9%拒识率的条件下错误率为1%。网络的输入为28x28的图像，输出为0-9这10个类。整个网络有4个隐含层，其中H1为4个5x5的卷积核，输出为4张24x24的特征图像。H2为下采样层，对H1的输出结果进行2x2的下采样，得到4张12x12的图像。H3有12个5x5的卷积核，输出为12张8x8的图像，这里输出图像每个通道的多通道卷积只作用于前一层输出图像的部分通道上，为什么采用这样方式？有两个原因：1.减少参数，2.这种不对称的组合连接的方式有利于提取多种组合特征。H2和H3的连接关系如下图所示：
![](https://img-blog.csdn.net/20180618114637302?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
H4为下采样层，对H3的输出图像进行2x2的下采样，得到12张4x4的特征图像。最后为输出层，接收H4特特征图像，输出10个类别的概率。
文献[3]的网络即为大家熟知的LeNet-5网络，这是第一个被广为流传的卷积网络，奠定了现代卷积神经网络的基础。整个网络的结构如下图所示：
![](https://img-blog.csdn.net/20180618114654685?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
下面是基于LeNet-5的手写体数字识别案例：
![](https://img-blog.csdn.net/20180618140946856?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这个网络的输入为32x32的图像，整个网络有2个卷层，2个池化层，2个全连接层，一个输出层，输出层有10个神经元，代表10个数字类。卷积层C1有6个5x5的卷积核，作用于灰度图像，产生6张28x28的输出图像。池化层S2作用于C1的输出图像，执行2x2的池化，产生6张14x14的输出图像。卷积层C3有16个5x5的卷积核，每个卷积核作用于前一层输出图像的部分通道上，产生16张10x10的输出图像。C3和S2的连接关系如下图所示：
![](https://img-blog.csdn.net/20180618114709250?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
池化层S4对C3的输出图像进行2x2的池化，得到16张5x5的输出图像。全连接层C5有120个节点，全连接层F6有64个节点。
网络的激活函数选用tanh函数，损失函数采用均方误差函数，训练时采用随机梯度下降法和反向传播算法。
早期的卷积网络被用于人脸检测[4][5]，人脸识别[6]，字符识别[7]等各种问题。但并没有成为主流的方法，其原因在SIGAI公众号之前的文章“卷积神经网络为什么能称霸计算机视觉领域？”中已经分析过了，主要是梯度消失问题、训练样本数的限制、计算能力的限制3方面因素。梯度消失的问题在之前就已经被发现，对于深层神经网络难以训练的问题，文献[8]进行了分析，但给出的解决方法没有成为主流。

深度卷积神经网络
在深入分析比较当前主流深度卷积神经网络的特点之前，我们从各网络在ImageNet 2012测试数据集的准确率以及网络的参数量和计算复杂度三个维度进行分析，希望读者对当前的主流网络结构有一个整体的认知。如下图所示：
![](https://img-blog.csdn.net/20180618114718859?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
深度卷积网络的大发展起步于2012年的AlexNet网络，在这之后各种改进的网络被不断的提出，接下来我们会介绍各种典型的网络结构。

AlexNet网络
现代意义上的深度卷积神经网络起源于AlexNet网络[9]，它是深度卷积神经网络的鼻祖。这个网络相比之前的卷积网络最显著的特点是层次加深，参数规模变大。网络结构如下图所示：
![](https://img-blog.csdn.net/20180618114732738?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这个网络有5个卷积层，它们中的一部分后面接着max-pooling层进行下采样；最后跟3个全连接层。最后一层是softmax输出层，共有1000个节点，对应ImageNet图集中 1000个图像分类。网络中部分卷基层分成2个group进行独立计算，有利于GPU并行化以及降低计算量。
这个网络有两个主要的创新点：1. 新的激活函数ReLU，2. dropout机制[10]。dropout的做法是在训练时随机的选择一部分神经元进行休眠，另外一些神经元参与网络的优化，起到了正则化的作用以减轻过拟合。
网络的输入图像为的彩色三通道图像。第1个卷积层有96组11x11大小的卷积核，卷积操作的步长为4。这里的卷积核不是2维而是3维的，每个通道对应有3个卷积核（所以是一组卷积核），具体实现时是用3个2维的卷积核分别作用在RGB通道上，然后将三张结果图像相加。下图为输入为3通道，卷积层参数为2组每组3个卷积核，输出结果为2通道的动态卷积过程
![](https://img-blog.csdn.net/20180618141014588?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
第2个卷积层有256组5x5大小的卷积核，分为两个group，即每个group通道数为128组，每组有48个卷积核。第3个卷积层有384组3x3大小的卷积核，每组有256个卷积核。第4个卷积层有384组3x3大小的卷积核，分为两个group，即每个group通道数为192组，每组有192个卷积核。第5个卷积层有256组，3x3大小的卷积核，分为两个group，即每个group为128组，每组有192个卷积核。
这个网络没有使用传统的sigmoid或tanh函数作为激活函数，而是使用了新型的ReLU函数[11]：
![](https://img-blog.csdn.net/20180618141145615?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其导数为符号函数sgn。ReLU函数和它的导数计算简单，在正向传播和反向传播时都减少了计算量。由于在时函数的导数值为1，可以在一定程度上解决梯度消失问题，训练时有更快的收敛速度。当时函数值为0，这使一些神经元的输出值为0，从而让网络变得更稀疏，起到了类似L1正则化的作用，也可以在一定程度上缓解过拟合。在SIGAI公众号上一篇文章“理解神经网络的激活函数”中我们已经对激活函数做了全面深入的介绍。
ZFNet网络
文献[12]提出通过反卷积（转置卷积）进行卷积网络可视化的方法，以此分析卷积网络的效果，并指导网络的改进，在AlexNet网络的基础上得到了效果更好的ZFNet网络。
该论文是在AlexNet基础上进行了一些细节的改动，网络结构上并没有太大的突破。该论文最大的贡献在于通过使用可视化技术揭示了神经网络各层到底在干什么，起到了什么作用。如果不知道神经网络为什么取得了如此好的效果，那么只能靠不停的实验来寻找更好的模型。使用一个多层的反卷积网络来可视化训练过程中特征的演化及发现潜在的问题；同时根据遮挡图像局部对分类结果的影响来探讨对分类任务而言到底那部分输入信息更重要。下图为典型反卷积网络示意图：
![](https://img-blog.csdn.net/20180618114752836?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
ZFNet网络结构如下图所示：
![](https://img-blog.csdn.net/20180618114805399?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
ZFNet在保留AlexNet的基本结构的同时利用反卷积网络可视化的技术对特定卷积层的卷积核尺寸进行了调整，第一层的卷积核从11*11减小到7*7，将stride从4减小到2，Top5的错误率比AlexNet比降低了1.7%。
GoogLeNet网络
文献[13]提出了一种称为GoogLeNet网络的结构（Inception-V1）。在AlexNet出现之后，针对图像类任务出现了大量改进的网络结构，总体来说改进的思路主要是增大网络的规模，包括深度和宽度。但是直接增加网络的规模将面临两个问题，首先，网络参数增加之后更容易出现过拟合，在训练样本有限的情况下这一问题更为突出。另一个问题是计算量的增加。GoogLeNet致力于解决上面两个问题。
GoogLeNet由Google在2014年提出，其主要创新是Inception机制，即对图像进行多尺度处理。这种机制带来的一个好处是大幅度减少了模型的参数数量，其做法是将多个不同尺度的卷积核，池化层进行整合，形成一个Inception模块。典型的Inception模块结构如下图所示：
![](https://img-blog.csdn.net/20180618114817170?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
上图的模块由3组卷积核以及一个池化单元组成，它们共同接受来自前一层的输入图像，有三种尺寸的卷积核，以及一个max pooling操作，它们并行的对输入图像进行处理，然后将输出结果按照通道拼接起来。因为卷积操作接受的输入图像大小相等，而且卷积进行了padding操作，因此输出图像的大小也相同，可以直接按照通道进行拼接。
从理论上看，Inception模块的目标是用尺寸更小的矩阵来替代大尺寸的稀疏矩阵。即用一系列小的卷积核来替代大的卷积核，而保证二者有近似的性能。
上图的卷积操作中，如果输入图像的通道数太多，则运算量太大，而且卷积核的参数太多，因此有必要进行数据降维。所有的卷积和池化操作都使用了1x1卷积进行降维，即降低图像的通道数。因为1x1卷积不会改变图像的高度和宽度，只会改变通道数。
GoogleNet网络结构如下图所示：
![](https://img-blog.csdn.net/20180618114834424?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
GoogleNet在ILSVRC 2014的比赛中取得分类任务的第一名，top-5错误率6.67%。相较于之前的AlexNet-like网络，GoogleNet的网络深度达到了22层，参数量减少到AlexNet的1/12，可以说是非常优秀且非常实用的模型。
为了降低网络参数作者做了2点尝试，一是去除了最后的全连接层，用全局平均池化替代。全连接层几乎占据了AlexNet中90%的参数量，而且会引起过拟合，去除全连接层后模型训练更快并且减轻了过拟合。用全局平均池化层取代全连接层的做法借鉴了Network In Network（以下简称NIN）论文[16]。二是GoogleNet中精心设计的Inception模块提高了参数的利用效率，这一部分也借鉴了NIN的思想，形象的解释就是Inception模块本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。不过GoogleNet比NIN更进一步的是增加了分支网络。
VGG网络
VGG网络由著名的牛津大学视觉组（Visual Geometry Group）2014年提出[14]，并取得了ILSVRC 2014比赛分类任务的第2名（GoogleNet第一名）和定位任务的第1名。同时VGGNet的拓展性很强，迁移到其他图片数据上的泛化性非常好。VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和池化尺寸（2x2）。到目前为止，VGGNet依然经常被用来提取图像特征，被广泛应用于视觉领域的各类任务。
VGG网络的主要创新是采用了小尺寸的卷积核。所有卷积层都使用3x3卷积核，并且卷积的步长为1。为了保证卷积后的图像大小不变，对图像进行了填充，四周各填充1个像素。所有池化层都采用2x2的核，步长为2。全连接层有3层，分别包括4096，4096，1000个节点。除了最后一个全连接层之外，所有层都采用了ReLU激活函数。下图为VGG16的结构：
![](https://img-blog.csdn.net/2018061811484486?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
VGG与Alexnet相比，做了以下改进：
1.去掉了LRN层，作者实验中发现深度卷积网络中LRN的作用并不明显。
2.采用更小的连续3x3卷积核来模拟更大尺寸的卷积核，例如2层连续的3x3卷积层可以达到一层5x5卷积层的感受野，但是所需的参数量会更少，两个3x3卷积核有18个参数（不考虑偏置项），而一个5x5卷积核有25个参数。后续的残差网络等都延续了这一特点。
残差网络
残差网络(Residual Network)[15]用跨层连接（Shortcut Connections）拟合残差项（Residual Representations）的手段来解决深层网络难以训练的问题，将网络的层数推广到了前所未有的规模，作者在ImageNet数据集上使用了一个152层的残差网络，深度是VGG网络的8倍但复杂度却更低，在ImageNet测试集上达到3.57%的top-5错误率，这个结果赢得了ILSVRC2015分类任务的第一名，另外作者还在CIFAR-10数据集上对100层和1000层的残差网络进行了分析。VGG19网络和ResNet34-plain及ResNet34-redisual网络对比如下：
![](https://img-blog.csdn.net/20180618114858180?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
之前的经验已经证明，增加网络的层数会提高网络的性能，但增加到一定程度之后，随着层次的增加，神经网络的训练误差和测试误差会增大，这和过拟合还不一样，过拟合只是在测试集上的误差大，这个问题称为退化。
为了解决这个问题，作者设计了一种称为深度残差网络的结构，这种网络通过跳层连接和拟合残差来解决层次过多带来的问题，这种做法借鉴了高速公路网络（Highway Networks）的设计思想，与LSTM有异曲同工之妙。这一结构的原理如下图所示：
![](https://img-blog.csdn.net/20180618114908369?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
后面有文献对残差网络的机制进行了分析。得出了以下结论：残差网络并不是一个单一的超深网络，而是多个网络指数级的隐式集成，由此引入了多样性的概念，它用来描述隐式集成的网络的数量；在预测时，残差网络的行为类似于集成学习；对训练时的梯度流向进行了分析，发现隐式集成大多由一些相对浅层的网络组成，因此，残差网络并不能解决梯度消失问题。
为了进一步证明残差网络的这种集成特性，并确定删除掉一部分跨层结构对网络精度的影响，作者进行了删除层的实验，在这里有两组实验，第一组是删除单个层，第二组是同时删除多个层。为了进行比较，作者使用了残差网络和VGG网络。实验结果证明，除了个别的层之外，删掉单个层对残差网络的精度影响非常小。相比之下，删掉VGG网络的单个层会导致精度的急剧下降。这个结果验证了残差网络是多个网络的集成这一结论。
第三组实验是对网络的结构进行变动，集调整层的顺序。在实验中，作者打乱某些层的顺序，这样会影响一部分路径。具体做法是，随机的交换多对层的位置，这些层接受的输入和产生的输出数据尺寸相同。同样的，随着调整的层的数量增加，错误率也平滑的上升，这和第二组实验的结果一致。
但是笔者认为作者的这种解释有些牵强。普通意义上的集成学习算法，其各个弱学习器之间是相互独立的，而这里的各个网络之间共享了一些层，极端情况下，除了一层不同之外，另外的层都相同。另外，这些网络是同时训练出来的，而且使用了相同的样本。
GoogleNet-Inception-Like网络改进系列
**Inception-V2（GoogleNet-BN）**
作者基于GoogleNet的基本结构进行了改进，Top1错误率相较减少了2个百分点，主要做了以下的改进：
1.加入了BN层，减少了Internal Covariate Shift（内部neuron的数据分布发生变化），使每一层的输出都规范化到一个N(0, 1)的高斯。
2.学习VGG用2个3x3的conv替代Inception模块中的5x5，既降低了参数数量，也加快了计算速度。
**Inception-V3**
Inception-V3一个最重要的改进是卷积核分解（Factorization），将7x7的卷积核分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），我们称为非对称分解，如下图所示。这样做既可以加速计算减少参数规模，又可以将1个卷积拆成2个卷积，使得网络深度进一步增加，增加了网络的非线性。
![](https://img-blog.csdn.net/20180618114920725?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
除此以外作者对训练优化的算法也做了改进：
1.通过改进AdaGrad提出了RMSProp一种新的参数优化的方式。RMSprop是Geoff Hinton提出的一种自适应学习率方法。AdaGrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解AdaGrad算法学习率下降较快的问题。 实验证明RMSProp在非凸条件下优化结果更好。
![](https://img-blog.csdn.net/2018061811495611?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
2.采用了Label Smoothing的策略，该方法是一种通过在输出标签中添加噪声，实现对模型进行约束，降低模型过拟合程度的一种正则化方法。
**Inception-V4**
Inception-v4相较于v3版本增加了Inception模块的数量，整个网络变得更深了。
**Xception**
Xception是Google针对Inception v3的另一种改进，主要是采用Depthwise Separable Convolution来替换原来Inception v3中的卷积操作, 在基本不增加网络复杂度的前提下提高了模型的效果。什么是Depthwise Separable Convolution？ 通常，在一组特征图上进行卷积需要三维的卷积核，也即卷积核需要同时学习空间上的相关性和通道间的相关性。Xception通过在卷基层加入group的策略将学习空间相关性和学习通道间相关性的任务分离，大幅降低了模型的理论计算量且损失较少的准确度。
Xception网络结构如下图所示：
![](https://img-blog.csdn.net/20180618115015150?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**Inception-ResNet v1/v2**
作者基于Inception-v3和Inception-v4将残差网络的思想进行融合，分别得到了Inception-ResNet-v1和Inception-ResNet-v2两个模型。不仅提高了分类精度而且训练的稳定性也得到增强。
Inception-ResNet-v2 网络结构如下图所示:
![](https://img-blog.csdn.net/20180618115027147?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**NASNet**
此论文由Google brain出品，是在之前的一篇论文NAS-Neural Architecture Search With Reinforcement Learning的基础做了突破性的改进，使得能让机器在小数据集（CIFAR-10数据集）上自动设计出CNN网络，并利用迁移学习技术使得设计的网络能够被很好的迁移到ImageNet数据集，验证集上达到了82.7%的预测精度，同时也可以迁移到其他的计算机视觉任务上（如目标检测）。该网络的特点为：
1.延续NAS论文的核心机制，通过强化学习自动产生网络结构。
2.采用ResNet和Inception等成熟的网络拓扑结构减少了网络结构优化的搜索空间，大型网络直接由大量的同构模块堆叠而成，提高学习效率。
3.在CIFAR-10上进行了架构搜索，并将最好的架构迁移到ImageNet图像分类和COCO物体检测上。
下图为采用AutoML设计的Block结构：
![](https://img-blog.csdn.net/20180618115047861?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**WRN（wide residual network）**
作者认为，随着模型深度的加深，梯度反向传播时，并不能保证能够流经每一个残差模块（residual block）的权重，以至于它很难学到东西，因此在整个训练过程中，只有很少的几个残差模块能够学到有用的表达，而绝大多数的残差模块起到的作用并不大。因此作者希望使用一种较浅的，但是宽度更宽的模型，来更加有效的提升模型的性能。
![](https://img-blog.csdn.net/20180618115101483?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
ResNet原作者针对CIFAR-10所使用的的网络，包含三种Residual Block，卷积通道数量分别是16、32、64，网络的深度为6*N+2。而在这里，WRN作者给16、32、64之后都加了一个系数k，也就是说，作者是通过增加Residual Block卷积通道的数量来使模型变得更宽，从而N可以保持很小的值，就可以是网络达到很好的效果。
CIFAR-10和CIFAR -100性能对比：
![](https://img-blog.csdn.net/20180618115113418?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
上述实验表明单独增加模型的宽度是对模型的性能是有提升的。不过也不能完全的就认为宽度比深度更好，两者只有相互搭配，才能取得更好的效果。
**ResNeXt**
作者提出 ResNeXt 的主要原因在于：传统的提高模型准确率的做法，都是加深或加宽网络，但是随着超参数数量的增加（比如通道数，卷积核大小等），网络设计的难度和计算开销也会增加。因此本文提出的 ResNeXt 结构可以在不增加参数复杂度的前提下提高准确率。
这篇论文提出了ResNeXt网络，同时采用了VGG堆叠的思想和Inception 的 split-transform-merge 思想，但是可扩展性比较强，可以认为是在增加准确率的同时基本不改变或降低模型的复杂度。这里提到一个名词cardinality，原文的解释是the size of the set of transformations，如下图(a)(b) cardinality=32所示：
![](https://img-blog.csdn.net/20180618115128505?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
通过实验给出了下面的结论：
1.证明ResNeXt比ResNet更好，而且Cardinality越大效果越好
2.增大Cardinality比增大模型的width或者depth效果更好
当时取得了state-of-art的结果，虽然后来被其它的网络结构超越，但就在最近Facebook 在图像识别技术上又有了新突破，基于ResNeXt 101-32x48d在ImageNet测试中准确度达到创纪录的 85.4%！（使用了35亿张图像，1.7万主题标签进行模型训练，规模史无前例！！！笔者这里不下什么结论，各位看官自行体会...）
**DenseNet**
DenseNet 是一种具有密集连接的卷积神经网络。在该网络中，任何两层之间都有直接的连接，也就是说，网络每一层的输入都是前面所有层输出的并集，而该层所学习的特征图也会被直接传给其后面所有层作为输入。DenseNet的一个优点是网络更窄，参数更少，很大一部分原因得益于dense block的设计，后面有提到在dense block中每个卷积层的输出feature map的数量都很小（小于100），而不是像其他网络一样动不动就几百上千的宽度。同时这种连接方式使得特征和梯度的传递更加有效，网络也就更加容易训练。下面是DenseNet 的示意图：
![](https://img-blog.csdn.net/20180618115144164?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
DenseNet可以有效地减少网络参数规模，达到减轻过拟合的效果，对小数据集合的学习很有效果。但是由于中间输出的feature map数量是多层Concat的结果，导致网络在训练和测试的时候显存占用并没有明显的优势，计算量也没有明显的减少！
**MobileNet**
MobileNets是Google针对手机等嵌入式设备提出的一种轻量级的深层神经网络，网络设计的核心Separable Convolution可以在牺牲较小性能的前提下有效的减少参数量和计算量。Separable Convolution将传统的卷积运算用两步卷积运算代替：Depthwise convolution与Pointwise convolution，如下图所示：
![](https://img-blog.csdn.net/20180618115224537?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
从图中可以明确的看出，由于输入图片为三通道，Depthwise conv的filter数量只能为3，而传统的卷积方法会有3x3总共9个filter。
后续的MobileNet-v2主要增加了残差结构，同时在Depthwise convolution之前添加一层Pointwise convolution，优化了带宽的使用，进一步提高了在嵌入式设备上的性能。可分离卷积的思路如下图所示：
![](https://img-blog.csdn.net/20180618115233485?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
深度神经网络优化策略汇总
接下来介绍卷积神经网络的各种改进措施，其中经典网络的改进措施已经在前面各个网络中介绍。针对卷积神经网络的改进措施主要在以下几个方面：卷积层，池化层，激活函数，损失函数，网络结构，正则化技术等方面。优化算法对网络的训练至关重要，在这里我们单独列出来了。
**卷积层**
卷积层的改进有以下几种：卷积核小型化，1x1卷积，Network In Network，Inception机制，卷积分解（Factorization），反卷积运算等，下面分别介绍。
Network In Network[16]的主要思想是用一个小规模的神经网络来替代卷积层的线性滤波器，在这篇文献中，小型网络是一个多层感知器卷积网络。显这种小型网络比线性的卷积运算有更强的的描述能力。
卷积核小型化是现在普遍接受的观点，在VGG网络中已经介绍了。1x1卷积可以用于通道降维，也可以用于全卷积网络，保证卷积网络能接受任意尺寸的输入图像，并能做逐像素的预测。Inception机制在GoogLeNet网络中已经介绍，这里也不在重复。
卷积操作可以转化为图像与一个矩阵的乘积来实现，反卷积[17]也称为转置卷积，它的操作刚好和这个过程相反，正向传播时左乘矩阵的转置，反向传播时左乘矩阵。注意这里的反卷积和信号处理里的反卷积不是一回事，它只能得到和原始输出图像尺寸相同的图像，并不是卷积运算的逆运算。反卷积运算有一些实际的用途，包括接下来要介绍的卷积网络的可视化；全卷积网络中的上采样，图像生成等。反卷积运算通过对卷积运算得到的输出图像左乘卷积矩阵的转置，可以得到和原始图像尺寸相同的一张图像。
**池化层**
池化层的改进主要有以下几种：L-P池化，混合池化，随机池化，Spatial pyramid pooling，ROI pooling。Spatial pyramid pooling在SPP网络中提出，之前的“基于深度学习的目标检测算法综述”文章中SIGAI已经做了讲解，这里不再重复。ROI pooling在Fast R-CNN算法中提出，同样的在目标检测文章中已经做了介绍。
**激活函数**
除了传统的sigmoid，tanh函数，深度卷积神经网络中出现了各种新的激活函数，主要的有：ReLU，ELU，PReLU等，它们取得了不错的效果，其中ReLU以及它的改进型在卷积网络中被普遍采用。这些激活函数在SIGAI公众号上一篇文章“理解神经网络的激活函数”中已经做了介绍。
**损失函数**
损失函数也是一个重要的改进点。除了欧氏距离损失之外，交叉熵，对比损失，合页损失等相继被使用。这些基本的损失函数SIGAI在后续的文章中会专门介绍，敬请期待！
在一些复杂的任务上，出现了多任务损失损失函数。典型的有目标检测算法，人脸识别算法，图像分割算法等，这些损失函数在人脸识别、目标检测系列综述文章中已经进行介绍，在这里不再重复。
**网络结构**
这里的网络结构指拓扑结构以及层的使用上。连接关系的改进如残差网络和DenseNet等结构在前面已经做了介绍。
全卷积网络Fully Convolutional Networks[31]，简称FCN，是在标准卷积网络基础上所做的改变，它将标准卷积网络的全连接层替换成卷积层，以适应图像分割、深度估计等需要对原始图像每个像素点进行预测的情况。一般情况下，全卷积网络最后几个卷积层采用1x1的卷积核。由于卷积和下采样层导致了图像尺寸的减小，为了得到与原始输入图像尺寸相同的图像，使用了反卷积层实现上采样以得到和输入图像尺寸相等的预测图像。
不同层的卷积核有不同的感受野，描述了图像在不同尺度的信息。多尺度处理也是卷积网络的一种常用手段，将不同卷积层输出图像汇总到一个层中进行处理可以提取图像多尺度的信息，典型的做法包括GoogLeNet，SSD，Cascade CNN，DenseBox。
**归一化技术**
神经网络在训练过程中每一层的参数会随着迭代的进行而不断变化，这会导致它后面一层的输入数据的分布不断发生变化，这种问题称为internal covariate shift。在训练时，每一层要适应输入数据的分布，这需要我们在迭代过程中调整学习率，以及精细的初始化权重参数。为了解决这个问题，我们需要对神经网络每一层的输入数据进行归一化。其中一种解决方案为批量归一化Batch Normalization[66]，它是网络中一种特殊的层，用于对前一层的输入数据进行批量归一化，然后送入下一层进行处理，这种做法可以加速神经网络的训练过程。
**优化算法**
除了标准的mini-batch随机梯度下降法之外，还有一些改进版本的梯度下降法，它们在很多实验和实际应用中取得了更好的效果，下面分别进行介绍。
AdaGrad[67]为自适应梯度，即adaptive gradient算法，是梯度下降法最直接的改进。唯一不同的是，AdaGrad根据前几轮迭代时的历史梯度值来调整学习率。AdaDelta算法[70]也是梯度下降法的变种，在每次迭代时也利用梯度值构造参数的更新值。Adam算法[68]全称为adaptive moment estimation，它由梯度项构造了两个向量m和v，它们的初始值为0。NAG算法是一种凸优化方法，由Nesterov提出。和标准梯度下降法的权重更新公式类似，NAG算法构造一个向量v，初始值为0。RMSProp算法[69]也是标准梯度下降法的变种，它由梯度值构造一个向量，初始化为0，
参数初始化和动量项对算法的收敛都至关重要，文献[32]对这两方面的因素进行了分析。它的观点认为，对于深度神经网络和循环神经网络的训练优化问题求解，权重初始值和动量项都很重要，二者缺一不可。如果初始值设置不当，即使使用动量项也很难收敛到好的效果；另一方面，如果初始值设置的很好，但不使用动量项，收敛效果也打折扣。
**理论解释**
卷积网络一般有很深的层次，要对它进行严格而细致的分析比较困难。与网络的应用和设计相比，对它的理论和运行机理分析与解释相对较少。如果我们能分析清楚卷积网络的运行机理，把卷积操作可视化的显示出来，无论是对于理解卷积网络，还是对于网络的设计都具有重要的意义。
对多层卷积神经网络的理论解释和分析来自两个方面。第一个方面是从数学角度的分析，对网络的表示能力、映射特性的数学分析；第二个方面是多层卷积网络和人脑视觉系统关系的研究，分析二者的关系有助于理解、设计更好的方法，同时也促进了神经科学的进步。
在SIGAI公众号之前的文章“卷积神经网络为什么能称霸计算机视觉领域？”中，我们介绍了对卷积网络的理论分析，感兴趣的读者可以阅读那篇文章，在这里不再重复介绍。
典型应用
卷积神经网络在诸多领域得到了成功的应用。接下来我们将介绍它在机器视觉，计算机图形学，自然语言处理这些典型领域的应用。对于这些应用问题和为它们设计的网络结构和算法，理解的关键点是：
1.网络的结构。即网络由那些层组成，各个层的作用是什么，它们的输入数据是什么，输出数据是什么。
2.训练目标即损失函数，这直接取决于要解决的问题。
**机器视觉**
卷积神经网络在图像分类问题上取得成功之后很快被用于人脸检测问题，在精度上大幅度超越之前的AdaBoost框架。直接用滑动窗口加卷积网络对窗口图像进行分类的方案计算量太大很难达到实时，使用卷积网络进行人脸检测的方法采用各种手段解决或者避免这个问题。在这些方法中，Cascade CNN，DenseBox，Femaleness-Net，MT-CNN是其中的代表。在SIGAI之前的公众号文章“人脸检测算法综述”中我们已经对这些算法进行了介绍，这里不再重复。
和人脸、行人等特定目标检测不同，通用目标检测的任务是同时检测图像中多种类型的目标。各类目标的形状不同，因此目标矩形的宽高比不同，难度更大。典型的算法是R-CNN，SPP网络，Fast R-CNN，Faster R-CNN，YOLO，SSD，R-FCN，FPN等。基于卷积神经网络的通用目标检测算法已经在SIGAI公众号之前的文章“基于深度卷积神经网络的目标检测算法综述”讲述，在这里不再重复，感兴趣的读者可以阅读这篇文章。
人脸关键点定位的目标是确定关键位置的坐标，如眼睛的中点，鼻尖和嘴尖等。它在人脸识别、美颜等功能中都有应用。这个问题是一个回归问题，要实现的是如下映射：
![](https://img-blog.csdn.net/20180618115300344?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[26]提出了一种用级联的卷积网络进行人脸关键点检测的方法，通过逐级细化的思路实现。本文检测5个关键点，分别是左右眼的中心LE和RE，鼻尖N，嘴的左右端LM和RM。采用了3个层次的卷积网络进行级联，逐步求精。第一个层次上包含3个卷积网络，分别称为F1，EN1，NM1，输入分别为整个人脸图像，眼睛和鼻子，鼻子和嘴巴。每个网络都同时预测多个关键点。对每个关键点，将这些网络的预测值进行平均以减小方差。系统的结构如下图所示：
![](https://img-blog.csdn.net/20180618115251751?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
人脸识别也是深度卷积神经网络成功应用的典型领域，在SIGAI之前的公众号文章“人脸识别算法演化史”中我们已经做了综述，感兴趣的读者可以参考那篇文章。
文字定位和识别也是卷积网络成功应用的方向[27][28][29][30]，后者属于图像分类问题。在这里我们不详细介绍。除了图像分类，目标检测等大类任务之后，接下来我们重点介绍卷积网络在机器视觉其他问题上的应用。
图像语义分割和图像识别是密切相关的问题。分割可看做对每个像素的分类问题。卷积网络在进行多次卷积和池化后会缩小图像的尺寸，最后的输出结果无法对应到原始图像中的单一像素，卷积层后面接的全连接层将图像映射成固定长度的向量，这也与分割任务不符。针对这两个问题有几种解决方案，最简单的做法是对一个像素为中心的一块区域进行卷积，对每个像素都这样的操作。这种方法有两个缺点：计算量大，利用的信息只是本像素周围的一小片区域。更好的方法是全卷积网络，这是我们接下来要介绍的重点。
文献[31]提出了一种称为全卷积网络FCN的结构来实现图像的语义分割，这种模型从卷积特征图像恢复出原始图像每个像素的类别。网络能够接受任意尺寸的输入图像，并产生相同尺寸的输出图像，输入图像和输出图像的像素一一对应。这种网络支持端到端、像素到像素的训练。
最简单的FCN的前半部分改装自AlexNet网络，将最后两个全连接层和一个输出层改成3个卷积层，卷积核均为1x1大小。解决卷积和池化带来的图像分辨率缩小的问题的思路是上采样。
网络的最后是上采样层，在这里用反卷积操作实现上采样，反卷积的卷积核通过训练得到。在实现时，在最后一个卷积层后面接上一个反卷积层，将卷积结果映射回和输入图像相等的尺寸。为了得到更精细的结果，可以将不同卷积层的反卷积结果组合起来。系统结构如下图所示：
![](https://img-blog.csdn.net/20180618115311307?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[34]提出了一种称为DeepLab的图像分割方法。这个方法的创新有3点：用上采样的滤波器进行卷积，称为atrous卷积，以实现密集的、对像素级的预测；采用了atrous空间金字塔下采样技术，以实现对物体的多尺度分割；第三点是使用了概率图模型，实现更精确的目标边界定位，通过将卷积网络最后一层的输出值与一个全连接的条件随机场相结合得到。算法运行结果如下图所示：
![](https://img-blog.csdn.net/20180618115320968?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[35]提出了一种称为SegNet的图像语义分割网络，这也是一个全卷积网络，其主要特点是整个网络由编码器和解码器构成。网络的前半部分是编码器，由多个卷积层和池化层组成。网络的后半部分为解码器，由多个上采样层和卷积层构成。解码器的最后一层是softmax层，用于对像素进行分类。
![](https://img-blog.csdn.net/20180618115329946?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
编码器网络的作用是产生有语义信息的特征图像；解码器网络的作用是将编码器网络输出的低分辨率特征图像映射回输入图像的尺寸，以进行逐像素的分类。解码器用编码器max池化时记住的最大元素下标值执行非线性上采样，这样上采样的参数不用通过学习得到。上采样得到的特征图像通过卷积之后产生密集的特征图像。整个框架实现了完全端到端的训练。
边缘检测的目标是找出图像中所有的边缘像素点。Sobel算子和拉普拉斯算子都可以通过卷积和阈值化的方式提取出图像的边缘。更复杂的方法有Canny算子，它首先用Sobel算子得到梯度图像，在进行阈值化之后进行非最大抑制，最后得到更为干净的边缘图。和图像分割一样，纯图像处理的方法只在像素一级进行操作，没有利用图像语义和结构信息。边缘和轮廓检测可以看做是二分类问题，正样本为边缘点的像素，负样本为非边缘像素。
文献[39]提出了一种称为DeepEdge的边缘提取方法，这是一种基于图像块的方法，卷积网络作用于原始图像中以每个像素为中心的小图像块，判断该像素是否为边缘像素。轮廓检测流程分为如下几步：
1.用Canny算子提取候选轮廓点，它输出的边缘图像中所有的边界点作为候选轮廓点。
2.为所有候选轮廓点提取4个尺度的子图像，将它们同时送入卷积网络中进行处理。
3.将卷积的结果送入2个子网络中进行处理，第一个网络用于分类，第二个网络用于回归。
4.将这两个网络的输出值进行加权平均，得到最后的分数值，这个分数值表示该候选轮廓点是否真的是轮廓点。
5.对上一步的输出分数进行阈值化，得到最终的轮廓图像。
![](https://img-blog.csdn.net/20180618115410191?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
边缘检测的结果如下图所示：
![](https://img-blog.csdn.net/20180618115419816?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[37]提出了一种称为DeepContour的物体轮廓提取算法，这也是一种基于图像块的方法。在这里将正样本即轮廓划分为多个子类，并且用不同的模型拟合这些子类。作者设计了一种新的损失函数，称为positive-sharing loss，各个子类共享正样本类的损失。在这里用卷积网络对小的图像块进行分类，这些图像块从整个图像中切分出来，可能包括轮廓，也可能不包括轮廓。
![](https://img-blog.csdn.net/20180618115721141?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[38]提出了一种称为整体式嵌套（Holistically-Nested）的边缘检测算法。整体式是指整个算法是端到端的，嵌套式指在整个边缘检测的过程中通过不断的细化求解，得到精确的边界图像。网络对输入图像进行了多尺度的处理，这通过卷积网络运行过程中得到的多个尺度的特征图像进行处理融合而实现。
![](https://img-blog.csdn.net/20180618115730590?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
风格迁移的任务是把输入图像变成另一种风格，如油画风格，但要保持和输入图像的内容相同，这是一个根据两张图像生成一张图像的问题。
文献[40]提出了一种用卷积网络进行风格迁移的方法。在这里将风格看成是纹理特征，风格迁移看成是提取待迁移图像的语义及内容信息，然后将纹理风格作用于该图像，得到想要的风格的输出图像。
算法的输入包括一张风格图像和一张要进行风格迁移的内容图像，输出的新图像内容和内容图像保持一致，风格和风格图像保持一致。处理流程为：
1.用卷积网络提取风格图像的风格特征，内容图像的内容特征。
2.从一张白噪声图像开始迭代生成目标图像，优化的目标是使得目标图像的风格特征与风格图像相似，内容特征与内容图像相似。
![](https://img-blog.csdn.net/20180618115742574?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图像增强的任务是提升图像的对比度。文献[90]提出了一种用卷积神经网络进行图像增强的方法。其基本思想是学习人工对图像进行增强调整的模型。这种方法达到了非常好的效果，而且可以在移动设备上做到实时处理。
在进行图像增强时，卷积网络输出的是原始图像的低分辨版本，进行双边空间中的一系列仿射变换，然后对这些仿射变换进行保边缘的上采样。然后将上采样后的变换作用于原始输出图像，得到增强后的图像。
卷积神经网络被成功的用于根据单张图像估计深度信息。文献[41]提出了一种用多尺度的卷积网络从单张图像估计深度的方法，在这里，深度信息只是相对数据，即图像中每个像素离摄像机的远近关系，而不是真实的物理距离。由于每个像素点都会预测出一个深度值，因此这是一个逐像素的回归问题。
系统的输入是单张RGB图像，输出是深度图，和输入图像尺寸相同。系统由两个卷积网络层叠组成，第一个网络对整个图像进行粗的全局深度预测，第二个卷积网络用局部信息对全局预测结果进行求精。
![](https://img-blog.csdn.net/2018061811584046?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
算法运行的结果如下图所示：
![](https://img-blog.csdn.net/20180618115851366?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
更进一步，文献[42]提出了一种用多尺度卷积神经网络从单张图像估计深度信息、法向量的方法。这个卷积网络的输入为单张RGB图像，输出为三张图像，分别为深度图，法向量图，以及物体分割标记图。
这个卷积网络包括三个尺度，形成级联结构。每个尺度的第一个层都接受原始RGB图像作为输入，另外还接受上一个级卷积网络的输出作为输入，这个输出是经过上采样的。
![](https://img-blog.csdn.net/20180618115916736?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
目标跟踪是机器视觉领域中的一个重要问题，它分为单目标跟踪与多目标跟踪两种问题。前者只跟踪单个目标，后者要对多个目标同时进行跟踪。单目标跟踪是一个状态预测问题，它根据目标在之前帧中的位置、大小、外观和运动信息估计在当前帧中的位置、大小等状态。
文献[44]用卷积神经网络来实现目标的检测以用于目标跟踪。网络的输入为固定尺寸的图像，包含3个卷积层，输出为概率图像，表示该位置为目标的概率。在卷积层和全连接层之间加入了SPP网络中的SPP池化层，以提高目标定位的精度。整个网络先用ImageNet的目标检测数据集进行离线训练，这样就具有区分目标和背景的能力。
文献[46]提出了一种用全卷积网络进行目标跟踪的方法，卷积网络的作用是目标检测。这种方法用一个在ImageNet数据集上预先训练好的卷积网络提取图像的特征，用于区分目标和背景，卷积网络采用VGG结构。另外也用卷积网络的特征生成热度图，表示每个位置处是目标的概率。
文献[45]提出了一种称为Multi-Domain的卷积网络结构实现目标跟踪。这个网络的前半部分是卷积层和全连接层，后面是多个domain-specific层， 它们用于实现目标的精确定位。
其他的目标跟踪文章见参考文献，在此不一一列举。
**图形学**
计算机图形学是计算机科学的一个重要分支，它的任务是用计算机程序生成图像，尤其是真实感图像。图形学中有3个主要的问题：几何模型的建立，物理模型的建立包括光照模型，渲染即由几何和物理模型生成最终的图像。
机器学习技术在图形学中的应用代表了数据驱动这类方法，它通过大量的训练样本得到要建立的模型的参数，或者直接由训练的模型生成图像。卷积网络适合处理图像、2D或者3D空间中的网格数据这里具有空间结构的数据，在图形学的很多问题上也取得了很好的效果。
文献[51]提出了一种用基于八叉树的卷积网络进行3D形状分析的方法，称为O-CNN。在这里用八叉树表示3D物体，将八叉树最精细叶子节点的法向量均值作为卷积网络的输入，执行3D卷积运算。这种卷积网络能对3D形状进行分类，检索和分割。
![](https://img-blog.csdn.net/20180618115931477?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在图形学中，物理模型包括对要绘制的物体进行力学和光学建模。前者主要针对运动的物体，包括刚体和流体。对所有要渲染的物体，都需要建立光学模型，包括物体表面材质的光学特征，以及光照模型。
文献[53]提出了一种使用单张图片估计物体表面反射函数的方法，该算法用卷积网络表示表面反射函数。表面反射函数定义了物体表面的光学反射特性，它决定了给定光照条件下物体表面的颜色和纹理，这对绘制物体至关重要。
流体模拟是图形学中一个重要的问题，它对液体、气体如烟雾等物体的运动进行建模和绘制。在仿真、游戏与动画、电影特技里都有这种技术的应用。经典的方法是基于物理的流体模拟。它主要由两步构成：对流体的运动进行建模，及对流体的表面进行绘制，前者的基础是流体力学。在流体力学领域，描述流体运动使用的是Navier-Stokes方程，这是一个复杂的偏微分方程组。用离散化的数值方法计算需要求解大规模的方程组，非常耗时，使得高精度的流体模拟很难实时进行。
文献[58]提出了一种用卷积网络加速流体模拟的方法，这种方法不再求解大规模的线性方程组，而是直接用卷积网络进行预测。这个网络用大量的仿真数据作为训练集，采用半监督的方法进行训练，目标是最小化长期速度散度。
![](https://img-blog.csdn.net/20180618115941918?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[52]提出了一种用卷积网络进行烟雾合成的方法，其关键是用卷积网络建立烟雾运动的力学模型。在这里采用了一个有4个卷积层和2个全连接层的卷积网络。卷积网络的作用是学习描述粗糙尺度烟雾模拟局部和精细尺度烟雾模拟局部对应关系的映射。在新场景中生成精细的烟雾特效时，只需进行快速的粗糙模拟，并根据卷积网络建立的映射得到与各局部相对应的精细模拟局部，然后将其细节形体信息转移过来即可。
![](https://img-blog.csdn.net/20180618115951127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
纹理合成是渲染时重要的一步，它从小的纹理样图生成大的纹理图像，然后映射到物体表面的曲面上，要保证生成的图像没有缝隙。和风格迁移一样，这也是一个从图像生成图像的问题。卷积神经网络的卷积输出值蕴含了图像的信息，因此可以根据它来计算纹理特征，用来衡量样例图像和生成的图像的相似度。
文献[55]提出了一种用卷积网络合成纹理的方案，其思想和前面介绍的风格迁移类似。这个方法分为两步。首先是纹理分析，它的输入是纹理样图，送入卷积网络处理之后，在各个卷积层的输出特征图像上计算Gram矩阵。第二步是纹理合成，它的输入是一张白噪声图像，送入卷积网络进行处理，用纹理模型在卷积网络的各个层上计算损失函数。然后用梯度下降法迭代更新这张白噪声图像，使得损失函数最小化。对白噪声图像的优化结果就是合成得到的纹理图像，它与纹理样例图像具有相同的Gram矩阵。
![](https://img-blog.csdn.net/20180618120002426?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
文献[56]提出了一种用卷积网络学习纹理的特征，然后合成纹理的方法。它们的方法思路和Leon 的类似，也是用一个卷积网络提取出图像在各个层的纹理特征，另外，用同样的网络对一张白噪声图像进行处理，提取出相同的纹理特征。然后用梯度下降法更新噪声图像，目标是使得二者的纹理特征相同。在这里，他们没有使用Gram矩阵描述纹理特征，而是使用了结构化能量，它基于输出图像的相关系数，捕捉纹理的自相似性和规则性。
![](https://img-blog.csdn.net/20180618120019854?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图像彩色化的目标是给定一张黑白图像，在少量的用户交互作用下生成对应的彩色图像。在这里的用户交互一般是让用户在黑白图像的某些位置设置颜色。
文献[60]提出了一种使用卷积网络将黑白图像彩色化的方法。卷积网络的输入是灰度图像以及少量的用户提示信息，输出数据是彩色图像。其目标是根据灰度图像的结构信息以及用户在几个典型位置的输入颜色，预测出每个像素的颜色值。系统由两个神经网络构成。第一个为局部提示网络，它接受稀疏的用户输入；第二个网络是全局提示网络，它使用图像的全局统计信息。
![](https://img-blog.csdn.net/20180618120210762?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
下图是彩色化的结果：
![](https://img-blog.csdn.net/20180618120222536?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
High Dynamic Range即高度动态范围，简称HDR，它确保在某些极端光照条件下，图像的高光和弱光区域都很清晰。普通照相机因为传感器量化范围的限制，产生的图像图像会有欠曝光或者过曝光区域，HDR是解决这个问题的一种方法。
产生HDR图像的做法一般是用相机拍摄多张有不同曝光度的LDR（Low Dynamic Range，低动态范围）的图像，然后合并成一张高动态范围的图像。生成HDR图像需要解决两个问题：1.需要将多张LDR图像对齐，2.将这些图像进行合并，生成HDR图像。第1个问题可以用光流法等手段解决，但会留下人工痕迹。
文献[54]提出了一种用机器学习的手段进行HDR图像合成的方法。这种方法能够根据3张不同曝光的LDR图像生成HDR图像。首先用光流法将高曝光与低曝光图像与中度曝光图像对齐，中度曝光图像为参考图像。最后生成的HDR图像与参考图像对齐，但包含另外两张图像即高曝光与低曝光图像的信息。然后将3张对齐的图像送入卷积网络中预测，生成HDR图像。
![](https://img-blog.csdn.net/20180618120246685?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**自然语言处理**
自然语言处理领域大多数的问题都是时间序列问题，这是循环神经网络擅长处理的问题，在下一章中我们将详细介绍。但对于有些问题，使用卷积网络也能进行建模并且得到了很好的结果，在这里我们重点介绍文本分类和机器翻译。
文献[64]设计了一种用卷积网络进行句子分类的方案。这个方法的结构很简单，使用不同尺寸的卷积核对文本矩阵进行卷积，卷积核的宽度等于词向量的长度，然后使用max池化。对每一个卷积核提取的向量进行操作，最后每一个卷积核对应一个数字，把这些数据拼接起来，得到一个表征该句子的向量。最后的预测都是基于该句子的。
文献[65]提出了一种用卷积网络进行机器翻译的方法。这篇文章用卷积网络实现了序列到序列的学习，而之前的经典做法是用循环神经网络构建序列到序列的学习框架。在WMT 14的英语-德语，英语-法语数据集上，这种方法的精度超越了Google的LSTM循环神经网络翻译系统。
工程优化
深度神经网络的模型需要占用大量的存储空间，网络传输时也会耗费大量的带宽和时间，这限制了在移动设备、智能终端上的应用。在Caffe中，AlexNet网络的模型文件超过200MB，VGG则超过500MB，这样的模型文件是不适合集成到app安装包中的。因此需要对模型进行压缩，在下一节中我们将介绍解决这一问题的典型方法。
复杂的模型不仅带来存储空间的问题，还有计算量的增加。运行在服务端的模型可以通过GPU、分布式等并行计算技术进行加速，运行在移动端和嵌入式系统中的模型由于成本等因素的限制，除了采用并行计算等进行加速之外，还需要对算法和模型本身进行裁剪或者优化以加快速度。在下一节中，我们将详细介绍加快网络运行速度的方法。
减少存储空间和计算量的一种方法是对神经网络的模型进行压缩。有多种实现手段，包括减小网络的规模，对模型的权重矩阵进行压缩，对模型的参数进行编码，神经网络二值化等，接下来分别介绍。
**权重剪枝**
文献[71]提出了一种卷积神经网络模型压缩方法。在不影响精度的前提下，能够将AlexNet网络模型的参数减少到1/9，VGG-16网络模型的参数减少到1/13。其做法是先按照正常的流程训练神经网络，然后去掉小于指定阈值的权重，最后对剪枝后的模型进行重新训练，反复执行上面的过程直到完成模型的压缩。
更进一步，文献[72]提出了一种称为deep compression的深度模型压缩技术，通过剪枝、量化和哈夫曼编码对模型进行压缩，而且不会影响网络的精度。整个方法分为3步，第1步对模型进行剪枝，只保留一些重要的连接。第2步通过权值量化来共享一些权值。第3步通过哈夫曼编码来进一步压缩数据。
![](https://img-blog.csdn.net/20180618120300662?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**二值化网络**
将网络的权重由浮点数转换为定点数甚至是二值数据可以大幅度的提高计算的速度，减少模型的存储空间。相比浮点数的加法和乘法运算，定点数要快很多，而二值化数据的运算可以直接用位运算实现，带来的加速比更大。
文献[73]提出了一种称为二值神经网络（简称BNN）的模型。二值神经网络的权重值和激活函数都是二值化的数据，这能显著减小模型存储空间，并且加快模型的计算速度。
文献[74]提出了一种称为二值权重网络和XNOR（同或门）网络的模型，这是对卷积神经网络的二值化逼近，也是对文献[17]方法的进一步优化。
二值权重网络的权重矩阵是二值化数据，输入数据是实数。XNOR网络的卷积核、卷积层、全连接层的输入数据都是二值化的。在不损失精度的前提下，XNOR网络能够把模型的存储空间压缩为1/32，速度提升58倍。
1 个月前
参考文献：
[1] .LeCun, B.Boser, J.S.Denker, D.Henderson, R.E.Howard, W.Hubbard, and L.D.Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989.
[2] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a back-propagation network. In David Touretzky, editor, Advances in Neural Information Processing Systems 2 (NIPS*89), Denver, CO, 1990, Morgan Kaufman.
[3] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, november 1998.
[4] H. Rowley, S. Baluja, and T. Kanade. Neural Network-Based Face Detection. In:Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. San Francisco, CA, USA: IEEE Computer Society, 1996. 203-208
[5] H. Rowley, S. Baluja, and T. Kanade. Rotation Invariant Neural Network-Based Face Detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Santa Barbara, CA, USA: IEEE Computer Society, 1998. 38-44
[6] S Lawrence, C L Giles, Ah Chung Tsoi, Andrew D Back.Face recognition: a convolutional neural-network approach.1997, IEEE Transactions on Neural Networks.
[7] P. Y. Simard, D. Steinkraus, and J. C. Platt, Best practices for convolutional neural networks applied to visual document analysis. in null. IEEE, 2003.
[8] X. Glorot, Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS, 2010.
[9] Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton. ImageNet Classification with Deep Convolutional Neural Networks.
[10] G.E.Hinton, N.Srivastava, A.Krizhevsky, I.Sutskever, and R.R.Salakhutdinov. Improving neural networks by preventing coadaptation of feature detectors. arXiv:1207.0580, 2012.
[11] Nair, V. and Hinton. Rectified linear units improve restricted Boltzmann machines. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML 2010).
[12] Zeiler M D, Fergus R. Visualizing and Understanding Convolutional Networks. European Conference on Computer Vision, 2013.
[13] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Going Deeper with Convolutions, Arxiv Link:[http://http://arxiv.org/abs/1409.4842](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.4842).
[14] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition.
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. 2015, computer vision and pattern recognition.
[16] Lin, Min, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400
[17] Zeiler M D, Krishnan D, Taylor G W, et al. Deconvolutional networks. Computer Vision and Pattern Recognition, 2010.
[18] Stephane Mallat. Understanding deep convolutional networks. 2016, Philosophical Transactions of the Royal Society A.
[19] Aravindh Mahendran, Andrea Vedaldi. Understanding Deep Image Representations by Inverting Them. CVPR 2015.
[20] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.
[21] Ross Girshick. Fast R-CNN. 2015, international conference on computer vision.
[22] Anelia Angelova, Alex Krizhevsky, Vincent Vanhoucke, Abhijit Ogale, Dave Ferguson. Real-Time Pedestrian Detection With Deep Network Cascades.
[23] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, Gang Hua. A convolutional neural network cascade for face detection. 2015, computer vision and pattern recognition
[24] Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu. DenseBox: Unifying Landmark Localization with End to End Object Detection. 2015, arXiv: Computer Vision and Pattern Recognition
[25] Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang. Faceness-Net: Face Detection through Deep Facial Part Responses.
[26] Yi Sun, Xiaogang Wang, Xiaoou Tang. Deep Convolutional Network Cascade for Facial Point Detection. 2013, computer vision and pattern recognition.
[27] Kobchaisawat T, Chalidabhongse T H. Thai text localization in natural scene images using Convolutional Neural Network. Asia-Pacific Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA). IEEE, 2014: 1-7.
[28] Guo Q, Lei J, Tu D, et al. Reading numbers in natural scene images with convolutional neural networks. Security, Pattern Analysis, and Cybernetics (SPAC), 2014 International Conference on. IEEE, 2014: 48-53.
[29] Xu H, Su F. A robust hierarchical detection method for scene text based on convolutional neural networks. Multimedia and Expo (ICME), 2015 IEEE International Conference on. IEEE, 2015: 1-6.
[30] Cireşan D C, Meier U, Gambardella L M, et al. Convolutional neural network committees for handwritten character classification. Document Analysis and Recognition (ICDAR), 2011 International Conference on. IEEE, 2011: 1135-1139.
[31] Long J, Shelhamer E, Darrell T, et al. Fully convolutional networks for semantic segmentation. Computer Vision and Pattern Recognition, 2015.
[32] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the Importance of Initialization and Momentum in Deep Learning. Proceedings of the 30th International Conference on Machine Learning, 2013.
[33] Hyeonwoo Noh, Seunghoon Hong, Bohyung Han. Learning Deconvolution Network for Semantic Segmentation. 2015, international conference on computer vision.
[34] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. 2016.
[35] Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.
[36] R. Girshick, J. Donahue, T. Darrell, J. Malik. Region-Based Convolutional Networks for Accurate Object Detection and Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, May. 2015.
[37] Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang. DeepContour: A deep convolutional feature learned by positive-sharing loss for contour detection. 2015 computer vision and pattern recognition.
[38] Saining Xie, Zhuowen Tu. Holistically-Nested Edge Detection. 2015. international conference on computer vision.
[39] Gedas Bertasius, Jianbo Shi, Lorenzo Torresani. DeepEdge: A multi-scale bifurcated deep network for top-down contour detection. 2015, computer vision and pattern recognition
[40] Gatys L A, Ecker A S, Bethge M. Image Style Transfer Using Convolutional Neural Networks. CVPR 2016.
[41] David Eigen, Christian Puhrsch, Rob Fergus. Depth Map Prediction from a Single Image using a Multi-Scale Deep Network. 2014, neural information processing systems.
[42] David Eigen, Rob Fergus. Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture. 2015, international conference on computer vision.
[43] Naiyan Wang, Dityan Yeung. Learning a Deep Compact Image Representation for Visual Tracking. 2013, neural information processing systems.
[44] Naiyan Wang, Siyi Li, Abhinav Gupta, Dityan Yeung. Transferring Rich Feature Hierarchies for Robust Visual Tracking. 2015, arXiv: Computer Vision and Pattern Recognition.
[45] Hyeonseob Nam, Bohyung Han. Learning Multi-domain Convolutional Neural Networks for Visual Tracking. 2016, computer vision and pattern recognition.
[46] Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu. Visual Tracking with Fully Convolutional Networks. 2015, international conference on computer vision.
[47] Chao Ma, Jiabin Huang, Xiaokang Yang, Minghsuan Yang. Hierarchical Convolutional Features for Visual Tracking. 2015, international conference on computer vision.
[48] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo Lim, Minghsuan. Hedged Deep Tracking. 2016, computer vision and pattern recognition.
[49] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, Philip H S Torr. Fully-Convolutional Siamese Networks for Object Tracking. 2016, european conference on computer vision.
[50] David Held, Sebastian Thrun, Silvio Savarese. Learning to Track at 100 FPS with Deep Regression Networks. 2016, european conference on computer vision.
[51] Pengshuai Wang, Yang Liu, Yuxiao Guo, Chunyu Sun, Xin Tong. O-CNN: octree-based convolutional neural networks for 3D shape analysis. 2017, ACM Transactions on Graphics.
[52] Mengyu Chu, Nils Thuerey. Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors. 2017, ACM Transactions on Graphics
[53] Xiao Li, Yue Dong, Pieter Peers, Xin Tong. Modeling surface appearance from a single photograph using self-augmented convolutional neural networks. 2017, ACM Transactions on Graphics.
[54] Nima Khademi Kalantari, Ravi Ramamoorthi. Deep high dynamic range imaging of dynamic scenes. 2017, ACM Transactions on Graphics.
[55] Leon A Gatys, Alexander S Ecker, Matthias Bethge. Texture synthesis using convolutional neural networks. 2015, neural information processing systems.
[56] Omry Sendik, Daniel Cohenor. Deep Correlations for Texture Synthesis. 2017, ACM Transactions on Graphics
[57] Michael Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoff, Fredo Durand. Deep Bilateral Learning for Real-Time Image Enhancement. 2017, ACM Transactions on Graphics.
[58] Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, Ken Perlin. Accelerating Eulerian Fluid Simulation With Convolutional Networks. 2016,international conference on machine learning.
[59] Peiran Ren,Yue Dong,Stephen Lin,Xin Tong,Baining Guo. Image based relighting using neural networks. 2015,international conference on computer graphics and interactive techniques.
[60] Richard zhang, Jun-Yan Zhu, Phillip Isola, XinYang Geng, Angela S. Lin, Tianhe Yu, Alexei A. Efros. Real-Time User-Guided Image Colorization with Learned Deep Priors.
[61] Yoon Kim. Convolutional Neural Networks for Sentence Classification. 2014, empirical methods in natural language processing.
[62] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. arXiv preprint arXiv:1509.01626, 2015.
[63] Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1408.5882, 2014.
[64] Phil Blunsom, Edward Grefenstette, Nal Kalchbrenner, et al. A Convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. 2015.
[65] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin. Convolutional Sequence to Sequence Learning. 2017.
[66] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv preprint arXiv:1502.03167 (2015).
[67] Duchi, E. Hazan, and Y. Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Machine Learning Research, 2011.
[68] D. Kingma, J. Ba. Adam: A Method for Stochastic Optimization. International Conference for Learning Representations, 2015.
[69] T. Tieleman, and G. Hinton. RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.Technical report, 2012.
[70] M. Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint, 2012.
[71] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.
[72] Song Han, Huizi Mao, William J Dally. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. 2016, international conference on learning representations.
[73] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran Elyaniv, Yoshua Bengio. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. 2016,arXiv: Learning.
[74] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. 2016, european conference on computer vision.
更多干货请关注V X公众号：SIGAI

