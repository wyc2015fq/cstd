# 论文阅读：《Recurrent Models of Visual Attention》 - Soul Joy Hub - CSDN博客

2018年05月30日 09:28:38[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：523


[https://blog.csdn.net/u011239443/article/details/80474983](https://blog.csdn.net/u011239443/article/details/80474983)

论文地址：[http://pdfs.semanticscholar.org/09a5/03095db2d68b439e48d67481399198ed0e5b.pdf](http://pdfs.semanticscholar.org/09a5/03095db2d68b439e48d67481399198ed0e5b.pdf)

# 摘要

将卷积神经网络应用于大型图像的计算量很大，因为计算量与图像像素数成线性关系。我们提出了一种新颖的循环神经网络模型，可以从图像或视频中提取信息，方法是自适应地选择一系列区域或位置，并仅以高分辨率处理选定区域。与卷积神经网络一样，所提出的模型具有内置的平移不变性程度，但其执行的计算量可以独立于输入图像大小进行控制。虽然模型是不可区分的，但可以使用强化学习方法来学习，以学习特定于任务的策略。我们在几个图像分类任务上评估我们的模型，其中它显着优于混乱图像上的卷积神经网络基线以及动态视觉控制问题，其中它学习跟踪简单对象而没有明确的训练信号。

# 1 介绍

基于神经网络的架构最近在极具挑战性的图像分类和物体检测数据集方面取得了巨大的成功。但是，他们出色的识别准确性在训练和测试时间都会带来高昂的计算成本。尽管输入图像被降采样以减少计算，但目前通常使用的大型卷积神经网络需要数天的时间才能在多个GPU上训练。在对象检测处理的情况下，当在单个GPU上运行时，测试时间的单个图像需要几秒钟，因为这些方法有效地遵循来自计算机视觉文献的经典滑动窗口范例，其中分类器被训练用于检测紧密裁剪中的对象边界框在不同位置和尺度上独立应用于数千个测试图像的候选窗口。尽管可以共享一些计算，但这些模型的主要计算费用来自于整个输入图像的卷积滤波映射，因此它们的计算复杂度至少在像素数量上是线性的。

人类感知的一个重要特性是不倾向于一次处理整个场景。 相反，人类有选择地将注意力集中在视觉空间的某些部分上，以获取需要的信息，并随时间将不同视角的信息相结合，以建立场景的内部表示，指导未来的眼球运动和决策制定。 由于需要处理更少的“像素”，因此将场景中的部分计算资源集中在一起可节省“带宽”。 但它也大大降低了任务的复杂性，因为感兴趣的对象可以放置在固定的中心，固定区域之外的视觉环境（“杂乱”）的不相关特征自然被忽略。

根据其基本作用，在神经科学和认知科学文献中广泛研究了人眼运动的指导。 虽然低层次场景的属性和自下而上的过程扮演着重要的角色，但人们注视的位置也显示出强烈的特定任务。在本文中，我们从这些结果中获得灵感，并开发了一个新颖的基于注意力的任务驱动的神经网络视觉处理‘框架。 我们的模型将视觉场景的基于注意力的处理视为控制问题，并且通用性足以应用于静态图像，视频或作为与动态视觉环境交互的代理的感知模块。

该模型是一个循环神经网络（RNN），它按顺序处理输入，一次一个地处理图像（或视频帧）内的不同位置，并递增地组合来自这些注视的信息以建立场景的动态内部表示，或环境。基于过去的信息和任务的需求，模型不是一次处理整个图像甚至是边界框，而是在每一步中选择下一个要注意的位置。我们的模型中的参数数量和它执行的计算量可以独立于输入图像的大小来控制，而卷积网络的计算需与图像像素的数量线性地成比例。我们描述了一个端到端的优化程序，该程序允许模型直接针对给定的任务进行训练，并最大限度地提高可能取决于模型做出的整个决策序列的性能测量。该过程使用反向传播来训练神经网络组件和策略梯度以解决由于控制问题导致的非差异性。

我们表明，我们的模型可以有效的学习特定于任务的策略，如多图像分类任务以及动态视觉控制问题。 我们的结果还表明，基于关注的模型可能比卷积神经网络更好地处理杂波和大输入图像。

# 2 前期工作

计算机视觉文献中计算限制受到了很多关注。 例如，对于对象检测，已经做了很多工作来降低广泛的滑动窗口范例的成本，主要着眼于减少评估完整分类器的窗口的数量，例如， 通过分类器级联，通过分类器输出上的分支和边界方法从考虑中去除图像区域，或者通过提出可能包含对象的候选窗口。 尽管使用这些方法可以获得实质性的加速，并且其中一些可以与CNN分类器相结合或用作CNN分类器的附加，但它们仍然牢牢扎根于用于对象检测的窗口分类器设计，并且只利用过去的信息来通知未来 以非常有限的方式处理图像。

显着性检测器是计算机视觉领域具有悠久历史，受人类感知强烈驱动的第二类方法。 这些方法优先处理潜在感兴趣（“显着”）图像区域的处理，该图像区域通常基于局部低级别特征对比度的某种度量来识别。 显着性检测器确实捕捉人眼运动的一些属性，但它们通常不会将信息整合到注视点上，它们的显着性计算大多是硬连线的，并且它们仅基于低级图像属性，通常忽略其他因素，例如 场景和任务需求的语义内容。

计算机视觉文献和其他文献中的一些作品已经将视觉作为一个顺序决策任务，就像我们在这里所做的那样。 在那里，和我们的工作一样，关于图像的信息是按顺序收集的，下一次参加的决定取决于以前的图像。

我们的工作也许与其他在深度学习框架中实施注意力处理的尝试最相似。 然而，我们的公式使用RNN来整合视觉信息，并决定如何行动，但是，我们的学习过程允许对顺序决策过程进行端到端的优化，而不是依赖贪婪的行为选择。 我们进一步展示了如何使用相同的通用体系结构在静止图像中进行高效的对象识别，并以任务驱动的方式与动态可视化环境进行交互。

# 3 循环注意力模型

在本文中，我们将关注问题视为目标导向代理与视觉环境交互的顺序决策过程。 在每个时间点，代理只通过一个带宽限制的传感器观察环境，即它不会完全感知环境。 它可能仅在本地区域或窄频带中提取信息。 但是，代理可以主动控制如何部署其传感器资源（例如，选择传感器位置）。 代理还可以通过执行操作来影响环境的真实状态。 由于只能部分观察环境，因此代理需要随时间整合信息，以确定如何采取行动以及如何最有效地部署传感器。 在每一步中，代理人都会收到一个标量奖励（这取决于代理人已执行并可能推迟的行动），代理人的目标是最大化这些奖励的总和。

该公式包含了各种任务，如静态图像中的对象检测和控制问题，如从屏幕上可见的图像流中播放电脑游戏。 对于游戏，环境状态将是游戏引擎的真实状态，并且代理的传感器将在屏幕上显示的视频帧上运行。 （请注意，对于大多数游戏，单个框架不会完全指定游戏状态）。 这里的环境行动将对应于操纵杆控制，并且奖励将反映得分。 对于静态图像中的对象检测，环境状态将是固定的，并与图像的真实内容相对应。 环境行动将与分类决定相对应（只有在固定数量的注视之后才能执行），并且奖励将反映决策是否正确。

## 3.1 模型

如图1所示，Agent是围绕循环神经网络构建的。 
![](https://upload-images.jianshu.io/upload_images/1621805-20a16a00e7443932.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 
图1：A）**Glimpse传感器**：给定Glimpse和输入图像的坐标，传感器提取以$l_{t-1}$为中心的包含多个分辨率块的视网膜样表示$ρ（x_t，l_{t-1}）$。 

  B）**Glimpse网络**：给定位置（$l_{t-1}$）和输入图像（$x_t$），使用Glimpse传感器来提取视网膜表示$ρ（x_t，l_{t-1}）$。 然后，视网膜表示和Glimpse位置被映射到使用分别由$θ_{g}^{0}$和$θ_{g}^{1}$参数化的独立线性层的隐藏空间，使用整流单元，接着是另一个线性层$θ_g^2$以组合来自两个分量的信息。 Glimpse网络$f_g(.;{θ_g^0,θ_g^1,θ_g^2})$定义了用于产生Glimpse表示$g_t$的注意力网络的可训练带宽限制传感器。 

  C）**模型架构**：总体而言，该模型是一个RNN。 模型$f_h (.; θ_h )$的核心网络将Glimpse表示$g_t$作为输入，并与上一时间步$h_{t-1}$的内部表示相结合，产生模型$h_t$的新内部状态。 位置网络$f_l（;θ_l）$和动作网络$f_a（;θ_a）$分别使用模型的内部状态$h_t$来产生下一个要注意的位置和动作/分类。 这个基本的RNN迭代有可变数量的步骤重复。

在每个时间步骤中，它处理传感器数据，随时间集成信息，并选择如何操作以及如何在下一步骤部署传感器：

**传感器**：在每个步骤t，代理以图像形式接收（部分）环境观察。 该代理不能完全访问该图像，而是可以通过其带宽受限传感器ρ从$x_t$中提取信息，例如， 通过将传感器聚焦在感兴趣的某个区域或频带上。

在本文中，我们假设带宽限制传感器从图像$x_t$中提取位置$l_{t-1}$周围的视网膜样表示$ρ（x_t，l_{t-1}）$。 它以高分辨率对$l$周围的区域进行编码，但是对于距离$l$更远的像素使用逐渐降低的分辨率，导致维度比原始图像$x$低得多的向量。 我们将把这个低分辨率表示看作是glimpse。 glimpse传感器用于我们称之为glimpse网络$f_g$的内部，以产生glimpse特征向量$gt = f_g（x_t，l_{t-1};θ_g）$，其中$θg= \{θ_g^0，θ_g^1，θ_g^2\}$（图1B）。

**内部状态**：代理维护一个内部状态，汇总从过去观察历史中提取的信息; 它对代理人的环境知识进行编码，并有助于决定如何采取行动以及在何处部署传感器。 这个内部状态由循环神经网络的隐含单元$h_t$组成，并且随时间由核心网络更新：$h_t = f_h(h_{t−1}, g_t; θ_h)$。 网络的外部输入是glimpse特征向量$g_t$。

**操作**：在每一步中，代理执行两个操作：它决定如何通过传感器控件$l_t$部署其传感器，以及可能影响环境状态的环境操作。环境行动的性质取决于任务。在这项工作中，位置动作是从位置网络$f_l（h_t;θ_l）$在时间t参数化的分布随机选择的：$l_t〜p（·| f_l（h_t;θ_l））$。在$〜p（·| f_a（h_t;θ_a））$处的第二个网络输出的条件下，环境行为同样得到。为了进行分类，它使用softmax输出进行制定，对于动态环境，其确切表述取决于为特定环境定义的动作集（例如，操纵杆运动，电机控制等）。最后，我们的模型还可以增加一个额外的动作，决定何时停止glimpse。例如，这可以用来学习一个成本敏感的分类器，通过给代理人每次glimpse给予一个负面的回报，迫使它代之以做出正确的分类和更多的glimpse的代价。

**奖励**：在执行一个行动之后，行动者接收一个对环境$x_{t + 1}$和奖励信号$r_{t + 1}$的新视觉观察。 代理的目标是最大化通常非常稀疏和延迟的奖励信号的总和：$R = \sum ^T_{t= 1} r_t$。 在对象识别的情况下，例如，如果在T步骤之后对象被正确分类，则$r_T = 1$，否则为0。

上述设置是RL社区中已知的作为部分可观察马尔可夫决策过程（POMDP）的特例。 环境的真实状态（可以是静态的或动态的）是不可见的。 在这个视图中，代理需要学习一个（随机）策略$π（（l_t，a_t）| s_{1:t};θ）$，参数θ表示在每个步骤t映射过去与环境交互的历史$s_{1:t} = x_1，l_1，a_1，... x_{t-1}，l_{t-1}，a_{t-1}$，$x_t$分配给当前时间步的动作分配，但不受传感器的限制。 在我们的例子中，策略π由上面概括的RNN定义，并且历史$s_t$在隐含单位$h_t$中被汇总。 我们将在第4节中描述上述组件的具体选择。

## 3.2 训练

我们的代理参数由glimpse网络，核心网络（图1C）和操作网络$θ=  \{θ_g，θ_h，θ_a\}$ 给予，并且我们学习这些以最大化当代理人与环境交互时可以期望的总奖励。

更正式地说，代理人的政策可能结合环境的动态（例如玩游戏），引发对可能的交互序列$s_{1:N}$的分配，并且我们的目标是在这种分配下最大化奖励:$J(θ) = E_{p(s_1:T ;θ)}[\sum ^T_{t= 1} r_t]= E_{p(s_1:T ;θ)}[R]$，其中$p(s_1:T ;θ)$取决于策略。

正确地最大化$J$不是简单的事，因为它涉及对高维相互作用序列的期望，这可能反过来涉及未知的环境动态。 然而，将问题看作是POMDP，可以让我们从RL文献中找到技术来承担：正如Williams所示，梯度的样本近似值由： 
![](https://upload-images.jianshu.io/upload_images/1621805-c90f7b356db23d57.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中$s_i$是通过针对$i = 1...M$事件运行当前代理$π_θ$获得的交互序列。

> 
根据情景，考虑折扣奖励总和可能更为合适，在远期未来获得的奖励贡献较少：$R = \sum ^T_{t= 1} γ^{t−1}r_t$。这种情况，我们有$T → ∞$

学习规则（1）也被称为REINFORCE规则，它涉及以当前策略运行代理以获取交互序列$s_{1:T}$的样本，然后调整智能体的参数θ，使得导致高累积奖励的选择行为增加，而产生低回报的行动减少。

等式（1）需要我们计算：$∇_θ logπ(u^i_t|s^i_{1:t};θ)$。但是，这只是RNN的梯度，它定义了我们的代理在时间步骤$t$评估并可以通过标准反向传播计算。

方差减少：方程（1）为我们提供了无偏估计的梯度，但它可能具有高方差。 因此通常考虑梯度估计： 
![](https://upload-images.jianshu.io/upload_images/1621805-bf4e881580aaf698.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，$R^i_t$是在执行行动$u_t^i$之后获得的累积奖励，$b_t$是基线，可以基于$s^i_{1:t}$ (如通过$h^i_t$) ,而不基于行动$u^i_t$本身。

这个估计值等于（1）的预期值，但可能有较低的方差。 选择$b_t =E_π[R_t] $是很自然的，这种形式的基线被称为强化学习文献中的价值函数。 由此产生的算法增加了一个行动的对数概率，其后跟随着一个大于预期的累积奖励，并且如果获得的累积奖励较小则降低概率。 我们使用这种类型的基线并通过减少$R_t^i$和$b_t$之间的平方误差来学习它。

$使用混合监督损失$：上述算法允许我们在“最佳”行为未知时训练代理，并且学习信号仅通过奖励提供。 举例来说，我们可能并不知道哪一系列的注视提供了关于未知图像的大部分信息，但是在一集结束时的总奖励将给我们指示尝试的序列是好还是坏。

但是，在某些情况下，我们确实知道要采取的正确操作：例如，在对象检测任务中，代理必须输出对象的标签作为最终操作。 对于训练图像，该标签将是已知的，并且我们可以直接优化策略以在观察序列结束时输出与训练图像相关的正确标签。这可以像监督学习中常见的那样通过给定来自图像的观察结果最大化真实标签的条件概率来实现，即通过最大化$logπ(a^∗_T|s_{1:T};θ)$，其中$a^∗_T$对应于与观察$s_1:T$所获得的图像相关的实况标签。 我们遵循这种分类问题的方法，我们优化交叉熵损失以训练操作网络$f_a$并通过核心和glimpse网络反向传播梯度。 位置网络$f_l$总是通过REINFORCE进行训练。

# 4 实验

我们在几个图像分类任务以及一个简单的游戏中评估了我们的方法。 我们首先描述所有实验中共同的设计选择：

**视网膜和位置编码**：视网膜编码$ρ（x，l）$提取以位置$l$为中心的$k$个正方形贴片，第一个贴片尺寸为$g_w×g_w$像素，并且每个连续贴片具有前一个宽度的两倍。 然后将$k$个补丁全部调整为$g_w×g_w$并连接。Glimpse位置$l$被编码为实值$（x，y）$坐标，其中$（0,0）$是图像$x$的中心，$（-1，-1）$是$x$的左上角。

**Glimpse网络**: Glimpse网络$f_g (x, l)$有两个全连接层。$Linear(x)$表示向量x的线性变换，$Linear(x) = W x+b $，$W$为权重矩阵，$b$为偏置。$Rect(x) = max(x, 0)$为整流器的非线性。glimpse网络的输出g被定义为$g=Rect(Linear(h_g)+Linear(h_l))$，其中$h_g =Rect(Linear(ρ(x,l))) $，$h_l = Rect(Linear(l))$。$h_g$和$h_l$的维度为128，而本文中训练的所有注意模型的维度为256。

**位置网络**：位置$l$的策略由具有固定方差的双分量高斯定义。 位置网络在时间$t$输出位置策略的均值并且被定义为$ f_l(h) = Linear(h)$，其中$h$是核心网络/ RNN的状态。

**核心网络**：对于核心$f_h$后面的分类实验，整流器单元网络定义为$h_t = f_h(h_{t−1}) = Rect(Linear(h_{t−1}) + Linear(g_t))$。在动态环境下进行的实验使用了LSTM单元的核心

## 4.1 图像分类

在以下分类实验中使用的注意力网络仅在最后一个时间步$t = N$做出分类决定。 动作网络$f_a$只是一个线性的softmax分类器，定义为$f_a（h）= exp（Linear（h））/ Z$，其中$Z$是一个归一化常数。 RNN状态向量$h$具有维度256。所有方法均使用随机梯度下降进行训练，其中尺寸为20，动量为0.9。 在训练过程中，我们将学习率从初始值线性地退化到0。 使用随机搜索来选择超参数，例如初始学习率和位置策略的方差。 如果代理商分类正确，则最后一步的奖励为1，否则为0。 所有其他时间步的奖励为0。

**居中数字**：我们首先测试了我们的训练方法通过使用它训练MNIST数字数据集上最多7次瞥见RAM模型的成功glimpse策略的能力。 这个实验的“视网膜”只是一个8×8的贴片，它只能容纳一个数字的一部分，因此该实验还测试了RAM结合多次glimpse信息的能力。 我们还训练了具有两个隐藏层的标准前馈和卷积神经网络作为基线。 

![](https://upload-images.jianshu.io/upload_images/1621805-492a31a2c3719a52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 
表1：在MNIST和翻译的MNIST数据集上的分类结果。 FC表示具有两层整流器单元的全连接网络。 卷积网络具有一个8步10×10个步长为5的滤波器，其后是每层之后的具有256个单元整流器的完全连接层。 注意力模型的实例标有glimpse的数量，视网膜中的片数量和视网膜的大小。

表1a列出了测试装置上不同模型的误差率。 我们发现RAM的性能通常会随着更多的glimpse而改善，并且它最终将胜过在全28×28居中数字上训练的基准模型。 这表明该模型可以成功地学习结合来自多个glimpse的信息。

**非居中数字**：我们考虑的第二个问题是对非居中数字进行分类。 我们创建了一个名为Translated MNIST的新任务，通过将MNIST数字放置在较大空白补丁的随机位置生成数据。 有效训练集大小为50000（MNIST训练集的大小）乘以可能的位置数。图2a包含60乘60转换MNIST任务的测试用例的随机样本。表1b显示了用60乘60图片在Translated MNIST任务上训练的几个不同模型的结果。 
![](https://upload-images.jianshu.io/upload_images/1621805-49dbf5c1d9277fbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 
图2：转换和杂乱转换MNIST任务的测试用例示例。

除了RAM和两个完全连接的网络之外，我们还训练了一个网络，其中一个卷积层由16个10×10的滤波器组成，步长为5，然后是整流器非线性，然后是256个整流器单元的完全连接层。 卷积网络，RAM网络和较小的全连接模型都具有大致相同的参数数量。 由于卷积网络内置了一定程度的平移不变性，因此与完全连接的网络相比，它的误码率显着降低到1.62％。 然而，具有4次瞥见的RAM性能比卷积网络略好，并且在6次和8次瞥见中进一步胜过它，达到1.2％的误差。 这是可能的因为注意模型可以将其视网膜集中在数字上并因此学习转化不变策略。 该实验还表明，当对象不居中时，注意模型能够成功地搜索大图像中的对象。

**凌乱的非中心位数**：分类真实世界图像最具挑战性的方面之一是存在广泛的杂波。 以全分辨率操作整个图像的系统特别容易混乱，并且必须学会不变。 注意机制的一个可能的优点是它可以通过关注图像的相关部分并且忽略不相关部分而使得在存在杂乱的情况下学习更容易。 我们用一些我们称之为Cluttered Translated MNIST的新任务进行了几次实验来验证这个假设。 该任务的数据是通过首先将MNIST数字放置在较大空白图像的随机位置，然后将来自其他随机MNIST数字的随机8乘8个子图像添加到图像的随机位置来生成的。 目标是分类图像中的完整数字。 图2b显示了60乘60混杂翻译MNIST任务的测试用例的随机样本。 
![](https://upload-images.jianshu.io/upload_images/1621805-301c3a6a7d27229e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 
表2：Cluttered Translated MNIST数据集上的分类。 FC表示具有两层整流器单元的全连接网络。卷积网络有一层8×10×10的滤波器，步长为5，其次是在60×60的情况下有256个单元的完全连接层，在100×100的情况下有86个单元，每层之后有整流器。关注模型的实例标有视线数量，视网膜大小和视网膜片数量。除大型全连接网络外，所有型号的参数数量大致相同。

表2a显示了我们用60块Cluttered Translated MNIST对4块杂波进行训练的模型的分类结果。 杂波的存在使得任务变得更加困难，但是关注模型的性能受到的影响要小于其他模型的性能。 具有4次glimpse的RAM达到4.96％的误差，其大幅超过完全连接的模型并且卷积神经网络超过3％，并且用6次和8次glimpse训练的RAM实现更低的误差。 由于RAM在杂波存在的情况下对卷积网络的相对误差提高较大，这些结果表明，基于注意力模型在处理杂波时可能比卷积网络更好，因为它们可以通过不看它而简单地忽略它。 学习策略的两个样本如图3所示，更多的样本包含在补充材料中。 
![](https://upload-images.jianshu.io/upload_images/1621805-70b957b69dad20f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 
图3：在60×60混杂转换的MNIST任务上学习策略的示例。 第1列：带有glimpse路径的输入图像以绿色覆盖。 第2-7列：网络选择的六个glimpse。 每个图像的中心显示全分辨率的glimpse，外部低分辨率区域通过将低分辨率glimpse升级为完整图像大小而获得。glimpse路径清楚地表明，学习策略避免了在输入空间的空白或嘈杂部分进行计算，并直接探索感兴趣对象周围的区域。

第一列显示覆盖了glimpse路径的原始数据点。第一次glimpse的位置用实心圆标记，并且最后glimpse的位置用空圆圈标记。 路径上的中间点用实线直线描绘。 右侧的每个连续图像都显示网络看到的glimpse。 可以看出，学习策略可以可靠地发现和探索感兴趣的对象，同时避免混乱。 最后，表2a还包括8-glimpse RAM模型的结果，其随机均匀地选择glimpse位置。 学习glimpse策略的RAM模型即使只有一半的glimpse，也可以实现低得多的错误率。

为了进一步检验这个假设，我们还对100×100 Cluttered Translated MNIST和8块杂波进行了实验。 表2b列出了我们比较的模型所实现的测试误差。 结果显示了卷积网络上RAM的类似改进。必须指出的是，我们的模型的整体容量和计算量并没有从60×60图像变为100×100，而连接到线性层的卷积网络的隐藏层随着输入中像素的数量线性增长。

## 4.2 动态环境

循环注意力模型的一个吸引人的特性是它可以像静态图像任务一样容易地应用于具有视觉输入的视频或交互式问题。 我们测试了我们的方法在动态视觉环境中学习控制策略的能力，同时通过训练它来玩简单的游戏，通过带宽有限的视网膜感知环境。 该游戏在24×24像素的二进制像素屏幕上播放，涉及两个对象：一个像素表示一个从屏幕顶部落下的球，同时弹出屏幕两侧，以及一个位于屏幕底部的双像素桨，该屏幕由控制器控制以捕捉球。 当下降像素到达屏幕底部时，如果桨与球交迭，则代理获得1奖励，否则奖励0。 游戏然后从头开始重新开始。

我们训练了循环注意力模型，只使用最终奖励作为输入来玩“Catch”游戏。 该网络在三个尺度上有一个6×6的视网膜作为其输入，这意味着该代理必须在6×6的最高分辨率区域捕捉球，以便知道其精确位置。 除了两个位置动作之外，关注模型还有三个游戏动作（左，右，并且什么都不做），并且动作网络$f_a$使用线性softmax来模拟游戏动作的分布。 我们使用了256个LSTM单元的核心网络。

我们进行随机搜索以找到合适的超参数并为每个代理训练2000万帧。 最佳代理的视频大约有85％的时间捕获球，可以从[http://www.cs.toronto.edu/vmnih/docs/retention.mov](http://www.cs.toronto.edu/vmnih/docs/retention.mov)下载。 该视频显示，循环注意力模型通过追踪屏幕底部附近的球来学习玩游戏。 由于代理人没有以任何方式被告知追踪球并且仅因为获得奖励而获得奖励，这一结果表明该模型有能力学习有效的特定任务的关注策略。

# 5 讨论

本文介绍了一种新颖的视觉注意模型，该模型被设计为一个单一的循环神经网络，它以一个glimpse窗口为输入，利用网络的内部状态来选择下一个要关注的位置，以及在动态的环境。尽管模型不可区分，但是所提出的统一架构是从像素输入到使用策略梯度方法的操作端到端地进行的。该模型有几个吸引人的属性。首先，可以独立于输入图像的大小来控制参数的数量和RAM执行的计算量。其次，该模型可以通过将视网膜对准相关区域来忽略图像中的杂乱。我们的实验显示RAM在杂乱的对象分类任务上明显胜过卷积体系结构，其参数数量可比。此外，我们的方法的灵活性允许一些有趣的扩展。例如，网络可以增加另一个动作，允许它在任何时间点终止并做出最终的分类决定。我们的初步实验表明，这可以让网络学会停止glimpse，一旦它有足够的信息做出自信的分类。网络也可以被允许控制视网膜对图像进行采样的尺度，从而允许其在固定尺寸的视网膜中适应不同尺寸的对象。在这两种情况下，额外的动作都可以简单地添加到动作网络$f_a$中，并使用我们描述的策略渐变过程进行训练。鉴于RAM所取得的令人鼓舞的成果，将该模型应用于大规模目标识别和视频分类是未来工作的一个自然方向。 
![](http://upload-images.jianshu.io/upload_images/1621805-e7dd694099f756b7?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

