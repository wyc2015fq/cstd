# 奇异值分解(SVD) - Soul Joy Hub - CSDN博客

2017年08月06日 11:58:14[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：532标签：[机器学习																[svd																[矩阵分解](https://so.csdn.net/so/search/s.do?q=矩阵分解&t=blog)](https://so.csdn.net/so/search/s.do?q=svd&t=blog)](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)
个人分类：[机器学习](https://blog.csdn.net/u011239443/article/category/6268728)



# [机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)

**版权声明：**

    本文由LeftNotEasy发布于[http://leftnoteasy.cnblogs.com](http://leftnoteasy.cnblogs.com/), 本文可以被全部的转载或者部分使用，但请注明出处，如果有问题，请联系[wheeleast@gmail.com](mailto:wheeleast@gmail.com)。也可以加我的微博: [@leftnoteasy](http://weibo.com/leftnoteasy)

**前言：**

    上一次写了关于[PCA与LDA](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)的文章，PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。在上篇文章中便是基于特征值分解的一种解释。特征值和奇异值在大部分人的印象中，往往是停留在纯粹的数学计算中。而且线性代数或者矩阵论里面，也很少讲任何跟特征值与奇异值有关的应用背景。奇异值分解是一个有着很明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识，实际上，人脸上的特征是有着无数种的，之所以能这么描述，是因为人天生就有着非常好的抽取重要特征的能力，让机器学会抽取重要的特征，SVD是一个重要的方法。

    在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing）

    另外在这里抱怨一下，之前在百度里面搜索过SVD，出来的结果都是俄罗斯的一种狙击枪（AK47同时代的），是因为穿越火线这个游戏里面有一把狙击枪叫做SVD，而在Google上面搜索的时候，出来的都是奇异值分解（英文资料为主）。想玩玩战争游戏，玩玩COD不是非常好吗，玩山寨的CS有神马意思啊。国内的网页中的话语权也被这些没有太多营养的帖子所占据。真心希望国内的气氛能够更浓一点，搞游戏的人真正是喜欢制作游戏，搞Data Mining的人是真正喜欢挖数据的，都不是仅仅为了混口饭吃，这样谈超越别人才有意义，中文文章中，能踏踏实实谈谈技术的太少了，改变这个状况，从我自己做起吧。

    前面说了这么多，本文主要关注奇异值的一些特性，另外还会稍稍提及奇异值的计算，不过本文不准备在如何计算奇异值上展开太多。另外，本文里面有部分不算太深的线性代数的知识，如果完全忘记了线性代数，看本文可能会有些困难。

**一、奇异值与特征值基础知识：**

    特征值分解和奇异值分解在机器学习领域都是属于满地可见的方法。两者有着很紧密的关系，我在接下来会谈到，特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。先谈谈特征值分解吧：

   1）**特征值：**

    如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/20110119222632467.png)

    这时候λ就被称为特征向量v对应的特征值，一个矩阵的一组特征向量是一组正交向量。特征值分解是将一个矩阵分解成下面的形式：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226327992.png)

    其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角阵，每一个对角线上的元素就是一个特征值。我这里引用了一些参考文献中的内容来说明一下。首先，要明确的是，一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。比如说下面的一个矩阵：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226323008.png)    它其实对应的线性变换是下面的形式：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226323913.png)    因为这个矩阵M乘以一个向量(x,y)的结果是：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226335026.png)    上面的矩阵是对称的，所以这个变换是一个对x，y轴的方向一个拉伸变换（每一个对角线上的元素将会对一个维度进行拉伸变换，当值>1时，是拉长，当值<1时时缩短），当矩阵不是对称的时候，假如说矩阵是下面的样子：



![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226336454.png)

    它所描述的变换是下面的样子：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226334536.png)

    这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最**主要的**变化方向（变化方向可能有不止一个），**如果我们想要描述好一个变换，那我们就描述好这个变换主要的变化方向就好了**。反过头来看看之前特征值分解的式子，分解得到的Σ矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）

    当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。也就是之前说的：**提取这个矩阵最重要的特征。**总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，**特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。**

   （说了这么多特征值变换，不知道有没有说清楚，请各位多提提意见。）

**   2）奇异值：**

    下面谈谈奇异值分解。特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，**我们怎样才能描述这样普通的矩阵呢的重要特征呢？**奇异值分解可以用来干这个事情，**奇异值分解是一个能适用于任意的矩阵的一种分解的方法**：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226332060.png)    假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量），从图片来反映几个相乘的矩阵的大小可得下面的图片

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226342650.png)

    那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到：![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226348223.png)    这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226346304.png)    这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，**在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了**。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下**部分奇异值分解**：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226358289.png)

    r是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子：



![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226356370.png)

    右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵：U、Σ、V就好了。

**二、奇异值的计算：**

    奇异值的计算是一个难题，是一个O(N^3)的算法。在单机的情况下当然是没问题的，matlab在一秒钟内就可以算出1000 * 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。Google的**吴军**老师在**数学之美**系列谈到SVD的时候，说起Google实现了SVD的并行化算法，说这是对人类的一个贡献，但是也没有给出具体的计算规模，也没有给出太多有价值的信息。

    其实SVD还是可以用并行的方式去实现的，在解大规模的矩阵的时候，一般使用迭代的方法，当矩阵的规模很大（比如说上亿）的时候，迭代的次数也可能会上亿次，如果使用Map-Reduce框架去解，则每次Map-Reduce完成的时候，都会涉及到写文件、读文件的操作。个人猜测Google云计算体系中除了Map-Reduce以外应该还有类似于MPI的计算模型，也就是节点之间是保持通信，数据是常驻在内存中的，这种计算模型比Map-Reduce在解决迭代次数非常多的时候，要快了很多倍。

[Lanczos迭代](http://en.wikipedia.org/wiki/Lanczos_algorithm)就是一种解**对称方阵部分特征值**的方法（之前谈到了，解A’* A得到的对称方阵的特征值就是解A的右奇异向量），是将一个对称的方程化为一个三对角矩阵再进行求解。按网上的一些文献来看，Google应该是用这种方法去做的奇异值分解的。请见Wikipedia上面的一些引用的论文，如果理解了那些论文，也“几乎”可以做出一个SVD了。

    由于奇异值的计算是一个很枯燥，纯数学的过程，而且前人的研究成果（论文中）几乎已经把整个程序的流程图给出来了。更多的关于奇异值计算的部分，将在后面的参考文献中给出，这里不再深入，我还是focus在奇异值的应用中去。

**三、奇异值与主成分分析（PCA）：**

     主成分分析在上一节里面也讲了一些，这里主要谈谈如何用SVD去解PCA的问题。PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226357275.png)     这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。

    一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。

    PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。

    还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226366436.png)

    而将一个m * n的矩阵A变换成一个m * r的矩阵，这样就会使得本来有n个feature的，变成了有r个feature了（r < n)，这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226364833.png)    但是这个怎么和SVD扯上关系呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量…我们回忆一下之前得到的SVD式子：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226366818.png)     在矩阵的两边同时乘上一个矩阵V，由于V是一个正交的矩阵，所以V转置乘以V得到单位阵I，所以可以化成后面的式子

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226374899.png)     将后面的式子与A * P那个m * n的矩阵变换为m * r的矩阵的式子对照看看，在这里，其实V就是P，也就是一个变化的向量。这里是将一个m * n 的矩阵压缩到一个m * r的矩阵，也就是对列进行压缩，如果我们想对行进行压缩（在PCA的观点下，对行进行压缩可以理解为，将一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉）怎么办呢？同样我们写出一个通用的行压缩例子：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226374060.png)    这样就从一个m行的矩阵压缩到一个r行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以U的转置U’

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226374408.png)    这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。

**四、奇异值与潜在语义索引LSI：**

     潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在[矩阵计算与文本处理中的分类问题](http://www.google.com.hk/ggblog/googlechinablog/2006/12/blog-post_8935.html)中谈到：

*    “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。”*

     上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial，具体的网址我将在最后的引用中给出：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226386634.png)      这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226397148.png)      左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。

      继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；

      其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。

      然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到：

![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226404739.png)     在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。

     不知道按这样描述，再看看吴军老师的文章，是不是对SVD更清楚了？:-D

**参考资料：**

1）A Tutorial on Principal Component Analysis, Jonathon Shlens 
     这是我关于用SVD去做PCA的主要参考资料    
2）[http://www.ams.org/samplings/feature-column/fcarc-svd](http://www.ams.org/samplings/feature-column/fcarc-svd)
     关于svd的一篇概念好文，我开头的几个图就是从这儿截取的    
3）[http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html](http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html)
     另一篇关于svd的入门好文    
4）[http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html](http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html)
     svd与LSI的好文，我后面LSI中例子就是来自此    
5）[http://www.miislita.com/information-retrieval-tutorial/svd-lsi-tutorial-1-understanding.html](http://www.miislita.com/information-retrieval-tutorial/svd-lsi-tutorial-1-understanding.html)
     另一篇svd与LSI的文章，也还是不错，深一点，也比较长    
6）Singular Value Decomposition and Principal Component Analysis, Rasmus Elsborg Madsen, Lars Kai Hansen and Ole Winther, 2004    
     跟1）里面的文章比较类似






   转载请声明出处[http://blog.csdn.net/zhongkejingwang/article/details/43053513](http://blog.csdn.net/zhongkejingwang/article/details/43053513)

    在网上看到有很多文章介绍SVD的，讲的也都不错，但是感觉还是有需要补充的，特别是关于矩阵和映射之间的对应关系。前段时间看了国外的一篇文章，叫A Singularly Valuable Decomposition The SVD of a Matrix，觉得分析的特别好，把矩阵和空间关系对应了起来。本文就参考了该文并结合矩阵的相关知识把SVD原理梳理一下。

   SVD不仅是一个数学问题，在工程应用中的很多地方都有它的身影，比如前面讲的PCA，掌握了SVD原理后再去看PCA那是相当简单的，在推荐系统方面，SVD更是名声大噪，将它应用于推荐系统的是Netflix大奖的获得者Koren，可以在Google上找到他写的文章；用SVD可以很容易得到任意矩阵的满秩分解，用满秩分解可以对数据做压缩。可以用SVD来证明对任意M*N的矩阵均存在如下分解：

![](https://img-blog.csdn.net/20150123160014873?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

这个可以应用在数据降维压缩上！在数据相关性特别大的情况下存储X和Y矩阵比存储A矩阵占用空间更小！

   在开始讲解SVD之前，先补充一点矩阵代数的相关知识。

## 正交矩阵

   正交矩阵是在欧几里得空间里的叫法，在酉空间里叫酉矩阵，一个正交矩阵对应的变换叫正交变换，这个变换的特点是不改变向量的尺寸和向量间的夹角，那么它到底是个什么样的变换呢？看下面这张图

![](https://img-blog.csdn.net/20150123124108372?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

假设二维空间中的一个向量OA，它在标准坐标系也即e1、e2表示的坐标是中表示为(a,b)’（用’表示转置），现在把它用另一组坐标e1’、e2’表示为(a’,b’)’，存在矩阵U使得(a’,b’)’=U(a,b)’，则U即为正交矩阵。从图中可以看到，正交变换只是将变换向量用另一组正交基表示，在这个过程中并没有对向量做拉伸，也不改变向量的空间位置，加入对两个向量同时做正交变换，那么变换前后这两个向量的夹角显然不会改变。上面的例子只是正交变换的一个方面，即旋转变换，可以把e1’、e2’坐标系看做是e1、e2坐标系经过旋转某个斯塔角度得到，怎么样得到该旋转矩阵U呢？如下

![](https://img-blog.csdn.net/20150123135846821)

![](https://img-blog.csdn.net/20150123135825015)

![](https://img-blog.csdn.net/20150123135926913)

a’和b’实际上是x在e1’和e2’轴上的投影大小，所以直接做内积可得，then

![](https://img-blog.csdn.net/20150123140252265)

从图中可以看到

![](https://img-blog.csdn.net/20150123131646169)![](https://img-blog.csdn.net/20150123131621812)

所以

![](https://img-blog.csdn.net/20150123140255328)

正交阵U行（列）向量之间都是单位正交向量。上面求得的是一个旋转矩阵，它对向量做旋转变换！也许你会有疑问：刚才不是说向量空间位置不变吗？怎么现在又说它被旋转了？对的，这两个并没有冲突，说空间位置不变是绝对的，但是坐标是相对的，加入你站在e1上看OA，随着e1旋转到e1’，看OA的位置就会改变。如下图：

![](https://img-blog.csdn.net/20150123142158281?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

如图，如果我选择了e1’、e2’作为新的标准坐标系，那么在新坐标系中OA（原标准坐标系的表示）就变成了OA’，这样看来就好像坐标系不动，把OA往顺时针方向旋转了“斯塔”角度，这个操作实现起来很简单：将变换后的向量坐标仍然表示在当前坐标系中。

旋转变换是正交变换的一个方面，这个挺有用的，比如在开发中需要实现某种旋转效果，直接可以用旋转变换实现。正交变换的另一个方面是反射变换，也即e1’的方向与图中方向相反，这个不再讨论。

总结：正交矩阵的行（列）向量都是两两正交的单位向量，正交矩阵对应的变换为正交变换，它有两种表现：旋转和反射。正交矩阵将标准正交基映射为标准正交基（即图中从e1、e2到e1’、e2’）

## 特征值分解——EVD

    在讨论SVD之前先讨论矩阵的特征值分解（EVD），在这里，选择一种特殊的矩阵——对称阵（酉空间中叫hermite矩阵即厄米阵）。对称阵有一个很优美的性质：它总能相似对角化，对称阵不同特征值对应的特征向量两两正交。一个矩阵能相似对角化即说明其特征子空间即为其列空间，若不能对角化则其特征子空间为列空间的子空间。现在假设存在mxm的满秩对称矩阵A，它有m个不同的特征值，设特征值为

![](https://img-blog.csdn.net/20150123145748984)

对应的单位特征向量为

![](https://img-blog.csdn.net/20150123145830171)

则有

![](https://img-blog.csdn.net/20150123150002344)

进而

![](https://img-blog.csdn.net/20150123150119033)

![](https://img-blog.csdn.net/20150123150331493)

![](https://img-blog.csdn.net/20150123150505140)

所以可得到A的特征值分解（由于对称阵特征向量两两正交，所以U为正交阵，正交阵的逆矩阵等于其转置）

![](https://img-blog.csdn.net/20150123150757364)

这里假设A有m个不同的特征值，实际上，只要A是对称阵其均有如上分解。

矩阵A分解了，相应的，其对应的映射也分解为三个映射。现在假设有x向量，用Ａ将其变换到Ａ的列空间中，那么首先由U’先对x做变换：

![](https://img-blog.csdn.net/20150123151414245)

U是正交阵U’也是正交阵，所以U’对x的变换是正交变换，它将x用新的坐标系来表示，这个坐标系就是A的所有正交的特征向量构成的坐标系。比如将x用A的所有特征向量表示为：

![](https://img-blog.csdn.net/20150123151915529)

则通过第一个变换就可以把x表示为[a1 a2 … am]’：

![](https://img-blog.csdn.net/20150123152046383)

紧接着，在新的坐标系表示下，由中间那个对角矩阵对新的向量坐标换，其结果就是将向量往各个轴方向拉伸或压缩：

![](https://img-blog.csdn.net/20150123152331057)

从上图可以看到，如果A不是满秩的话，那么就是说对角阵的对角线上元素存在0，这时候就会导致维度退化，这样就会使映射后的向量落入m维空间的子空间中。

最后一个变换就是U对拉伸或压缩后的向量做变换，由于U和U’是互为逆矩阵，所以U变换是U’变换的逆变换。

因此，从对称阵的分解对应的映射分解来分析一个矩阵的变换特点是非常直观的。假设对称阵特征值全为1那么显然它就是单位阵，如果对称阵的特征值有个别是0其他全是1，那么它就是一个正交投影矩阵，它将m维向量投影到它的列空间中。

根据对称阵A的特征向量，如果A是2*2的，那么就可以在二维平面中找到这样一个矩形，是的这个矩形经过A变换后还是矩形：

![](https://img-blog.csdn.net/20150123155218277)

这个矩形的选择就是让其边都落在A的特征向量方向上，如果选择其他矩形的话变换后的图形就不是矩形了！

## 奇异值分解——SVD

   上面的特征值分解的A矩阵是对称阵，根据EVD可以找到一个（超）矩形使得变换后还是（超）矩形，也即A可以将一组正交基映射到另一组正交基！那么现在来分析：对任意M*N的矩阵，能否找到一组正交基使得经过它变换后还是正交基？答案是肯定的，它就是SVD分解的精髓所在。

   现在假设存在M*N矩阵A，事实上，A矩阵将n维空间中的向量映射到k（k<=m）维空间中，k=Rank(A)。现在的目标就是：在n维空间中找一组正交基，使得经过A变换后还是正交的。假设已经找到这样一组正交基：

![](https://img-blog.csdn.net/20150123160515876)

则A矩阵将这组基映射为：

![](https://img-blog.csdn.net/20150123160626263)

如果要使他们两两正交，即

![](https://img-blog.csdn.net/20150123160744762)

根据假设，存在

![](https://img-blog.csdn.net/20150123160916671)

所以如果正交基v选择为A’A的特征向量的话，由于A’A是对称阵，v之间两两正交，那么

![](https://img-blog.csdn.net/20150123161147171)

这样就找到了正交基使其映射后还是正交基了，现在，将映射后的正交基单位化：

因为

![](https://img-blog.csdn.net/20150123161911890)

所以有

![](https://img-blog.csdn.net/20150123162005218)

所以取单位向量

![](https://img-blog.csdn.net/20150123162032674)

由此可得

![](https://img-blog.csdn.net/20150123162324773)

当k < i <= m时，对u1，u2，…，uk进行扩展u(k+1),…,um，使得u1，u2，…，um为m维空间中的一组正交基，即

![](https://img-blog.csdn.net/20150123162811221)

同样的，对v1，v2，…，vk进行扩展v(k+1),…,vn（这n-k个向量存在于A的零空间中，即Ax=0的解空间的基），使得v1，v2，…，vn为n维空间中的一组正交基，即

![](https://img-blog.csdn.net/20150123202328388)

则可得到

![](https://img-blog.csdn.net/20150123165814334)

继而可以得到A矩阵的奇异值分解：

![](https://img-blog.csdn.net/20150123170018218)

![](https://img-blog.csdn.net/20150123170122018)

现在可以来对A矩阵的映射过程进行分析了：如果在n维空间中找到一个（超）矩形，其边都落在A’A的特征向量的方向上，那么经过A变换后的形状仍然为（超）矩形！

vi为A’A的特征向量，称为A的右奇异向量，ui=Avi实际上为AA’的特征向量，称为A的左奇异向量。下面利用SVD证明文章一开始的满秩分解：

![](https://img-blog.csdn.net/20150123170421531)

利用矩阵分块乘法展开得：

![](https://img-blog.csdn.net/20150123171001047)

可以看到第二项为0，有

![](https://img-blog.csdn.net/20150123171137580)

令

![](https://img-blog.csdn.net/20150123171202821)

![](https://img-blog.csdn.net/20150123171215379)

则A=XY即是A的满秩分解。

整个SVD的推导过程就是这样，后面会介绍SVD在推荐系统中的具体应用，也就是复现Koren论文中的算法以及其推导过程。

参考文献：[A Singularly Valuable Decomposition The SVD of a Matrix](http://www-users.math.umn.edu/~lerman/math5467/svd.pdf)



