# 《机器学习技法》学习笔记15——矩阵分解 - Soul Joy Hub - CSDN博客

2017年08月05日 21:22:32[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：1658
所属专栏：[机器学习技法与实战](https://blog.csdn.net/column/details/16096.html)



[http://blog.csdn.net/u011239443/article/details/76735871](http://blog.csdn.net/u011239443/article/details/76735871)

# 线性网络模型

Netflix在2006年给出了一个数据集 

（用户id，电影id，电影评分） 

让我们来预测用户未评分的电影评分分数。 

我们可以讲用户id进行二分向量编码，然后同意用户的电影评分组成一个向量，即得到：

![](http://upload-images.jianshu.io/upload_images/1621805-10b8c6061d00328f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-34e3086393ffdd43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因为向量x只有一个值为1，所以模型可以变成：

![](http://upload-images.jianshu.io/upload_images/1621805-f766bcfbd4faca6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而对于某一个电影的预测评分可以写作：

![](http://upload-images.jianshu.io/upload_images/1621805-2d2ee286a32746e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 矩阵分解基础

损失函数为：

![](http://upload-images.jianshu.io/upload_images/1621805-76ecdf1b4b8a7c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们可以讲电影评分预测看作矩阵分解：

![](http://upload-images.jianshu.io/upload_images/1621805-63bf9f6b83b7db40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 
交替最小二乘法（ALS） 

  我们使用用户喜好特征矩阵$U(m*k)$中的第i个用户的特征向量$u_i$，和产品特征矩阵$V(n*k)$第j个产品的特征向量$v_j$来预测打分矩阵$A(m*n)$中的$a_{ij}$。我们可以得出一下的矩阵分解模型的损失函数为：

$\large C = \sum\limits_{(i,j)\in R}[(a_{ij} - u_iv_j^T)^2+\lambda(u_i^2+v_j^2)]$

有了损失函数之后，下面就开始介绍优化方法。通常的优化方法分为两种：交叉最小二乘法（alternative least squares）和随机梯度下降法（stochastic gradient descent）。Spark使用的是交叉最小二乘法（ALS）来最优化损失函数。算法的思想就是：我们先随机生成然后固定它求解，再固定求解，这样交替进行下去，直到取得最优解$min(C)$。因为每步迭代都会降低误差，并且误差是有下界的，所以 ALS 一定会收敛。但由于问题是非凸的，ALS 并不保证会收敛到全局最优解。但在实际应用中，ALS 对初始点不是很敏感，是否全局最优解造成的影响并不大。

算法的执行步骤：
- 先随机生成一个。一般可以取0值或者全局均值。
- 
固定，即认为是已知的常量，来求解： 
$\large C = \sum\limits_{(i,j)\in R}[(a_{ij} - u_i^{(0)}v_j^T)^2+\lambda((u_i^2)^{(0)}+v_j^2)]$

  由于上式中只有$v_j$一个未知变量，因此C的最优化问题转化为最小二乘问题，用最小二乘法求解$v_j$的最优解： 

  固定$ j , j\in (1,2,...,n)$，则：等式两边关于为$v_j$求导得：

$\large \frac{d(c)}{d(v_j)} $
$\large= \frac{d}{d(v_j)}(\sum\limits_{i=1}^{m}[(a_{ij} - u_i^{(0)}v_j^T)^2+\lambda((u_i^2)^{(0)}+v_j^2)])$
$\large= \sum\limits_{i=1}^m[2(a_{ij} - u_i^{(0)}v_j^T)(- (u_i^T)^{(0)})+2\lambda v_j]$
$\large= 2\sum\limits_{i=1}^m[( u_i^{(0)}(u_i^T)^{(0)}+\lambda)v_j-a_{ij}(u_i^T)^{(0)}]$

令$\large \frac{d(c)}{d(v_j)} =0$，可得： 
$\large\sum\limits_{i=1}^m[( u_i^{(0)}(u_i^T)^{(0)}+\lambda)v_j]=\sum\limits_{i=1}^m a_{ij}(u_i^T)^{(0)} $
$\large  => (U^{(0)}(U^T)^{(0)} + \lambda E)v_j = a_j^TU^{(0)}$

令 $M_1 = U^{(0)}(U^T)^{(0)} + \lambda E , M_2 = a_j^TU^{(0)}$，则$v_j = M_1^{-1}M_2$

  按照上式依次计算$v_1，v_2，...，v_n$，从而得到$V^{(0)}$

- 
同理，用步骤2中类似的方法: 
$\large C = \sum\limits_{(i,j)\in R}[(a_{ij} - u_i(v_j^T)^{(0)})^2+\lambda(u_i^2+(v_j^2)^{(0)})]$

  固定$ i , i\in (1,2,...,m)$，则：等式两边关于为$u_i$求导得：

$\large \frac{d(c)}{d(u_i)} $
$\large= \frac{d}{d(u_i)}(\sum\limits_{j=1}^{n}[(a_{ij} - u_i(v_j^T)^{(0)})^2+\lambda((u_i^2)+(v_j^2)^{(0)})])$
$\large= \sum\limits_{j=1}^n[2(a_{ij} - u_i(v_j^T)^{(0)})(- (v_j^T)^{(0)})+2\lambda u_i]$
$\large= 2\sum\limits_{j=1}^n[( v_j^{(0)}(v_j^T)^{(0)}+\lambda)u_i-a_{ij}(v_j^T)^{(0)}]$

令$\large \frac{d(c)}{d(u_i)} =0$，可得： 
$\large\sum\limits_{j=1}^n[( v_j^{(0)}(v_j^T)^{(0)}+\lambda)u_i]=\sum\limits_{j=1}^n a_{ij}(v_j^T)^{(0)} $
$\large  =>( (V^{(0)}(V^T)^{(0)} + \lambda E)u_i = a_i^TV^{(0)}$

  令 $M_1 = V^{(0)}(V^T)^{(0)} + \lambda E , M_2 =a_i^TV^{(0)}$，则$u_i = M_1^{-1}M_2$

  按照上式依次计算$u_1，u_2，...，u_n$，从而得到$U^{(1)}$

- 循环执行步骤2、3，直到损失函数C的值收敛（或者设置一个迭代次数N，迭代执行步骤2、3，N次后停止）。这样，就得到了C最优解对应的矩阵U、V。

线性自动编码器和矩阵分解对比：

![](http://upload-images.jianshu.io/upload_images/1621805-b735485b60d5a8d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 随机梯度下降

我们尝试使用随机梯度下降的方法来最小化损失函数：

![](http://upload-images.jianshu.io/upload_images/1621805-0f0ed3bf52319ea5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

于是我们的优化模型就可以表示成：

![](http://upload-images.jianshu.io/upload_images/1621805-10bd4ecf480120cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当训练集的数据时间上要早于测试集，这么一来，时间上越晚的数据应该权重越高。于是我们可以使用随机梯度下降变形——基于时间的梯度下降——在训练后期只选择时间靠后的数据。 
![这里写图片描述](https://img-blog.csdn.net/20170904105701712?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTIzOTQ0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

