# 基于神经网络的智能对话系统（二）——机器学习背景知识 - Soul Joy Hub - CSDN博客

2018年12月03日 15:48:15[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：599
所属专栏：[基于神经网络的智能对话系统](https://blog.csdn.net/column/details/18953.html)



# 2. 机器学习背景知识

本章简要回顾了深度学习和强化学习，这些学习与后续章节中的会话AI最相关。

## 2.1 机器学习基础

Mitchell（1997）将机器学习广义地定义为包括任何计算机程序，该计算机程序通过经验E来改善其在某个任务T（由P测量）的性能。

如表1.2所示，对话是一个明确定义的学习问题，T，P和E规定如下：

•T：与用户进行对话以实现用户的目标。

•P：表1.2中定义的累积奖励。

•E：一组对话，每个对话都是一系列用户 - 代理交互。

举一个简单的例子，通过人类标记的问答配对的经验，单转QA对话代理可以通过QA任务生成的答案的准确性或相关性来衡量其绩效。

使用监督学习（SL）构建ML代理的常见配方包括数据集，模型，成本函数（a.k.a.损失函数）和优化过程。

•数据集由$（x，y ^ *）$对组成，其中对于每个输入$x$，存在实际的输出$y ^*$。在QA中，$x$由输入问题和生成答案的文档组成，$y^*$是由知识渊博的外部检查人提供的期望答案。

•模型通常具有$y = f（x;θ）$的形式，其中f是由$θ$参数化的函数（例如，神经网络），其将输入$x$映射到输出$y$。

•成本函数是形式L（y *，f（x;θ））。 L（。）通常被设计为平滑的误差函数，并且是可微分的w.r.t. θ。满足这些标准的常用成本函数是均方误差或MSE，定义为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127175755612.png)

•可以将优化视为搜索算法，以识别最小化$L（.）$的最佳$θ$。鉴于L是可微分的，最广泛使用的深度学习优化程序是小批量随机梯度下降（SGD），其在每批后更新θ
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127175918230.png)

其中M是批量大小，α是学习率。

> 
1 如表1.2所示，对话学习被表述为RL，其中代理学习策略π，在每个对话中，基于对话状态s从集合A中选择适当的动作a，以便实现最大的累积奖励。

虽然SL从固定数据集中学习，但在诸如对话1之类的交互式问题中，获得所需行为的示例通常是不切实际的，这些行为既正确又代表了代理必须采取行动的所有状态。在未开发的领域，代理必须学习如何通过与环境相互作用来进行操作，称为强化学习（RL），其中代理与其体验之间存在反馈循环。换句话说，虽然SL从先前经验丰富的外部主管提供的经验中学习，但RL通过自己的经验来学习。 RL在几个重要方面与SL不同（Sutton和Barto，2018; Mitchell，1997）

•勘探开发权衡。在RL中，代理需要从环境中收集奖励信号。这提出了哪个实验策略产生最有效学习的问题。代理人必须利用它已经知道的东西来获得回报，但它也必须探索未知的状态和行动，以便在将来做出更好的行动选择。

•延迟奖励和临时信用分配。在RL中，训练信息不像SL中那样以$（x，y *）$的形式提供。相反，当代理执行一系列操作时，环境仅提供延迟的奖励。例如，我们不知道对话是否成功完成任务直到会话结束。因此，代理人必须确定其序列中的哪些动作被记入产生最终奖励，这是一种称为临时信用分配的问题。

•部分观察到的状态。在许多RL问题中，从每个步骤的环境中感知到的观察，例如每个对话转向中的用户输入，仅提供关于整个环境状态的部分信息，代理根据该信息选择下一个动作。神经方法通过编码在当前和过去步骤中观察到的所有信息（例如，所有先前的对话轮次和来自外部数据库的检索结果）来学习深度神经网络以表示状态。

SL和RL的核心挑战是泛化 - 代理在未见输入上表现良好的能力。已经提出了许多学习理论和算法来通过例如寻求在可用训练经验的量与模型能力之间的良好折衷以避免欠拟合和过度拟合来成功地解决该挑战。与以前的技术相比，神经网络方法通过利用深度神经网络的表示学习能力提供了一种可能更有效的解决方案，我们将在下一节简要回顾。

## 2.2 深度学习

深度学习（DL）涉及训练神经网络，其原始形式由单层（即感知器）组成（Rosenblatt，1957）。感知器甚至无法学习逻辑异或等简单函数，因此后续工作探索了“深层”架构的使用，这增加了输入和输出之间的隐藏层（Rosenblatt，1962; Minsky和Papert，1969），通常称为多层感知器（MLP）或深度神经网络（DNN）的神经网络。本节介绍NLP和IR的一些常用DNN。有兴趣的读者可以参考Goodfellow等人。 （2016）进行全面讨论。

### 2.2.1 基础

考虑文本分类问题：通过诸如“sport”和“politics”之类的域名标记文本字符串（例如，文档或查询）。如图2.1（左）所示，经典ML算法首先使用一组手工设计的特征（例如，单词和字符n-gram，实体和短语等）将文本字符串映射到矢量表示x，然后学习具有softmax层的线性分类器以计算域标签的分布y = f（x; W），其中W是使用SGD从训练数据学习的矩阵以最小化误分类错误。设计工作主要集中在特征工程上。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127191258429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEyMzk0NDM=,size_16,color_FFFFFF,t_70)

而不是使用手工设计的x特征，DL方法使用DNN联合优化特征表示和分类，如图2.1（右）所示。我们看到DNN由两部分组成。上半部分可视为线性分类器，类似于图2.1（左）中的传统ML模型，但其输入向量h不是基于手工设计的特征，而是使用下半部分学习可以将DNN视为与端到端方式的分类器一起优化的特征生成器。与经典ML不同，设计DL分类器的工作主要是优化DNN架构以进行有效的表示学习。

对于NLP任务，根据我们希望在文本中捕获的语言结构的类型，我们可以应用不同类型的神经网络（NN）层结构，例如用于局部词依赖性的卷积层和用于全局词序列的循环层。这些层可以组合和堆叠以形成深层体系结构，以在不同的抽象级别捕获不同的语义和上下文信息。下面描述了几种广泛使用的NN层：

> 
2 我们经常在本文中省略用于简化符号的偏差项。

**单词嵌入层**：在符号空间中，每个单词表示为单热矢量，其维数N是预定义词汇表的大小。词汇量通常很大;例如，N> 100K。我们应用（预训练的）单词嵌入模型，该模型由线性投影矩阵$W_e∈R^{N×M}$参数化，以将每个单热矢量映射到嵌入的神经空间中的M维实值向量（M << N）。在语义上更相似的单词的向量彼此更接近。

**完全连接层** :它们执行线性投影为$W^Tx$ 2。我们可以通过在每个线性投影之后引入非线性激活函数g来堆叠多个完全连接的层以形成深度前馈NN（FFNN）。如果我们将文本字符串视为Bag-Of-Words（BOW）并且让x是文本中所有单词的嵌入向量的总和，则深FFNN可以提取高度非线性的特征来表示文本的隐藏语义主题。不同的层，例如，第一层的$h（1）=g(W^{（1）_T}x)$，第二层的$h（2）=g(W^{（2）_T}h（1）)$，依此类推，其中W是可训练的矩阵。

**卷积最大池化层**：一个例子如图2.1（右）所示。卷积层在两个步骤中形成字$w_i$的局部特征向量，表示为$u_i$。它首先通过连接$w_i$的单词嵌入向量及其由固定长度窗口定义的周围单词来生成上下文向量$c_i$。然后执行投影以获得$u_i = g（W^T_c c_i）$，其中$W_c$是可训练矩阵并且g是激活函数。最大池操作在由卷积层计算的向量序列的每个“时间”i上应用最大操作以获得全局特征向量h，其中每个元素被计算为$h_j =max_{1≤i≤L}u_{i，j}$ 。

**递归层**：递归神经网络（RNN）的一个例子如图2.2所示。 RNNs通常用于句子嵌入，我们将文本字符串视为单词序列而不是一个BOW。他们将文本字符串映射到密集和低维度的语义向量顺序地和循环地处理每个单词，并将子序列映射到当前单词到低维矢量为$h_i = RNN（x_i，h_{i-1}）:= g（W^T_{ih}x_i+W_r^Th_{i-1}）$，其中$x_i$是在文本中嵌入第i个单词的单词，$W_{ih}$和$W_r$是可训练的矩阵，$h_i$是直到第i个单词的单词序列的语义表示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127205236856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEyMzk0NDM=,size_16,color_FFFFFF,t_70)

### 2.2.2 DSSM的案例研究

DSSM代表深度结构化语义模型，或更一般地，深度语义相似性模型。 DSSM是一种用于测量一对输入（x，y）的语义相似性的深度学习模型。根据（x，y）的定义，它们可以应用于各种任务。例如，（x，y）是用于Web搜索排名的查询 - 文档对（Huang et al。，2013; Shen et al。，2014），推荐中的文档对（Gao et al。，2014b），一个问题QA中的一对（Yih等，2015a），机器翻译中的一对句子（Gao et al。，2014a），以及图像字幕中的图像 - 文本对（Fang et al。，2015）和等等。

如图2.3所示，DSSM由一对DNN（f1和f2）组成，它们将输入x和y映射到公共低维语义空间中的相应向量。然后通过两个矢量的余弦距离测量x和y的相似性。 f1和f2可以是不同的体系结构，具体取决于x和y。例如，为了计算图像 - 文本对的相似性，f1可以是深度卷积NN，f2可以是RNN。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127205259354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEyMzk0NDM=,size_16,color_FFFFFF,t_70)

设θ为f1和f2的参数。学习θ以识别x和y的最有效的特征表示，直接针对最终任务进行优化。换句话说，我们学习一个隐藏的语义空间，用θ参数化，其中空间中矢量之间距离的语义由任务定义，或者更具体地说，是任务的训练数据。例如，在Web文档排名中，距离测量查询 - 文档相关性，并且使用成对排名损失来优化θ。考虑查询x和两个候选文档y +和y-，其中y +比y-与x更相关。令simθ（x，y）为由θ参数化的语义空间中x和y的相似度
$\large sim_θ(x, y) = cos(f_1(x), f_2(y)).$

我们想要最大化$Δ=simθ（x，y^+）- simθ（x，y^-）$。我们通过优化平滑损失函数来实现：
$\large L(∆; θ) = log (1 + exp (−γ∆)) , (2.2)$

其中γ是比例因子，使用公式的SGD  2.1。

## 2.3 强化学习

本节简要回顾强化学习，这与后面章节中的讨论最为相关。对于全面的调查，感兴趣的读者可以参考优秀的教科书和评论，如Sutton和Barto（2018）;凯尔林等人。 （1996）; Bertsekas和Tsitsiklis（1996）; Szepesva ri（2010）; Wiering和van Otterlo（2012年）;李（2019年）。

### 2.3.1 基础

强化学习是一种学习范式，智能代理通过与最初未知的环境进行交互来学习做出最佳决策（Sutton和Barto，2018）。与监督学习相比，RL中的一个独特挑战是在没有教师的情况下学习（即没有监督标签）。正如我们将要看到的，这将导致RL经常独有的算法考虑因素。

如图2.4所示，代理 - 环境交互通常被建模为离散时间马尔可夫决策过程，或MDP（Puterman，1994），由五元组$M =⟨S，A，P，R，γ⟩$描述：

•$S$是环境可能存在的无限状态集;

•$A$是代理人可以在一个州内采取的一系列可能的行动;

•$P（s'| s，a）$给出在状态s中采取行动a后环境着陆在新状态s’中的转移概率;

•$R（s，a）$是代理人在采取行动a后立即收到的平均奖励;和

•$γ∈（0,1]$是折扣因子。

可以将交叉点记录为轨迹$（s_1，a_1，r_1，...）$，如下生成：在步骤$t =1,2，...，$

•代理观察环境的当前状态$s_t∈S$，并在$a_t∈A$时采取行动;

•环境转换到下一个状态$s_{t + 1}$，根据转移概率$P（·| s_t，a_t）$分布;

•与过渡相关联的是即时奖励$r_t∈R$，其平均值为$R（s_t，a_t）$。

省略下标，每一步都会产生一个称为转换的元组$（s，a，r，s'）$。 RL代理的目标是通过采取最佳行动（即将定义）来最大化长期奖励。它的行动选择政策，用$π$表示，可以是确定性的或随机的。在任何一种情况下，我们使用$a ∼π（s）$来表示通过跟随状态s中的π来选择动作。给定策略π，状态s的价值是该状态的平均折扣长期奖励：

$\large V^π(s):=E[r_1 +γr_2 +γ^2r_3 +···|s_1 =s,a_i ∼π(s_i),∀_i≥1].$

我们有兴趣优化策略，以便$V^π$最大化所有状态。由$π^*$表示最优策略，而$V ^*$表示其对应的值函数（也称为最优值函数）。在许多情况下，使用称为Q函数的另一种形式的值函数更方便：

$\large Q^π(s,a):=E[r_1 +γr_2 +γ^2r_3 +···|s_1 =s,a_1 =a,a_i ∼π(s_i),∀_i>1]$

它通过首先选择状态s然后遵循政策π来衡量平均折扣长期奖励。对应于最优策略的最佳Q函数由$Q^*$表示。

### 2.3.2 基本算法

我们现在简要介绍两种流行的算法，例如Q-learning和policy gradient两种算法。

**Q学习**。第一类基于观察，即如果最佳Q函数可用，则可以立即检索最优策略。具体而言，可以通过确定最优策略：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203135918882.png)

因此，一大类RL算法专注于学习$Q ^*（s，a）$，并且统称为基于价值函数的方法。

实际上，当表格问题很大时，用表格表示$Q（s，a）$是很昂贵的，每个$（s，a）$表示一个条目。例如，Go游戏中的状态数量大于$2×10^{170}$（Tromp和Farnebäck，2006）。因此，我们经常使用紧凑形式来表示Q.特别地，我们假设Q函数具有预定义的参数形式，由一些向量$θ∈R^d$参数化。一个例子是线性近似：

![$ \large Q(s, a; θ) = φ(s, a)Tθ $](https://img-blog.csdnimg.cn/20181203140539904.png)

其中$φ（s，a）$是状态 - 动作对$（s，a）$的d维手动编码特征向量，θ是要从数据中学习的对应系数向量。通常，$Q（s，a;θ）$可以采用不同的参数形式。例如，在深Q网络（DQN）的情况下，$Q（s，a;θ）$采用深层神经网络的形式，例如多层感知器（Tesauro，1995; Mnih等，2015），递归网络（Hausknecht和Stone，2015; Li等，2015）等。此外，可以使用决策树以非参数方式表示Q函数（Ernst et al。，2005）或高斯过程（Engel等，2005），这超出了本介绍部分的范围。

为了学习Q函数，我们在观察状态转换$（s，a，r，s'）$后使用以下更新规则修改参数θ：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203140918534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEyMzk0NDM=,size_16,color_FFFFFF,t_70)

上述更新被称为Q-learning（Watkins，1989），它对θ应用了一个小变化，

由步长参数α控制并根据时间差计算（Sutton，1988）。

虽然很受欢迎，但在实践中，Q学习可能不稳定，并且在达到$Q^*$的良好近似之前需要许多样本。两种修改通常在实践中有所帮助。第一个是经验重播（Lin，1992），由Mnih等人推广。 （2015年）。不是使用观察到的过渡来使用(2.3)更新θ，而是将其存储在重放缓冲区中，并定期从其中过滤以执行Q学习更新。这样，每次转换都可以多次使用，从而提高了样品效率。此外，当更新参数θ时，它有助于防止数据分布随时间过快变化，从而有助于稳定学习。

第二个是双网络实施（Mnih等，2015）。在这里，学习者维护一个额外的Q函数副本，称为目标网络，由$θ_{target}$参数化。在学习期间，$θ_{target}$是固定的并且用于计算时间差以更新θ。具体来说，Eqn 2.3现在变成：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203143627532.png)

周期性地，$θ_{target}$被更新为θ，并且该过程继续。这实际上是更一般的拟合值迭代算法的实例（Munos和Szepesva ri，2008）。

最近对上述基本Q学习进行了一些改进，例如决斗Q-network（Wang等，2016），双Q学习（van Hasselt等，2016），以及最近的SBEED算法，数据有效且可证明收敛（Dai等，2018b）。

> - 我们在更简单的有界长度轨迹情况下描述了策略梯度，尽管当轨迹长度无界时它可以扩展到问题（Baxter和Bartlett，2001; Baxter等，2001）。

策略梯度。另一类算法试图直接优化策略，而不必学习Q函数。这里，策略本身由$θ∈R^d$直接参数化，而$π（s;θ）$通常是动作上的分布。给定任何θ，该策略自然地通过它在长度H的轨迹中得到的平均长期回报来评估，$τ=（s_1，a_1，r_1，...，s_H，a_H，r_H）：^3$
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203144403967.png)

> - 随机梯度上升只是在否定目标函数上的随机梯度下降。

如果可以从采样轨迹估计梯度$∇_θJ$，可以进行随机梯度上升以最大化J：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203144743454.png)

其中α又是一个步长参数

一种这样的算法，称为REINFORCE（Williams，1992），估计梯度如下。令τ为由$π（·;θ）$生成的长度为H的轨迹;也就是说，对于每个t，在$a_t  ∼ π（s_t;θ）$处。然后，给出基于该单个轨迹的随机梯度：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203152621596.png)

REINFORCE在实践中可能会遇到很大的差异，因为它的梯度估计直接取决于沿整个轨迹的奖励总和。通过使用当前政策的估计值函数可以减少其方差，通常被称为行为者 - 批评者算法中的批评者（Sutton等，1999a; Konda和Tsitsiklis，1999）。现在计算决策梯度：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203152827177.png)

其中
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203153034375.png)

是当前策略$π（s;θ）$的估计值函数，用于近似方程2.6中的 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203153426263.png)

估计值函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203153034375.png)

可以通过标准时间差方法（类似于已经描述的Q学习）来学习，但是存在许多变体如何从数据中学习
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203153034375.png)

。此外，关于如何计算比Eqn2.7中最速下降更有效的梯度$∇_θJ$，已经有很多工作。有兴趣的读者可以参考一些相关的着作和其中的参考资料以获得更多细节（Kakade，2001; Peters等，2005; Schulman等，2015a，b; Mnih等，2016; Gu等，2017; ; Dai等，2018a; Liu等，2018a）。

### 2.3.3 探索

到目前为止，我们已经描述了在转换作为输入时更新值函数或策略的基本算法。通常，强化学习代理还必须确定如何选择动作来收集所需的学习转换。总是选择看起来最好的行动（“利用”）是有问题的，因为不选择新的行动（在目前为止收集的数据中代表性不足甚至空缺），称为“探索”，可能会导致不看到可能更好的结果。有效平衡探索和开发是强化学习中的独特挑战之一。

基本的探索策略被称为ε-greedy。我们的想法是选择具有高概率（用于利用）的最佳动作，以及具有小概率的随机动作（用于探索）。在DQN的情况下，假设θ是Q函数的当前参数，则状态s的动作选择规则如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181203154638456.png)

在许多问题中，这种简单的方法是有效的（尽管不一定是最佳的）。第 4.5.2节中有关探索的更深入讨论。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127171010511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEyMzk0NDM=,size_16,color_FFFFFF,t_70)

