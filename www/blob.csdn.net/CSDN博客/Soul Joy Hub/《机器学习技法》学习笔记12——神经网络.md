# 《机器学习技法》学习笔记12——神经网络 - Soul Joy Hub - CSDN博客

2017年08月04日 13:48:38[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：1235
所属专栏：[机器学习技法与实战](https://blog.csdn.net/column/details/16096.html)



[http://blog.csdn.net/u011239443/article/details/76680704](http://blog.csdn.net/u011239443/article/details/76680704)

# 动因

单隐藏层神经网络：

![](http://upload-images.jianshu.io/upload_images/1621805-3657bd5ec7894382.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

单隐藏层神经网络做“与”运算：

![](http://upload-images.jianshu.io/upload_images/1621805-74746716987b45fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是单隐藏层神经网无法做异或运算：

![](http://upload-images.jianshu.io/upload_images/1621805-75be5c4597717297.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到上面最右边的图，就算是映射到高维的空间中，依旧是线性不可分的。我们可以使用多层的神经网络来解决这个问题：

![](http://upload-images.jianshu.io/upload_images/1621805-3766fd6400ccd292.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 神经网络假说

神经网络基本上的模型为：

![](http://upload-images.jianshu.io/upload_images/1621805-b701d3c277dafcc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

由于阶梯函数不好求导优化；整个网络的激活函数都是线性函数的话，和只使用一个线性函数没太多的区别；所以我们更多使用S形函数，这里使用双曲正切函数，写作tanh：

![](http://upload-images.jianshu.io/upload_images/1621805-5bd4f10776853cb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我的可以得到新的基本模型：

![](http://upload-images.jianshu.io/upload_images/1621805-e3a65be1417368d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 神经网络学习

我们学习的目标的：学习到各层之间的w，使得最终输出的误差最小。记误差为：

![](http://upload-images.jianshu.io/upload_images/1621805-54053d497d4c71c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么我们是想使用（随机）梯度下降来计算：

![](http://upload-images.jianshu.io/upload_images/1621805-5cec184d63f72c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-30029b588780a7cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们先来看下对于最后一层w该如何计算：

![](http://upload-images.jianshu.io/upload_images/1621805-4d133e11f39ecb65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如果不是最后一层：

![](http://upload-images.jianshu.io/upload_images/1621805-135538f27b62ec1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-fbb592924df03ae3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如何计算δ呢？我们先来想想s和e的关系：

![](http://upload-images.jianshu.io/upload_images/1621805-bfbe75b2255fb3aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

于是，我们由链式求导可以得到以下递推的公式：

![](http://upload-images.jianshu.io/upload_images/1621805-441610f623d81745.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样一来我们就可以从最后一层开始计算得到前一层，一直到第一层。这就是著名的“反向传播”：

![](http://upload-images.jianshu.io/upload_images/1621805-c2139d4e313c073e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于每一条数据我们都经过1~3的步骤，向前传播一次，再向后传播一次，这样十分耗时。`mini-batch`的策略，让一批数据并行的进行1~3的步骤，将各个$x_i^{l−1}δ_j^l$的平均值代入步骤4。

# 优化与正则化

我们知道随机梯度下降/梯度下降我们只能找到局部的最优解。所以神经网络模型的效果，对于w的初始化十分敏感。过大的w值会使得在S函数上的变化变得很小，导致梯度消失。建议对w进行随机的小数值取值。

复杂的神经网络会产生过拟合，需要引入正则化：
- L1，不好求导。
- L2，本质上是在对原来的w根据一定比例缩小，然而原来w中大权重缩小后还是比原来小的权重大。
- 权消去正则化：

![](http://upload-images.jianshu.io/upload_images/1621805-229f276b6805f189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 根据验证集的结果，早点停止迭代。

![这里写图片描述](https://img-blog.csdn.net/20170804134951993?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTIzOTQ0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

