# 论文阅读：《A Deep Relevance Model for Zero-Shot Document Filtering》 - Soul Joy Hub - CSDN博客

2018年08月26日 12:44:28[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：157


ACL 2018

一种用于零样本文档过滤的深度相关性模型

A Deep Relevance Model for Zero-Shot Document Filtering

武汉大学、阿里巴巴集团

Wuhan University、Alibaba Group

【摘要】在大数据时代，能否在短时间内针对某些主题的文档信息进行分析是非常重要的需求，而信息过滤是实现这一目标不可或缺的任务之一。本文中我们提出了一种用于零样本文档过滤的深度相关性模型——DAZER，结合预训练的词向量，DAZER在词向量空间构建了文档和类别的交互信息，通过门卷积网络等结构来提取、综合相关性信号，并结合对抗学习技术进一步提升模型泛化能力。实验结果表明，DAZER在零样本文档过滤任务中大大的优于其他技术方案。

![](http://t12.baidu.com/it/u=4210163489,3402620280&fm=173&app=25&f=JPEG?w=640&h=88)

1 引言

在互联网以及大数据时代，有很多的场合都需要过滤不相关信息、提炼并分析符合某种需求的信息，例如突发事件的追踪等等。而信息需求由于动态变化往往具有不可预见性，潜在的类别种类是无穷的，我们不可能为所有潜在的类别标注好训练文档。对于这种需要对模型训练阶段未见过的新类别进行文档过滤，我们定义它为零样本文档过滤问题。

近期Dataless文本分类方法可以有效减少用于模型训练的文档标注的需求，它们用少部分种子词表示该类别，在不需要任何文档训练数据的情况下完成文档集的分类任务。但它们仅用于分类，无法直接用来做文档过滤。

因此我们提出了针对零样本文档过滤问题的DAZER模型，我们同样用少量种子词来表示类别，通过在词向量空间上构建类别与文档的交互信息，加上基于类别的门机制来有效控制卷积网络的信息流动来提炼相关性信息，并通过对抗学习进一步提高模型的泛化能力。

如果把种子词看作检索语句，信息检索模型也能用于该任务，然而信息检索非常依赖于精确的关键词匹配信号（Fang和Zhai, 2006; Wu 等，2007; Eickhoff 等, 2015; Guo 等, 2016a,b)，我们认为在零样本文档过滤任务中，种子词更有可能是语义性、概念性的，这和信息检索任务中是不同的，所以信息检索模型不能很好的解决该问题，实验部分亦能看出这一点。据我们所知DAZER是第一个用于零样本文档过滤的深度学习模型，在两个真实数据集（文本分类数据集20NG和电影评论数据集MovieReview）上都体现出了非常明显的优势。

![](http://t12.baidu.com/it/u=4210163489,3402620280&fm=173&app=25&f=JPEG?w=640&h=88)

2 模型

模型结构如图1所示，包含两个主要的部分：相关信号提取以及相关性信号综合。

![](http://t12.baidu.com/it/u=3238604420,344298694&fm=173&app=25&f=JPEG?w=640&h=188&s=0DD0E81395746C231CDD88CA000010B3)

图1:DAZER模型框架图

2.1 相关信号提取

给定文档

![](http://t11.baidu.com/it/u=4106928547,1402615203&fm=173&app=25&f=JPEG?w=179&h=36&s=1AAA7A23EF17EA110855E8D20000C0B1)

，类别c的种子词集合

![](http://t12.baidu.com/it/u=1923923051,2675260227&fm=173&app=25&f=JPEG?w=86&h=35&s=1EAA7C22CF44FC114CDDD5C60000A0B1)

，我们把每个单词w映射为词向量表达

![](http://t11.baidu.com/it/u=967677166,1591938642&fm=173&app=25&f=JPEG?w=72&h=44&s=1AAA7423CBC0EC130C705CC20000A0B1)

（词向量是由外界大量数据集预训练得来）,然后用词向量平均的方法将类别c映射到词向量空间: 

![](http://t11.baidu.com/it/u=1601778814,109081797&fm=173&app=25&f=JPEG?w=177&h=55&s=5AAC3C62C586DD115C5415DE0000E0B1)

。

2.1.1 基于交互的表达

由于词向量蕴含了语意、句法等诸多信息（Mikolov等, 2013; Pennington等, 2014)，我们认为类似于余弦相似度这样的数值信息太过于粗糙，损失了大量信息，所以我们用两种元素级别的交互来表示交互信息，分别为元素减法：

![](http://t10.baidu.com/it/u=1926546552,2589334723&fm=173&app=25&f=JPEG?w=130&h=44&s=3AAA7A23C5F4E0230CF580D2000080B1)

以及元素相乘:

![](http://t12.baidu.com/it/u=1900723826,735039322&fm=173&app=25&f=JPEG?w=126&h=44&s=1AAA7A238574EC2308F454C20000C0B1)

。为检验这两类信息的有效性，我们进行了简单的实验来佐证，结果如表1、表2所示。最终我们拼接单词w的词向量以及两种交互向量得到

![](http://t11.baidu.com/it/u=2187558961,3783703568&fm=173&app=25&f=JPEG?w=213&h=44&s=1AA87822CD85CC1308557DC3000060B1)

，对文档d中每个单词依次进行处理便得到了交互信息矩阵

![](http://t12.baidu.com/it/u=4214284097,4261230722&fm=173&app=25&f=JPEG?w=164&h=45&s=0AAA7022CDB768114A7565D30000A0B1)

。

![](http://t11.baidu.com/it/u=3780145390,1181438462&fm=173&app=25&f=JPEG?w=400&h=115&s=CD8AF91B90E849011A5914DB0200C0B1)

表1:验证元素减法有效性的例子

![](http://t11.baidu.com/it/u=1263431455,612927924&fm=173&app=25&f=JPEG?w=482&h=98&s=6802E51BCE59C40150D514C6010010B0)

表2:验证元素乘法有效性的例子

2.1.2 卷积以及最大K池化

我们用1维的卷积来对上一步得到的交互矩阵进行线性变换:

![](http://t12.baidu.com/it/u=2520687476,4247440917&fm=173&app=25&f=JPEG?w=168&h=44&s=4AAA3C62E9B7CC134A5DA9D2000080B1)

。在通过类别门机制（见下小节）后，我们运用最大K池化的方法，拼接每个卷积核里最大的前K个值形成一个向量： 

![](http://t12.baidu.com/it/u=2868240146,77827107&fm=173&app=25&f=JPEG?w=326&h=28)

。

2.1.3 类别门机制

考虑到对于不同的类别，各个卷积核的重要性可能是不一样的，于是我们通过

![](http://t11.baidu.com/it/u=1319186905,3259045171&fm=173&app=25&f=JPEG?w=167&h=25)

得到一个类别门向量，其维度等同于卷积核的个数，激活函数为sigmoid，该门向量因此可以视作卷积核的开关向量。在进行最大K池化之前我们将门向量与卷积后的矩阵进行元素相乘：

![](http://t12.baidu.com/it/u=924957530,974006661&fm=173&app=25&f=JPEG?w=232&h=44&s=1EAC7C22C797CE314CF568D70000C0B1)

。我们这里运用的门卷积网络与 (Dauphin 等, 2017)提出的Gated Linear Units (GLU) 有些类似，与他们不同的是，我们这里卷积和门的计算使用的是不同的输入。

2.2相关性信号的综合

对于最大K池化后得到的相关性信号

![](http://t10.baidu.com/it/u=2133261330,4136377029&fm=173&app=25&f=JPEG?w=30&h=35)

，我们先用一层激活函数为tanh的全连接层

![](http://t12.baidu.com/it/u=236990468,1467530823&fm=173&app=25&f=JPEG?w=199&h=27)

将其化为隐相关性特征

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

；再通过一层全连接

![](http://t12.baidu.com/it/u=3941976926,439996385&fm=173&app=25&f=JPEG?w=230&h=27)

将特征向量

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

转化为相关性得分

![](http://t11.baidu.com/it/u=516003379,2432791698&fm=173&app=25&f=JPEG?w=51&h=28)

。

![](http://t12.baidu.com/it/u=4210163489,3402620280&fm=173&app=25&f=JPEG?w=640&h=88)

3 模型的训练

3.1对抗学习

为了使模型的相关性综合模块具备对未知类别的泛化能力，我们期望隐性特征

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

是类别无关的，然而事实可能并不如此，于是我们运用对抗学习。具体做法是在

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

之上引入一个线性对抗分类器

![](http://t12.baidu.com/it/u=3769164215,1991738496&fm=173&app=25&f=JPEG?w=324&h=38)

，该分类器的目的是正确的将

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

分类，所以目标函数为：

![](http://t10.baidu.com/it/u=583670481,1391674945&fm=173&app=25&f=JPEG?w=314&h=62&s=1AAE7C22EF47F0010CD5D1C60000C0B1)

我们在分类器与

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

之间引入了梯度逆转层GRL，GRL可视为一个伪函数，公式为：

![](http://t10.baidu.com/it/u=3828096021,72368973&fm=173&app=25&f=JPEG?w=198&h=48&s=1AAA7423C9A0CC131C55B0CE0000E0B1)

其中

![](http://t10.baidu.com/it/u=1108531275,1948803913&fm=173&app=25&f=JPEG?w=12&h=28)

控制了对抗学习的权重，对于DAZER模型，我们采用排序模型中常见的pairwsemargin loss来进行训练，目标函数如下：



结合L2正则化、对抗学习、pairwisemargin loss，最终得到的目标函数为：

![](http://t12.baidu.com/it/u=874081202,4088977224&fm=173&app=25&f=JPEG?w=374&h=28)

其中

![](http://t10.baidu.com/it/u=3002314233,484114953&fm=173&app=25&f=JPEG?w=23&h=35)

控制了L2正则项的权重。

![](http://t12.baidu.com/it/u=4210163489,3402620280&fm=173&app=25&f=JPEG?w=640&h=88)

4 实验

在本节中，我们通过两个真实数据集来评测DAZER的效果。

4.1 Baseline模型

我们运用传统信息检索模型：BM25（Robertson 和 Walker,1994），基于神经网络的检索模型：DRMM(Guo 等, 2013)，K-NRM (Xiong 等, 2017)，DeepRank (Pang 等, 2017)，深度语义匹配模型：DSSM （Huang 等, 2013），基于种子词的支持向量机（SSVM）等模型进行对比实验。（对于SSVM我们首先基于种子词构建伪训练样本，然后训练一个one-classSVM，最后再进行文档过滤实验）

4.2 数据集

20-Newsgroup：一个广泛用于文本分类的数据集，它包含接近2万个文档，这些文档来自20个类。我们实验中运用的是bydate版本，一共包含18846篇文档，训练集测试集比例为6:4。

MovieReview：一个英文的电影评论数据集，包含5006篇文档，每个文档包含一个得分，根据得分我们将这些评论划分为五类：非常负面、负面、中立、正面、非常正面，各包含167，1030，1786，1682，341篇评论，训练集测试集比例为8:2。

我们运用在840万词下预训练好的300维Glove词向量，词向量在实验中不再更新（DAZER及其他Baseline均如此）。停用词以及未出现在Glove中的词均被除去，每一个单词均转化为小写形式。

4.3 评价策略及指标

对于给定的待测试类别（训练中不会出现），我们用训练集中其他类的文档来训练模型，用测试集的所有文档来做测试。对于待测试的未见类别，模型目的是让属于这个类别的文档排名尽量靠前，于是我们运用常见的mean averageprecision （MAP）来进行评价。除SSVM和BM25外，每个模型随机运行五次，取结果的平均值，同时对于所有的神经网络模型，我们随机选取了训练集中一个类别的数据作为验证集，用来作为训练过程中的earlystop信号。

4.4 种子词的挑选

20NG数据集我们直接运用来自于（Song和 Roth, 2014)的种子词。Movie Review数据集的种子词选取我们按照（Chen 等, 2015)里的方法，借助LDA来进行人工挑选，其种子词如表3所示。20NG数据集中，每个类平均有5.2个种子词，MovieReview数据集中，每个类则平均有4.6个种子词。

![](http://t12.baidu.com/it/u=3885895628,1358327818&fm=173&app=25&f=JPEG?w=506&h=144&s=CC02F51BD4F479805EF550CE0100C0A2)

表3:MovieReview数据集的种子词

4.5 参数设置

对于DAZER，卷积核数目为50，宽度为5，最大K池化中的K为3，相关性综合中的

![](http://t10.baidu.com/it/u=3796870809,2136610926&fm=173&app=25&f=JPEG?w=32&h=35&s=0EAA7A238D85E911D8D500D2000080B1)

的维度为75，学习率为1e-5，batch大小为16，L2正则化权重为1e-4，对抗学习权重为0.1，优化器采用Adam。

4.6 结果对比

对于20NG数据集，我们创建了9个任务，对于Movie Review数据集我们把每个类别当作一次未见类来进行测试，最终实验结果如表4所示。

![](http://t11.baidu.com/it/u=2245185775,1618157255&fm=173&app=25&f=JPEG?w=640&h=358&s=E098EC33B1CFC8EB4E5DE0DB0100A0B1)

表4:七个模型在零样本文档过滤任务中的MAP表现。每个任务中，最好和次好的结果分别用加粗和下划线表示。表示该结果和最好的结果之间的显著性差异在0.05的级别。Avg：所有任务的平均MAP值。

4.7 实验结果分析

——DAZER效果明显最好，所有任务下的平均MAP为0.671，在文本分类任务和情感分析任务中结果都非常稳定；

——第二好的模型是K-NRM，然而DAZER相比K-NRM提升约30.8%；

——DSSM作为语义匹配模型在情感分析任务上优于文本分类任务，说明情感分析更像是一个语义匹配任务；

——SSVM的结果在两个任务中是均是最差的，说明对于有监督学习方法来说，高质量的标注数据非常重要，而用种子词构建的伪训练集包含了过多的噪音；

——BM25的表现不稳定，情感分析任务中它的效果明显下降，这说明在情感分析任务中有多种方式表达情感，难以用少量的种子词准确表达某一情感，这也证明了DAZER的优越性。

4.8 DAZER的分析

为了验证每个模块对DAZER模型的作用，我们在Movie Review数据集上进行了消蚀测试，结果如表5所示。

![](http://t12.baidu.com/it/u=3716115704,1392159354&fm=173&app=25&f=JPEG?w=622&h=120&s=4982E513CE49E0015ADD31DA0100D0B2)

表5:DAZER中不同模块的影响。最好的结果用加粗表示。

![](http://t10.baidu.com/it/u=2744091573,1886013993&fm=173&app=25&f=JPEG?w=57&h=44&s=7BAC346285B549824C5574CF0000A0B1)

表示没有元素减法，

![](http://t11.baidu.com/it/u=2150478286,3214665913&fm=173&app=25&f=JPEG?w=59&h=44&s=5AAC3C6285B549824E55E5DF0000A0B1)

表示没有元素乘法，

![](http://t11.baidu.com/it/u=770135509,2133169198&fm=173&app=25&f=JPEG?w=55&h=28)

表示没有基于类别的门机制，

![](http://t11.baidu.com/it/u=1365065383,3767177511&fm=173&app=25&f=JPEG?w=51&h=28)

表示没有对抗学习。可以看出每一个模块都对于模型有重要的作用。

![](http://t12.baidu.com/it/u=4210163489,3402620280&fm=173&app=25&f=JPEG?w=640&h=88)

5 相关工作

5.1文档过滤

对于给定的主题，区分相关和不相关的文档即文档过滤任务。基于分类和排序的方法都被提出过（Harman, 1994; Robertson 和 Soboroff, 2002;Soboroff 和 Robertson, 2003），早期的过滤系统主要用来协助文档排序 （Mostafa等, 1997)。基于词项的模式挖掘技术也广泛的用在了文档过滤任务上，（Nanas 等, 2010)运用一个基于网络的主题特征表来挖掘词项间的关联，（Gao等, 2013, 2015)  通过隐式主题来挖掘频繁词项模式，进而来做文档过滤，近期基于事件的微博过滤也用到了频繁模式挖掘的方法（Proskurnia 等, 2017）。然而这些都是有监督学习的方法，需要大量的标注数据。近期一些工作在以实体为中心的方式下进行文档过滤，这一任务是将文档分配给在外部数据库里定义好的实体。然而很多信息需求并不能表示为一个特定的实体，这些方法只适用于数据库相关的任务，例如数据库加速（Frank 等, 2012）。（Balog 和 Ramampiaro， 2013）测试了在过滤任务下基于分类和排序的方法的优劣，他们发现基于排序的方法更适合过滤任务。据此我们将零样本文档过滤任务定义为一个相关性排序任务。

5.2信息检索

在相关性排序的任务框架下，信息检索可以直接进行运用。传统的信息检索模型根据关键词匹配现象来估计相关性，其中BM25（Robertson 和 Walker，1994）可以视作这一类型方法中最优的一个。近期词向量的进展使得基于词向量的神经相关性排序模型大量出现，K-NRM（Xiong等, 2017）和DRMM（Guo等, 2016a）都是基于词向量间的余弦相似度信号来进行相关性估计的神经网络模型。DeepRank（Pang等, 2017）首先基于关键词匹配提取关键词周围的上下文，然后通过卷积得到相关性信号，最后用RNN用来综合相关性信号。这些基于神经网络的方法较之传统信息检索，有不同程度的效果提升。

5.3深度语义匹配模型

衡量检索语句和文档之间的相关性也可以视作一个文本间的匹配任务。目前有许多深度匹配模型，例如DSSM（Huang 等, 2013），ARC-Ⅱ（Hu 等, 2014），MatchPyramid(Pand 等, 2016），Match-SRNN（Wan 等, 2016）。这些模型主要用来完成特定的语义匹配任务，例如同义句检测任务等，这类任务中句子的结构常常需要考虑在内，而在零样本文档过滤中，种子词并不包含句法结构。同时（Guo等, 2016a）指出语义匹配模型在基于关键词的相关性估计任务上表现不佳。

![](http://t12.baidu.com/it/u=4210163489,3402620280&fm=173&app=25&f=JPEG?w=640&h=88)

6 结论

在本文中，我们提出了一种用于零样本文档过滤的深度相关性模型——DAZER，为了让DAZER具备良好的泛化能力，我们运用了两种词向量空间上的交互信息、基于类别的门卷积网络以及对抗学习等技术。最终在两种不同的任务（文本分类和情感分析）上均取得了最优的结果，验证了模型的优越性。在未来我们考虑丰富DAZER的结构，使之能够应用于少量样本学习的场景中。

