# 机器学习面试问题集（2018-3-13更新） - Soul Joy Hub - CSDN博客

2017年07月30日 11:43:27[卓寿杰_SoulJoy](https://me.csdn.net/u011239443)阅读数：10179


[http://blog.csdn.net/u011239443/article/details/76360294](http://blog.csdn.net/u011239443/article/details/76360294)

# 1 基础概念

## 1.1 熵、联合熵、条件熵、交叉熵与相对熵的意义？

![](http://upload-images.jianshu.io/upload_images/1621805-f0cd052ffe0a391a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1.2 归一化方法？

1、线性函数归一化(Min-Max scaling) 

线性函数将原始数据线性化的方法转换到[0 1]的范围，归一化公式如下： 
![](http://upload-images.jianshu.io/upload_images/1621805-09555fe9831dfc1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

2、0均值标准化(Z-score standardization) 

均值归一化方法将原始数据集归一化为均值为0、方差1的数据集，归一化公式如下：

![](http://upload-images.jianshu.io/upload_images/1621805-9df2949da8d9f004.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1.3 准确率,召回率,F值,ROC,AUC？

![](http://upload-images.jianshu.io/upload_images/1621805-95140faffaf35de7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 准确率

![](http://upload-images.jianshu.io/upload_images/1621805-09ce0c72d6272012.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

理解为你预测对的正例数占你预测正例总量的比率

### 召回率

![](http://upload-images.jianshu.io/upload_images/1621805-594ea5acc6d83ab3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

理解为你预测对的正例数占真正的正例数的比率

### F值

![](http://upload-images.jianshu.io/upload_images/1621805-f918c947642288eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

为准确率和召回率的调和平均值

### ROC和AUC

[http://blog.csdn.net/ybdesire/article/details/51999995](http://blog.csdn.net/ybdesire/article/details/51999995)

## 1.4 过拟合与正则化

### 1.4.1 为什么过拟合不好？

训练集中的数据有噪音，过拟合会学习到这些噪音，导致泛化性能变差。

### 1.4.2 正则化

![这里写图片描述](http://upload-images.jianshu.io/upload_images/1621805-d4a9c95b6af0aecb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

缺点：L1，不好求导。 L2，本质上是在对原来的w根据一定比例缩小，然而原来w中大权重缩小后还是比原来小的权重大。解决方法—— 权消去正则化：

![](http://upload-images.jianshu.io/upload_images/1621805-229f276b6805f189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1.5 优化方法

### 描述梯度下降？

![](http://upload-images.jianshu.io/upload_images/1621805-b89128651c44754f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 牛顿法 拟牛顿法？

#### 牛顿法

![](http://upload-images.jianshu.io/upload_images/1621805-fdbd5de9d4f176db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![牛顿法动图](http://upload-images.jianshu.io/upload_images/1621805-e76e41fe0f3f3340?imageMogr2/auto-orient/strip)

![](http://upload-images.jianshu.io/upload_images/1621805-724f6084c628611d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-57679ca3e70c1662.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 拟牛顿法

![](http://upload-images.jianshu.io/upload_images/1621805-c0c32a901b3264ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-4cc65479c5ce254b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-2562e77436947a71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### DFP

![](http://upload-images.jianshu.io/upload_images/1621805-3cd0ee6278b3fb72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-81987d8c9815d74d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-635779e70c1a0890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### BFGS

![](http://upload-images.jianshu.io/upload_images/1621805-5ab27c3ed18f6479.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-1ffbf1d429be6485.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-90ffcdaaee42bfff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1.6 各算法的优缺点
|算法|优点|缺点|
|----|----|----|
|KNN|1.简单、有效。2.重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的）3.计算时间和空间线性于训练集的规模（在一些场合不算太大）4.由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。5.该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。|1.KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多。2.类别评分不是规格化的（不像概率评分）。3.输出的可解释性不强，例如决策树的可解释性较强。4.该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。5.计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。|
|朴素贝叶斯|1.朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。2.NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单，常用于文本分类。3.对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。|1.理论上，NBC模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的（可以考虑用聚类算法先将相关性较大的属性聚类），这给NBC模型的正确分类带来了一定影响。在属性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。2.需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。3.由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。4.对输入数据的表达形式很敏感。|
|决策树|1.决策树易于理解和解释.人们在通过解释后都有能力去理解决策树所表达的意义。2.对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。3.能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。4.决策树是一个白盒模型。如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。5.易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。6.在相对短的时间内能够对大型数据源做出可行且效果良好的结果。7.可以对有许多属性的数据集构造决策树。8.决策树可很好地扩展到大型数据库中，同时它的大小独立于数据库的大小。|1.对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征。2.决策树处理缺失数据时的困难。3.过度拟合问题的出现。4. 忽略数据集中属性之间的相关性。|
|支持向量机|1.可以解决小样本情况下的机器学习问题。2.可以提高泛化性能。3.可以解决高维问题。4.可以解决非线性问题。5.可以避免神经网络结构选择和局部极小点问题。|1.对缺失数据敏感。2.对非线性问题没有通用解决方案，必须谨慎选择Kernel function来处理。|
|Adaboosting|1.adaboost是一种有很高精度的分类器。2.可以使用各种方法构建子分类器，Adaboost算法提供的是框架。3.当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。4.简单，不用做特征筛选。5.不用担心overfitting。||
|神经网络|1.分类的准确度高2.并行分布处理能力强,分布存储及学习能力强3.对噪声神经有较强的鲁棒性和容错能力4.能充分逼近复杂的非线性关系5.具备联想记忆的功能等。|1.神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值2.不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度3.学习时间过长,甚至可能达不到学习的目的。|

## 1.7 机器学习项目流程？
- 理解实际问题。抽象为机器学习能处理的数学问题 理解实际业务场景问题是机器学习的第一步，机器学习中特征工程和模型训练都是非常费时的，深入理解要处理的问题，能避免走很多弯路。
- 获取数据。“ 数据决定机器学习结果的上限，而算法只是尽可能的逼近这个上限”。总的来说数据要有具有“代表性”，对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。不仅如此还要对评估数据的量级，样本数量、特征数量，估算训练模型对内存的消耗。如果数据量太大可以考虑减少训练样本、降维或者使用分布式机器学习系统。
- 特征工程 

特征工程包括从原始数据中特征构建、特征提取、特征选择，非常有讲究。深入理解实际业务场景下的问题，丰富的机器学习经验能帮助我们更好的处理特征工程。特征工程做的好能发挥原始数据的最大效力，往往能够使得算法的效果和性能得到显著的提升，有时能使简单的模型的效果比复杂的模型效果好。数据挖掘的大部分时间就花在特征工程上面，是机器学习非常基础而又必备的步骤。数据预处理、数据清洗、筛选显著特征、摒弃非显著特征等等都非常重要。
- 模型训练、诊断、调优 

模型诊断中至关重要的是判断过拟合、欠拟合，常见的方法是绘制学习曲线，交叉验证。通过增加训练的数据量、降低模型复杂度来降低过拟合的风险，提高特征的数量和质量、增加模型复杂来防止欠拟合。诊断后的模型需要进行进一步调优，调优后的新模型需要重新诊断，这是一个反复迭代不断逼近的过程，需要不断的尝试，进而达到最优的状态。
- 模型验证、误差分析 

通过测试数据，验证模型的有效性，观察误差样本，分析误差产生的原因，往往能使得我们找到提升算法性能的突破点。误差分析主要是分析出误差来源与数据、特征、算法。
- 模型融合 

提升算法的准确度主要方法是模型的前端（特征工程、清洗、预处理、采样）和后端的模型融合。

## 1.8 样本不均衡问题

主要三个方面，数据，模型和评估方法。
- 
数据上重采样和欠采样，使之均衡；

- 
模型上选对样本不均衡问题不敏感的模型，如决策树；

- 
评估方法，想之前所说查全率，查准率之类。

## 1.9 损失函数
- 0-1损失
- 感知损失
- Hinge损失
- log损失、交叉熵
- 平方损失
- 指数损失
- 绝对值损失

![](http://www.csuldw.com/assets/articleImg/4DFDU.png)

参阅：[]()[http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/)

# 2 监督学习

## 2.1 朴素贝叶斯

### 描述朴素贝叶斯？

[http://blog.csdn.net/u011239443/article/details/53735609#t35](http://blog.csdn.net/u011239443/article/details/53735609#t35)

### 朴素贝叶斯对输入的数据有何要求？

朴素贝叶斯对输入数据的表达形式敏感。NBC模型假设属性之间相互独立。

### 极大似然估计与EM算法的区别？

#### 极大似然估计

![](http://upload-images.jianshu.io/upload_images/1621805-52ea3ba56bb90883.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-8ccbcf6b5a366048.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上式可以理解为，右边为在向量c条件下得到数据集$D_c$的概率，那么它便等于左边的向量c条件下得到数据集$D_c$中各条数据的概率的乘积。

![](http://upload-images.jianshu.io/upload_images/1621805-8c52c4b2df20615e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-4178dc7c68066d43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### EM算法

![](http://upload-images.jianshu.io/upload_images/1621805-21849141326c8877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-792e58fd484d0de1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-96cf49b049495d58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-b6369852499b8883.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-f5fd4888f6a0344c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-56e257f72819bdfe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 属性之间不完全相互独立，如何优化？

![](http://upload-images.jianshu.io/upload_images/1621805-3ccd3cb98e35dafd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-b37273b1daf09d47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 2.2 决策树

### 2.2.1 如何划分节点？
- p(x):分类结果x的概率，即分类结果为x的数据量/总数据量
- 信息：l(x) = -log2(p(x))
- 信息熵：信息的期望值 p(x1)l(x1) + p(x2)l(x2) + …… ，可以评价一组不同类别的划分结果的混沌度。

计算信息增益，选择信息增益最大的特征：

![](http://upload-images.jianshu.io/upload_images/1621805-c1f719d877f215f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-b088bb47e6a80dfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.2.2 如何做剪枝？

![](http://upload-images.jianshu.io/upload_images/1621805-ae0d8a3355fd1399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-e47ecb63648b2fe1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-8de8434dbc98a9ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-0dfd18dc4d3dc212.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-3ba99b0ccdd14fb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.2.3 信息增益缺点？

![](http://upload-images.jianshu.io/upload_images/1621805-a8dec2594ddf4f27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.2.4 基尼系数？

![](http://upload-images.jianshu.io/upload_images/1621805-10d404b1d504789b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.2.5 决策树 有哪些类？哪些适合做gbdt ？

决策树有ID3算法，2.2.1 所描述的就是ID3算法。除了ID3算法外，还有C4.5算法，C4.5和ID3的不同是使用了信息增益率，解决了2.2.3 所讲的信息增益的缺点。但是，GBDT使用的树是回归树。回归树算法有CART的最小二乘回归树： 
![](http://upload-images.jianshu.io/upload_images/1621805-573b1c434448e4c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-dd772dc089342bc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

CART的分类树思想与回归树类似，不同的是 需要优化的是 2.2.4 所示的基尼系数。

## 2.3 SVM

### 2.3.1 推导svm？

[http://blog.csdn.net/u011239443/article/details/76572743](http://blog.csdn.net/u011239443/article/details/76572743)
[http://blog.csdn.net/u011239443/article/details/76574969](http://blog.csdn.net/u011239443/article/details/76574969)
[http://blog.csdn.net/u011239443/article/details/76598872](http://blog.csdn.net/u011239443/article/details/76598872)

### 2.3.2 支撑向量是什么？

那些在间隔区边缘的训练样本点

### 2.3.3 svm的损失函数？

![](http://upload-images.jianshu.io/upload_images/1621805-21f244754adbe61f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-d89255b29069e0f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.3.4 如何解决线性不可分的情况？
- 少数点不可分，加入松弛变量。
- 整体都不可分的话，核函数映射到高维空间使得线性可分

### ‌ 2.3.5 SVM与逻辑回归的联系与区别？

#### 软间隔SVM与逻辑回归的联系？

损失函数 

软间隔SVM表示样本数据不必要求全部正确分类，允许少量的数据点犯错。于是将硬间隔SVM的优化目标由：

![](http://upload-images.jianshu.io/upload_images/1621805-e630bb2755ecd63d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在上面的优化目标中加上了惩罚项，C越大惩罚越大允许的错误越小。但是直接使用0/1损失函数的话其非凸、非连续，数学性质不好优化起来比较复杂，因此需要使用其他的数学性能较好的函数进行替换，替代损失函数一般有较好的数学性质。常用的三种替代函数： 
![这里写图片描述](http://upload-images.jianshu.io/upload_images/1621805-d2d7f70f5e6f6e1d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

　一般的软间隔SVM采用的是hinge损失函数（合页损失函数）进行替代，可以得到常见的软件的SVM的优化目标函数。如果采用的是对数损失函数进行替代那么就和逻辑回归的优化目标几乎相同，这就得到了软间隔SVM与逻辑回归的数学上的联系，因此一般来说SVM的性能和逻辑回归的性能差不多。

![](http://upload-images.jianshu.io/upload_images/1621805-84ff59376a55d63a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 软间隔SVM与逻辑回归的区别
- 逻辑回归通过输出预测概率后根据阈值进行判断类别，SVM则直接输出分割超平面，然后使用0/1函数对距离进行分类，不能直接输出概率值，如果需要SVM输出概率值则需要进行特殊处理，可以根据距离的大小进行归一化概率输出。
- 逻辑回归可以使用多阈值然后进行多分类，SVM则需要进行推广。
- SVM在训练过程只需要支持向量的，依赖的训练样本数较小，而逻辑回归则是需要全部的训练样本数据，在训练时开销更大。

### 2.3.6 核函数有哪些？

[http://blog.csdn.net/u011239443/article/details/76598872](http://blog.csdn.net/u011239443/article/details/76598872)

## 2.4 集成学习

## 2.4.1 gbdt

### 2.4.1.1 gbdt、rf的区别？

随机森林是 bagging 集成学习，是多棵树进行投票。 

gbdt  
![](http://upload-images.jianshu.io/upload_images/1621805-dfe6b4a2aa4591a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

更多可见 ： [http://blog.csdn.net/u011239443/article/details/77435463](http://blog.csdn.net/u011239443/article/details/77435463)

# 3 无监督学习

## 3.1 k-means

### 3.1.1 k-means 是时间、空间复杂度？

时间复杂度：O(T * n * k * m) 

空间复杂度：O(（ n + k ） * m) 

n:元素个数，k:聚类中心数，m:每个元素的特征项个数，T:迭代的次数

# 4 深度学习

## 4.1 神经网络

### 4.1.1 描述神经网络？推导反向传播？

[http://blog.csdn.net/u011239443/article/details/76680704](http://blog.csdn.net/u011239443/article/details/76680704)

### 4.1.2 对比激活函数（注意优缺点）？

[http://blog.csdn.net/cyh_24/article/details/50593400](http://blog.csdn.net/cyh_24/article/details/50593400)

### 4.1.3 描述dropout？

![](http://upload-images.jianshu.io/upload_images/1621805-17a863a65fda6a5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-20ee8d06a9759031.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-58acfaf90ace34ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-d256fcc416e82f13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-2058380f7e504ede.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/1621805-8cb8b6a44d004482.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 4.1.4 ‌什么情况下，浅层神经网络更好？什么情况下，深层神经网络更好？
|浅层神经网络|深层神经网络|
|----|----|
|更加有效的训练|难以训练|
|更简单的结构选择|复杂的结构选择|
|更具有理论依据，但可能难以提取特征|更加武断的依据，简化特征提取|
|适合多项式组合比较明确的情况下|适合多项式组合复杂未知的情况下|

### 4.1.5 梯度消失产生的原因？
- 
反向传播，前一层的梯度是由后一层梯度的乘积计算得到。假如每一层上的梯度小于1，越乘越小，到最前面的层就会梯度消失。

- 
S型神经元，在01附近梯度很小，而w和b的梯度是有S函数的梯度因式，从而导致梯度变小。

## 4.2 CNN

### 4.2.1 feature map 计算?

> 
(n+2p-f)/s + 1

- n：上一层长
- p：填充数
- f：过滤器长
- s：步长

### 4.2.2 池化层

#### 常见的池化方式有哪些？

最大池，平均池，l2池

#### 检测图像的纹理，用什么池化层？

max-pooling，可以进行信息压缩，不关心纹理的具体位置，而是否出现，大概在哪。

#### 池化层作用
- invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)
- 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力

[https://www.zhihu.com/question/36686900](https://www.zhihu.com/question/36686900)

## 4.3 LSTM
- 遗忘门限 
![](http://img.ptcms.csdn.net/article/201511/25/5655a201d61f2.jpg)
- 输入门限 
![](http://img.ptcms.csdn.net/article/201511/25/5655a21e6a118.jpg)
![](http://img.ptcms.csdn.net/article/201511/25/5655a2399979f.jpg)
- 输出门限 
![](http://img.ptcms.csdn.net/article/201511/25/5655a25bb7fdf.jpg)

参考：[http://blog.csdn.net/u011239443/article/details/73196473](http://blog.csdn.net/u011239443/article/details/73196473)

# 5 平台架构

## 5.1 Spark

### 5.1.1 spark 为什么不适合做神经网络？

（待~）

## 5.2 Tensorflow

### 5.2.1  描述下 Tensorflow 的基本架构？

[http://blog.csdn.net/u011239443/article/details/78945486](http://blog.csdn.net/u011239443/article/details/78945486)

# 6 业务场景题

## 问题一

> 
数据格式： (用户特征，商品特征)，预测用户是否会购买该商品？

这里给出之前在导师公司做的一个解决方法： 

1.根据用户特征，按照一定规则给用户打上基础标签。 

2.根据基础标签，做聚类，得到K个聚类中心。 

3.各个聚类中心下的各个用户的所有（商品特征，是否购买）作为训练集，训练出K个二分类模型。 

4.测试数据根据其用户特征与各中心的距离，将其归为最近的聚类重心C下。根据商品特征，使用对应的二分类模型，进行预测。

## 问题二

> 
打车软件从定价到下单，预测转化率，可以做哪些特征？

![](http://upload-images.jianshu.io/upload_images/1621805-bbd4d8ae36190647?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

