# 大话机器学习之数据预处理与数据筛选 - lyx的专栏 - CSDN博客





2016年06月14日 21:04:46[钱塘小甲子](https://me.csdn.net/qtlyx)阅读数：3593








      数据挖掘和机器学习这事，其实大部分时间不是在做算法，而是在弄数据，毕竟算法往往是现成的，改变的余地很小。

      数据预处理的目的就是把数据组织成一个标准的形式。

1.归一化

      归一化通常采用两种方法。

      a.最简单的归一化，最大最小值映射法


      P_New=(P-MI)/（MA-MI）

      P是原始数据，MI是这一属性中的最小值，MA是这一属性中的最大值。这样处理之后，所有的值都会限定在0-1之间。

      b.标准差标准化

      P_New=（P-AVG(P)）/SD(P)
      其中AVG（P）为变量均值，SD(P)为标准差.

     
 这个方法还有一个好处，就是当你发现如此处理之后，有的数字很离奇，就可以认为是异常值，直接剔除。

2、离散化

     
 如果你的数值是连续的，有时候不是那么好处理，比如年龄。往往把数字离散成小孩，少年，青年等等更加有意义。


3、缺失值问题

     
 这个首先要考虑缺失值的多少，如果过多，不如直接删除属性；如果在可接受范围内，则利用平均值、最大值或者别的适合的方案来补充。

     
 当然还有一种方法，先用方法1对不缺失的记录建模，然后用该方法预测缺失值；然后用方法2最终建模。当然，这里存在许多问题，比如方法一的准确度、方法1和方法2使用同一种方法的时候产生的信息冗余。

4、异常数据点

     
 实际的数据集有很多是异常数据，可能是由于录入错误或者采集中受到干扰等因素产生的错误数据。通常剔除异常数据的方法最常用的有如下两种。

     
 寻找附近的点，当最近的点的距离大于某一个阈值的时候，就认为是异常点。当然也可以在限定距离内，包含的数据点少于某个数目的时候认为是异常点。

     
 前者是基于距离，后者是基于密度。当然，还可以把两者结合，指定距离的同时也指定数目，这叫做COF。

5、数据的筛选

     
 我们在预处理好数据之后，有时候数据的维度是很大的，出于经济性考虑，当然，需要降维或者特征选择。有时候降为和特征选择也会增加准确度。

     
 降维通常使用PCA，主成分分析。直观上，就是把几个变量做线性组合，变成一个变量；特征选择则比较简单，就是选择相关性强的特征。

     
 当然，PCA其实设计到矩阵的奇异值分解，具体的数学原理就不展开了。
















