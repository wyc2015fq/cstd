# 如何产生好的词向量 - miner_zhu的博客 - CSDN博客





2019年01月04日 17:17:24[miner_zhu](https://me.csdn.net/miner_zhu)阅读数：96








词向量、词嵌入（word vector，word embedding）也称分布式表示（distributed representation）。如今词向量已经被广泛应用于各自NLP任务中，研究者们也提出了不少产生词向量的模型并开发成实用的工具供大家使用。但在使用这些工具产生词向量时，不同的训练数据，参数，模型等都会对产生的词向量有所影响，那么如何产生好的词向量对于工程来说很重要。

## **词的表示方法**

### **1.1 独热表示技术（早期传统的表示技术）**

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103141031-262301510.png)

### **1.2 分布表示技术**

**与独热表示技术相对应，基于分布式假说[即上下文相似的词，其语义也相似]，把信息分布式地存储在向量的各个维度中的表示方法，具有紧密低维，捕捉了句法、语义信息特点**
- **基于矩阵的分布表示**

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103142235-1654825543.png)
- **基于聚类的分布表示**

通过聚类手段构建词与其上下文之间的关系。代表模型：布朗聚类（Brown clustering）。
- 
**基于神经网络的分布表示（这是我们下面要研究的主要方法，在此介绍几种代表性模型）**


**神经网络语言模型（NNLM）**

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103142844-1441290270.png)

**Log双线性语言模型（LBL）**

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103143360-1884313078.png)

**C&W模型**

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103143985-111558173.png)

**Continuous Bag-of-Words（CBOW）Skip-gram（SG）**

Word2vec工具中的两个模型

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103144750-1004728549.png)

**Order模型**

在上面CBOW模型的在输入层是直接进行求和，这样并没有考虑词之前的序列顺序，所以来博士把直接求和改为了词向量之间的顺序拼接来保存序列顺序信息。

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103145406-611413496.png)

**模型理论比较**

![](https://images2015.cnblogs.com/blog/670089/201606/670089-20160626103146016-1552738452.png)

## **各种模型的实验对比分析**

整个实验是围绕下面几个问题进行的:
- 如何选择合适的模型？
- 训练语料的大小及领域对词向量有什么影响？
- 
如何选择训练词向量的参数？
- 迭代次数
- 词向量维度


**模型比较**
- 对于实际的自然语言处理任务，各模型的差异不大，选用简单的模型即可。
- 简单模型在小语料上整体表现更好，而复杂的模型需要更大的语料作支撑。

**语料影响**
- 同领域的语料，一般语料越大效果越好
- 领域内的语料对相似领域任务的效果提升非常明显，但在领域不契合时甚至会有负面作用。
- 在自然语言任务上，同领域的语料10M效果明显差，但是100M以上扩大语料，任务结果的差异较小。

**规模和领域的权衡**
- 语料的领域纯度比语料规模更重要。（特别是在任务领域的语料比较小时，加入大量其他领域的语料可能会有很负面的影响）

**参数选择**

**迭代次数**
- 根据词向量的损失函数选择迭代次数不合适。
- 条件允许的话，选择目标任务的验证集性能作为参考标准。
- 具体任务性能指标趋势一样，可以选简单任务的性能峰值。

**词向量维度**
- 对于分析词向量语言学特性的任务，维度越大效果越好。
- 对于提升自然语言处理任务而言，50维词向量通常就足够好。（这里我觉得只能说是某些任务，不过趋势是一致的，随着词向量维度的增加，性能曲线先增长后趋近于平缓，甚至下降）

## **总结**
- 选择一个合适的模型。复杂的模型相比简单的模型，在较大的语料中才有优势。
- 选择一个合适领域的语料，在此前提下，语料规模越大越好。使用大规模的语料进行训练，可以普遍提升词向量的性能，如果使用领域内的语料，对同领域的任务会有显著的提升。（训练语料不要过小，一般使用同领域语料达到100M规模）
- 训练时，迭代优化的终止条件最好根据具体任务的验证集来判断，或者近似地选取其它类似的任务作为指标，但是不应该选用训练词向量时的损失函数。（迭代参数我一般使用根据训练语料大小，一般选用10~25次）
- 
词向量的维度一般需要选择50维及以上，特别当衡量词向量的语言学特性时，词向量的维度越大，效果越好。




##  参考文章

[https://www.cnblogs.com/robert-dlut/p/5617466.html](https://www.cnblogs.com/robert-dlut/p/5617466.html)



