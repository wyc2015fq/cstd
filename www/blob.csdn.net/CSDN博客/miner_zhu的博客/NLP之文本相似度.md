# NLP之文本相似度 - miner_zhu的博客 - CSDN博客





置顶2018年08月13日 11:12:22[miner_zhu](https://me.csdn.net/miner_zhu)阅读数：944标签：[NLP																[TD-IDF																[文本相似																[LCS																[推荐](https://so.csdn.net/so/search/s.do?q=推荐&t=blog)
个人分类：[NLP](https://blog.csdn.net/miner_zhu/article/category/7896187)





### 相似度

相似度度量(从字面上和语义上两方面来度量)：计算个体间相似程度（得到一个分数，通过分数来度量相似度，范围[0,1]）

    -文本角度（TF-IDF、LCS）：这件衣服真好看，这件衣服真难看

    -语义角度（协同过滤）：真好玩，真有趣

    -文本+语义角度（word2vec）

### 余弦相似度

先介绍文本相似中最常用最简单的方法：余弦相似度。

    – 一个向量空间中两个向量夹角的余弦值作为衡量两个个体之间差异的大小

    – 余弦值接近1，夹角趋于0，表明两个向量越相似

![](https://img-blog.csdn.net/20180331105745679)

简单的例子分析：

![](https://images2017.cnblogs.com/blog/1209698/201801/1209698-20180114155451488-245359421.png)

### 计算步骤

 • 得到了文本相似度计算的处理流程是:

– 找出两篇文章的关键词；

– 每篇文章各取出若干个关键词，合并成一个集合，计算每篇文章对于这个集合中的词的词频

– 生成两篇文章各自的词频向量；

– 计算两个向量的余弦相似度，值越大就表示越相似。

# 一、TFIDF

TF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

## 1、词频——TF

• 假设：如果一个词很重要，应该会在文章中多次出现

• 词频TF（Term Frequency）：一个词在文章中出现的次数

• **也不是绝对的！**出现次数最多的是“的”“是”“在”，这类最常用的词，叫做**停用词（stop words）通常列为黑名单**

• 停用词对结果毫无帮助，必须过滤掉的词

• 过滤掉停用词后就一定能接近问题么？

• 进一步调整假设：如果某个词较为少见，但是它在这篇文章中多次出现，那么它很可能反映了这篇文章的特性，正是我们所需要的关键词

![](https://img-blog.csdn.net/20180810170727748?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



## 2、反文档频率——IDF

• 在词频的基础上，赋予每一个词的权重，进一步体现该词的重要性，

• 最常见的词（“的”、“是”、“在”）给予最小的权重

• 较常见的词（“国内”、“中国”、“报道”）给予较小的权重

• 较少见的词（“养殖”、“维基”）给予较大权重

• 将TF和IDF进行相乘，就得到了一个词的TF-IDF值，某个词对文章重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。

![](https://img-blog.csdn.net/20180810170800343?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

    针对这个公式，可能有人会有以下的疑问：

    1.为什么+1？是为了处理分母为0的情况。假如所有的文章都不包含这个词，分子就为0，所以+1是为了防止分母为0的情况。

    2.为什么要用log函数？log函数是单调递增，求log是为了归一化，保证反文档频率不会过大。

    3.会出现负数？肯定不会，分子肯定比分母大

## 3.TF—IDF

TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上就是 TF*IDF，其中 TF（Term Frequency），表示词条在文章Document 中出现的频率；IDF（Inverse Document Frequency），其主要思想就是，如果包含某个词 Word的文档越少，则这个词的区分度就越大，也就是 IDF 越大。对于如何获取一篇文章的关键词，我们可以计算这边文章出现的所有名词的 TF-IDF，TF-IDF越大，则说明这个名词对这篇文章的区分度就越高，取 TF-IDF 值较大的几个词，就可以当做这篇文章的关键词。

![](https://img-blog.csdn.net/20180810170832408?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



![](https://img-blog.csdn.net/20180331112819122)

## **利用TF-IDF计算相似文章**

    1)使用TF-IDF算法，找出两篇文章的关键词

    2)每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）

    3)生成两篇文章各自的词频向量

    4)计算两个向量的余弦相似度，值越大就表示越相似

## **自动摘要**

"自动摘要"就是要找出那些包含信息最多的句子。句子的信息量用"关键词"来衡量。如果包含的关键词越多，就说明这个句子越重要。只要关键词之间的距离小于“门槛值” ，它们就被认为处于同一个簇之中，如果两个关键词之间有5个以上的其他词，就可以把这两个关键词分在两个簇。下一步，对于每个簇，都计算它的重要性分值

![](https://img-blog.csdn.net/20180813103455814?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

有时候，为了方便实际的操作可以化简为不考虑簇的用法，

1)计算原始文本的词频，生成词频数组 [(2, '你好'), (10, '大数据'), (4, '智能'), (100, '的')...]

2)过滤停用词，得出新的词频数组  [(2, '你好'), (10, '大数据'), (4, '智能')...]

3)按照词频进行排序 [(2, '你好'), (4, '智能'), (10, '大数据')...]

4)将文章分为句子

5)选择关键词首先出现的句子 (一般文章首段和最后一段，每段的首句和末句是比较重要的)

6)将选中的句子按照出现顺序，组成摘要

   这样做的优点是简单快速，结果比较符合实际情况。缺点是单纯以“词频”做衡量标准，不够全面，词性和词的出现位置等因素没有考虑到，而且有时重要的词可能出现的次数并不多。这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。） 

# 二、LCS

## 1.LCS介绍

• 最长公共子序列（Longest Common Subsequence）

• 一个序列S任意删除若干个字符得到的新序列T，则T叫做S的子序列

• 两个序列X和Y的公共子序列中，长度最长的那个，定义为X和Y的最长公共子序列

– 字符串12455与245576的最长公共子序列为2455

– 字符串acdfg与adfc的最长公共子序列为adf

• 注意区别最长公共子串（Longest Common Substring）

– 最长公共子串要求连续

一般用动态规划求解

## 2.LCS用法

• 求两个序列中最长的公共子序列算法

– 生物学家常利用该算法进行基金序列比对，以推测序列的结构、功能和演化过程。

• 描述两段文字之间的“相似度”

– 辨别抄袭，对一段文字进行修改之后，计算改动前后文字的最长公共子序列，将除此子序列外的部分提取出来，该方法判断修改的部分](https://so.csdn.net/so/search/s.do?q=LCS&t=blog)](https://so.csdn.net/so/search/s.do?q=文本相似&t=blog)](https://so.csdn.net/so/search/s.do?q=TD-IDF&t=blog)](https://so.csdn.net/so/search/s.do?q=NLP&t=blog)




