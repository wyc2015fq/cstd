# 命名实体识别（NER）的发展历程 - miner_zhu的博客 - CSDN博客





2019年01月04日 15:39:44[miner_zhu](https://me.csdn.net/miner_zhu)阅读数：292








命名实体识别（Named Entity Recognition，NER）简单说就是从一段自然语言文本中找出相关实体，并标注出其位置以及类型。一般我们归为序列标注问题（sequence labeling problem）中的一种。与分类问题相比，序列标注问题中当前的预测标签不仅与当前的输入特征相关，还与之前的预测标签相关，即预测标签序列之间是有强相互依赖关系的。例如，使用BIO标签进行NER时，正确的标签序列中标签O后面是不会接标签I的。

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215630660-471018493.png)

NER的发展从早期基于词典和规则的方法，到传统机器学习的方法，到近年来基于深度学习的方法，大致如下。

![](https://oscimg.oschina.net/oscnet/b9ce883b60e2c06120448e5b09481a4b943.jpg)

在传统机器学习中，CRF（条件随机场）是NER目前的主流模型。它的目标函数不仅考虑输入的状态特征函数，而且还包含了标签转移特征函数。在训练时可以使用SGD学习模型参数。在已知模型时，给输入序列求预测输出序列即求使目标函数最大化的最优序列，是一个动态规划问题，可以使用维特比算法进行解码。CRF的优点在于其为一个位置进行标注的过程中可以利用丰富的内部及上下文特征信息。

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215631176-479509507.png)

在传统机器学习方法中，常用的特征如下：

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215632644-1517806781.png)

近年来，随着算力的发展以及词的分布式表示（word embedding）的提出，神经网络可以有效处理许多NLP任务。这类方法对于序列标注任务（如CWS、POS、NER）的处理方式是类似的：将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。

这种方法使得模型的训练成为一个端到端的过程，而非传统的pipeline，不依赖于特征工程，但神经网络种类繁多、对参数设置依赖大，模型可解释性差。此外，这种方法的一个缺点是对每个token打标签的过程是独立的进行，不能直接利用上文已经预测的标签（只能靠隐含状态传递上文信息），进而导致预测出的标签序列可能是无效的，例如标签I-PER后面是不可能紧跟着B-PER的，但Softmax不会利用到这个信息。所以进一步提出DL-CRF模型做序列标注。在神经网络的输出层接入CRF层(重点是利用标签转移概率)来做句子级别的标签预测，使得标注过程不再是对各个token独立分类。

# 2 NER中的神经网络结构

## 2.1 NN/CNN-CRF模型

    《Natural language processing (almost) from scratch》是较早使用神经网络进行NER的代表工作之一。在这篇论文中，作者提出了窗口方法与句子方法两种网络结构来进行NER。这两种结构的主要区别就在于窗口方法仅使用当前预测词的上下文窗口进行输入，然后使用传统的NN结构；而句子方法是以整个句子作为当前预测词的输入，加入了句子中相对位置特征来区分句子中的每个词，然后使用了一层卷积神经网络CNN结构。

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215633504-1416487236.png)

    在训练阶段，作者也给出了两种目标函数：一种是词级别的对数似然，即使用softmax来预测标签概率，当成是一个传统分类问题；另一种是句子级别的对数似然，其实就是考虑到CRF模型在序列标注问题中的优势，将标签转移得分加入到了目标函数中。后来许多相关工作把这个思想称为结合了一层CRF层，所以我这里称为NN/CNN-CRF模型。

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215634347-54796442.png)

在作者的实验中，上述提到的NN和CNN结构效果基本一致，但是句子级别似然函数即加入CRF层在NER的效果上有明显提高。

## 2.2 RNN-CRF模型

借鉴上面的CRF思路，出现了一系列使用RNN结构并结合CRF层进行NER的工作。模型结构如下图：

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215635894-1441255049.png)

    它主要有Embedding层（主要有词向量，字符向量以及一些额外特征），双向RNN层，tanh隐层以及最后的CRF层构成。它与之前NN/CNN-CRF的主要区别就是他使用的是双向RNN代替了NN/CNN。这里RNN常用LSTM或者GRU。实验结果表明RNN-CRF获得了更好的效果，已经达到或者超过了基于丰富特征的CRF模型，成为目前基于深度学习的NER方法中的最主流模型。在特征方面，该模型继承了深度学习方法的优势，无需特征工程，使用词向量以及字符向量就可以达到很好的效果，如果有高质量的词典特征，能够进一步获得提高。

# 3 最近的一些工作

    近年在基于神经网络结构的NER研究上，主要集中在两个方面：一是使用流行的注意力机制来提高模型效果（Attention Mechanism），二是针对少量标注训练数据或无标注数据进行的一些研究。

## 3.1 Attention-based

    《Attending to Characters in Neural Sequence Labeling Models》该论文还是在RNN-CRF模型结构基础上，重点改进了词向量与字符向量的拼接。使用attention机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习attention的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215636472-1440311259.png)

    另一篇论文《Phonologically aware neural model for named entity recognition in low resource transfer settings》，在原始BiLSTM-CRF模型上，加入了音韵特征，并在字符向量上使用attention机制来学习关注更有效的字符，主要改进如下图。

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215637207-1758787810.png)

## 3.2 少量标注数据

    对于深度学习方法，一般需要大量标注数据，但是在一些领域并没有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据进行NER也是最近研究的重点。其中包括了迁移学习《Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks》和半监督学习。这里我提一下最近ACL2017刚录用的一篇论文《Semi-supervised sequence tagging with bidirectional language models》。该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向RNN-CRF模型中。实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高NER效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始RNN-CRF模型的效果。整体模型结构如下图：

![](https://images2015.cnblogs.com/blog/670089/201705/670089-20170512215637754-267340132.png)

# 4 总结

    最后进行一下总结，目前将神经网络与CRF模型相结合的NN/CNN/RNN-CRF模型成为了目前NER的主流模型。我认为对于CNN与RNN，并没有谁占据绝对的优势，各自有相应的优点。由于RNN有天然的序列结构，所以RNN-CRF使用更为广泛。基于神经网络结构的NER方法，继承了深度学习方法的优点，无需大量人工特征。只需词向量和字符向量就能达到主流水平，加入高质量的词典特征能够进一步提升效果。对于少量标注训练集问题，迁移学习，半监督/无监督学习应该是未来研究的重点。



### 参考文章

[http://www.cnblogs.com/robert-dlut/p/6847401.html](http://www.cnblogs.com/robert-dlut/p/6847401.html)
[https://my.oschina.net/datagrand/blog/2251431](https://my.oschina.net/datagrand/blog/2251431)



