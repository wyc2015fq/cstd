# 微软亚研院：NLP趋势展望 - miner_zhu的博客 - CSDN博客





2018年12月21日 16:25:54[miner_zhu](https://me.csdn.net/miner_zhu)阅读数：108








**趋势热点：值得关注的 NLP 技术**

从最近的 NLP 研究中，我们认为有一些技术发展趋势值得关注，这里总结了五个方面：

**热点一，预训练神经网络**

如何学习更好的预训练的表示，在一段时间内继续成为研究的热点。

通过类似于语言模型的方式来学习词的表示，其用于具体任务的范式得到了广泛应用。这几乎成为自然语言处理的标配。这个范式的一个不足是词表示缺少上下文，对上下文进行建模依然完全依赖于有限的标注数据进行学习。实际上，基于深度神经网络的语言模型已经对文本序列进行了学习。如果把语言模型关于历史的那部分参数也拿出来应用，那么就能得到一个预训练的上下文相关的表示。这就是 Matthew Peters 等人在 2018 年 NAACL 上的论文「Deep Contextualized Word Representations」的工作，他们在大量文本上训练了一个基于 LSTM 的语言模型。最近 Jacob Delvin 等人又取得了新的进展，他们基于多层 Transformer 机制，利用所谓「MASKED」模型预测句子中被掩盖的词的损失函数和预测下一个句子的损失函数所预训练得到的模型「BERT」，在多个自然语言处理任务上取得了当前最好的水平。以上提到的所有的预训练的模型，在应用到具体任务时，先用这个语言模型的 LSTM 对输入文本得到一个上下文相关的表示，然后再基于这个表示进行具体任务相关的建模学习。结果表明，这种方法在语法分析、阅读理解、文本分类等任务都取得了显著的提升。最近一段时间，这种预训练模型的研究成为了一个研究热点。

如何学习更好的预训练的表示在一段时间内将继续成为研究的热点。在什么粒度（word，sub-word，character）上进行预训练，用什么结构的语言模型（LSTM，Transformer 等）训练，在什么样的数据上（不同体裁的文本）进行训练，以及如何将预训练的模型应用到具体任务，都是需要继续研究的问题。现在的预训练大都基于语言模型，这样的预训练模型最适合序列标注的任务，对于问答一类任务依赖于问题和答案两个序列的匹配的任务，需要探索是否有更好的预训练模型的数据和方法。将来很可能会出现多种不同结构、基于不同数据训练得到的预训练模型。针对一个具体任务，如何快速找到合适的预训练模型，自动选择最优的应用方法，也是一个可能的研究课题。

**热点二，迁移学习和多任务学习**

对于那些本身缺乏充足训练数据的自然语言处理任务，迁移学习有着非常重要和实际的意义。多任务学习则用于保证模型能够学到不同任务间共享的知识和信息。

不同的 NLP 任务虽然采用各自不同类型的数据进行模型训练，但在编码器（Encoder）端往往是同构的。例如，给定一个自然语言句子 who is the Microsoft founder，机器翻译模型、复述模型和问答模型都会将其转化为对应的向量表示序列，然后再使用各自的解码器完成后续翻译、改写和答案生成 (或检索) 任务。因此，可以将不同任务训练得到的编码器看作是不同任务对应的一种向量表示，并通过迁移学习（Transfer Learning）的方式将这类信息迁移到目前关注的目标任务上来。对于那些本身缺乏充足训练数据的自然语言处理任务，迁移学习有着非常重要和实际的意义。

多任务学习（Multi-task Learning）可通过端到端的方式，直接在主任务中引入其他辅助任务的监督信息，用于保证模型能够学到不同任务间共享的知识和信息。Collobert 和 Weston 早在 2008 年就最早提出了使用多任务学习在深度学习框架下处理 NLP 任务的模型。最近 Salesforce 的 McCann 等提出了利用问答框架使用多任务学习训练十项自然语言任务。每项任务的训练数据虽然有限，但是多个任务共享一个网络结构，提升对来自不同任务的训练数据的综合利用能力。多任务学习可以设计为对诸任务可共建和共享网络的核心层次，而在输出层对不同任务设计特定的网络结构。

**热点三，知识和常识的引入**

如何在自然语言理解模块中更好地使用知识和常识，已经成为目前自然语言处理领域中一个重要的研究课题。

随着人们对人机交互（例如智能问答和多轮对话）要求的不断提高，如何在自然语言理解模块中更好地使用领域知识，已经成为目前自然语言处理领域中一个重要的研究课题。这是由于人机交互系统通常需要具备相关的领域知识，才能更加准确地完成用户查询理解、对话管理和回复生成等任务。

最常见的领域知识包括维基百科和知识图谱两大类。机器阅读理解是基于维基百科进行自然语言理解的一个典型任务。给定一段维基百科文本和一个自然语言问题，机器阅读理解任务的目的是从该文本中找到输入问题对应的答案短语片段。语义分析是基于知识图谱进行自然语言理解的另一个典型任务。给定一个知识图谱（例如 Freebase）和一个自然语言问题，语义分析任务的目的是将该问题转化为机器能够理解和执行的语义表示。目前，机器阅读理解和语义分析可以说是最热门的自然语言理解任务，它们受到了来自全世界研究者的广泛关注和深入探索。

常识指绝大多数人都了解并接受的客观事实，例如海水是咸的、人渴了就想喝水、白糖是甜的等。常识对机器深入理解自然语言非常重要，在很多情况下，只有具备了一定程度的常识，机器才有可能对字面上的含义做出更深一层次的理解。然而获取常识却是一个巨大的挑战，一旦有所突破将是影响人工智能进程的大事情。另外，在 NLP 系统中如何应用常识尚无深入的研究，不过出现了一些值得关注的工作。

**热点四，低资源的 NLP 任务**

引入领域知识（词典、规则）可以增强数据能力、基于主动学习的方法增加更多的人工标注数据等，以解决数据资源贫乏的问题。

面对标注数据资源贫乏的问题，譬如小语种的机器翻译、特定领域对话系统、客服系统、多轮问答系统等，NLP 尚无良策。这类问题统称为低资源的 NLP 问题。对这类问题，除了设法引入领域知识（词典、规则）以增强数据能力之外，还可以基于主动学习的方法来增加更多的人工标注数据，以及采用无监督和半监督的方法来利用未标注数据，或者采用多任务学习的方法来使用其他任务甚至其他语言的信息，还可以使用迁移学习的方法来利用其他的模型。

以机器翻译为例，对于稀缺资源的小语种翻译任务，在没有常规双语训练数据的情况下，首先通过一个小规模的双语词典（例如仅包含 2000 左右的词对），使用跨语言词向量的方法将源语言和目标语言词映射到同一个隐含空间。在该隐含空间中, 意义相近的源语言和目标语言词具有相近的词向量表示。基于该语义空间中词向量的相似程度构建词到词的翻译概率表，并结合语言模型，便可以构建基于词的机器翻译模型。使用基于词的翻译模型将源语言和目标语言单语语料进行翻译，构建出伪双语数据。于是，数据稀缺的问题通过无监督的学习方法产生伪标注数据，就转化成了一个有监督的学习问题。接下来，利用伪双语数据训练源语言到目标语言以及目标语言到源语言的翻译模型，随后再使用联合训练的方法结合源语言和目标语言的单语数据，可以进一步提高两个翻译系统的质量。

为了提高小语种语言的翻译质量，我们提出了利用通用语言之间大规模的双语数据，来联合训练四个翻译模型的期望最大化训练方法（Ren et al., 2018）。该方法将小语种（例如希伯来语）作为有着丰富语料的语种（例如中文）和（例如英语）之间的一个隐含状态，并使用通用的期望最大化训练方法来迭代地更新 X 到 Z、Z 到 X、Y 到 Z 和 Z 到 Y 之间的四个翻译模型，直至收敛。

**热点五，多模态学习**

视觉问答作为一种典型的多模态学习任务，在近年来受到计算机视觉和自然语言处理两个领域研究人员的重点关注。

婴儿在掌握语言功能前，首先通过视觉、听觉和触觉等感官去认识并了解外部世界。可见，语言并不是人类在幼年时期与外界进行沟通的首要手段。因此，构建通用人工智能也应该充分地考虑自然语言和其他模态之间的互动，并从中进行学习，这就是多模态学习。

视觉问答作为一种典型的多模态学习任务，在近年来受到计算机视觉和自然语言处理两个领域研究人员的重点关注。给定一张图片和用户提出的一个自然语言问题，视觉问答系统需要在理解图片和自然语言问题的基础上，进一步输入该问题对应的答案，这需要视觉问答方法在建模中能够对图像和语言之间的信息进行充分地理解和交互。

我们在今年的 CVPR 和 KDD 大会上分别提出了基于问题生成的视觉问答方法（Li et al., 2018）以及基于场景图生成的视觉问答方法（Lu et al., 2018），这两种方法均在视觉问答任务上取得了非常好的结果，实现了 state-of-the-art 的效果。除视觉问答外，视频问答是另一种最近广受关注的多模态任务。该任务除了包括带有时序的视频信息外，还包括了音频信息。目前，视频问答作为一种新型的问答功能，已经出现在搜索引擎的场景中。可以预见，该任务在接下来一定还会受到更多的关注。

**未来展望：理想的 NLP 框架和发展前景**

我们认为，未来理想状态下的 NLP 系统架构可能是如下一个通用的自然语言处理框架：

首先，对给定自然语言输入进行基本处理，包括分词、词性标注、依存分析、命名实体识别、意图/关系分类等。

其次，使用编码器对输入进行编码将其转化为对应的语义表示。在这个过程中，一方面使用预训练好的词嵌入和实体嵌入对输入中的单词和实体名称进行信息扩充，另一方面，可使用预训练好的多个任务编码器对输入句子进行编码并通过迁移学习对不同编码进行融合。

接下来，基于编码器输出的语义表示，使用任务相关的解码器生成对应的输出。还可引入多任务学习将其他相关任务作为辅助任务引入到对主任务的模型训练中来。如果需要多轮建模，则需要在数据库中记录当前轮的输出结果的重要信息，并应用于在后续的理解和推理中。

显然，为了实现这个理想的 NLP 框架需要做很多工作：
- 
需要构建大规模常识数据库并且清晰通过有意义的评测推动相关研究；

- 
研究更加有效的词、短语、句子的编码方式，以及构建更加强大的预训练的神经网络模型；

- 
推进无监督学习和半监督学习，需要考虑利用少量人类知识加强学习能力以及构建跨语言的 embedding 的新方法；

- 
需要更加有效地体现多任务学习和迁移学习在 NLP 任务中的效能，提升强化学习在 NLP 任务的作用，比如在自动客服的多轮对话中的应用；

- 
有效的篇章级建模或者多轮会话建模和多轮语义分析；

- 
要在系统设计中考虑用户的因素，实现用户建模和个性化的输出；

- 
构建综合利用推理系统、任务求解和对话系统，基于领域知识和常识知识的新一代的专家系统；

- 
利用语义分析和知识系统提升 NLP 系统的可解释能力。


未来十年，NLP 将会进入爆发式的发展阶段。从 NLP 基础技术到核心技术，再到 NLP+的应用，都会取得巨大的进步。比尔盖茨曾经说过人们总是高估在一年或者两年中能够做到的事情，而低估十年中能够做到的事情。



原文链接：[https://www.jiqizhixin.com/articles/2018-11-25](https://www.jiqizhixin.com/articles/2018-11-25)



