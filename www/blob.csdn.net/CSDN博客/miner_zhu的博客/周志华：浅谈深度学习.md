# 周志华：浅谈深度学习 - miner_zhu的博客 - CSDN博客





2018年12月20日 16:16:02[miner_zhu](https://me.csdn.net/miner_zhu)阅读数：163








我们都知道**直接掀起人工智能热潮的最重要的技术之一，就是深度学习技术。**今天，其实深度学习已经有各种各样的应用，到处都是它，不管图像也好，视频也好，声音自然语言处理等等。那么我们问一个问题，什么是深度学习？

### 深度学习的理论基础尚不清楚

我想大多数人的答案，就是深度学习差不多就等于深度神经网络。有一个非常著名的学会叫SIAM，是国际工业与应用数学学会，他们有一个旗舰的报纸叫SIAM news。在去年的 6 月份，这个报纸的头版上就有这么一篇文章，直接就说了这么一句话，说**深度学习是机器学习中使用深度神经网络的的子领域**。

如果我们要谈深度学习的话，是绕不开**深度神经网络**的。首先我们必须从神经网络说起。神经网络其实并不是一个新生事物，神经网络可以说在人工智能领域已经研究了超过半个世纪。但是以往的话，一般我们会用这样的神经网络，就是中间有一个隐层，或者有两个隐层。在这样的神经网络里面，它的每一个单元是个非常简单的计算模型。我们收到一些输入，这些输入通过一些连接放大，它就是这么一个非常简单的公式。所谓的神经网络，是很多这样的公式经过嵌套迭代得到的一个系统。那么今天当我们说用深度神经网络的时候，其实我们指的是什么？简单来说，就是我们用的层数会很深很深，很多层。这是个非常庞大非常巨大的系统，把这么一个系统训练出来，难度是非常大的。

有一点非常好的消息。**神经网络里面的计算单元，最重要的激活函数是连续的、可微的。**比如说我们在以往常用这样的sigmoid函数，它是连续可微的，现在大家常用的ReLu函数或者它的变体，也是这样。这使得我们可以容易地进行梯度计算，这样就可以很容易用著名的BP算法来训练。通过这样的算法，我们的神经网络已经取得了非常多的胜利。

我们知道一个机器学习模型，它的复杂度实际上和它的容量有关，而容量又跟它的学习能力有关。所以就是说学习能力和复杂度是有关的。机器学习界早就知道，**如果我们能够增强一个学习模型的复杂度，那么它的学习能力能够提升**。那怎么样去提高复杂度，对神经网络这样的模型来说，有两条很明显的途径。一条是我们把模型变深，一条是把它变宽。**如果从提升复杂度的角度，那么变深是会更有效。**当你变宽的时候，你只不过是增加了一些计算单元，增加了函数的个数，在变深的时候不仅增加了个数，其实还增加了它的嵌入的程度。所以从这个角度来说，我们应该尝试去把它变深。

这里涉及到另外一个问题，我们把**机器学习的学习能力变强了，这其实未必是一件好事。**因为机器学习一直在斗争的一个问题，就是经常会碰到过拟合（overfit）。当你的能力非常非常强的时候，你可能就把一些特性学出来，当成一般规律。所以我们以往通常不太愿意用太复杂的模型。

那现在我们为什么可以用这样的模型？有很多因素。总结一下就是：第一我们有了更大的数据；第二我们有强力的计算设备；第三我们有很多有效的训练技巧。这导致我们可以用高复杂度的模型，而深度神经网络恰恰就是一种很便于实现的高复杂度模型。

### **深度神经网络里面最本质的东西到底是什么？**

今天我们的回答是，**表示学习的能力**。以往我们用机器学习解决一个问题的时候，首先我们拿到一个数据，比如说这个数据对象是个图像，然后我们就用很多特征把它描述出来，比如说颜色、纹理等等。这些特征都是我们人类专家通过手工来设计的，表达出来之后我们再去进行学习。而今天我们有了深度学习之后，现在**不再需要手工去设计特征**了。你把数据从一端扔进去，模型从另外一端就出来了，中间所有的特征完全可以通过学习自己来解决。所以这就是我们所谓的特征学习，或者说表示学习。这和以往的机器学习技术相比可以说是一个很大的进步。我们不再需要依赖人类专家去设计特征了。

有些朋友经常说的一个东西是**端到端学习**。对这个其实我们要从两方面看，一方面，当我们把特征学习和分类器的学习联合起来考虑的时候，可以达到一个联合优化的作用，这是好的方面。但是另外一方面，如果这里面发生什么我们不清楚，这样的端到端学习就不一定真的是好的。因为里面很可能第一个部分在往东，第二个部分在往西，合起来看，好像它往东走的更多一点，其实内部已经有些东西在抵消了。

**真正重要的还是特征学习，或者表示学习。**那如果我们再问下一个问题，表示学习最关键的又是什么呢？我们现在有这么一个答案，就是**逐层的处理**。我引述最近非常流行的一本书，《深度学习》这本书里面的一个图，当我们拿到一个图像的时候，我们如果把神经网络看作很多层，首先它在最底层，好像我们看到的是一些像素这样的东西。当我们一层一层往上的时候，慢慢的可能有边缘，再网上可能有轮廓，甚至对象的部件等等。当然这实际上只是个示意图，在真正的神经网络模型里面不见得会有这么清楚的分层。但是总体上当我们逐渐往上的时候，它确实是不断在对对象进行抽象。我们现在认为这好像是深度学习为什么成功的关键因素之一。因为扁平神经网络能做很多深层神经网络能做的事，但是有一点它是做不到的。当它是扁平的时候，它就没有进行这样的一个深度的加工。 所以深度的逐层抽象这件事情，可能是很关键的。

大家可能就会问，“逐层地处理”在机器学习里面也不是新东西。比如说决策树就是一种逐层处理，这是非常典型的。决策树模型已经有五六十年的历史了，但是它为什么做不到深度神经网络这么好呢？我想答案是这样。**首先它的复杂度不够**，决策数的深度，如果我们只考虑离散特征的话，它最深的深度不会超过特征的个数，所以它的模型复杂度是有限的。**第二，整个决策树的学习过程中，它内部没有进行特征的变换，始终是在一个特征空间里面进行的。**这可能也是它的一个问题。大家如果对高级点的机器学习模型了解，你可能会问，那boosting呢？比如说现在很多获胜的模型，xgboost 等等都属于这个boosting的一类，它也是一层一层的往下走。你说他为什么没有取得像深度神经网络这样的成功呢？我想其实问题是差不多的，首先它的复杂度还不够。第二可能是更关键的一点，它始终是在原始空间里面做事情，所有的这些学习器都是在原始特征空间，中间没有进行任何的特征变化。所以现在我们的看法是，深度神经网络到底为什么成功？或者成功的关键原因是什么？我想第一是逐层地处理，第二我们要有一个内部的特征变换。

### 深度学习成功的三个因素

**这是我们现在的一个认识：第一，我们要有逐层的处理；第二，我们要有特征的内部变换；第三，我们要有足够的模型复杂度。**这三件事情是我们认为深度神经网络为什么能够成功的比较关键的原因。或者说，这是我们给出的一个猜测。

### 深度学习存在的问题

那如果满足这几个条件，我们其实马上就可以想到，那我不一定要用神经网络。神经网络可能只是我可以选择的很多方案之一，我只要能够同时做到这三件事，那我可能用别的模型做也可以，并不是一定只能是用深度神经网络。

第一，凡是用过深度神经网络的人都会知道，**你要花大量的精力来调它的参数，因为这是个巨大的系统**。那这会带来很多问题。首先我们调参数的经验其实是很难共享的。有的朋友可能说，你看我在第一个图像数据集上调参数的经验，当我用第二个图像数据集的时候，这个经验肯定是可以重用一部分。但是我们有没有想过，比如说我们在图像上面做了一个很大的深度神经网络，这时候如果要去做语音的时候，其实在图像上面调参数的经验，在语音问题上基本上不太有借鉴作用。所以当我们跨任务的时候，这些经验可能就很难共享。

第二个问题，今天大家都非常关注我们做出来的结果的可重复性，不管是科学研究也好，技术发展也好，都希望这个结果可重复。 而**在整个机器学习领域，可以说深度学习的可重复性是最弱的。**我们经常会碰到这样的情况，有一组研究人员发文章说报告了一个结果，而这个结果其他的研究人员很难重复。因为哪怕你用同样的数据，同样的方法，只要超参数的设置不一样，你的结果就不一样。

还有很多问题，比如说我们**在用深度神经网络的时候，模型复杂度必须是事先指定的**。因为我们在训练这个模型之前，我们这个神经网络是什么样就必须定了，然后我们才能用 BP算法等等去训练它。其实这会带来很大的问题，因为我们在没有解决这个任务之前，我们怎么知道这个复杂度应该有多大呢？所以实际上大家做的通常都是设更大的复杂度。

如果大家关注过去 3、4 年深度学习这个领域的进展，你可以看到**很多最前沿的工作在做的都是在有效的缩减网络的复杂度。**比如说 RestNet 这个网络通过加了shortcuts，有效地使得复杂度变小。还有最近大家经常用的一些模型压缩，甚至权重的二值化，其实都是在把复杂度变小。实际上它是先用了一个过大的复杂度，然后我们再把它降下来。那么我们有没有可能在一开始就让这个模型的复杂度随着数据而变化，这点对神经网络可能很困难，但是对别的模型是有可能的。

**还有很多别的问题，比如说理论分析很困难，需要非常大的数据，黑箱模型等等。**那么从另外一个方面，有人可能说你是做学术研究，你们要考虑这些事，我是做应用的，什么模型我都不管，你只要能给我解决问题就好了。其实就算从这个角度来想，我们研究神经网络之外的模型也是很需要的。

虽然在今天深度神经网络已经这么的流行，这么的成功，但是其实我们可以看到**在很多的任务上，性能最好的不见得完全是深度神经网络**。比如说如果大家经常关心Kaggle上面的很多竞赛，它有各种各样的真实问题，有买机票的，有订旅馆的，有做各种的商品推荐等等。我们去看上面获胜的模型，在很多任务上的胜利者并不是神经网络，它往往是像随机森林，像xgboost等等这样的模型。深度神经网络获胜的任务，往往就是在图像、视频、声音这几类典型任务上。而在别的凡是涉及到混合建模、离散建模、符号建模这样的任务上，其实它的性能可能比其他模型还要差一些。那么，有没有可能做出合适的深度模型，在这些任务上得到更好的性能呢？

我们从学术的观点来总结一下，今天我们谈到的深度模型基本上都是深度神经网络。如果用术语来说的话，它是**多层、可参数化的、可微分的非线性模块所组成的模型，而这个模型可以用 BP算法来训练。**

### 探索深度学习之外的方法：深度森林

那么这里面有两个问题。第一，我们现实世界遇到的各种各样的问题的性质，并不是绝对都是可微的，或者用可微的模型能够做最佳建模的。第二，过去几十年里面，我们的机器学习界做了很多很多模型出来，这些都可以作为我们构建一个系统的基石，而中间有相当一部分模块是不可微的。那么这样的东西能不能用来构建深度模型？能不能通过构建深度模型之后得到更好的性能，能不能通过把它们变深之后，使得深度模型在今天还比不上随机森林等等这些模型的任务上，能够得到更好的结果呢？现在有这么一个很大的挑战，这不光是学术上的，也是技术上的一个挑战，就是我们能不能用不可微的模块来构建深度模型？

这个问题一旦得到了回答，我们同时就可以得到很多其他问题的答案。比如说深度模型是不是就是深度神经网络？我们能不能用不可微的模型把它做深，这个时候我们不能用BP算法来训练，那么同时我们能不能让深度模型在更多的任务上获胜？我们提出这个问题之后，在国际上也有一些学者提出了一些相似的看法。可能大家都知道，深度学习非常著名的领军人物Geoffery Hinton教授，他也提出来说，希望深度学习以后能摆脱 BP 算法来做，他提出这件事比我们要晚一些。

我想这样的问题是应该是站在一个很前沿的角度上探索。刚才跟大家分析所得到的三个结论，**第一我们要做逐层处理，第二我们要做特征的内部变换，第三，我们希望得到一个充分的模型复杂度**。我自己领导的研究组最近在这方面做了一些工作。我们最近提出了一个叫做[**Deep Forest（深度森林）**](http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651994082&idx=1&sn=3a1f21ab37ea8322c6700f660b71648a&chksm=f1214313c656ca05de3d7b134570470333e2e4d9601548dad6a5bde9842c42444075a01cdfbf&scene=21#wechat_redirect)的方法。这个方法是一个基于树模型的方法，它主要是借用了**集成学习**里面的很多的想法。第二，在很多不同的任务上，它的模型得到的结果可以说和深度神经网络是高度接近的。除了一些大规模的图像任务，这基本上是深度神经网络的杀手锏应用，它在很多的其它任务上，特别是跨任务的表现非常好。我们可以用同样一套参数，用不同的任务，性能都还不错，就不再需要逐任务的慢慢去调参数，同时它要调的超参数少很多，容易调的多。还有一个很重要的特性，它有自适应的模型复杂度，可以根据数据的大小，自动的来判定模型该长到什么程度。

另外一方面，我们要看到，这实际上是在深度学习这个学科领域发展思路上一个全新的探索。所以今天虽然它已经能够解决一部分问题了，但是我们应该可以看到它再往下发展下去，它的前景可能是今天我们还不太能够完全预见到的。

我经常说**我们其实没有什么真正的颠覆性的技术，所有的技术都是一步一步发展起来的。**比方说现在深度神经网络里面最著名的CNN，从首次提出到ImageNet上获胜是经过了30年，从算法完全成形算起，到具备在工业界广泛使用的能力也是经过了20年，无数人的探索改进。所以，今天的一些新探索，虽然已经能够解决一些问题，但更重要的是再长远看，经过很多进一步努力之后，可能今天的一些探索能为未来的技术打下重要的基础。

以前我们说深度学习是一个黑屋子，这个黑屋子里面有什么东西呢？大家都知道，有深度神经网络。现在我们把这个屋子打开了一扇门，把深度森林放进来了，那我想以后可能还有很多更多的东西。可能这是从学科意义来看，这个工作更重要的价值。



参考文章:

[周志华：深度学习为什么深？](https://yq.aliyun.com/articles/581994)





