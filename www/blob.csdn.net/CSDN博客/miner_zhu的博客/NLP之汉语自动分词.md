# NLP之汉语自动分词 - miner_zhu的博客 - CSDN博客





置顶2018年09月21日 21:47:45[miner_zhu](https://me.csdn.net/miner_zhu)阅读数：384








汉语自动分词就是让计算机识别出汉语文本中的‘词’，在词与词之间自动加上空格或其他边界标记。

**目录**

[一.汉语自动分词中的基本问题](#%E4%B8%80.%E6%B1%89%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98)

[1.1分词规范问题](#1.1%E5%88%86%E8%AF%8D%E8%A7%84%E8%8C%83%E9%97%AE%E9%A2%98)

[2.2歧义切分问题](#2.2%E6%AD%A7%E4%B9%89%E5%88%87%E5%88%86%E9%97%AE%E9%A2%98)

[3.未登录词问题](#3.%E6%9C%AA%E7%99%BB%E5%BD%95%E8%AF%8D%E9%97%AE%E9%A2%98)

[二.汉语分词方法](#%E4%BA%8C.%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95)

[1.N-最短路径方法](#1.N-%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E6%96%B9%E6%B3%95)

[2.基于词的n元语法模型的分词方法](#2.%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84n%E5%85%83%E8%AF%AD%E6%B3%95%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95)

[3.由字构词的汉语分词方法](#3.%E7%94%B1%E5%AD%97%E6%9E%84%E8%AF%8D%E7%9A%84%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95)

[4.基于词感知机算法的汉语分词方法](#4.%E5%9F%BA%E4%BA%8E%E8%AF%8D%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E7%9A%84%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95)

[5.基于字的生成式模型和区分式模型相结合的汉语分词方法](#5.%E5%9F%BA%E4%BA%8E%E5%AD%97%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%8C%BA%E5%88%86%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9B%B8%E7%BB%93%E5%90%88%E7%9A%84%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95)

[6.其他分词方法](#6.%E5%85%B6%E4%BB%96%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95)

[三、分词方法比较](#%E4%B8%89%E3%80%81%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83)

# 一.汉语自动分词中的基本问题

 汉语自动分词的主要困难来自如下三个方面：**分词规范、歧义切分、未登录词的识别**。

### 1.1**分词规范问题**

“词是什么”（词的抽象定义）及“什么是词”（词的具体界定），这两个基本问题迄今拿不出一个公认的、具有权威性的词表来。

主要困难出自两个方面：一方面是单字词与词素之间的划界，另一方面是词与短语（词组）的划界。

此外，对于汉语“词”的认识，普通说话人的语感与语言学家的标准也有较大的差异。

从计算的严格意义上说，自动分词是一个没有明确定义的问题 。

### 2.2**歧义切分问题**

歧义字段在汉语文本中普遍存在。

梁南元最早对歧义字段进行了比较系统的考察。他定义了以下两种基本的切分歧义类型。

**1.交集型切分歧义**

汉字串ABC称作交集型切分歧义，如果满足AB、BC同时为词（A、B、C分别为汉字串）。此时汉字串B称作交集串。

例如：大学生、结合成……

为了区分交集型歧义字段的复杂度，我们引入链长的概念。

**链长**： 一个交集型切分歧义所拥有的交集串的集合称为交集串链，它的个数称为链长。

例如，交集型切分歧义“结合成分子”、“结合”、“合成”、“成分”、“分子”均成词，交集串的集合为｛“合”，“成”，“分”｝，因此，链长为3。类似地，“为人民工作”交集型歧义字段的链长为3，“中国产品质量”字段的链长为4，“部分居民生活水平”字段的链长为6。

**2.组合型切分歧义**

 汉字串AB称作多义组合型切分歧义，如果满足A、B、AB同时为词。

例如，多义组合型切分歧义：“起身”

在如下两个例子中，“起身”分别有两种不同的切分：（a）他站│起 │身│来。（b）他明天│起身│去北京。

类似地，“将来”、“现在”、“才能”、“学生会”等，都是组合型歧义字段。



汉语词语边界的歧义切分问题比较复杂，处理这类问题时往往需要进行复杂的上下文语义分析，甚至韵律分析，包括语气、重音、停顿等。

### 3.未登录词问题

未登录词又称为生词（unknown word），可以有两种解释：一是指已有的词表中没有收录的词；二是指已有的训练语料中未曾出现过的词。在第二种含义下，未登录词又称为集外词（out of vocabulary, OOV），即训练集以外的词。

通常情况下将OOV与未登录词看作一回事。

未登录词可以粗略划分为如下几种类型：

①新出现的普通词汇，如博客、房奴、给力等，尤其在网络用语中这种词汇层出不穷。

②专有名词（proper names）。专有名词在早期主要是指人名、地名和组织机构名这三类实体名称。1996年第六届信息理解会议对此进行了扩展，首次提出了命名实体（named entity）的概念，新增了时间和数字表达（日期、时刻、时段、数量值、百分比、序数、货币数量等），并且地名被进一步细化为城市名、州（省）名和国家名称等。

③专业名词和研究领域名称。特定领域的专业名词和新出现的研究领域名称也是造成生词的原因之一，如三聚氰胺、苏丹红、禽流感、堰塞湖等。

④其他专用名词，如新出现的产品名，电影、书籍等文艺作品的名称，等等。

根据黄昌宁等人（2003）的统计，在真实文本的切分中，未登录词总数的大约九成是专有名词（人名、地名、组织机构名），其余的为新词（包括专业术语）。当然，这个统计比例与语料所属的领域密切相关。

对于大规模真实文本来说，未登录词对于分词精度的影响远远超过了歧义切分。在汉语分词系统中对于未登录词的处理，尤其是对命名实体的处理，远比对切分歧义词的处理重要得多。

在20世纪八九十年代和21世纪初期，针对汉语分词问题，其一般做法是：首先依据从各类命名实体库中总结出来的统计知识（如人名姓氏用字及其频度）和人工归纳出来的某些命名实体结构规则，在输入句子中猜测可能成为命名实体的汉字串并给出其置信度，然后利用对该类命名实体具有标识意义的紧邻上下文信息（如称谓、组织机构标识词等），以及全局统计量和局部统计量，作进一步鉴定。

实践中，外国译名的识别效果最好，中国人名次之，中国地名再次之，组织机构名最差，而任务本身的难度实质上也正是循这个顺序由小到大。未登录词的识别面临很多困难。一方面，很多未登录词都是由普通词汇构成的，长度不定，也没有明显的边界标志词；另一方面，有些专有名词的首词和尾词可能与上下文中的其他词汇存在交集型歧义切分。另外，在汉语文本中夹杂着其他语言的字符或符号，也是常见的事情。

# 二.汉语分词方法

刘源等（1994）曾简要介绍了16种不同的分词方法，包括正向最大匹配法（forward maximum matching method, FMM）、逆向最大匹配法（backward maximum matching method, BMM）、双向扫描法、逐词遍历法等，这些方法基本上都是在20世纪80年代或者更早的时候提出来的。由于这些分词方法大多数都是基于词表进行的，因此，一般统称为基于词表的分词方法。

随着统计方法的迅速发展，人们又提出了若干基于统计模型（包括基于HMM和n元语法）的分词方法，以及规则方法与统计方法相结合的分词技术，使汉语分词问题得到了更加深入的研究。

接下来主要介绍几种性能较好的基于统计模型的分词方法，并对这些分词技术进行简要的比较。

### 1.N-最短路径方法

考虑到汉语自动分词中存在切分歧义消除和未登录词识别两个主要问题，因此，有专家将分词过程分成两个阶段：首先采用切分算法对句子词语进行初步切分，得到一个相对最好的粗分结果，然后，再进行歧义排除和未登录词识别。当然，粗切分结果的准确性与包容性（即必须涵盖正确结果）直接影响后续的歧义排除和未登录词识别模块的效果，并最终影响整个分词系统的正确率和召回率。

张华平等（2002）提出了旨在提高召回率并兼顾准确率的词语粗分模型——基于N-最短路径方法的汉语词语粗分模型。

这种方法的基本思想是：根据词典，找出字串中所有可能的词，构造词语切分有向无环图。每个词对应图中的一条有向边，并赋给相应的边长（权值）。然后针对该切分图，在起点到终点的所有路径中，求出长度值按严格升序排列（任何两个不同位置上

的值一定不等，下同）依次为第1、第2、…、第i、…、第N（N≥1）的路径集合作为相应的粗分结果集。如果两条或两条以上路径长度相等，那么，它们的长度并列第i，都要列入粗分结果集，而且不影响其他路的排列序号，最后的粗分结果集合大小大于或等于N。

考虑到切分有向无环图G中每条边边长（或权重）的影响，张华平等人又将该方法分为**非统计粗分模型**和**统计粗分模型**两种。所谓的非统计粗分模型即假定切分有向无环图G中所有词的权重都是对等的，即每个词对应的边长均设为1。

**非统计粗分模型：**

假设NSP为结点V0到Vn的前N个最短路径的集合，RS是最终的N-最短路径粗分结果集。那么，N-最短路径方法将词语粗分问题转化为如何求解有向无环图G的集合NSP。

求解有向无环图G的集合NSP可以采取贪心技术，张华平等使用的算法是基于求解单源最短路径问题的Dijkstra贪心算法的一种简单扩展。改进之处在于：每个结点处记录N个最短路径值，并记录相应路径上当前结点的前驱。如果同一长度对应多条路径，必须同时记录这些路径上当前结点的前驱，最后通过回溯即可求出NSP。

**例子：**

以句子“他说的确实在理”为例，给出了3-最短路径的求解过程。

![](https://img-blog.csdn.net/20180921193118310?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

图中，虚线是回溯出的是第一条最短路径，对应的粗分结果为：“他／说／的／确实／在理／”，Table（2），Table（3）…Table（7）分别为结点2、3、…、7对应的信息记录表，Table（0）、 Table（1）的信息记录表没有给出。每个结点的信息记录表里的编号为路径不同长度的编号，按由小到大的顺序排列，编号最大不超过N。如Table（5）表示从结点0出发到达结点5有两条长度为4的路径（分别为0-1-2-4-5和0-1-2-3-5）和一条长度为5的路径（0-1-2-3-4-5）。前驱（i, j）表示沿着当前路径到达当前结点的最后一条边的出发结点为i，即当前结点的前一个结点为i，相应的边为结点i的信息记录表中编号为j的路径。如果j＝0，表示没有其他候选的路径。如结点7对应的信息记录表Table（7）中编号为1的路径前驱（5,1）表示前一条边为结点5的信息表中第1条路径。类似地，Table（5）中的前驱（3,1）表示前驱为结点3的信息记录表中的第1条路径。Table（3）中的（2,0）表示前驱边的出发点为结点2，没有其他候选路径。信息记录表为系统回溯找出所有可选路径提供了依据。

Dijkstra算法的时间复杂度为O（n^2），它求的是图中所有点到单源点的最短路径，而应用于切分有向图时，有两个本质区别：首先有向边的源点编号均小于终点编号，即所有边的方向一致；其次，算法最终求解的是有向图首尾结点之间的N-最短路径。因此，在该算法中，运行时间与n（字串长度）、N（最短路径数）以及某个字作为词末端字的平均次数k（等于总词数除以所有词末端字的总数，对应的是切分图中结点入度的平均值）成正比。所以，整个算法的时间复杂度是O（n×N×k）。

考虑到在非统计模型构建粗切分有向无环图的过程中，给每个词对应边的长度赋值为1。随着字串长度n和最短路径数N的增大，长度相同的路径数急剧增加，同时粗切分结果数量必然上升。这样，大量的切分结果对后期处理以及整个分词系统性能的提高非常不利。因此，又给出了一种基于**统计信息的粗分模型**。

假定一个词串W经过信道传送，由于噪声干扰而丢失了词界的切分标志，到输出端便成了汉字串C。N-最短路径方法词语粗分模型可以相应地改进为：求N个候选切分W，使概率P（W|C）为前N个最大值：![](https://img-blog.csdn.net/20180921194301117?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)其中，P（C）是汉字串的概率，它是一个常数，不必考虑。从词串恢复到汉字串的概率P（C|W）＝1（只有唯一的一种方式）。

因此，粗分的目标就是确定P（W）最大的N种切分结果。为了简化计算，张华平等人采用一元统计模型。假设W＝w1w2…wm是字串S＝c1c2…cn的一种切分结果。wi是一个词，P（wi）表示词wi出现的概率，在大规模语料训练的基础上通过最大似然估计方法求得。切分W的概率为：

![](https://img-blog.csdn.net/2018092119433512?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

为了处理方便，令P*（W）＝-lnP（W）＝![](https://img-blog.csdn.net/20180921194348375?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)这样，-lnP（wi）就可以看作是词wi在切分有向无环图中对应的边长（做适当的数据平滑处理）。于是，求式（7-2）的最大值问题转化为求P*（W）的最小值问题。

针对修改了边长后的切分有向无环图G*，直接使用非统计粗分模型的求解算法，就可以获得问题的最终解。

### 2.基于词的n元语法模型的分词方法

基于词的n元文法模型是一个典型的生成式模型，早期很多统计分词方法均以它为基本模型，然后配合其他未登录词识别模块进行扩展。其基本思想是：首先根据词典（可以是从训练语料中抽取出来的词典，也可以是外部词典）对句子进行简单匹配，找出所有可能的词典词，然后，将它们和所有单个字作为结点，构造的n元的切分词图，图中的结点表示可能的词候选，边表示路径，边上的n元概率表示代价，最后利用相关搜索算法（如Viterbi算法）从图中找到代价最小的路径作为最后的分词结果。

以输入句子“研究生物学”为例，下图给出了基于二元文法的切分词图。

![](https://img-blog.csdn.net/20180921194554888?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

于未登录词的识别是汉语分词过程中的关键问题之一，因此，很多家认为未登录词的识别与歧义切分应该是一体化处理的过程，而不是相互分离的。1996曾提出了基于加权的有限状态转换机模型与未登录词识别一体化切分的实现方法。受这种方法的启发，J.Gao等人2003提出了基于改进的信源信道模型的分词方法。现在简要介绍一下这种基于统计语言模型的分词方法。

为了给自动分词任务一个明确的定义，J.Gao等人对文本中的词给出了一个可操作的定义，把汉语词定义成下列4类：

（1）待切分文本中能与分词词表中任意一个词相匹配的字段为一个词。

（2）文本中任意一个经词法派生出来的词或短语为一个词，如重叠形式（高高兴兴，说说话、天天）、前缀派生（非党员、副部长）、后缀派生（全面性、朋友们）、中缀派生（看得出、看不出）、动词加时态助词（克服了、蚕食着）、动词加趋向动词（走出、走出来）、动词的分离形式（长度不超过3个字，如：洗了澡、洗过澡），等等。

（3）文本中被明确定义的任意一个实体名词（如：日期、时间、货币、百分数、温度、长度、面积、体积、重量、地址、电话号码、传真号码、电子邮件地址等）是一个词。

（4）文本中任意一个专有名词（人名、地名、机构名）是一个词。

在这个定义中没有考虑文本中的新词问题。

假设随机变量S为一个汉字序列，W是S上所有可能切分出来的词序列，分词过程应该是求解使条件概率P（W|S）最大的切分出来的词序列W*，即![](https://img-blog.csdn.net/20180921201144405?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

根据贝叶斯公式，式（7-3）改写为

![](https://img-blog.csdn.net/20180921201214656?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

由于分母为归一化因子，因此，

![](https://img-blog.csdn.net/2018092120123513?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

为了把4类词纳入同一个统计语言模型框架，黄昌宁等分别把专有名词的人名（PN）、地名（LN）、机构名（ON）各作为一类，实体名词中的日期（dat）、时间（tim）、百分数（per）、货币（mon）等作为一类处理，简称为实体名，对词法派生词（MW）和词表词（LW）则每个词单独作为一类。这样，按表7-2可以把一个可能的词序列W转换成一个可能的词类序列C＝c1c2…cN，那么，式（7-5）可被改写成式（7-6）：

![](https://img-blog.csdn.net/20180921201322274?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中，P（C）就是大家熟悉的语言模型，我们不妨将P（S|C）称为生成模型。

![](https://img-blog.csdn.net/20180921201340855?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

根据之前中对语言模型的介绍，如果P（C）采用三元语法，可以表示为：

![](https://img-blog.csdn.net/20180921201409697?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

三元模型的参数可以通过最大似然估计在一个带有词类别标记的训练语料上计算，并采用回退平滑算法解决数据稀疏问题。

生成模型在满足独立性假设的条件下，可以近似为：

![](https://img-blog.csdn.net/20180921201435954?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

式（7-8）认为，任意一个词类ci生成汉字串si的概率只与ci自身有关，而与其上下文无关。例如，如果“教授”是词表词，则P（si＝教授|ci＝LW）＝1

黄昌宁等介绍的实验系统中，词表含有98668个词条，词法派生词表收入59285条派生词。训练语料由88MB新闻文本构成。

模型的训练由以下三步组成：

①在上述两个词表的基础上，用正向最大匹配法（FMM）切分训练语料，专有名词通过一个专门模块标注，实体名词通过相应的规则和有限状态自动机标注，由此产生一个带词类别标记的初始语料；

②用带词类别标记的初始语料，采用最大似然估计方法估计统计语言模型的概率参数；

③采用得到的语言模型对训练语料重新进行切分和标注（见式（7-6）～式（7-8）），得到一个刷新的训练语料。重复第②、③步，直到系统性能不再有明显的提高为止。

另外，对于交集型歧义字段（OAS），该方法的处理措施是：首先通过最大匹配方法（包括正向最大匹配和反向最大匹配）检测出这些字段，然后，用一个特定的类〈GAP〉取代全体OAS，依次来训练语言模型P（C）。类〈GAP〉的生成模型的参数通过消歧规则或机器学习方法来估计。

对于组合型歧义字段（CAS），该方法通过对训练语料的统计，选出最高频、且其切分分布比较均衡的70条CAS，用机器学习方法为每一条CAS训练一个二值分类器，再用这些分类器在训练语料中消解这些CAS的歧义。

### 3.由字构词的汉语分词方法

字构词的汉语分词方法的思想并不复杂，它是将分词过程看作字的分类问题。在以往的分词方法中，无论是基于规则的方法还是基于统计的方法，一般都依赖于一个事先编制的词表，自动分词过程就是通过查词表作出词语切分的决策。与此相反，由字构词的分词方法认为每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位）。假如规定每个字只有4个词位：词首（B）、词中（M）、词尾（E）和单独成词（S）。

这里所说的“字”不仅限于汉字，也可以指标点符号、外文字母、注音符号和阿拉伯数字等任何可能出现在汉语文本中的文字符号，所有这些字符都是由字构词的基本单元。

分词结果表示成字标注形式之后，分词问题就变成了序列标注问题。对于一个含有n个字的汉语句子c1c2…cn，可以用下面的公式来描述分词原理：

![](https://img-blog.csdn.net/20180921202914313?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中，tk表示第k个字的词位，即tk∈{B, M, E, S}。

通常情况下，使用基于字的判别式模型时需要在当前字的上下文中开一个w个字的窗口（一般取w＝5，前后各两个字），在这个窗口里抽取分词相关的特征。常用的特征模板有：

（a）    ck（k＝-2，-1,0,1,2）

（b）    ckck＋1（k＝-2，-1,0,1）

（c）    c－1c1

（d）    T（c－2）T（c－1）T（c0）T（c1）T（c2）

前面三类特征模板（a）～（c）是窗口内的字及其组合特征，T（ci）是指字ci的字符类别，例如，阿拉伯数字，中文数字，标点符号，英文字母等等。假设当前字是“北京奥运会”中的“奥”字，那么模板（a）将生成以下特征：c-2＝北，c-1＝京，c0＝奥，c1＝运，c2＝会；模板（b）：c-2c-1＝北京，c-1c0＝京奥，c0c1＝奥运，c1c2＝运会；模板（c）：c-1c1＝京运。模板（d）与定义的字符类别信息有关，主要是为了处理数字、标点符号和英文字符等有明显特征的词。有了这些特征以后，我们就可以利用常用的判别式模型，如最大熵、条件随机场、支持向量机和感知机（感知器）等进行参数训练，然后利用解码算法找到最优的切分结果。

由字构词的分词技术的重要优势在于，它能够平衡地看待词表词和未登录词的识别问题，文本中的词表词和未登录词都是用统一的字标注过程来实现的，分词过程成为字重组的简单过程。在学习构架上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词识别模块，因此，大大简化了分词系统的设计。

### 4.基于词感知机算法的汉语分词方法

2007年的ACL国际大会上提出了一种基于词的判别式模型，该模型采用平均感知机（averaged perceptron）作为学习算法，直接使用词相关的特征，而不是基于字的判别式模型中经常使用的字相关特征。

简要介绍平均感知机算法。假设x∈X是输入句子，y∈Y是切分结果，其中X是训练语料集合，Y是X中句子标注结果集合。我们用GEN（x）表示输入句子x的切分候选集，用Φ（x, y）∈Rd表示训练实例（x, y）对应的特征向量，α表示参数向量，其中Rd是模型的特征空间。那么，给定一个输入句子x，其最优切分结果满足如下条件：

![](https://img-blog.csdn.net/20180921203547506?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

平均感知机用来训练参数向量α。首先将α中所有参数初始化为0，然后在训练解码过程中不断更新。对每一个训练样本，用当前的模型参数进行解码得到切分结果，如果切分结果与标注结果不一致，则更新模型参数。每一次都保留参数的加和，直到进行多轮迭代以后，取参数的平均值以避免模型过拟合。

平均感知机训练算法如下：

输入：训练样本（xi，yi）

初始化：α＝0，v＝0

算法过程：
![](https://img-blog.csdn.net/20180921203622629?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

输出：α

基于感知机算法的汉语自动分词方法的基本思路是，对于任意给定的一个输入句子，解码器每次读一个字，生成所有的候选词。生成候选词的方式有两种：①作为上一个候选词的末尾，与上一个候选词组合成一个新的候选词；②作为下一个候选词的开始。这种方式可以保证在解码过程中穷尽所有的分词候选。在解码的过程中，解码器维持两个列表：源列表和目标列表。开始时，两个列表都为空。解码器每读入一个字，就与源列表中的每个候选组合生成两个新的候选（合并为一个新的词或者作为下一个词的开始），并将新的候选词放入目标列表。当源列表中的候选都处理完成之后，将目标列表中的所有候选复制到源列表中，并清空目标列表。然后，读入下一个字，如此循环往复直到句子结束。最后，从源列表中可以获取最终的切分结果。

这个算法有点类似于全切分方法，理论上会生成所有的2l－1个切分结果（l为句长）。为了提升切分速度，可以对目标列表tgt中候选词的数目进行限制，每次只保留B个得分最高的候选（如B＝16）。那么，如何对tgt列表中的切分候选进行打分和排序呢？Y. Zhang和S. Clark使用了平均感知机作为学习算法，使用的特征如表7-3所示。

![](https://img-blog.csdn.net/2018092120414830?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

例如，假定源列表src中有一个候选为“如今／有些／露宿”，当解码器读入一个新的字“者”时，就会生成两个候选：（a）“如今／有些／露宿者”；（b）“如今／有些／露宿／者”。候选（a）将激活表7-3中编号为1～6和8～14的特征，候选（b）将激活编号为7的特征。这里c0为“者”。表7-4详细描述了两个候选（a）和（b）对应的特征。

![](https://img-blog.csdn.net/20180921204224973?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

根据这些特征，就可以利用平均感知机分类器对切分候选进行打分和排序。

### 5.基于字的生成式模型和区分式模型相结合的汉语分词方法

根据前面的介绍，在汉语分词中基于词的n元语法模型（生成式模型）和基于字的序列标注模型（区分式模型）是两大主流方法。其中，基于词的生成式模型对于集内词（词典词）的处理可以获得较好的性能表现，而对集外词（未登录词）的分词效果欠佳；基于字的区分式模型则恰好相反，它一般对集外词的处理有较好的鲁棒性，对集内词的处理却难以获得很好的性能，比基于词的生成式模型差很多。其主要原因归结为两种方法采用的不同处理单元（分别是字和词）和模型（分别为生成式模型和区分式模型），在基于字的区分式模型中，将基本单位从词换成字以后，所有可能的“字-标记”对（character-tag-pairs）候选集要远远小于所有可能的词候选集合。通过实验分析发现，两个处于词边界的字之间的依赖关系和两个处于词内部的字之间的依赖关系是不一样的。图7-4给出了logP（ci|ci－1）在两种情况下的分布图。 

![](https://img-blog.csdn.net/20180921212246162?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

基于词的生成式模型实际上隐含地考虑了这种处于不同位置字之间的依赖关系，而在基于字的判别式模型中却无法考虑这种依赖关系。但是，区分式模型能够充分利用上下文特征信息等，有较大的灵活性。因此，基于字的区分式模型具有较强的鲁棒性。基于这种考虑，提出了利用基于字的n元语法模型以提高其分词的鲁棒性，并将基于字的生成式模型与区分式模型相结合的汉语分词方法，获得了很好的分词效果。

在提出的基于字的生成式模型中，将词替换成相应的“字-标记”对，即

![](https://img-blog.csdn.net/2018092121234367?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

根据贝叶斯公式并参考式三元语言模型的计算公式：![](https://img-blog.csdn.net/20180921212356163?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

式（7-11）可以进一步简化为：

![](https://img-blog.csdn.net/20180921212406310?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

这样，基于字的生成式模型仍然以字作为基本单位，但考虑了字与字之间的依赖关系，与基于字的判别式模型相比，处理词典词的能力有了大幅改观。但是，该模型仍然有缺陷，它并没有考虑未来信息（当前字后面的上下文）。例如，“露宿者”中的“者”字，是一个明显的后缀，当使用基于字的判别式模型切分“宿”时，能够利用后续信息判断“宿”应该标记为“M”，而基于字的生成式模型却由于只考虑了“宿”字左边的上下文，错误地将“宿”标为“E”。因此，基于字的生成式模型处理未登录词的能力仍然弱于基于字的判别式模型。为了利用基于字的判别式模型和基于字的生成式模型对词典词和未登录词进行互补性处理，利用线性插值法将这两个模型进行了整合，提出了一个集成式分词模型：

![](https://img-blog.csdn.net/20180921212936375?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中α（0≤α≤1.0）为加权因子。

该模型同时融合了基于字的生成模型和基于字的判别式模型的优点，因此，分词性能比两个基本模型有了大幅度的提升。

### 6.其他分词方法

Gao et al. （2005）提出了一种汉语分词的语用方法（pragmatic approach）。该方法与多数已有方法的主要区别有三点：第一，在传统的分词方法中一般采用理论语言学家根据各种语言学标准给出的汉语词的定义，而在本方法中汉语的词是根据它们在实际使用和处理中的需要从语用上定义的切分单位；第二，该方法中提出了一种语用数学框架，在这个框架中切分已知词和检测不同类型的生词能够以一体化的方式同步进行；第三，在这种方法中假设不存在独立于具体应用的通用切分标准，而是认为根据不同的语言处理任务需要多重切分标准和不同的词汇粒度。该方法已经应用于微软亚洲研究院开发的适应性汉语分词系统MSRSeg）中。

自从薛念文等人提出由字构词的分词方法以后，该模型迅速成为汉语分词的主流方法。为了提升该模型的词典词召回率（recall），张瑞强等人提出了基于“子词”（sub-word）的判别式模型方法：首先用最大匹配方法切分出常用词，然后将sub-word和字混合作为标注的基本单位。赵海等人还比较了不同词位数量对该模型的影响，他们的实验表明，基于6个词位的效果最好。从近几年的研究情况来看，基于字的判别式模型仍然是汉语分词的主流方法。

基于词的判别式模型有效提升了基于词的分词方法的性能。孙薇薇比较了基于字的判别式模型和基于词的判别式模型的性能，并利用Bagging将两种方法结合，取得了不错的效果。

# 三、分词方法比较

为了公平地对比各种分词方法，我们采用SIGHAN规定的“封闭测试”原则：模型训练和测试过程中，仅允许使用SIGHAN提供的数据集进行训练和测试，其他任何语料、词典、人工知识和语言学规则都不能使用。评价指标包括：准确率（P）、召回率（R）、F-测度（F-measure，简写为F）、未登录词的召回率（ROOV）和词典词的召回率（RIV）。各指标的计算公式如下：

![](https://img-blog.csdn.net/20180921214210363?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmVyX3podQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

在实际应用中，F值计算时一般取β＝1，所以，F值又称为F1值。

总之，随着自然语言处理技术整体水平的提高，尤其近几年来新的机器学习方法和大规模计算技术在汉语分词中的应用，分词系统的性能一直在不断提升。特别是在一些通用的书面文本上，如新闻语料，领域内测试（训练语料和测试语料来自同一个领域）的性能已经达到相当高的水平。但是，跨领域测试的性能仍然很不理想，例如用计算机领域或者医学领域的测试集测试用新闻领域的数据训练出来的模型。由于目前具有较好性能的分词系统都是基于有监督的学习方法训练出来的，需要大量有标注数据的支撑，而标注各个不同领域的语料需要耗费大量的人力和时间，因此，如何提升汉语自动分词系统的跨领域性能仍然是目前面临的一个难题。

另外，随着互联网和移动通信技术的发展，越来越多的非规范文本大量涌现，如微博、博客、手机短信等。这些网络文本与正规出版的书面文本有很大的不同，大多数情况下它们不符合语言学上的语法，并且存在大量网络新词和流行语，有很多还是非正常同音字或词的替换，例如，“温拿”、“卢瑟”等。这些文字的出现都给分词带来了一定的困难，传统的分词方法很难直接使用，而且这些网络新词的生命周期都不是很长，消亡较快，所以，很难通过语料规模的随时扩大和更新来解决这些问题。

综上所述，汉语分词是中文信息处理研究的一项基础性工作，经过几十年的研究开发，已经取得了丰硕的成果，但仍面临若干颇具挑战性的难题。



