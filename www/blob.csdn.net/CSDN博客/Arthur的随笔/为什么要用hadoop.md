# 为什么要用hadoop - Arthur的随笔 - CSDN博客
2011年04月10日 23:10:00[largetalk](https://me.csdn.net/largetalk)阅读数：4063
        以前公司的数据日志是每天从生产数据库导出到数据服务器，再通过一个python脚本分析这些日志并存入mysql当中，这种方式在数据量小的情况下还没什么事，数据量一大，所需时间是几何增长。有段时间每天光apache log解压之后就有几十个G，虽然通过很多手段比如减少查询，减少单条数据插入，使用LOAD将数据导入数据库，但所需时间还是要很久。为了减少每天log分析的时间以及数据的稳定性，决定搭建一个hadoop系统，使用hadoop map/reduce来并行的处理log。
       具我分析，目前数据分析系统的主要瓶颈在于数据文件分析，数据的提取以及数据库的查询操作，而不在于数据的插入操作，我们知道mysql load100万条数据也就只要几十秒种，如果能使用多台机器并行的数据分析，提取操作，势必能减少总时间。但需要注意的是，数据的查询和少量的更新仍需要想办法减少或去除，在并行的处理过程中，这些操作不仅会导致程序执行变慢还会产生数据一致性问题。
      为保证程序执行过程中一些全局变量的一致，考虑用membase存放这些变量，为了方便调试和保留生产环境中产生的log，考虑用logy或redis来收集log，具体哪个还没想好，logy与python logging结合的很好，代码简单，比较好查看，但调试时比较不容易使用，因为需要用到浏览器， redis很稳定，消息不容易丢失（毕竟写的人牛啊），但不太好查看，python redis client也没研究过，不知道好不好用，但用redis调试起来应该会比较方便，写个shell脚本就可以了。
       目前想法是每个hadoop task处理一个用户或一个ip的数据，但每个task要不要去连mysql还没考虑好，毕竟每个task都连mysql的话load操作就变成了insert操作，如果不连，就只能通过hive最后一起导入mysql，又增加了系统的复杂度。
       还有一个现在没考虑好的问题是hadoop key value的取值问题，感觉这个跟数据分析部门的工作相关性非常大，如果作的好，兄弟部门的人就直接可以使用hive来作数据分析了，mysql只用来为报表系统提供数据就可以了。
