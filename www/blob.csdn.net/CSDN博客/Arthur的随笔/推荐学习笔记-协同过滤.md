# 推荐学习笔记-协同过滤 - Arthur的随笔 - CSDN博客
2012年12月04日 23:28:59[largetalk](https://me.csdn.net/largetalk)阅读数：816
协同过滤是推荐系统中用的比较多的算法，也是容易理解较简单的算法，而且效果也不错。协同过滤又分为：
item-based collaborative filtering: 喜欢这个物品的人还喜欢什么， 代表有amazon
user-based collaborative filtering: 和我相似的人还喜欢什么， 代表有digg
两种方法计算类似，先要找出相似的item或user。计算相似度（距离）的方法也不少，列举一下：(参考[http://www.chinaz.com/web/2011/1008/212684.shtml](http://www.chinaz.com/web/2011/1008/212684.shtml)）
曼哈顿距离：∑|xi - yi|, 就是每个维度之差相加
欧几理德距离： sqrt( ∑(xi - yi)2 ), 直线距离，非常好懂
明氏距离：明氏距离是欧氏距离的推广， ( ∑|xi - yi|p)1/p , p =2 时就是欧几理德距离， p=1是曼哈顿距离, P越大表示单个维度的差异会更大影响总距离
切比雪夫距离： ![Chebyshev Distance](http://upload.chinaz.com/2011/1008/1318060559321.png)
马哈拉诺比斯距离: 降各维度标准化之后再使用欧式距离
**相似度：**
余弦相似度：![Cosine Similarity](http://upload.chinaz.com/2011/1008/1318060559877.png)， ∑(xiyi) / ( sqrt(∑xi2) * sqrt(∑yi2) ) ,相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。较多用于text
 mining.
皮尔森系数：![Pearson Correlation Coefficient](http://upload.chinaz.com/2011/1008/1318060559622.png) , 用户打分可能有不同尺度，有的偏高，有的偏低， 皮尔森系数能较好忽略这些差异，而察觉到相关性。结果在1到-1之间，正数表示正相关，负数表示负相关。数值越大，相关性越强，0表示不相关。
Jaccard相似系数: Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数，公式如：![Jaccard Coefficient](http://upload.chinaz.com/2011/1008/1318060559590.png)
**K近邻**
一般算出最相似的item(user)不会是一个，而是k个，综合每个邻居的权重和推荐给出最后推荐。一般就是相似度（权重）乘以评分之后得到推荐评分。
