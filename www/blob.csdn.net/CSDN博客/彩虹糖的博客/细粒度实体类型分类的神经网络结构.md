# 细粒度实体类型分类的神经网络结构 - 彩虹糖的博客 - CSDN博客





2018年04月08日 18:59:32[彩虹糖梦](https://me.csdn.net/caozixuan98724)阅读数：310








# 实体分类相关论文阅读第二篇：

# Neural Architectures for Fine-grained Entity Type Classification

这篇论文的基础是[引入注意力机制的细粒度实体分类](https://blog.csdn.net/caozixuan98724/article/details/79834673)，里面论文的主体构架和上一篇论文是相同的，因此关于该篇论文的大部分内容不再重复叙述，重点关注本篇论文的创新点：将手工提取的feature与注意力网络结合起来提高实体分类的精确度。

![](https://img-blog.csdn.net/2018040817544285)


这是在上篇论文中出现的注意力神经网络，可以看到这个网络的输入是通过LSTM得到的上下文矩阵，输出的两个a分别与上文与下文有关。

这样的方式有什么问题呢？对于不同属性，不同特点的实体，我们不能针对实体的区别去影响到我们的神经网络，因此，要想去提高这个模型的表现，我们下一步的目标便是将实体本身的特性引入到这个神经网络中去。

关于实体特征的抽取和表示，作者参考了另一篇论文Context-Dependent Fine-Grained Entity Type Tagging的工作，为了理解这篇论文，我们再把作者提到的这篇论文的部分内容拿出来看一下。

这篇论文来自谷歌发表于2014年。作者使用到这篇论文的地方有两点，手工特征的选取和实体分类的表示（这一点我们在后文还会提到）。

首先看在这篇论文中手工特征的提取：

![](https://img-blog.csdn.net/20180408182529595)


在这张表中，作者人工提取出了实体词汇的这9个特征，比如它的形式，它的上下文，它是主语还是宾语，它所在的文档的主题等等，关于Cluster，原文给出的解释是The word clusters are derived from the class-based exchange clustering algorithm described by Uszkoreit and Brants(2008).具体是什么意思再去相关论文找去，大致应该是另一篇论文中提到的词群分类。

提取出特征我们应该把它表示成向量，一个很严肃的问题在这里，我看不懂原文是怎么把特征转化为向量的。原文如下：

For each mention of a resolved entity with at least one type, we extract a training instance (x, y), where x is a vector of binary feature indicators and y ∈ {0, 1} |T | is the binary vector of label indicators. The feature set includes the lexical and syntactic features described in Table 4, similar to those used in previous work. We also use a more semantic document topic feature, the result of training a simple bag-of-words topic model with eight topics (arts, business, entertainment, health, mayhem, politics, scitech, sport), to try to capture longer-range context. The word clusters are derived from the class-based exchange clustering algorithm described by Uszkoreit and Brants (2008).


从上述表格来看，上面的特征绝对不是二分类的问题，那作者是怎么表示成01向量的？这是我没有理解的地方，先假定其表达为01向量，继续往下看。

再回到我们要分析的这篇论文，通过作者引用的这篇论文的工作，我们已经得到了手工提取特征的向量，怎么把它引入神经网络呢？作者给出了下列公式：

![](https://img-blog.csdn.net/20180408183958840)


再来对比一下上篇论文中的输出公式：

![](https://img-blog.csdn.net/20180408184030362)


显而易见，我们在输出函数的变量中增加了一个feature向量，而这个feature向量是由01向量乘以一个投影向量得到的。

本篇论文的另一个创新点在于Wy这个参数矩阵上。在论文中，作者给出了这样的公式：

![](https://img-blog.csdn.net/2018040818435881)


让我们关注一下Vy和S分别代表了什么。

Vy很简单，是一个通过训练得到的一个权重矩阵，而S则是一个系数矩阵，上述公式的含义和S的含义在原文中论述如下：

![](https://img-blog.csdn.net/20180408184653767)


![](https://img-blog.csdn.net/20180408184702864)


作者将分类转化为稀疏矩阵的想法也是来自我们在上文中提到的那篇论文Context-Dependent Fine-Grained Entity Type Tagging。

贴一张完整的多级分类图示意：

![](https://img-blog.csdn.net/20180408185449600)


最后再看一眼总的模型：

![](https://img-blog.csdn.net/20180408190413142)


总体上看，和上篇论文其实没有区别。

## 总结：

论文看到这里，精华就差不多看完了。总结一下，这篇论文的创新点主要在两个方面。

一.输出函数的变量中增加了一个feature向量，更多的引入了实体的知识。

二.输出函数中的参数矩阵同样引入了分类信息，更能突出重点。












