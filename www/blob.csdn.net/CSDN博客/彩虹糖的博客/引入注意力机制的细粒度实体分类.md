# 引入注意力机制的细粒度实体分类 - 彩虹糖的博客 - CSDN博客





2018年04月06日 19:17:04[彩虹糖梦](https://me.csdn.net/caozixuan98724)阅读数：1314








# 实体分类相关论文阅读第一篇：

## **An Attentive Neural Architecture for Fine-grained Entity Type Classification**



这篇论文来自UCL自然语言处理实验室，发表于2016年。

细粒度实体分类是在构建知识图谱过程中非常重要的内容，关于实体分类相关的文献也比较多，也有不少分类方法，但是我们如何在非结构化的文本中确定出一个我们想要的细粒度实体，并把它分入相应的大类中去呢？

举个例子，“掘金胜森林狼，约基奇准三双助球队赢下争八关键战。”我们该如何从文本中识别出“掘金”，“森林狼”属于NBA球队，“约基奇”属于篮球运动员呢？本篇论文给出了解决方案。

![](https://img-blog.csdn.net/20180406165341956)


这是整篇论文实现的模型，接下来我们会一一进行讲解。




### 一.模型概览

首先这个神经网络输入的是一段话，比如上图中的"She got a Ph.D from New Your in Feb. 1995."，这句话被分为了三个部分，第一部分是被标注为实体的New York，第二部分是它的上文got a Ph.D from，最后一部分是它的下文in Feb. 1995.输入的均是这些词的词向量。（词向量：通过word embedding的方法将词汇映射到稠密的连续的向量空间）

输入的模型如下：

![](https://img-blog.csdn.net/20180406170143663)


C代表选取的上文或下文的长度，l,r代表上下文中的一个单词，M代表实体的长度，m代表实体中的一个单词。如果原句的长度达不到上下文的长度，比如实体在句末，不存在下文，相应的位置用占位符代替，比如用“#”代替。

在这个模型中我们的输出是一组yk，假如我们把实体分为K类，yk代表该实体是某一类型的可能性。当yk>0.5且是所有可能性中最高的那个时，我们认为该实体属于k这个大类。

输出公式如下：

![](https://img-blog.csdn.net/2018040617125167)


符号意义如下：

![](https://img-blog.csdn.net/20180406171415396)![](https://img-blog.csdn.net/20180406171419573)![](https://img-blog.csdn.net/20180406171426160)


vm和vc分别代表由实体和上下文获得的向量。

损失函数的定义如下：

![](https://img-blog.csdn.net/2018040617170878)


这是分类问题中常用的损失函数，可以根据最大似然估计推导得出。

### 二.实体的表示

论文中关于实体的表示，作者选取的模型十分简单，直接取了实体各个单词词向量的平均值。

![](https://img-blog.csdn.net/2018040618252027)


作者在论文中提到，在实体表示方面，用神经网络做实体表示效果反而并不好，他们认为这和标签在测试集和训练集中的差异性有关。举个例子，Monday可能在测试集中被标记为time，而训练集中被标注为别的，而显示声明的日期如Feb. 24则在测试集训练集中都被标记为time，这种差异性越是应用在越复杂的模型上，产生的危害就越大。（虽然根据我为数不多的经验，这种取平均值的做法并不能体现实体原有的语义，效果并不好。）

### 三.上下文的表示

#### 1.使用词向量平均值表示

![](https://img-blog.csdn.net/20180406183540809)


（这种方法效果肯定不大好）

#### 2.LSTM编码

LSTM是RNN的一种特例，可以避免常规RNN的梯度消失的问题，因而在工业界得到广泛应用。

这是传统RNN的模型：

![](https://img-blog.csdn.net/20180406184920689)


![](https://img-blog.csdn.net/20180406184929370)


这是LSTM模型：

![](https://img-blog.csdn.net/20180406184943316)


关于LSTM的详细介绍，参考这篇博文：[LSTM模型和前向反向传播算法](http://www.cnblogs.com/pinard/p/6519110.html)

![](https://img-blog.csdn.net/2018040618524033)


根据上文中介绍的LSTM模型，我们不难知道当前输出和当前细胞状态是有三个变量决定的：之前输出，之前细胞状态和当前输入。并且，我们可以将上下文正序输入，接着再倒叙输入可以得到两个向量，这最终组成了我们的上下文向量。

#### 三.引入注意力机制的LSTM编码

这是全文的精华重点所在，具体我也不是很明白（主要是注意力机制没怎么接触过）。

注意力机制的作用：注意力机制可以让一个神经网络能够只关注其输入的一部分信息，它能够选择特定的输入。

下图是核心公式：

![](https://img-blog.csdn.net/20180406190522391)


f 是一个 attention network，其生成一个 attention vector a, 再讲 a 与输入 x 的 特征向量 z 相乘，这个 a 取值范围是 [0,1]，当我们说 soft attention 时，其取值是 0 到 1，当我们说 hard attention 其取值就只有 0 或 1。


关于注意力机制，参考这篇博文：[神经网络注意力机制](https://blog.csdn.net/zhangjunhit/article/details/78249184)

![](https://img-blog.csdn.net/20180406190739624)


上图是论文中注意力机制的完整实现，作者用两层神经网络去训练这两个a，从公式可以看出这是一个soft attention。

论文讲解到这里，本篇文章第一张图就相当清晰明了了。

在第一张图中，实体的处理很简单，直接选取词向量的平均值，上下文输入处理比较麻烦，依次经过了词嵌入，LSTM编码，注意力层，最终得到的结果与实体一起丢入output layer得到最终的答案。

### 四.效果展示

当然了，作者给出的方法效果最好(￣▽￣)~*

下图是作者的衡量标准：

![](https://img-blog.csdn.net/20180406191456938)


下图是数据展示：

![](https://img-blog.csdn.net/20180406191556506)


可以看到，在大部分情况下，引入注意力机制的LSTM模型取得的效果最好。















