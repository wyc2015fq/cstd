# 还在熬夜憋思路？这12篇最新论文打包送给你 | 本周值得读 - Paper weekly - CSDN博客





2018年06月04日 15:34:03[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1940

















在碎片化阅读充斥眼球的时代，越来越少的人会去关注每篇论文背后的探索和思考。





在这个栏目里，你会快速 get 每篇精选论文的亮点和痛点，时刻紧跟 AI 前沿成果。




点击本文底部的「**阅读原文**」即刻加入社区，查看更多最新论文推荐。
这是 PaperDaily 的第 **78** 篇文章



**Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms**

**@EricShen 推荐**

#Word Embedding

这篇发表在 ACL 2018 上的论文来自于杜克大学 Lawrence Carin 教授的实验室。**文章重新审视了 deep learning models（例如 CNN, LSTM）在各类 NLP tasks 中的的必要性**。

通过大量的实验探究（17 个数据集），**作者发现对于大多数的 NLP 问题，在 word embedding 矩阵上做简单的 pooling 操作就达到了比 CNN encoder 或者 LSTM encoder 更好的的结果**。这类模型被作者命名为 SWEM (Simple Word-Embedding-based Models)。

**文章进一步提出了一种新型的 hierarchical pooling 操作：**在考虑到部分 word-order 信息的同时，保持了模型的简单性。值得一提的是，SWEM 模型相较于 LSTM 模型在训练速度上提高了 10 倍之多，在参数量上也大大减少。







论文链接

https://www.paperweekly.site/papers/1987



代码链接

https://github.com/dinghanshen/SWEM





**Deep Reinforcement Learning For Sequence to Sequence Models**

**@guohao916 推荐**

#Seq2Seq

**针对 Seq2Seq 模型中存在的两类问题：**1. exposure bias; 2. inconsistency between train/test measurement，**本文从强化学习的角度出发，结合强化学习方法在决策上的优势和 Seq2Seq 模型在长期记忆方面的优势，提出了基于深度强化学习的序列到序列的模型**，从而能够更好地解决复杂情况下的 Seq2Seq任务。






论文链接

https://www.paperweekly.site/papers/1973



代码链接

https://github.com/yaserkl/RLSeq2Seq






**Training Classifiers with Natural Language Explanations**

**@erutan 推荐**

#Relation Extraction

本文是斯坦福大学发表于 ACL 2018 的工作。在关系抽取中，**本文利用标注时标注者提供的自然语言解释，使用极弱的基于规则的领域无关的 parser 将其转化为标注规则，并自动去除了大多数矛盾的规则，将其运用在大量未标注数据中获取弱标注信息**，利用 weakly-supervised 的方法训练并取得不错的效果。 

比较有意思的点在于，一个是**利用了标注者给出的基于自然语言的解释**，这是一种相对而言可以较为廉价获得（相对于更专业的结构化语言）的资源。另一个是**使用了领域无关的弱 parser**，却非常简单地筛掉了大多数错误规则并且指出部分“细微的错误”还会带来一些泛化能力上的提升。







论文链接

https://www.paperweekly.site/papers/1986



代码链接

https://github.com/HazyResearch/babble








**What you can cram into a single vector: Probing sentence embeddings for linguistic properties**

**@lipaper9527 推荐**

#Sentence Embedding

本文是 Facebook AI Research 发表于 ACL 2018 的工作，**文章构建了一系列的句子级别的任务来检测不同模型获得的句子向量的质量**。

任务包含表层的信息如预测句子长度或某个字是否出现在句子中，也包含句法信息如句法树的深度，语义信息如时态、主语个数、宾语个数等。论文旨在比较不同模型获得的句子向量的质量。非常有意思且有价值。






论文链接

https://www.paperweekly.site/papers/1977








**Learning Semantic Textual Similarity from Conversations**

**@xwzhong 推荐**

#Sentence Embedding

**本文来自 Google Research，****文章提出使用对话数据+迁移学习（此处使用了 SNLI 数据集）来生成句向量**，从而用于 QA 中 question rerank，answer rerank 和 sentence 相似度计算等任务中。






论文链接

https://www.paperweekly.site/papers/1984




论文笔记

https://www.paperweekly.site/papers/notes/397








**R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering**

**@xiaolu 推荐**

#Visual Question Answering

**本文是清华大学和微软发表在 KDD ’18 的工作**，近来一些 VQA 工作引入了高级语义概念，例如利用计算机视觉领域的一些方法挖掘图像中的语义实体和属性，从而为 VQA 模型提供丰富的语义信息。

相比于一元形式的实体和属性，关系事实（Relation Fact）由主体实体、关系、对象实体三个要素组成，可以构造大量的事实组合，因此具有更强大的语义表达能力。

然而，这些工作存在着明显的局限性。一方面，它们使用高层次的语义概念为一元形式的实体或属性的，只能表达有限的语义知识。另一方面，利用在其它任务或数据集中训练得到的模型提取图像的候选概念，可能 VQA 任务中的问题内容无关。 

**为了更好地利用隐含在图像中的语义知识，本文提出了一个新的模型框架用来学习 VQA 任务中的视觉关系事实**。具体而言，本文基于 Visual Genome 数据集，通过计算文本之间的语义相似度构建 Relation-VQA（R-VQA）数据集，其中每一个数据由问题、正确答案和相关的支持关系事实组成。本文设计了一种关系事实检测器可以预测与给定视觉问题相关的关系事实。 

**本文进一步提出了由视觉注意力机制和语义注意力机制组成的多步注意力模型，分别提取图像中的视觉知识和语义知识**。本文在两个公开的 VQA 数据集上进行了全面的实验，证明本文的模型实现了目前最好的性能，同时验证了视觉关系事实在 VQA 任务中的效果。






论文链接

https://www.paperweekly.site/papers/1970








**Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering**

**@huilyu 推荐**

#Visual Question Answering

本文是亚利桑那州立大学发表于 AAAI 2018 的工作。VQA 的 reasoning 方面有待加强。**本文利用 PSL（Probabilistic Soft Logic）engine 来定义 inputs 和 rules 并列出 top evidences 提供解释**。其中，inputs 由三部分构成：image captioning and parsing into relation triples；question parsing into relation triples；phrasal similarity。






论文链接

https://www.paperweekly.site/papers/1988



代码链接

https://github.com/adityaSomak/PSLQA








**SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again**

**@SOC1 推荐**

#Pose Estimation

**本文是慕尼黑工业大学发表于 ICCV 2017 的工作**，论文贡献如下：

1. 基于扩展 SSD 的 6D 姿态估计；

2.不需要深度信息，只通过单幅 RGB 图像就能估计出 6D 姿态。






论文链接

https://www.paperweekly.site/papers/1954




代码链接

https://github.com/wadimkehl/ssd-6d








**Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering**

**@xaj 推荐**

Visual Question Answering

本文是杭州电子科技大学发表于 ICCV 2017 的工作，**论文提出了一种新的 bilinear pooling 方法，即 MFB**。此外，论文还引入了 co-attention 机制，来学习 image 和 question 的 attention。






论文链接

https://www.paperweekly.site/papers/1989




代码链接

https://github.com/yuzcccc/vqa-mfb








**H-DenseUNet: Hybrid Densely Connected UNet for Liver and Liver Tumor Segmentation from CT Volumes**

**@yunfeinihao 推荐**

#Image Segmentation

本文来自香港中文大学，**论文使用 U-Net 分割三维医学图像，可以借鉴到其他的医学图像中**。此外，论文还使用了混合 dense 优化网络，进一步提升效果。






论文链接

https://www.paperweekly.site/papers/1968








**Aesthetic-based Clothing Recommendation**

**@somtian 推荐**

#Recommender System

本文是清华大学发表于 WWW 18 的工作，论文利用图片增强效果，**传统的方法只考虑 CNN 抽取的图像特征；而本文考虑了图片中的美学特征对于推荐的影响**；作者利用 BDN 从图片中学习美学特征，然后将其融合到 DCF 中，增强用户-产品，产品-时间矩阵，从而提高了推荐效果；在亚马逊和 AVA 数据集上都取得了良好的效果。






论文链接

https://www.paperweekly.site/papers/1975








**Hindsight Experience Replay**

** @ChenjiaBai 推荐**

#Reinforcement Learning

本文来自 OpenAI，**论文提供了解决强化学习 Multi-Goal 问题的思路**，扩展了 Universal Value Function，并提供了实验环境。






论文链接

https://www.paperweekly.site/papers/1992



代码链接

https://github.com/openai/baselines







**#****推 荐 有 礼#**




本期所有入选论文的推荐人

均将获得**PaperWeekly纪念周边一份**









**▲ **机器学习主题行李牌/卡套





**▲ **深度学习主题防水贴纸



**想要赢取以上周边好礼？**

**点击阅读原文**即刻加入社区吧！









**点击以下标题查看往期推荐：**




- 
[来不及想标题了，我要去打包收藏了](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488060&idx=1&sn=8214e778bfc381bf684ab634a34dbf34&chksm=96e9cdbca19e44aac9ffb5bb402861e3295bb3e42b98fe7d964c422995b06875f92333fd85d1&scene=21#wechat_redirect)

- 
[快醒醒，一大波最新 AI 论文加开源代码来袭！](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488566&idx=1&sn=5af3e8b73b2d71003e9af12fb04a43b7&chksm=96e9cbb6a19e42a08722cba30ca9b663d38406f23f882fb123b5802f686391510036ad0a2da0&scene=21#wechat_redirect)


- 
[15 篇最新 AI 论文来袭！NLP、CV...人人有份](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489270&idx=1&sn=4fa88468dba51738df921da45573a927&chksm=96e9c976a19e4060c20453f9cb275966ba25522292b9b638d712963edf208822686486b2cbb7&scene=21#wechat_redirect)

- 
[选对论文，效率提升50% | 本周值得读](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487741&idx=1&sn=b61741b47e602626a236f5984a0b1cb4&chksm=96e9cf7da19e466b953b4f0fb4e0003b868045fd1a4eb1b38a2b6cfe5316c60bcd368f4ee985&scene=21#wechat_redirect)

- 
[本周 AI 论文良心推荐，你想 pick 谁？](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489436&idx=1&sn=111fefd080fd4459d2a80defa94880e3&chksm=96e9c81ca19e410a975df7747ea79fc9cfba4d8fa0910112e48bf66b8f2a8520f5eafe61630f&scene=21#wechat_redirect)






[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488603&idx=2&sn=7320cb23efba3e7b5a381be83b7fe3ad&chksm=96e9cbdba19e42cd5840d3d51e86da4709b3d5273b2cf2512c32d84ab2b42ac4e7f13bf9ba63&scene=21#wechat_redirect)

[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)

**▲**戳我查看招募详情




**#****作 者 招 募#**



****[让你的文字被很多很多人看到，喜欢我们不如加入我们](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)****






**关于PaperWeekly**




PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。









▽ 点击 | 阅读原文| 加入社区刷论文




