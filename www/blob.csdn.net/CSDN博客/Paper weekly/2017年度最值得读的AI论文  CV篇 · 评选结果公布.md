# 2017年度最值得读的AI论文 | CV篇 · 评选结果公布 - Paper weekly - CSDN博客





2018年01月31日 00:00:00[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1155









2017 年，这些计算机视觉论文是你心中的最佳么？




历时九天，我们收到了近千份有效读者投票，2017 年度最值得读的 AI 论文评选也正式结束。




我们根据读者的投票情况，选出了[**自然语言处理**](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487348&idx=1&sn=8ee8bf57418342a419fe73829cb14e75&chksm=96e9d0f4a19e59e288dcb105bd90b1e13f419ee7268ac69eba7cd6dac12e2e64aa84c56e5c07&scene=21#wechat_redirect)**和计算机视觉领域“2017 年最值得读的十大论文”**。让我们一起来看看过去一整年，**在 PaperWeekly 读者心中排名前十的计算机视觉论文都有哪些？**还有给我们留言的读者，在表达对这十篇论文的喜爱之情时都说了些什么？




此外，小编也在所有留言中选出了自己最钟意的五条，还在所有成功参与投票的读者中随机抽取了 13 位，他们都将获得 PaperWeekly 精心准备的新年礼物。




![640?wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfiaYPH1umbEmlcjYjJODWjXibFoJbjRwp3salFsajJyplKFgnWIULfShA/640?wxfrom=5&wx_lazy=1)







![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFicp8z5UIc0gb8hicEQNSRH7nVZp1ahsCBLIme11y0n6f3xl7eMqxc1Ww/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfezFHgLejyooaxDU0SAk9jcKgpYSsOicicF04sDlPsyAT37B6zE1BfXNg/640)




■ 论文 | Mask R-CNN

■ 链接 | https://www.paperweekly.site/papers/672

■ 源码 | https://github.com/CharlesShang/FastMaskRCNN




Mask R-CNN 是 ICCV 2017 的最佳论文。Faster R-CNN 用于目标检测，FCN 用于物体分割，概念基本深入人心。本文提出一个高效实体分割+目标检测+关键点检测框架，各任务之间并行实现，速率 5fps（在单 GPU 运行时间是 200ms/帧，使用 8 GPU 卡，在 COCO 数据集训练只需要 2 天时间），模型简洁，没有靠 trick 提升性能，网络框架主体就是 Faster R-CNN+FCN。




实体分割需要正确检测图片所有的物体并实现像素级分割。在论文之前的实现方式是分割之后做分类，而 Mask-RCNN 的检测和分割是并行出结果。该网络还很容易扩展到其他领域，像目标检测、分割和人物关键点检测等任务。



**扩展阅读：**




- 
Mask R-CNN阅读笔记

[https://www.paperweekly.site/papers/notes/222](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247484790&idx=2&sn=8fd971fad080bf3639cd659f2893308a&chksm=96e9daf6a19e53e0435230cf761eb9cbfafc645b973201980e086d5f23e9bb1dda62b66b1e84&scene=21#wechat_redirect)








![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFqKlYkb6nID08TeFSic2aqaxHqqE5mxUknXZ9QX9dApsF6g4lSLGtubQ/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfiaLTGhXNx1VerL8NjRMQCYnPumcpQptMhL6J5vu0OjDG0DffOJvdzXA/640)




■ 论文 | Image-to-Image Translation with Conditional Adversarial Networks

■ 链接 | https://www.paperweekly.site/papers/1401

■ 源码 | https://github.com/phillipi/pix2pix




将 GAN 的对抗 loss 引入有监督图像转换任务的经典之作。




原有的传统图像转换任务中 L1、L2 等人工设计的损失函数并不能产生令人满意的视觉效果，本文提出的 pix2pix 模型则借助了条件判别网络来充当一个隐式的损失函数，让它在与生成网络对抗的过程中超越人工设计的损失函数，取得良好的视觉效果。




本文提出的 PatchGAN 要求判别网络只对图像的一小块区域进行判别，专注捕捉高频信息，这也成为后续很多图像转换论文的常见做法。







![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFBZIemcweDLShTPibm6GTjZcPYNGwzLDcibYibTYUCB1gibnibkQ0icoymmnQ/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfPhW4McLss9csYRia5ed38D5IicicKcKWyte5nB6R6vf2Nib6x7znibGhmGA/640)




■ 论文 | A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection

■ 链接 | https://www.paperweekly.site/papers/314

■ 源码 | https://github.com/xiaolonw/adversarial-frcnn




遮挡和形变在物体检测中是很难的一类样本，而它们又具有长尾性，即使收集一个很大的数据集也很难涵盖不常见的情况。本文提出用 GAN 来生成遮挡和形变的样本，这是第一篇将 GAN 引入物体检测的文章。




这两类样本的生成都是在特征层面，而不是在图片层面。对于遮挡，作者采用一个 ASDN 网络，它的目标是对 ROI-pooling 的特征生成一个 mask，通过 mask 遮挡掉部分特征，以骗过分类器。




类似的，对于形变，通过 STN 网络在一定范围内生成一组旋转、缩放、平移的参数，再作用到特征上，使得分类器分错。而分类器的目标是尽可能地避免被这两类生成样本欺骗。




ASDN、ASTN 和 Fast-RCNN 可以联合训练，以避免在某些固定的生成模式下过拟合。实验表明，A-Fast-RCNN 在 VOC07 和 12 的数据上都有 2% 以上的 mAP 提升。







![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFwaSENhG4AmvNNKNxicSyRaArLhtIPXh7WjTibIc6UxxYe2Xiap3o5L02w/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfOE4U3uIKkSdIqXm7v7q7WiauW4d8BMFxg39jKicbG7XpbnGtfEmU8Flw/640)




■ 论文 | Bayesian GAN

■ 链接 | https://www.paperweekly.site/papers/1102

■ 源码 | https://github.com/andrewgordonwilson/bayesgan/




本文将贝叶斯公式引入到做无监督和半监督学习的 GAN 模型中，采用哈密顿蒙特卡罗随机梯度算法优化生成器和判别器。作者指出，在不需要 feature matching 和 mini-batch discrimination 等 tricks 的情况下，能够取得不错的分类性能。




此外，Bayesian GAN 还能避免模式坍塌（mode collapse）。文章在 SVHN、CelebA 和 CIFAR-10 等数据集上取得了 state-of-the-art 的半监督分类效果。







![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFwpspllDz4J2d382BQVNoT7PDJib0zSGuJicQ4vyA4AnnbAJXKjgMqLbA/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfLZJUfpUGFdlZasZlyMWG04JZ2QNmzX1KvD7ibWh8RZLDGY2h9BtOAPg/640)




■ 论文 | Interpretable R-CNN

■ 链接 | https://www.paperweekly.site/papers/1215

■ 源码 | 暂无




本文使用 R-CNN 展示了一种学习定性可解释模型的方法。R-CNN 由一个区域建议网络和一个感兴趣区域预测网络（RoI，Region of interest）组成。通过使用可解释的模型，可在检测中（对任何部分都不使用监督的情况下）自动地、同步地学习展开目标实例的隐藏部分结构。




本文还提出了一种 AOG 解析算子来取代 R-CNN 中常用的 RoI 池化算子，因此该方法可以适用于很多基于卷积神经网络的顶尖目标检测系统。




在实验中，作者在 R-FCN 之上创建模型并在 PASCAL VOC 2007、 PASCAL VOC 2012 数据集上进行测试，最终的性能与目前最先进的方法具有可比性。







![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoF52WCTYGKu6Tfzx8Zycr5oCX8fbJ9CCQT5aArb8tmeKx3iamZLSMShfA/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfVBz0J0SYOwA8sicicBRTQ8cfIT0sTHghe0haN3cK4tUJT1KKRAeIr9ibQ/640)




■ 论文 | Learning Feature Pyramids for Human Pose Estimation

■ 链接 | https://www.paperweekly.site/papers/1325

■ 源码 | https://github.com/bearpaw/PyraNet




本文是香港中文科技大学王晓刚教授团队之作，目前在 MPII 官网 Single Person 领域，PCKh @ 0.5 evaluation measure，取得 state-of-the-art 水平。 




论文在 Stacked Hourglass 基础上，提出 Pyramid Residual Module，金字塔残差模块，通过学习 DCNNs 中的特征金字塔来增强深度模型的尺度的不变性，而模型复杂度只有很小的增加。




本文针对具有多个输入或多个输出分支图层的 DCNNs 初始化问题，提出了有效的初始化方案，可用于 inception 和 ResNets 等模型。此外，本文还解决了由 identity mapping 引起的激活方差积累的问题。




**扩展阅读：**




- 
PyraNet阅读笔记

[https://www.paperweekly.site/papers/notes/229](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247484790&idx=2&sn=8fd971fad080bf3639cd659f2893308a&chksm=96e9daf6a19e53e0435230cf761eb9cbfafc645b973201980e086d5f23e9bb1dda62b66b1e84&scene=21#wechat_redirect)








![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFo6hme0lPNgvam9QHNHiaickDMkg3VNuZsZQnLnPwAHpBbR30DHfKRDUw/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfcFteibygZFLEbCGGYq166nD1Kfo9AksQUDkFRPphHjYW15x9U7Yrn2A/640)




■ 论文 | Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks

■ 链接 | https://www.paperweekly.site/papers/807

■ 源码 | https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix




本文可能是 GAN 在 CV 领域最著名的应用案例。通过 Cycle Consistency 的思想，在不需要成对数据的条件下实现了无监督的图像转换。




如果说 pix2pix 的结果还在“嗯这样能 work 倒也可以想象”的范畴之内，那么 CycleGAN 带来的则是令人惊异的飞跃，因为它仅仅通过“保真”和“可逆”这两个间接性的要求，就能够让模型完成合乎人类预期的风格转换。




可能是由于 CycleGAN 的实验效果更为吸睛，其知名度和引用量都远超同时期的另外两个兄弟 DualGAN 和 DiscoGAN。




**扩展阅读：**




- 
CycleGan论文笔记

[https://www.paperweekly.site/papers/notes/229](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247484790&idx=2&sn=8fd971fad080bf3639cd659f2893308a&chksm=96e9daf6a19e53e0435230cf761eb9cbfafc645b973201980e086d5f23e9bb1dda62b66b1e84&scene=21#wechat_redirect)








![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFWrc8LFUY53uYRvh3OnJvmyUmKrXvyZo6uz6KWnlzsApNhPzFEsmVMA/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfQJZicrb2XCwkApVxgEWVChdJ9vrXfrFicKDN7GjEEw2RPDKWmhvJxvFA/640)




■ 论文 | High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs

■ 链接 | https://www.paperweekly.site/papers/1278

■ 源码 | https://github.com/NVIDIA/pix2pixHD




自从 LAPGAN 将“由粗到细、分阶段生成”的做法引入了 GAN 之后，很多论文都在沿着这个方向做，而英伟达这篇论文提出的 pix2pixHD 模型同样沿袭了上述思想，在有监督条件下做到了迄今为止最好的高分辨率（2048 x 1024）视觉效果。




此外，本文还将他们的方法扩展到交互式 semantic manipulation，这对于传统的 rendering photo-realistic images 是一个颠覆性的工作。




**扩展阅读：**




- 
[利用条件GANs的pix2pix进化版：高分辨率图像合成和语义操作](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247486623&idx=1&sn=09c9930244579727dd35341a2f13a7ed&chksm=96e9d31fa19e5a0972eae1190c20659632caa27dfa6575a5684ea30de4a759406a4afcd69bf6&scene=21#wechat_redirect)









![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFU22rwuzxD8WpJWQwxM1e3L4CDoI1JO8GGxzmB4w5axYaicibibJ39KJBg/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfX5ibicbz2kMQU9TmG170qYXbia84NC0A3kQNQZI6r8tUWLZicQbPTtWyyA/640)




■ 论文 | Triple Generative Adversarial Nets

■ 链接 | https://www.paperweekly.site/papers/345

■ 源码 | https://github.com/zhenxuan00/triple-gan




从博弈角度来说，TripleGAN 的博弈涉及三方，判别器、生成器和分类器。其中，判别器和生成器有对抗；判别器和分类器（在训练前期）有对抗；生成器和分类器有协助作用。




可以从斗地主的角度来看，判别器是地主，生成器和分类器是农民。拆掉分类器，它就是一个 CGAN。拆掉生成器，它就是一个半监督的 GAN。 




此外，我们还能从对偶学习的角度进行解读，生成器对 p(x|y) 进行建模，而分类器则对 p(y|x) 建模。两者在判别器的统筹下达成 p(x,y) 的一致性，这是很漂亮的对偶思想。可以说这篇文章对三方博弈的设计非常巧妙。







![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglZA1hx1ZicXygLhZwFpXQoFf3FYdY4MGpricXia1TcYiaauTznIG0WT0iaKSN0oGIGgqryGicOBtbjJ2dg/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgn05MyicvduVY0TwV1dPPibZfibm1hJ83288IPQu8pfLHfWabiap3ias16E2pRRHOrtOHibQOiaA8ddm18mA/640)




■ 论文 | Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields

■ 链接 | https://www.paperweekly.site/papers/784

■ 源码 | https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation




本文发表于 CVPR 2017，首次提出了基于深度学习的实时多人二维姿态估计。本文最大的亮点在于其融合了 PCM 和 PAF 的级联 cascade 形网络结构。




本文算法主要流程如下：输入一幅图像，经过卷积网络提取特征，得到一组特征图，然后分成两个岔路，分别使用 CNN 网络提取 Part Confidence Maps 和 Part Affinity Fields ，得到这两个信息后，再使用图论中的 Bipartite Matching 将同一个人的关节点连接起来得到最终的结果。




**点击查看自然语言处理榜单![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/b96CibCt70iaajvl7fD4ZCicMcjhXMp1v6UibM134tIsO1j5yqHyNhh9arj090oAL7zGhRJRq6cFqFOlDZMleLl4pw/640)：**

[2017年度最值得读的AI论文 | NLP篇 · 评选结果公布](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487348&idx=1&sn=8ee8bf57418342a419fe73829cb14e75&chksm=96e9d0f4a19e59e288dcb105bd90b1e13f419ee7268ac69eba7cd6dac12e2e64aa84c56e5c07&scene=21#wechat_redirect)








![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)






读者福利名单


**** 读者留言精选 ****




**@silver**：投了三篇 paper，但是看见要求说最喜欢的论文的原因，最喜欢的是 IRGAN，上交和伦敦大学的那篇，对于 GAN 的应用让我耳目一新，尤其是在推荐系统上的应用，是我以前从未想过的，为了深入理解这篇 paper，还读了他们之前的工作，在 AAAI2017 上的 SeqGAN，一个完整连贯的工作体系，对于离散型数据在 GAN 的应用，以及优化设计上提供了思路，对于强化学习的应用也是让我获益匪浅，也在这个工作领域给我带来了很多新思路，非常感谢，也非常喜欢。




**@朱昊**：Selective Attention 那篇现在已经成为了做 Distant Supervision 的 Relation Extraction 必须比较的 baseline。把 attention 的方法用在 bag level 上对标注打分是一种非常有意思的思想。




**@Bruce Pan**：Convolutional Sequence to Sequence Learning 把 CNN 运用到 seq2seq 任务中，利用了 CNN 并行计算的优势，还有详细介绍了各种 Trick。 顺便也为 Attention Is All You Need 打 call，两篇我都很喜欢。




**@empty**：在噪声数据上利用强化学习进行关系分类，对这篇印象深刻，当时听了这个 talk 受益匪浅，作者把强化学习用到关系抽取上。还有 KBQA 那篇，对这个领域进行了一些介绍。在这里立个 flag，2018 年要把候选 paper 都看一遍![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/b96CibCt70iaajvl7fD4ZCicMcjhXMp1v6UOibKXzkxofLKcb20fz8gpuYeNFdVdRLk4r2JmOzSdOKdKjIHRZO3OkA/640)




**@Shunzhou Wang**：Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields 效果很赞，估姿态很准，从 CPM 到 Real time 2D 再到 openpose，一系列的工作都喜欢，喜欢笔记本和行李牌，求抽中![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/b96CibCt70iaajvl7fD4ZCicMcjhXMp1v6UmiabnGdKYsNFmq4Ks2WLbiaiajUhjfeiaQO1iajIvjhlcaiaEXjVb9z1V9pw/640)![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/b96CibCt70iaajvl7fD4ZCicMcjhXMp1v6UmiabnGdKYsNFmq4Ks2WLbiaiajUhjfeiaQO1iajIvjhlcaiaEXjVb9z1V9pw/640)




**![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)**



**** 13位幸运读者 ****




杨海宏

JunLee

蒋锐滢

猪宝

Liam

徐大帅

阿敏

XuanH

匿名用户

武永亮

lhf

dreamlike

robert




* 请以上 18 位读者

在以下福利清单中任选一种




并在本条微信留言回复

** “ 姓名 + 手机 + 地址 + A / B / C”**

（如选择手机壳，请注明手机型号）




**** 福利清单 ****




**A：**PaperWeekly定制手机壳 x 3份 

**B：**PaperWeekly定制笔记本 x 5份 

**C：**PaperWeekly定制行李牌 x 10份




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgnED8CgalBpSNcrqdibvRwqESxVO3lbKOIEIBdyzBIykjB2Vq91YlFTibBUylIC8ItotiadPyluHEn0w/640)

△ 我们长这样哦~



**** 领奖方式 ****




**请所有中奖者在本文底部留言**

**礼物数量有限，先到先选**




**回复截止时间**

? 2018年2月3日10:00









******关于PaperWeekly******




PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgl9qrwuXS7D8F2ZLyZNmqfWibCVlSbGBVCrd80blia0iaiaKuVk5p1tWP8tCaIiaYxiaQwiacIOlu9yOw6Mg/640?)

▽ 点击 | 阅读原文| 加入社区




