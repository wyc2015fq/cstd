
# Call for Papers | IJCNN 2019 Special Section å¾ç¨¿é€šé“å¼€å¯ - Paper weekly - CSDNåšå®¢


2018å¹´12æœˆ07æ—¥ 14:05:26[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)é˜…è¯»æ•°ï¼š497


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgmR30auxdkOGrcaSx93zxKhs71W6SyYlJiaqiawZzTfjOho32mZACICEqTBSesM1MXKoP8tAI9J7MBw/640)


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkPG6PI3D61CBCFmZsx2gEetfY8vQ2TRYmia2xRMaW0jUHL0ibH2O2FzxAvOD7iavZu4xPqzpqYMKvRA/640)

**Special Section: Transferable neural models for language understanding**

Language understanding, dealing with machine reading comprehension in various forms such as question answering, machine translation and language dialog, has been an aspiration of the artificial intelligence community, but has limited success until recently. Due to the success of deep neural networks, there is a resurgence of interest in research on deep neural networks applied to language understanding. The most recent research in language understanding aims to build deep neural network models that can be used for various language understanding tasks, such as paraphrasing, question answering, machine translation, spoken dialog, and text categorization. However, these models are (1) very data hungry â€“ requiring large training data; (2) very task specific â€“ hard to generalize the model for one task to other related tasks. To solve these problems, recently, transfer learning has been applied to language understanding. Transfer learning is a learning paradigm that aims to apply knowledge gained while solving one problem to a different but related problem. Transfer learning builds a neural model for one language understanding task with large training data, and then the model is retrained for another task with small training data.

IJCNN 2019 ä¼šè®®è®ºæ–‡æˆªæ­¢æ—¥æœŸä¸º**2018 å¹´ 12 æœˆ 15 æ—¥**ï¼Œå½•ç”¨é€šçŸ¥æ—¥æœŸä¸º**2019 å¹´ 1 æœˆ 30 æ—¥**ã€‚

**å¾ç¨¿ä¸»é¢˜åŒ…æ‹¬ä½†ä¸é™äºï¼š**è‡ªç„¶è¯­è¨€ç†è§£ï¼Œæ¨ç†å’Œç”Ÿæˆï¼Œæ·±åº¦å­¦ä¹ ï¼Œè¿ç§»å­¦ä¹ ï¼Œä¸»åŠ¨å­¦ä¹ ï¼Œè‡ªæˆ‘å­¦ä¹ ï¼Œé¢†åŸŸé€‚åº”å­¦ä¹ ï¼Œåºåˆ—å¯¹åºåˆ—å­¦ä¹ ï¼Œæœºå™¨ç¿»è¯‘ï¼Œå¤è¿°ï¼Œé—®ç­”ç³»ç»Ÿï¼Œä¿¡æ¯æŠ½å–ç­‰ã€‚

**åœ¨çº¿æŠ•ç¨¿ï¼š**

https://ieee-cis.org/conferences/ijcnn2019/upload.php

è¯·é€‰æ‹© Main research topic ä¸º S33 â€” Transferable neural models for language understandingã€‚

è‹¥æƒ³è¿›ä¸€æ­¥äº†è§£ï¼Œè¯·è”ç³» Dr. Zhiwei Linï¼ˆz.lin@ulster.ac.ukï¼‰ã€‚
![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkPG6PI3D61CBCFmZsx2gEesOmFo38yCd8XvOOm9LcLpA9qQA9XYP3K4A4tOoSYuoUMUUooZ377rQ/640)


ğŸ”

ç°åœ¨ï¼Œåœ¨**ã€ŒçŸ¥ä¹ã€**ä¹Ÿèƒ½æ‰¾åˆ°æˆ‘ä»¬äº†
è¿›å…¥çŸ¥ä¹é¦–é¡µæœç´¢**ã€ŒPaperWeeklyã€**
ç‚¹å‡»**ã€Œå…³æ³¨ã€**è®¢é˜…æˆ‘ä»¬çš„ä¸“æ å§


**å…³äºPaperWeekly**

PaperWeekly æ˜¯ä¸€ä¸ªæ¨èã€è§£è¯»ã€è®¨è®ºã€æŠ¥é“äººå·¥æ™ºèƒ½å‰æ²¿è®ºæ–‡æˆæœçš„å­¦æœ¯å¹³å°ã€‚å¦‚æœä½ ç ”ç©¶æˆ–ä»äº‹ AI é¢†åŸŸï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·åå°ç‚¹å‡»**ã€Œäº¤æµç¾¤ã€**ï¼Œå°åŠ©æ‰‹å°†æŠŠä½ å¸¦å…¥ PaperWeekly çš„äº¤æµç¾¤é‡Œã€‚

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgmR30auxdkOGrcaSx93zxKhvGFeOM5zzBoJCz0RMyliatXia8kbU7qLhYQamCibVHDWpzfejNqGXIzaQ/640)
â–½ ç‚¹å‡» |é˜…è¯»åŸæ–‡| åœ¨çº¿æŠ•ç¨¿


