
# 基于深度迁移学习进行时间序列分类 - Paper weekly - CSDN博客


2018年11月15日 12:20:50[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：332


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgnC9iaic8hDbiadLafh7TtCZS6icEYddVmMqZBksDV7cQkKmAu95h53FxyibqmZOS1yQgHibJT0WYD2s1Zw/640)
![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgl7VHx00TkzicBMAfz1dFT8icD4HwmJZpt0Jiccw6ns7c3co7MpZslIia8VAuZicUTSuoPaq6hE4KbxWPg/640?)

在碎片化阅读充斥眼球的时代，越来越少的人会去关注每篇论文背后的探索和思考。

在这个栏目里，你会快速 get 每篇精选论文的亮点和痛点，时刻紧跟 AI 前沿成果。

点击本文底部的「**阅读原文**」即刻加入社区，查看更多最新论文推荐。
这是 PaperDaily 的第**117**篇文章
作者丨王晋东
学校丨中国科学院计算技术研究所博士生
研究方向丨迁移学习和机器学习

本文是法国上阿尔萨斯大学发表于 IEEE Big Data 2018 上的工作。迁移学习和深度学习已经被广泛应用于计算机视觉和自然语言处理领域。但是在时间序列分类方面，至今没有完整的有代表性的工作。

**本文是第一篇系统探讨基于深度迁移学习进行时间序列分类的论文。**在内容上与今年 CVPR 最佳论文*Taskonomy: Disentangling Task Transfer Learning*[1]相似，都是做了大量实验来验证一些迁移学习方面的结论。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglJoFWOh3I8G966JxYRrnQjJQiazEfDUnuNRicSdq7YQkdWn6P92QH3aooM8EW5ItLRfL4frjmibQnibA/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglJoFWOh3I8G966JxYRrnQjHQPggvjhdCxy999P8hm1NvX9gtNUAKj6JEugfUPeGw5cF8fOMQicPlA/640)

# 论文动机

通常来说，用传统的机器学习方法（例如 KNN、DTW）进行时间序列分类能取得比较好的效果。但是，**基于深度网络的时间序列分类往往在大数据集上能够打败传统方法。**另一方面，深度网络必须依赖于大量的训练数据，否则精度也无法超过传统机器学习方法。在这种情况下，进行数据增强、收集更多的数据、实用集成学习模型，都是提高精度的方法。这其中，迁移学习也可以被用在数据标注不足的情况。

从深度网络本身来看，有研究者注意到了，针对时间序列数据，深度网络提取到的特征，与 CNN 一样，具有相似性和继承性。因此，作者的假设就是，**这些特征不只是针对某一数据集具有特异性，也可以被用在别的相关数据集。**这就保证了用深度网络进行时间序列迁移学习的有效性。

# 论文方法

**本文基本方法与在图像上进行深度迁移一致：**先在一个源领域上进行 pre-train，然后在目标领域上进行 fine-tune。

然而，与图像领域有较多的经典网络结构可选择不同，时间序列并没有一个公认的经典网络架构。因此，作者为了保证迁移的效果不会太差，选择了之前研究者提出的一种**全卷积网络**（FCN，Fully Convolutional Neural Network）。这种网络已经在之前的研究中被证明具有较高的准确性和鲁棒性。

网络的结构如下图所示。网络由 3 个卷积层、1 个全局池化层、和 1 个全连接层构成。使用全连接层的好处是，在进行不同输入长度序列的 fine-tune 时，不需要再额外设计池内化层。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglJoFWOh3I8G966JxYRrnQjEv9wsKjZj5MpsndAPpvEynMAGIwWPjibw4B5Aw3qjH2twmdrUGnx80g/640)

**与图像的区别就是，输入由图片换成了时间序列。**注意到，图片往往具有一定的通道数（如常见的 R、G、B 三通道）；时间序列也有通道：即不同维的时间序列数据。最简单的即是1维序列，可以认为是 1 个通道。多维时间序列则可以认为是多个通道。

**网络迁移适配**

Fine-tune 的基本方法就是，不改变除 softmax 层以外的层的结构，只改变 softmax 层的构造。例如，预训练好的网络可能是一个分 5 类的网络，而目标领域则是一个 10 类的分类问题。这时候，就需要改变预训练网络的 softmax 层，使之由原来的 5 层变为 10 层，以适应目标领域的分类。

因此，源领域和目标领域的网络相比，除最后一层外，其他都相同。当然，相同的部分，网络权重也相同。

**作者对整个网络都在目标领域上进行了fine-tune，而不是只fine-tune最后一层。**因为以往的研究标明，在整个网络上进行 fine-tune，往往会比只 fine-tune 某些层效果好。

**选择合适的源领域：数据集间相似性**

在进行迁移学习前，一个重要的问题就是：**给定一个目标域，如何选择合适的源领域？**如果选择的源域与目标域相似性过小，则很可能造成负迁移。

度量时间序列相似性的另一个问题是，如何度量不同维度的时间序列的相似性。作者提出把多维时间序列规约成每类由一维序列构成，然后利用 DTW（Dynamic Time Warping）来度量两个时间序列的相似性。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglJoFWOh3I8G966JxYRrnQjVU7piauHuAcLzfYMpnkHnmoonFRhpqyoUekkXS9rQwaXaiaxyganoHqg/640)

在进行规约时，作者利用了之间研究者提出的*DTW Barycenter Averaging (DBA)*[2]方法进行了时间序列的规约。经过规约后，两个数据集便可度量相似性。

然而，这种方法具有很大的局限性。例如，它没有考虑到数据集内部不同维度之间的关联性。作者自己也承认这种方法不够好，但是由于他们的主要关注点是如何迁移，因此，并未在这个方面多做文章。

经过相似度计算，可以针对 n 个数据集，得到一个 n×n 的相似性矩阵。此矩阵表示了不同数据集之间的相似度。相似度高的两个数据集，迁移效果最好。

# 实验

作者利用了 UCI 机器学习仓库中的**85 个**时间序列分类数据集，构建了**7140 对**迁移学习任务。为了进行如此大量的实验，他们用了来自英伟达的**60 个**GPU（只想说，有钱真好）。

**实验非常充分，这里简要说一下部分结论：**

1. 利用迁移往往效果比不迁移好，并且，几乎不会对原来的网络产生负面作用；

2. 同一个目标域，不同的源域，产生的迁移效果千差万别：总能找到一些领域，迁移效果比不迁移好；

3. 在选择正确的源域上，有时，随机选择的效果不一定会比经过作者的方法计算出来的要差。这说明，计算领域相似性的方法还有待加强。

# 总结

本文利用大量的时间序列进行了深度迁移学习分类的实验。用众多的实验结果证明了迁移学习对于时间序列分类的有效性。作者还提出了一种简单比较时间序列相似性从而选择源领域的方法。

作者也非常慷慨地开源了他们的实验代码：

https://github.com/hfawaz/bigdata18

# 参考文献

[1] A. Zamir, S. Sax*, W. Shen*, L. Guibas, J. Malik, S. Savarese. Taskonomy: Disentangling Task Transfer Learning. 2018 IEEE Conference on Computer Vision and Pattern Recognition.
[2] F. Petitjean and P. Ganc¸arski, “Summarizing a set of time series by averaging: From steiner sequence to compact multiple alignment,” Theoretical Computer Science, vol. 414, no. 1, pp. 76 – 91, 2012.
**本文由 AI 学术社区 PaperWeekly 精选推荐，社区目前已覆盖自然语言处理、计算机视觉、人工智能、机器学习、数据挖掘和信息检索等研究方向，点击「****阅读原文****」即刻加入社区！**
![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)

**点击标题查看往期内容推荐：**

[自动机器学习（AutoML）最新综述](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492654&idx=1&sn=b9047d5cca7657f02dc7f6685ef04037&chksm=96ea3baea19db2b8dc1c1267801d0c585b3cf072531af86abdeb73c6fb4c07dc3325c2d13d57&scene=21#wechat_redirect)
[自然语言处理中的语言模型预训练方法](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492317&idx=1&sn=e823a75d9463257ed9ea7b3e4677c1ae&chksm=96ea3d5da19db44be0872ff4e29043aa72c7a624a116196bfeeca092a15f9209d7cf8ce46eb5&scene=21#wechat_redirect)
[从傅里叶分析角度解读深度学习的泛化能力](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491082&idx=1&sn=d7c1cb39c3be43154c658ca5a791eb4c&chksm=96e9c18aa19e489c32fe36671e4208ce42bf200e3a7adeda200fa2785462d16f85c58bb455b4&scene=21#wechat_redirect)
[深度](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492065&idx=1&sn=a91d7ae724eed652ca87f647910bf666&chksm=96ea3e61a19db777c615d64abf564110ee4d1c32ecca2a629a7c0158a98bf6ccb2ec0fc05814&scene=21#wechat_redirect)[解读DeepMind新作：史上最强GAN图像生成器](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492065&idx=1&sn=a91d7ae724eed652ca87f647910bf666&chksm=96ea3e61a19db777c615d64abf564110ee4d1c32ecca2a629a7c0158a98bf6ccb2ec0fc05814&scene=21#wechat_redirect)
[论文复现 | ICML 2017大热门：Wasserstein GAN](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492542&idx=1&sn=c9fde79aeed10c8848e93872f229fdd0&chksm=96ea3c3ea19db528773e7489ff921609ff5bac380dddcc45c18d0441784c8bd6f987bde6b7c0&scene=21#wechat_redirect)
[收下这 16 篇最新论文，周会基本不用愁](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492583&idx=1&sn=85ec5352079218745428d66ab8ee97d4&chksm=96ea3c67a19db5718f5412c64f4c11d28cab5eda2826350fd5f15ac3e888f6ae7a9137eb31bd&scene=21#wechat_redirect)
TensorSpace：超酷炫3D神经网络可视化框架




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)**\#****投 稿 通 道****\#**
**让你的论文被更多人看到**

如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？**答案就是：你不认识的人。**

总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。

PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是**最新论文解读**，也可以是**学习心得**或**技术干货**。我们的目的只有一个，让知识真正流动起来。

📝**来稿标准：**
• 稿件确系个人**原创作品**，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向）
• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接
• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志

**📬 投稿邮箱：**
• 投稿邮箱：hr@paperweekly.site
• 所有文章配图，请单独在附件中发送
• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通



🔍

现在，在**「知乎」**也能找到我们了
进入知乎首页搜索**「PaperWeekly」**
点击**「关注」**订阅我们的专栏吧


**关于PaperWeekly**

PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgl9qrwuXS7D8F2ZLyZNmqfWibCVlSbGBVCrd80blia0iaiaKuVk5p1tWP8tCaIiaYxiaQwiacIOlu9yOw6Mg/640?)
▽ 点击 |阅读原文| 下载论文 & 源码


