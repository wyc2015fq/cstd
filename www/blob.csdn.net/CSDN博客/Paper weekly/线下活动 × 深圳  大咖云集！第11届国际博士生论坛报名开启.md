# çº¿ä¸‹æ´»åŠ¨ Ã— æ·±åœ³ | å¤§å’–äº‘é›†ï¼ç¬¬11å±Šå›½é™…åšå£«ç”Ÿè®ºå›æŠ¥åå¼€å¯ - Paper weekly - CSDNåšå®¢





2018å¹´12æœˆ03æ—¥ 13:19:53[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)é˜…è¯»æ•°ï¼š201









![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgnf0hRxEnhz41HGSxAIBQpCrszXP71KUJaSMbAR6kujdp3yibvnDp2J8XlLkWibK7O1swQ8jxIUEaLw/640)



**å›½é™…åšå£«ç”Ÿè®ºå›ï¼ˆInternational Doctoral Forumï¼‰æ˜¯ç”±æ¸…åå¤§å­¦å’Œé¦™æ¸¯ä¸­æ–‡å¤§å­¦äº 2006 å¹´è”åˆå‘èµ·çš„ä¸€é¡¹å­¦æœ¯äº¤æµæ´»åŠ¨**ï¼Œè‡³ä»Šå·²æœ‰ 12 å¹´çš„å†å²ã€‚2014 å¹´èµ·è¥¿åŒ—å·¥ä¸šå¤§å­¦å‚ä¸æ‰¿åŠã€‚è®ºå›ç”±æ¸…åå¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€è¥¿åŒ—å·¥ä¸šå¤§å­¦è½®æµæ‰¿åŠï¼Œæ—¨åœ¨æ¨è¿›åŒ—äº¬ã€é¦™æ¸¯ã€æ·±åœ³ã€è¥¿å®‰å„é«˜æ ¡ç›¸å…³é¢†åŸŸçš„è€å¸ˆå’Œå­¦ç”Ÿçš„äº¤æµä¸åˆä½œã€‚Â 




è®ºå›ä¸»é¢˜æ¶µç›–å¤šåª’ä½“ï¼ˆMultimediaï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼‰ã€äº’è”ç½‘æ•°æ®æŒ–æ˜ï¼ˆWeb Miningï¼‰ã€ç½‘ç»œåŠå¤§æ•°æ®ï¼ˆNetworking and Big Dataï¼‰ã€äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼‰ç­‰å¤šä¸ªé¢†åŸŸï¼Œå¹¶é‚€è¯·æ¥è‡ªç›¸å…³é¢†åŸŸå­¦æœ¯ç•Œã€äº§ä¸šç•Œçš„å­¦è€…ã€ä¸“å®¶äº²ä¸´æŒ‡å¯¼ã€ä½œç‰¹é‚€æŠ¥å‘Šç­‰ã€‚è®ºå›æœŸé—´è¿˜ä¼šå¬å¼€åœ†æ¡Œä¼šè®®äº¤æµã€å­¦æœ¯æ€æƒ³ç§€ç­‰ä¸“é¢˜æ´»åŠ¨ï¼Œä¿ƒè¿›ä¸ä¼šçš„è€å¸ˆå’ŒåŒå­¦ä»¬çš„äº¤æµå’Œåˆä½œã€‚Â 




åšå£«ç”Ÿè®ºå›ä¸ºåŒå­¦ä»¬æ­å»ºäº†å­¦ä¹ äº¤æµçš„å¹³å°ï¼Œè˜è¯·æ¸…åå¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€è¥¿åŒ—å·¥ä¸šå¤§å­¦ç›¸å…³ä¸“ä¸šé¢†åŸŸçš„é¢†å¯¼ã€è€å¸ˆæ‹…ä»»æŒ‡å¯¼å§”å‘˜ä¼šï¼Œç”±åŒå­¦ä»¬äº²è‡ªç­–åˆ’å’Œç»„ç»‡ï¼ŒåŒ…æ‹¬è®®é¢˜ç¡®å®šã€è®ºæ–‡æŠ•ç¨¿å’Œå®¡ç¨¿ã€å¤§ä¼šæŠ¥å‘Šé‚€è¯·ã€è®ºå›æ—¥ç¨‹å®‰æ’ã€ä¼˜ç§€è®ºæ–‡è¯„å®¡ã€æœ¬åœ°ç»„ç»‡ç­‰å„ä¸ªæ–¹é¢ï¼Œæœ‰æ•ˆé”»ç‚¼äº†ä¸ä¼šåŒå­¦ä»¬å„æ–¹é¢çš„èƒ½åŠ›ã€‚å€ŸåŠ©è®ºå›æ­å»ºçš„å¹³å°ï¼Œä¸å°‘å‚åŠ å¾€å±Šåšå£«ç”Ÿè®ºå›çš„åŒå­¦ä¸šå·²æˆé•¿ä¸ºåœ¨ç›¸å…³é¢†åŸŸæœ‰è¾ƒå¤§å½±å“åŠ›çš„ä¼˜ç§€é’å¹´å­¦è€…ã€‚Â 




è®ºå›è‡ª 2006 å¹´é¦–æ¬¡ä¸¾åŠä»¥æ¥ï¼Œä»Šå¹´å·²æ˜¯ç¬¬åä¸€å±Šã€‚å†å±Šè®ºå›çš„ä¸¾åŠåœ°åˆ†åˆ«ä¸ºåŒ—äº¬ï¼ˆ2006 å¹´ã€2008 å¹´ã€2010 å¹´ã€2015 å¹´ï¼‰ï¼Œé¦™æ¸¯ï¼ˆ2007 å¹´ã€2009 å¹´ã€2016 å¹´ï¼‰ï¼Œæ·±åœ³é¦™æ¸¯è”åˆï¼ˆ2011 å¹´ï¼‰ï¼Œè¥¿å®‰ï¼ˆ2014 å¹´ã€2017 å¹´ï¼‰ã€‚å†å±Šè®ºå›è·å¾—äº†æ¥è‡ªå„é«˜æ ¡è€å¸ˆå’ŒåŒå­¦ä»¬çš„ç§¯ææ”¯æŒå’Œå‚ä¸ï¼Œä¹Ÿå–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚ä»¥è®ºå›ä¸ºå¥‘æœºï¼Œç›¸å…³é¢†åŸŸçš„è€å¸ˆå’ŒåŒå­¦ä»¬æ·±å…¥äº¤æµå’Œç›¸äº’è®¨è®ºå­¦ä¹ ï¼Œå¹¶è¾¾æˆäº†è¯¸å¤šåˆä½œï¼Œå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚è®ºå›çš„å½±å“åŠ›ä¹Ÿåœ¨ä¸æ–­æ‰©å¤§ï¼Œè·å¾—äº†æ¥è‡ªä¸­å›½å¤§é™†ã€é¦™æ¸¯ã€å°æ¹¾ã€æ¾³é—¨ã€ä¹ƒè‡³æµ·å¤–è¯¸å¤šé™¢æ ¡çš„æ”¯æŒä¸å‚ä¸ã€‚Â 




**ä»Šå¹´çš„å›½é™…åšå£«ç”Ÿè®ºå›äº 12 æœˆ 6 æ—¥-7 æ—¥åœ¨æ·±åœ³ä¸¾è¡Œï¼Œç”±æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢å…·ä½“æ‰¿åŠï¼Œå°†ä¸¾è¡Œä¸ºæœŸ 2 å¤©çš„å­¦æœ¯ç ”è®¨å’Œäº¤æµæ´»åŠ¨ã€‚**è®ºå›ä¸»é¢˜ä¸ºï¼šMultimediaã€Intelligent Speech Interactionã€Web Miningã€Networking and Big Dataã€‚è®ºå›é¦–æ—¥åŒ…æ‹¬å¼€å¹•å¼ï¼ˆOpening Ceremonyï¼‰ã€å¹¶å®‰æ’ 4 ä¸ªç‰¹é‚€æŠ¥å‘Šï¼ˆInvited Talkï¼‰ã€ç§‘æŠ€å›­é«˜ç§‘æŠ€ä¼ä¸šå‚è§‚ï¼ˆUBTech and Tencent Binhai Mansionï¼‰ï¼›è®ºå›ç¬¬äºŒå¤©å®‰æ’ 3 ä¸ªç‰¹é‚€æŠ¥å‘Šï¼ˆInvited talkï¼‰ã€1 ä¸ªç‰¹æ®Šè®®é¢˜ä¸“é¢˜æŠ¥å‘Šï¼ˆSpecial Session: Dialogue with AI Companiesï¼‰ã€10 ç»„å£å¤´åˆ†è®ºå›æŠ¥å‘Šï¼ˆOral Sessionsï¼‰ã€æ™šå®´åŠé¢å¥–ä»ªå¼ï¼ˆBanquet and Best Paper Award Ceremonyï¼‰ç­‰ã€‚Â 




**è®ºå›ç‰¹é‚€æŠ¥å‘Šè®²è€…åŒ…æ‹¬ï¼š**ä½æ²»äºšç†å·¥æé”¦è¾‰æ•™æˆï¼Œé¦™æ¸¯ä¸­æ–‡å¤§å­¦é‚¢å›½è‰¯ã€å‘¨åšç£Šæ•™æˆï¼Œè¥¿åŒ—å·¥ä¸šå¤§å­¦è°¢ç£Šæ•™æˆï¼Œæ¸…åå¤§å­¦åˆ˜çŸ¥è¿œã€è´¾çˆã€è¢æ˜¥æ•™æˆã€‚ç‰¹æ®Šè®®é¢˜ä¸“é¢˜æŠ¥å‘Šå®‰æ’äººå·¥æ™ºèƒ½ç›¸å…³ä¼ä¸šä»‹ç»æœ€æ–°çš„ç ”ç©¶æˆæœï¼Œå¹¶å’Œå¤§å®¶è¿›è¡Œäº¤æµï¼Œè®²è€…åŒ…æ‹¬å¥½æœªæ¥äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼ˆTAL AI Labï¼‰æ¨åµ©ã€æ·±åœ³å£¹ç§˜ç§‘æŠ€é™ˆæ–‡æ˜ã€æ·±åœ³å£°å¸Œç§‘æŠ€åˆ˜é¹é£ã€‚Â 




å‚åŠ è®ºå›çš„å­¦ç”Ÿæ€»äººæ•°ä¸º 60-70 äººï¼Œå‚åŠ è®ºå›çš„è€å¸ˆäººæ•°ä¸º 10 äººå·¦å³ã€‚




æ¬¢è¿å¤§å®¶æŠ¥åå‚åŠ è®ºå›å¼€å¹•å¼ã€ç‰¹é‚€æŠ¥å‘Šä»¥åŠç‰¹æ®Šè®®é¢˜ç¯èŠ‚ã€‚Â 




**è®ºå›å¼€å¹•å¼ï¼š**12 æœˆ 6 æ—¥ï¼Œ9:00-9:45Â 

**ç¬¬ä¸€ç»„ç‰¹é‚€æŠ¥å‘Šï¼š**12 æœˆ 6 æ—¥ï¼Œ9:45-12:15ï¼ˆåŒ…æ‹¬ç‰¹é‚€æŠ¥å‘Š 1-4ï¼‰Â 

**ç¬¬äºŒç»„ç‰¹é‚€æŠ¥å‘Šï¼š**12 æœˆ 7 æ—¥ï¼Œ8:30-10:00ï¼ˆåŒ…æ‹¬ç‰¹é‚€æŠ¥å‘Š 5-7ï¼‰Â 

**Dialogue with AI Companiesç‰¹æ®Šè®®é¢˜æŠ¥å‘Šï¼š**12 æœˆ 7 æ—¥ï¼Œ10:30-12:00ï¼ˆåŒ…æ‹¬ç‰¹æ®Šè®®é¢˜æŠ¥å‘Š 1-3ï¼‰



01**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 6 æ—¥ï¼Œ9:45-10:45**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEdwIKkianyplkiajuJXO0CdcGkaugJbQe3w49zOEBwOI9x1ibQNxxIpnlg/640)



**Â ä½æ²»äºšç†å·¥æé”¦è¾‰æ•™æˆÂ **




Chin-Hui Lee is a professor at School of Electrical and Computer Engineering, Georgia Institute of Technology. Before joining academia in 2001, he had accumulated 20 years of industrial experience ending in Bell Laboratories, Murray Hill, as a Distinguished Member of Technical Staff and Director of the Dialogue Systems Research Department. Dr. Lee is a Fellow of the IEEE and a Fellow of ISCA. He has published over 500 papers and 30 patents, with more than 42,000 citations and an h-index of 80 on Google Scholar. He received numerous awards, including the Bell Labs President's Gold Award in 1998. He won the SPS's 2006 Technical Achievement Award for â€œExceptional Contributions to the Field of Automatic Speech Recognitionâ€. In 2012 he gave an ICASSP plenary talk on the future of automatic speech recognition. In the same year he was awarded the ISCA Medal in scientific achievement for â€œpioneering and seminal contributions to the principles and practice of automatic speech and speaker recognitionâ€.




** Â æŠ¥å‘Šé¢˜ç›®Â ****Â Knowledge-rich Speech Processing: Beyond Current Deep Learning**




Deep neural networks (DNNs) are becoming ubiquitous in designing speech processing algorithms. However, the robustness issues that have hindered a wide-spread deployment of speech technologies for decades still have not been fully resolved. In this talk, we first discuss capabilities and limitations of deep learning technologies. Next, we illustrate three knowledge-rich techniques, namely: (1) automatic speech attribute transcription (ASAT) integrating acoustic phonetic knowledge into speech processing and computer assisted pronunciation training (CAPT), (2) Bayesian DNNs leveraging upon speaker information for adaptation and system combination, and (3) DNN-based speech pre-processing, demonstrating better acoustics leads to more accurate speech recognition. Finally, we argue that domain knowledge in speech, language and acoustics is heavily needed beyond current blackbox deep learning in order to formulate sustainable whitebox solutions to further advance speech technologies.






02**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 6 æ—¥ï¼Œ10:45-11:15**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEKA0j3v0U4Lvs9EXSHv7WAaBvleL4bjzCWibx85MsLHloTTnzuX5ibZkw/640)



**Â æ¸…åå¤§å­¦åˆ˜çŸ¥è¿œæ•™æˆÂ **




Zhiyuan Liu is an associate professor at the Department of Computer Science and Technology, Tsinghua University. He received his Ph.D. degree in Computer Science from Tsinghua in 2011. His research interests include representation learning, knowledge graphs and social computation, and has published more than 60 papers in top-tier conferences and journals of AI and NLP including ACL, IJCAI and AAAI, cited by more than 3500 according to Google Scholar.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Knowledge-Guided Natural Language ProcessingÂ **




Recent years have witnessed the advances of deep learning techniques in various areas of NLP. However, as a typical data-driven approach, deep learning suffers from the issue of poor interpretability. A potential solution is to incorporate large-scale symbol-based knowledge graphs into deep learning. In this talk, I will present recent works on knowledge-guided deep learning methods for NLP.






03**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 6 æ—¥ï¼Œ11:15-11:45**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvErxib8JXBRXI2yLRDtSge6oliaIlrjEp5T8M8F1pYI5ccJqASsJU58g7g/640)


**Â é¦™æ¸¯ä¸­æ–‡å¤§å­¦é‚¢å›½è‰¯æ•™æˆÂ **




Guoliang Xing is currently a Professor in the Department of Information Engineering, the Chinese University of Hong Kong. Previously, he was a faculty member at Michigan State University, U.S. His research interests include Embedded AI, Edge/Fog Computing, Cyber-Physical Systems, Internet of Things (IoT), security, and wireless networking. He received the B.S. and M.S degrees from Xiâ€™an Jiao Tong University, China, in 1998 and 2001, the D.Sc. degree from Washington University in St. Louis, in 2006. He is an NSF CAREER Award recipient in 2010. He received two Best Paper Awards and five Best Paper Nominations at several first-tier conferences including ICNP and IPSN. Several mobile health technologies developed in his lab won Best App Awards at the MobiCom conference and were successfully transferred to the industry. He received the Withrow Distinguished Faculty Award from Michigan State University in 2014. He serves as the General Chair for IPSN 2016 and TPC Co-Chair for IPSN 2017.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Edge AI for Data-Intensive Internet of ThingsÂ **




Internet of Things (IoT) represent a broad class of systems which interact with the physical world by tightly integrating sensing, communication, and compute with physical objects. Many IoT applications are data-intensive and mission-critical in nature, which generate significant amount of data that must be processed within stringent time constraints. Itâ€™s estimated that 0.75 GB of data can be produced by an autonomous vehicle each second. The existing Cloud computing paradigm is inadequate for such applications due to significant or unpredictable delay and concerns on data privacy.Â 




In this talk, I will present our recent work on Edge AI, which aims to address the challenges of data-intensive IoT by intelligently distributing compute, storage, control and networking along the continuum from Cloud to Things. First, I will present ORBIT, a system for programming Edge systems and partitioning compute tasks among network tiers to minimize the system power consumption while meeting application deadlines. ORBIT has been employed in several systems for seismic sensing, vision-based tracking, and multi-camera 3D reconstruction. Second, I will briefly describe several systems we developed for mobile health, smart cities, volcano and aquatic monitoring, which integrate domain-specific physical models with AI algorithms. We have conducted several large-scale field deployments for these systems, including installing a seismic sensor network at two live volcanoes in Ecuador and Chile.



04**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 6 æ—¥ï¼Œ11:45-12:15**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEgoK5TnYvibI75yVYXK4e7riaNkwBpUmvUYtZysnQ35IOBA4Xbic3LSVAg/640)



**Â æ¸…åå¤§å­¦è´¾çˆæ•™æˆÂ **




Jia Jia is a tenured associate professor in Department of Computer Science and Technology, Tsinghua University. Her main research interest is affective computing and human computer speech interaction. She has been awarded ACM Multimedia Grand Challenge Prize (2012), Scientific Progress Prizes from the National Ministry of Education as the First Person-in-charge (2016), IJCAI Early Career Spotlight (2018), ACM Multimedia Best Demo Award (2018) and ACM SIGMM Emerging Leaders (2018). She has authored about 70 papers in leading conferences and journals including T-KDE, T-MM, T-MC, T-ASLP, T-AC, ACM Multimedia, AAAI, IJCAI, WWW etc. She also has wide research collaborations with Tencent, SOGOU, Huawei, Siemens, MSRA, Bosch, etc.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Mental Health Computing via Harvesting Social Media DataÂ **




Psychological stress and depression are threatening peopleâ€™s health. It is non-trivial to detect stress or depression timely for proactive care. With the popularity of social media, people are used to sharing their daily activities and interacting with friends on social media platforms, making it feasible to leverage online social media data for stress and depression detection. In this talk, we will systematically introduce our work on stress and depression detection employing large-scale benchmark datasets from real-world social media platforms, including 1) stress-related and depression-related textual, visual and social attributes from various aspects, 2) novel hybrid models for binary stress detection, stress event and subject detection, and cross-domain depression detection, and finally 3) several intriguing phenomena indicating the special online behaviors of stressed as well as depressed people. We would also like to demonstrate our developed mental health care applications at the end of this talk.



05**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 7 æ—¥ï¼Œ8:30-9:00**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEtcWEafFFsYNHt3FQz2e08TCpvy1kL73XXibwBdYfxYibckOFibCFlv8NQ/640)



**Â è¥¿åŒ—å·¥ä¸šå¤§å­¦è°¢ç£Šæ•™æˆÂ **




Lei Xie is currently a Professor in the School of Computer Science, Northwestern Polytechnical University, Xian, China. From 2001 to 2002, he was with the Department of Electronics and Information Processing, Vrije Universiteit Brussel (VUB), Brussels, Belgium, as a Visiting Scientist. From 2004 to 2006, he worked in the Center for Media Technology (RCMT), City University of Hong Kong. From 2006 to 2007, he worked in the Human-Computer Communications Laboratory (HCCL), The Chinese University of Hong Kong. His current research interests include audio, speech and language processing, multimedia and human-computer interaction. He is currently an associate editor of IEEE/ACM Trans. on Audio, Speech and Language Processing. He has published more than 140 papers in major journals and proceedings, such as IEEE TASLP, IEEE TMM, Signal Processing, Pattern Recognition, ACM Multimedia, ACL, INTERSPEECH and ICASSP.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Meeting the New Challenges in Speech Processing: Some NPU-ASLP ApproachesÂ **




Speech has become a popular human-machine interface due to fast development of deep learning, big data and super-computing. We can see many applications in smartphones, TVs, robots and smart speakers. However, for further wide deployments of speech interfaces, there are still many challenges we have to face, such as noise interferences, inter- and intra-speaker variations, speaking styles and low-resource scenarios. In this talk, I will introduce several approaches, recently developed in the Audio, Speech and Language Processing Group, Northwestern Polytechnical University (NPU-ASLP) team, to meet these challenges in speech recognition, speech enhancement and speech synthesis.



06**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 7 æ—¥ï¼Œ9:00-9:30**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEPS23Mq7QorEsuYEiceMvGcibzibs8SnRv7tFDBWIyWMxJpIOEpX4jkP4g/640)



**Â é¦™æ¸¯ä¸­æ–‡å¤§å­¦å‘¨åšç£Šæ•™æˆÂ **




Bolei Zhou is an Assistant Professor with the Information Engineering Department at the Chinese University of Hong Kong. He received his PhD in computer science at Massachusetts Institute of Technology (MIT). His research is in computer vision and machine learning, focusing on visual scene understanding and interpretable deep learning. He received the Facebook Fellowship, Microsoft Research Fellowship, MIT Greater China Fellowship, and his research was featured in media outlets such as TechCrunch, Quartz, and MIT News.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Deep Visual Scene UnderstandingÂ **




Deep learning has made great progress in computer vision, achieving human-level object recognition. However, visual scene understanding, which aims at interpreting objects and their spatial relations in complex scene context, remains challenging. In this talk I will first introduce the recent progress of deep learning for visual scene understanding. From the 10-million image dataset Places to the pixel-level annotated dataset ADE20K, I will show the power of data and its synergy with interpretable deep neural networks for better scene recognition and parsing. Then I will talk about the trend of visual recognition from supervised learning towards more active learning scenario. Applications including city-scale perception and spatial navigation will be discussed.



07**ç‰¹é‚€æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 7 æ—¥ï¼Œ9:30-10:00**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEJSekbAyt8KcGPykBSJLKlbgRic7U4zs23fsQIrMAhFibbPpzI5R0B3Og/640)



**Â æ¸…åå¤§å­¦è¢æ˜¥æ•™æˆÂ **




Chun Yuan is currently an Associate Professor with the Division of Information Science and Technology at Graduate school at Shenzhen, Tsinghua University. He received his M.S. and Ph.D. degrees from the Department of Computer Science and Technology, Tsinghua University, Beijing, China, in 1999 and 2002, respectively. He once worked at the INRIA-Rocquencourt, Paris, France, as a Post-doc research fellow from 2003 to 2004. In 2002, he worked at Microsoft Research Asia, Beijing, China, as an intern. His research interests include computer vision, machine learning and multimedia technologies. He is now the executive vice director of â€œTsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systemsâ€.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Event Level Video Captioning based on Attentional RNNÂ **




Video understanding is a hotspot and challenge subject featured by jointly knowledge of natural language processing (NLP) and computer vision. More and more commercial application of online multimedia content requires better automatic understanding of video events. Unlike image captioning, video captioning faces more obstacles. First, video is complex data form to get and utilize feature, comparing to image. The temporal change makes sufficient information and different methods have their own shortages in mining temporal information. Second, in the task of captioning, the generation of sentence is required to extract dynamic information from videos. While some methods deal well with short ant monotone actions, mining with longer and more complex actions is next goal. Third, some new tasks like captioning multiple events, call for new algorithm to get event-level processing. When generating sentences, correctly generate words like â€œcontinueâ€ or â€œanotherâ€ is one manifestation of good exploit context information.



01**ç‰¹æ®Šè®®é¢˜æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 7 æ—¥ï¼Œ10:30-11:00**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEwg7EPMT2CR9wIUXTpiay3SCpHrWGVuJBqhCaeLZiaByt4Lafia0JXsguA/640)



**Â å£°å¸Œç§‘æŠ€ï¼ˆSpeechXï¼‰CTO åˆ˜é¹é£åšå£«Â **




Dr. Pengfei Liu received his B.E. and M.E. degrees from The East China Normal University and the Ph.D. degree from The Chinese University of Hong Kong. His research areas are natural language processing and deep learning, particularly on sentiment analysis and dialog systems. He developed the SEEMGO system which ranked 5th in the task of aspect-based sentiment analysis at SemEval-2014, and received the Technology Progress Award in JD Dialog Challenge in 2018. Dr. Liu previously worked at SAP Labs China in Shanghai, The Chinese University of Hong Kong, and Wisers AI lab in Hong Kong, where he led a team to conduct research on deep learning-based sentiment analysis. He is currently the CTO of SpeechX.




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â Developing a Personalized Emotional Conversational Agent for Learning Spoken EnglishÂ **




The spoken English skill is critical but challenging for non-native learners in China due to lack of enough practice, while improving spoken English is in large demand among learners of different ages. This talk presents our ongoing project at SpeechX on developing a personalized emotional conversational agent which aims to provide a virtual partner for language learners to practice their spoken English. Such an agent is personalized based on each learnerâ€™s English level and interests, and meanwhile gives appropriate responses according to the learnerâ€™s emotions. Developing the agent involves a lot of research challenges such as consistency and personalization in dialog systems, multimodal emotion recognition, expressive speech synthesis and so on. In this talk, we will briefly introduce our work responding to these challenges, present a preliminary proof-of-concept prototype and discuss future research perspectives.



02**ç‰¹æ®Šè®®é¢˜æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 7 æ—¥ï¼Œ11:00-11:30**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEtCl8BwXAEO1T2xmMgibWxfbAH6euJqZfLYp4qkSygVwEbkrEXeBGafg/640)



**Â æ·±åœ³å£¹ç§˜ç§‘æŠ€ï¼ˆeMeetï¼‰CEO é™ˆæ–‡æ˜Â **




é™ˆæ–‡æ˜ï¼Œæ·±åœ³å£¹ç§˜ç§‘æŠ€æœ‰é™å…¬å¸åˆ›å§‹äººï¼Œä¸­æ¬§å›½é™…å·¥å•†å­¦é™¢ EMBAã€‚åœ¨éŸ³è§†é¢‘ã€æ™ºèƒ½è¯­éŸ³ã€æ™ºèƒ½å®¶å±…ã€ç‰©è”ç½‘é¢†åŸŸå·¥ä½œ 18 å¹´ï¼›æ›¾äº TCL å°±èŒ 10 ä½™å¹´ï¼Œå†ä»»ç ”å‘æ€»ç»ç†ã€äº§å“æ€»ç»ç†ã€ç”µå£°äº‹ä¸šéƒ¨æ€»ç»ç†ã€åˆ›æ–°äº‹ä¸šéƒ¨æ€»ç»ç†ï¼›2016 å¹´ 8 æœˆåˆ›ç«‹æ·±åœ³å£¹ç§˜ç§‘æŠ€æœ‰é™å…¬å¸ã€‚




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â ä¸“ä¸šå•†åŠ¡æ™ºèƒ½è¯­éŸ³çš„åº”ç”¨åŠæŒ‘æˆ˜Â **




æ·±åœ³å£¹ç§˜ç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº 2016 å¹´ï¼Œä¸“æ³¨ç§»åŠ¨åŠå…¬äº§å“åˆ›æ–°åŠæ™ºèƒ½æœåŠ¡ã€‚ç ”å‘çš„äººå·¥æ™ºèƒ½ä¼šè®®æœåŠ¡ç³»ç»Ÿï¼ŒåŸºäºæ™ºèƒ½è¯­éŸ³å‰ç«¯é˜µåˆ—ç®—æ³•æŠ€æœ¯ã€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€ç½‘ç»œé€šè®¯æŠ€æœ¯ï¼ŒæœåŠ¡äºå…¨çƒç§»åŠ¨åŠå…¬åŠæ™ºèƒ½ä¼šè®®å¸‚åœºã€‚æŠ¥å‘Šå°†ä»¥å£¹ç§˜äº§å“åŠæœåŠ¡çš„åº”ç”¨åœºæ™¯åŠå¸‚åœºæ½œåŠ›ä½œä¸ºåˆ‡å…¥ç‚¹ï¼Œåˆ†äº«æ·±åœ³å£¹ç§˜ç§‘æŠ€æœ‰é™å…¬å¸äº‰åšæ™ºèƒ½è¯­éŸ³å•é¡¹æŠ€æœ¯åº”ç”¨å† å†›çš„å¿ƒè·¯å†ç¨‹ï¼Œè¿›è€Œä»å‰ç«¯è¯­éŸ³å¤„ç†çš„æŠ€æœ¯ç“¶é¢ˆã€åç«¯è¯­è¨€å¤„ç†æŠ€æœ¯çš„æŒ‘æˆ˜æœºé‡ä¸¤æ–¹é¢é˜è¿°ä¸“ä¸šå•†åŠ¡æ™ºèƒ½è¯­éŸ³çš„åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚



03**ç‰¹æ®Šè®®é¢˜æŠ¥å‘Š**



**Â æ—¶é—´Â ****Â 2018 å¹´ 12 æœˆ 7 æ—¥ï¼Œ11:30-12:00**

**Â åœ°ç‚¹Â ****Â æ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ï¼ŒCII ä¸€æ¥¼å¤šåŠŸèƒ½å…**


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEmibJlABbx8nQ2Ls1vvJS4BmNyibicqvtY74ibxqKPUcrdMZ16dibcqSYOCg/640)



**Â å¥½æœªæ¥AILABè¯­éŸ³æŠ€æœ¯è´Ÿè´£äººæ¨åµ©Â **




æ¨åµ©ï¼Œå†ä»»æ€å¿…é©°é«˜çº§è¯­éŸ³å·¥ç¨‹å¸ˆã€è‹å·é©°å£°ç ”å‘ä¸»ç®¡ã€å¥½æœªæ¥ AILAB è¯­éŸ³æŠ€æœ¯è´Ÿè´£äººã€‚ç ”ç©¶æ–¹å‘ä¸ºè¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è¯„æµ‹ã€‚ä¸€ç›´è‡´åŠ›äºä¸­é«˜è€ƒè‹±è¯­å£è¯­æœºå™¨è¯„åˆ†ï¼Œåœ¨çº¿æ•™è‚²è¯¾å ‚è´¨é‡è‡ªåŠ¨åŒ–è¯„ä¼°ç­‰æ–¹é¢å·¥ä½œï¼Œåœ¨è¯¥é¢†åŸŸæ‹¥æœ‰å¤šé¡¹ä¸“åˆ©ã€‚2014 å¹´è·ä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šé¢å‘çš„â€œå´æ–‡ä¿Šäººå·¥æ™ºèƒ½ç§‘å­¦æŠ€æœ¯å¥–è¿›æ­¥å¥–â€ã€‚




****Â æŠ¥å‘Šé¢˜ç›®Â ******Â AIåœ¨æ•™è‚²é¢†åŸŸè½åœ°çš„æ¢ç´¢Â **




å¥½æœªæ¥æ•™è‚²é›†å›¢ä»¥â€œç§‘æŠ€æ¨åŠ¨æ•™è‚²è¿›æ­¥â€ä½œä¸ºè‡ªå·±çš„ä½¿å‘½ï¼Œæ·±å…¥å‘æ˜AIæŠ€æœ¯å’Œæ•™è‚²åœºæ™¯çš„ç»“åˆç‚¹ã€‚é’ˆå¯¹æ•™å­¦èµ„æºä¸å‡è¡¡ï¼Œä¼˜è´¨å¸ˆèµ„ä¸è¶³çš„ç°çŠ¶ï¼Œå¤§åŠ›å‘å±•å„ä¸ªåœºæ™¯çš„â€œAI è€å¸ˆâ€ï¼›é’ˆå¯¹å­¦ç”Ÿèƒ½åŠ›å‘å±•ä¸å¹³è¡¡ï¼Œæ¨å¹¿ä¸ªæ€§åŒ–æ•™å­¦ã€‚æ­¤å¤–ä¸ºæ•™è‚²çš„å„ä¸ªç¯èŠ‚å¼•å…¥ä¸åŒçš„AIè¯„æµ‹æŠ€æœ¯ï¼›åœ¨çº¿ä¸‹è¯¾å ‚æ•™å­¦ä¸­æä¾›æ™ºæ…§æ•™å®¤çš„è§£å†³æ–¹æ¡ˆï¼Œè®©æ•™å®¤æ‹¥æœ‰çœ¼ç›ï¼ˆæ‘„åƒå¤´ï¼‰ï¼Œè€³æœµï¼ˆéº¦å…‹é£ï¼‰ï¼Œå¤§è„‘ï¼ˆäº‘ï¼‰åŠå…¶ä»–å™¨å®˜ï¼ˆç­”é¢˜å™¨ï¼ŒiPadï¼‰ï¼Œå¼•å…¥éŸ³è§†é¢‘é‡åŒ–æ•™å­¦è¿‡ç¨‹ï¼Œè¯„ä»·è¯¾å ‚çš„æ•™å­¦è´¨é‡ï¼›åœ¨çº¿ä¸Šè¯¾å ‚é€šè¿‡è¯†åˆ«å’Œåˆ†æè¯¾å ‚å†…å®¹ï¼Œè¯„ä»·å¸ˆç”Ÿé—´çš„äº¤äº’çŠ¶å†µï¼ŒæŠ½å–ç›¸å…³ç‰¹å¾å¯¹å¸ˆç”Ÿè¿›è¡ŒåŒ¹é…ï¼Œæé«˜æ•™å­¦æ•ˆç‡ã€‚å¥½æœªæ¥ä»¥ AI æŠ€æœ¯ä¸ºå¼•æ“ï¼ŒæŒç»­æ¢ç´¢æœªæ¥æ•™è‚²çš„æ–°æ¨¡å¼ã€‚



**è®ºå›æŠ¥åæ–¹å¼**





é•¿æŒ‰è¯†åˆ«äºŒç»´ç ï¼Œé©¬ä¸ŠæŠ¥åï¼

â–¼




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEwqveapHDicNHa8jKuyACxHhciaib0kichpiaMcE5QHg0afJ80ggUpAFpRyg/640)



**è®ºå›ä¸¾åŠåœ°ç‚¹**






**æ·±åœ³å¸‚å—å±±åŒºä¸½æ°´è·¯ 2279 å·ï¼Œæ¸…åå¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢ CII ä¸€æ¥¼å¤šåŠŸèƒ½å…ï¼ˆå›½é™…ä¼šè®®ä¸­å¿ƒï¼‰**





![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEMDOrsA1MtZaUHlDEuyg5e7GZZQXWSC8I3bGPK6ghNhg1VXpY8gXbtQ/640)



**è®ºå›æ‰¿åŠå•ä½**



![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvEa1YGlAE3vtF1Dn1773WmQehiagQ1NdIUurPRffhcu68wu7U0UgycUoQ/640)



**è®ºå›ä¸»åŠå•ä½**






![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvE4de1opI00uzveDyTqiawhSPl3YN45Ohq6tdsTvSvPSmibAtGe0zucbuw/640)



**è®ºå›æ”¯æŒå•ä½**



![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvE5ibs4zkicSzNxohPfj68H1Qq9Fkp6BGIN3claGu6oacJicVB1DEotfhMQ/640)


**è®ºå›åˆä½œåª’ä½“**








![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnKiacZ6l1Bib8yAkLmbAcJvE2S1LicBIhA1wYMXJsagpqbrJBtibJDpo1NmAys0n0VBQqicTQMMxRDSXw/640)







ğŸ”




ç°åœ¨ï¼Œåœ¨**ã€ŒçŸ¥ä¹ã€**ä¹Ÿèƒ½æ‰¾åˆ°æˆ‘ä»¬äº†

è¿›å…¥çŸ¥ä¹é¦–é¡µæœç´¢**ã€ŒPaperWeeklyã€**

ç‚¹å‡»**ã€Œå…³æ³¨ã€**è®¢é˜…æˆ‘ä»¬çš„ä¸“æ å§







**å…³äºPaperWeekly**



PaperWeekly æ˜¯ä¸€ä¸ªæ¨èã€è§£è¯»ã€è®¨è®ºã€æŠ¥é“äººå·¥æ™ºèƒ½å‰æ²¿è®ºæ–‡æˆæœçš„å­¦æœ¯å¹³å°ã€‚å¦‚æœä½ ç ”ç©¶æˆ–ä»äº‹ AI é¢†åŸŸï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·åå°ç‚¹å‡»**ã€Œäº¤æµç¾¤ã€**ï¼Œå°åŠ©æ‰‹å°†æŠŠä½ å¸¦å…¥ PaperWeekly çš„äº¤æµç¾¤é‡Œã€‚




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgl9qrwuXS7D8F2ZLyZNmqfWibCVlSbGBVCrd80blia0iaiaKuVk5p1tWP8tCaIiaYxiaQwiacIOlu9yOw6Mg/640?)

â–½ ç‚¹å‡» |Â é˜…è¯»åŸæ–‡| ç«‹åˆ»æŠ¥å




