# 你说我导！微软玩转标题描述生成视频 - Paper weekly - CSDN博客





2018年09月25日 12:18:15[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：477









![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgnC9iaic8hDbiadLafh7TtCZS6icEYddVmMqZBksDV7cQkKmAu95h53FxyibqmZOS1yQgHibJT0WYD2s1Zw/640)

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgl7VHx00TkzicBMAfz1dFT8icD4HwmJZpt0Jiccw6ns7c3co7MpZslIia8VAuZicUTSuoPaq6hE4KbxWPg/640?)





在碎片化阅读充斥眼球的时代，越来越少的人会去关注每篇论文背后的探索和思考。





在这个栏目里，你会快速 get 每篇精选论文的亮点和痛点，时刻紧跟 AI 前沿成果。




点击本文底部的「**阅读原文**」即刻加入社区，查看更多最新论文推荐。
这是 PaperDaily 的第 **107** 篇文章


本期推荐的论文笔记来自 PaperWeekly 社区用户 **@TwistedW**。**本文来自微软亚洲研究院，****根据描述生成视频是此篇文章实现的目的。**通过 LSTM 对描述标题提取特征，再结合噪声经过 3D 反卷积生成视频，利用 GAN 的思想优化生成质量，判别的时候根据视频和描述的匹配关系来判断真假，通过视频、视频帧、帧之间的判别实现了描述到视频的生成。

 如果你对本文工作感兴趣，点击底部**阅读原文**即可查看原论文。

# 关于作者：武广，合肥工业大学硕士生，研究方向为图像生成。

■ 论文 | To Create What You Tell: Generating Videos from Captions

■ 链接 | https://www.paperweekly.site/papers/2315

■ 作者 | Yingwei Pan / Zhaofan Qiu / Ting Yao / Houqiang Li / Tao Mei




视频生成在计算机视觉上已经是很困难的工作了，按照描述去生成视频是更具有挑战性的工作。***To Create What You Tell: Generating Videos from Captions*****简称为 TGANs-C，在实验上实现了由描述的标题生成相对应的视频**，这个工作很有意义，整体思想上采取 GAN 为主题框架实现，我们一起来读一下。




# 论文引入




视频生成的困难在于视频是视觉上连贯和语义相关的帧的序列，也就是在时序序列上做生成，一涉及到时序就存在大量的不确定性，这也是语音和视频生成上的难点所在。




视频通常伴随有文本描述，例如标签或字幕，因此学习视频生成模型对文本进行调节从而减少了采样不确定性，这个是具有很大的潜在实际应用。GAN 在实现时序上的生成我们之前有写到 Temporal GAN 论文解读 [1]、VGAN 论文解读 [2]，整体的思想都是采用 3D 卷积处理视频序列从而实现视频的生成。 




TGANs-C 和 TGAN 的区别在与 TGANs-C 实现了由描述性文字到视频的生成，**这篇论文在基础上借鉴了文本到图片生成的 GAN-CLS 即采用配对的思想**，这个我们后续再谈，如果你对 **GAN-CLS**[3] 印象很深的话这篇文章读起来会很轻松。 




**通常，在采用标题调节的视频生成中存在两个关键问题：跨视频帧的时间一致性以及标题描述与生成的视频之间的语义匹配。**前者产生了对生成模型学习的见解，相邻视频帧通常在视觉上和语义上是连贯的，因此应该随着时间的推移而平滑地连接，这可以被视为产生视频的内在和通用属性。后者追求的模型能够创建与给定标题描述相关的真实视频。




因此，一方面考虑条件处理以创建类似于训练数据的视频，另一方面考虑通过整体利用字幕语义和视频内容之间的关系来规范生成能力，这正是 TGANs-C 所考虑的。 




**总结一下 TGANs-C 的优势：**



- 
这是第一个在标题描述下生成视频的工作之一 

- 
实现了视频/帧内容与给定标题对齐 

- 
通过一系列广泛的定量和定性实验，验证了 TGANs-C 模型的有效性





# TGANs-C模型结构




我们还是先来看一下模型的网络结构：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhuGIttiahLtWrYunDHz5Pjo6iagpg9PCNd5wIdzqwHMiccEM73Mnfrdtnw/640)




整体上模型分为左右两块，左边是生成器，右边就是判别器，所以说 **TGANs-C 是以 GAN 为主体框架的模型**，我们分开来分析。




**生成网络**




**生成网络下由两块组成，下面是标题描述的文本编码，上面就是在噪声和文本编码特征作为输入的生成器。**对于文本编码，文章采用的是双向 LSTM 做的编码。文字描述的时序和语义结构是紧密相关的，所以需要保留时序信息，所以 RNN 的思想是处理的关键。对于文本编码过程中采用逐个单词双向处理，双向 LSTM 可以保证文本上更加紧密的时序和语义结构，最终编码到特征维度为![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAh59ibNGDnMwBbiboKmDGTIsu7BJg59qOticzPvXWRzib0cWpQOwDIsrzOLQ/640)文中取的是 256 维。




将文本编码得到的特征向量 S 和先验噪声 z 做 concat![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhmC5tsFJxxibTd32ibNkaDACibS082I7WDGrH5ZNkoMC3VCqULe17AukmQ/640)送入 3D 反卷积网络做视频的生成，整个过成处理上为![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhA7fq0PomRX7LLiaLibKuibsOqk8Hwlb8t40cHA5q8JkgIiaqMF4ZGbZpWg/640)，这里的![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhykOaia9agTkoZX5SgaQGeSkdJksEU2KRicKVeejNgxDywkmEsaZicYBeA/640)代表的是通道数、帧、高、宽，这里的高和宽对应的是视频一帧图像的长宽，帧代表反卷积下包括多少视频帧，比如最后的输出文中取的是 16 帧作为视频的输出。




可以看到，经过 3D 反卷积后，由噪声和文本编码最后生成了彩色的 16 帧大小为 48×48 的视频输出。整个生成器我们可以将标题描述特征作为条件，整个生成器是类似于条件生成器，由标题描述特征作为条件生成对应的视频。




**判别网络**




TGANs-C 有着强大的判别网络，文章为了实现判别效果设计了 3 个判别器，图中对应的是右半边上、中、下。




上面一路的判别器命名为![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhBfyMxNpT45RxUhLbo46a1q1BFxQBthDzPmXTUVQFrbDXUvs8aCdYibw/640)，它的目的是为了区别生成的视频和真实的视频的真假，为了保证与标题描述对应，在最后嵌入了标题特征做匹配。这个思想在 GAN-CLS 最早被应用，为了实现和描述文本的匹配，在判别器的设计上增强了判别器的能力。




判别器不仅判断视频的真假还判断视频是否和标题描述对应，配对就这样产生了，由 3 组配对关系：真实视频和正确标题描述![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhDd5wvl5jaduoxfx4l63wcJCuZVm8L5hM9vhvr6XgBZPsWs4ptBxYDQ/640)、生成视频和真实标题描述![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhA4X8xvibUhLroU57nk8y0FfcjpADeS67GnoNlphRVMmW2P1HZLI8L2w/640)还有就是真实视频和错误标题描述![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAheZkpMw5WkEiadtpxSAbMjY5icLg00iayrqEtEN8TfYgcJbKM2tBwOomkA/640)。判别器![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhBfyMxNpT45RxUhLbo46a1q1BFxQBthDzPmXTUVQFrbDXUvs8aCdYibw/640)只有在真实视频和正确标题对应上才判断为真，否则为假，即![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhDd5wvl5jaduoxfx4l63wcJCuZVm8L5hM9vhvr6XgBZPsWs4ptBxYDQ/640)真，![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhA4X8xvibUhLroU57nk8y0FfcjpADeS67GnoNlphRVMmW2P1HZLI8L2w/640)、![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAheZkpMw5WkEiadtpxSAbMjY5icLg00iayrqEtEN8TfYgcJbKM2tBwOomkA/640)为假。与之对应的损失函数为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhlR944eCjEhA0q9RPE864e0pic747BSTxbTMsRDfSSuF7juzHjjF2zbg/640)




中间一路的判别器命名为 D1，它的目的是为了区分对应的视频帧的真假，同样的加入了与标题描述的匹配，用![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhCthIsHk9WibF5W3IxlfhpxvHdb9cYcnNLQvsB8QUFVuu2QCEMbDL3mA/640)描述视频的第 i 帧对应的图像，对于整个视频一共有![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhgbYT6RLzicUjcrMQAyl5picNopjxnicibzoPDcoIBfjrQhibXOuognWiasXg/640)帧，这个判别器对应的损失为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAh5BBkzdvsbBUfUsns6S4eNz5FDgBzejdZd6RCLfuTTn56smIO0g7AeQ/640)




下面一路的判别器命名为 D2，它的目的是为了在时序上调整前后帧的关系，一般视频中前后帧之间不会有太大的变动，由此思想文章设计了时序关联损失。它的作用是保证视频的前后帧之间不会有太大的差异，用 D 表示：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhGtnNHqCRRibNM3OvsyNQQEibHQO5vLiamWezO3FQyrN0rEAL4FQibblKCA/640)




由于决定生成的视频帧的帧之间的关系的是生成器，对于真实视频没必要再做差异优化，所以这部分主要作用的是生成器，这一块的损失可以写为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhRGqr2qY1s1qQyibV5RmwoGLVm3eF91Ys8qS3qzKRpnicrV2M8j6Dyficw/640)




为什么这里的上标为 1 呢，因为对应的还有 2，这一部分是考虑到生成视频帧之间的关联差异，从动态差异上实现对抗又会怎么样呢？这就是另一种实现时序关联的方法。 这一部分用 Φ2 判断真假，此损失表示为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAh4wR15omnwwf4ub7S0qzuhU2KR6bc0V3iamQwIDcTU89rKQx1SEFLCibA/640)




文中对时序关联上采取的方法 (1) 时间相干约束损失命名为 TGANs-C-C，对于方法 (2) 时间相干性对抗性损失命名为 TGANs-C-A。从后续的实验上验证出 TGANs-C-A 的方法效果更好，所以文章的名字 TGANs-C 其实指的是 TGANs-C-A。 




整合一下，对于方法 (1) TGANs-C-C 对应的判别器和生成器最终损失为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhZUVuuZLzoonv29ibcdZJvWsk6KHotcCTHObLbXpB3cxy9nFaqCkr9WA/640)




对于方法 (2) TGANs-C-A 对应的判别器和生成器最终损失为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhhhPKSIbUbwticJcCeJz4xpQMMuTPUAMEg6NZ32RDarxeMUvRnkWlvQw/640)




最后贴上实现整个 TGANs-C 的伪代码：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhyiaQhKiajrEABJljY2VgwhVjfsO8Gwdm2UDglmFt577weoTyiccSBMyTw/640)




# TGANs-C实验




实验的数据集选择了单数字边界 MNIST GIF（SBMG），两位数的弹跳 MNIST GIF（TBMG）和微软研究视频描述语料库（MSVD）。




**SBMG 是通过在 64×64 帧内弹出单个手写数字而产生的。**它由 12,000 个 GIF 组成，每个 GIF 长 16 帧，包含一个 28×28 左右移动的数字左右或上下。数字的起始位置是随机均匀选择的。每个 GIF 都附有描述数字及其移动方向的单句。



**TBMG 是 SBMG 的扩展合成数据集，包含两个手写数字弹跳**，生成过程与 SBMG 相同，每个 GIF 中的两个数字分别左右或上下移动。MSVD 包含从 YouTube 收集的 1,970 个视频片段。每个视频大约有 40 个可用的英文描述。在实验中，手动过滤掉有关烹饪的视频，并生成 518 个烹饪视频的子集。数据集的部分描述如下图：



![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhhRqzUuRnUxcyKicgaVX1NjFbnpc315BAcptrcicmAiafWHibYbK9lunyoQ/640)




TGANs−C1 为仅考虑视频对抗![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhBfyMxNpT45RxUhLbo46a1q1BFxQBthDzPmXTUVQFrbDXUvs8aCdYibw/640)，TGANs−C2 为考虑了![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhGzxj8Dqsia5WWUHGGKL8lic9sJhib9lrVmz3bpYhnW7ApgicsHbnGdiaAEQ/640)未考虑![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhjm3eJVYX621w3uoG1sfRDBqWVPUyDJIQombV3Zum3B9OmVFr5TI7cw/640)，TGANs-C-C 和 TGANs-C-A 都已经知道了构成，这几个对比结果为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhF0ibQbRXIvEXOpiarJwLImbZZkDBtWkrkRDIh2C0ibxryAbOmsvnbuXBg/640)




定性分析不同模型产生的结果如下图，主要对比了 VGAN、Sync-DRAW（基于 VAE 实现的）、GAN-CLS 和 TGANs-C。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhcoepicruqS4eH3Gzaw5X7Z6hY9nK0QgwdI4vI1v4tsrozTSPu70Kjnw/640)




定量上以不同人的选择，给分越低效果越好，TGANs-C 也展示了很好的效果：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm5Xlia8U0YsDDzj1jzwAzAhCE7IXxGRwNUKrjwRpugwKmyynnt8UnPw8TiaAkCcWb7pic7icsVM95H3A/640)




# 总结




TGANs-C 实现了标题描述到视频的生成，正如文章的题目说的那样 **To Create What You Tell！**虽然这个题目有点大，但是确实在理论上是可以行得通的。匹配的思想对于严格的固定生成来说是很重要的一个技术环节，可以借鉴在很多地方，正是这种严格的配对关系往往限制了一些发展，因为这种算是全监督式学习了。无监督下条件生成是最为困难的，这个也是未来大家一起努力的地方。




# 参考文献




[1]. https://www.paperweekly.site/papers/notes/443

[2]. https://www.paperweekly.site/papers/notes/449

[3]. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
Bernt Schiele, and Honglak Lee. 2016. Generative adversarial
text to image synthesis. In ICML.

**本文由 AI 学术社区 PaperWeekly 精选推荐，社区目前已覆盖自然语言处理、计算机视觉、人工智能、机器学习、数据挖掘和信息检索等研究方向，点击「****阅读原文****」即刻加入社区！**

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)




**点击标题查看更多论文解读：**




- 
[ECCV 2018最佳论文：基于解剖结构的面部表情生成](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491543&idx=1&sn=e40163badcbeb30cc079c659677b04b5&chksm=96e9c057a19e4941dbf6fea886067f10e7ba4a62b8ea684e2d8852045125f201c0d90d63207e&scene=21#wechat_redirect)

- 
[神经网络架构搜索（NAS）综述](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491248&idx=1&sn=d2946d8a37f7c6567b1a767a497006fb&chksm=96e9c130a19e48267f72ad32c527ec4a1697741e409d865d9233c5d7035a1f66a59b5e40792d&scene=21#wechat_redirect)

- 
[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487949&idx=1&sn=e09391933f3c4493cfb737b0ea2cf0af&chksm=96e9ce4da19e475b0c789088d403a0f49449b8ba0c43734aa835c5d2a7cb69c3d839c7ce056c&scene=21#wechat_redirect)[从傅里叶分析角度解读深度学习的泛化能力](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491082&idx=1&sn=d7c1cb39c3be43154c658ca5a791eb4c&chksm=96e9c18aa19e489c32fe36671e4208ce42bf200e3a7adeda200fa2785462d16f85c58bb455b4&scene=21#wechat_redirect)


- 
[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488238&idx=1&sn=06ffb033332a54279e600c511e1c5c5f&chksm=96e9cd6ea19e44781ee1313b349e0e77631781a2a163e2fd845c841dc2200d988424bd73c4c7&scene=21#wechat_redirect)[ECCV 2018 | 从单帧RGB图像生成三维网格模型](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491465&idx=1&sn=a6871b72d460debf90d2daa0bed719c8&chksm=96e9c009a19e491f8c247f36a53bad0a6812c462f3fb5b2d9ae74b38be673b946b82a4b44330&scene=21#wechat_redirect)

- 
[ECCV 2018 | 基于三维重建的全新相机姿态估计方法](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491494&idx=1&sn=2922261ba0775f00ad67042dda355b52&chksm=96e9c026a19e493039bf5a90a1d523a46167df4058ee16f7f0797f10f6d392e0c98ef510acf2&scene=21#wechat_redirect)

- 
[ECCV 2018 | 腾讯AI Lab提出视频再定位任务](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491347&idx=1&sn=e1f2cc16c9fcfcc5d2935118f09ee094&chksm=96e9c093a19e49855931cf621ec7f715c1d2dd5041e3343bc311aea2e5069ae03aaa1367e8f9&scene=21#wechat_redirect)

- 
[杜伦大学提出GANomaly：无负例样本实现异常检测](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491630&idx=1&sn=394ffec2969cff23f63022526684f259&chksm=96ea3faea19db6b830104036bd176201ec17f3539608009bf2b17c4628331f4e4ea569b26eb9&scene=21#wechat_redirect)











**![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)****#****投 稿 通 道#**

** 让你的论文被更多人看到 **





如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ **答案就是：你不认识的人。**



总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 




PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是**最新论文解读**，也可以是**学习心得**或**技术干货**。我们的目的只有一个，让知识真正流动起来。




📝 **来稿标准：**

• 稿件确系个人**原创作品**，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向） 

• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接 

• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志



**📬 投稿邮箱：**

• 投稿邮箱：hr@paperweekly.site

• 所有文章配图，请单独在附件中发送 

• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通










🔍




现在，在**「知乎」**也能找到我们了

进入知乎首页搜索**「PaperWeekly」**

点击**「关注」**订阅我们的专栏吧







**关于PaperWeekly**



PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgl9qrwuXS7D8F2ZLyZNmqfWibCVlSbGBVCrd80blia0iaiaKuVk5p1tWP8tCaIiaYxiaQwiacIOlu9yOw6Mg/640?)

▽ 点击 | 阅读原文| 下载论文




