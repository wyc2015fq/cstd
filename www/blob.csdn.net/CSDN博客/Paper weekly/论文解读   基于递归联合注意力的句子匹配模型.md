
# 论文解读 | 基于递归联合注意力的句子匹配模型 - Paper weekly - CSDN博客


2018年07月12日 12:07:00[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：254


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgnC9iaic8hDbiadLafh7TtCZS6icEYddVmMqZBksDV7cQkKmAu95h53FxyibqmZOS1yQgHibJT0WYD2s1Zw/640)
![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgl7VHx00TkzicBMAfz1dFT8icD4HwmJZpt0Jiccw6ns7c3co7MpZslIia8VAuZicUTSuoPaq6hE4KbxWPg/640?)

在碎片化阅读充斥眼球的时代，越来越少的人会去关注每篇论文背后的探索和思考。

在这个栏目里，你会快速 get 每篇精选论文的亮点和痛点，时刻紧跟 AI 前沿成果。

点击本文底部的「**阅读原文**」即刻加入社区，查看更多最新论文推荐。
这是 PaperDaily 的第**89**篇文章
本期推荐的论文笔记来自 PaperWeekly 社区用户**@zhkun**。**本文主要集中在句子匹配任务上，作者将 DenseNet 的一些想法引入到了 stack RNN 中，提出了一种基于递归联合注意力的句子匹配模型。**
如果你对本文工作感兴趣，点击底部**阅读原文**即可查看原论文。

# 关于作者：张琨，中国科学技术大学博士生，研究方向为自然语言处理。
■ 论文 | Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information
■ 链接 | https://www.paperweekly.site/papers/2082
■ 作者 |Seonhoon Kim / Jin-Hyuk Hong / Inho Kang / Nojun Kwak

句子匹配（Sentence Matching）是自然语言理解任务中一个非常重要的任务，例如 Natural Language Inference，Paraphrase Identification，Question Answering 等都可以归属于这个任务。这个任务主要就是理解句子语义，理解句子之间的语义关系。因此如何去表示这些内容就变得十分重要了。

为了更好的利用原始特征信息，作者参考 DenseNet，提出了一种 densely-connected co-attentive recurrent neural network 模型，**该模型最突出的地方就是可以从最底层到最顶层一直保留原始信息以及利用 co-attention 得到的交互信息**。接下来，就对文章进行详细了解。

# 模型结构

首先是模型图：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1ln3kJBI0XvvNuiaTjmEHvajqMuIXKb34TGQMkIjOjeOOrEgRibQibz2aw/640)

不得不说，这个图还是很粗糙的，一点都不够精致，但模型的基本单元以及整体框架已经完全包含进去了，我们姑且用这个图对模型进行分析吧。

**输入层**

自然语言的任务首先就是输入层，对每个词的 one-hot 表示进行 embedding。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1eOQwZrTcRBnia5EWjMAWWo6V33KKaQJKIQVYZ2u7od0Yic1tHqiazdEgA/640)

这几个公式很好理解，首先作者将词的 embedding 分为两部分，一部分参与训练，即![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1xVyumGwhicbBS2luoqjbiaA6Rp1T3bTIYrozdobtNo24qZwe6BQEUo6g/640)，另一部分是固定不动的，即![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1hIwS2yXZHT8CkzzmHAlHbjQia5ZS9TaBxWk1sZJRQRGFvq4voaeoVHQ/640)，然后就是词级别的表示 char-Conv，以及一些 exact match 的匹配特征，主要是 a 中的每个词是否在 b 中有对应的词，然后将这些表示拼接起来，就得到了每个词的最后表示![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1ibEge0wvdichajJNtNRGUdbsKkFh9RwT1KPdZmJdRviclW3bYyMSoW5gQ/640)。

**密集连接层**

在这一层，作者受 DenseNet 启发，使用了密集连接和 RNN 结合的方法来实现对对句子的处理。首先![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I12MV3WoGTglKN4HdM71dUTNlYDibnnwy0ZiagOYuLich8jLDDCxKhKdl5A/640)表示的是第 l 层的 RNN 的第 t 的隐层状态。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1QoYjwLGVmx2EXbfGMQY6paarHoRCy5tlVFDbkUHoicl9Kq11UOtn6hw/640)

式 2.1 是传统的多层 RNN 结构，前一层 RNN 的隐层状态作为当前层的输入，然后就是 RNN 的计算方式，式 2.2 借鉴了残差网络，当前层的输入不仅包含了前一层的隐层状态，同时包含了前一层的输入，但他们是相加的方式，作者认为这种相加的形式很可能会阻碍信息的流动，因此借鉴 DenseNet，作者使用了拼接了方式，这样不仅保留了两部分信息，同时拼接方法也最大程度的保留了各自的独有信息。

但这就有一个问题了，多层的 RNN 的参数就不一样了，因为拼接的方式导致了每一层输入对应的参数规模是在不断变大的，这样就不能做得很深了。

**密集连接注意力**

因为句子匹配考虑的两个句子之间关系，因此需要建模两个句子之间的交互，目前来说，注意力机制是一种非常好的方法，因此作者在这样也使用了注意力机制。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1XkLCxa41kW2FuQmUHVjCar8WlXUdCCnBxnNT7swYbPWibaH4LUCakeg/640)

这个就是传统的 co-attention 计算方法，计算两个序列之间的在每个词上的对应关系，不过作者这里比较粗暴，直接使用了余弦相似度来计算每两个词之间的相似，这里也可以使用一个简单的 MLP 来计算。有意思的地方在下边：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I1icVEPaYLJpCKAu9hGqGZEAYTykTibeiaJVXA6m3kkA1ia02Zn8icFicI5wqQ/640)

这个就很有意思了，我们传统的做法是得到每个词在对方句子上的概率分布之后，使用对方句子中每个词向量的加权和作为当前词的向量表示，而这里**作者直接使用了计算出来的权值分布，将其作为一个特征引入到当前层的输入当中**，这个感觉还是很有意思的。

**瓶颈处理层**

正如前边提到的，这种 dense 连接方式直接导致的一个问题就是随着模型的加深，参数量会变的越来越多，这样最后全连接层的压力就会特别大。因此**作者在这里使用了一个 AutoEncoder 来解决这个问题**。AutoEncoder 可以帮助压缩得到的巨大向量表示，同时可以保持原始的信息。这个操作还是很不错的。

**分类层**

这是处理两个句子关系常用的一种匹配方法，作拼接，相减，点乘，不过作者在这里也是用了相减的绝对值，然后将最终拼接的向量通过一个全连接层，然后根据任务进行 softmax 分类，我个人做过实验，相减的效果要好于相减的绝对值，因为相减不仅可以表示差异，同时可以表明信息流方向，而相减的绝对值就更专注于差异了，两个都用应该是效果比只用一个好的。

# 实验结果

照例，上图，**作者在 NLI 任务和 Question Pair 两个任务上进行了模型验证**，效果当然是十分不错的。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I10LXMgmlx4QwvkuK7bf3ZRQWYWoc8kWXH63RqGTKL1BnYop6KRoiaCaw/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgly531RnBAtvzAXf6Cnu4I18gkoEgHCOgibYNcS9N3fPRSL44RtPjcLYia8dIVBOiaJU27ibHsiaK1jvpA/640)

# 感想

这篇文章主要集中在句子匹配任务上，将 DenseNet 的一些想法引入到了 stack RNN 中，还是可以给人一些灵感的，比如说从残差连接到 DenseNet，比如说注意力权值的使用方法，比如说利用 AutoEncoder 来压缩向量，这些还是十分值得学习的。
**本文由 AI 学术社区 PaperWeekly 精选推荐，社区目前已覆盖自然语言处理、计算机视觉、人工智能、机器学习、数据挖掘和信息检索等研究方向，点击「****阅读原文****」即刻加入社区！**
![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)

**点击标题查看更多论文解读：**

[ACL2018高分论文：混合高斯隐向量文法](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490152&idx=1&sn=ee9c70c701d5ba74423318865ecdb44f&chksm=96e9c5e8a19e4cfeddb4d92d86415c54f511427f8851c5f22b596c68128b85512bf7a62cf729&scene=21#wechat_redirect)
[基于词向量的简单模型 | ACL 2018论文解读](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490031&idx=1&sn=e307230ffbffb648b213b1a775372d06&chksm=96e9c66fa19e4f7996bb13ed2d944d5e49bd538174bd192e41abaf4d2a8863d29135b034cf9c&scene=21#wechat_redirect)
COLING 2018最佳论文：序列标注经典模型复现
[图像压缩哪家强？请看这份超详细对比](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490260&idx=1&sn=6e27f266fdf0ccb08822f7a34aa3fed6&chksm=96e9c554a19e4c42a2b74d6271b388c0327a702b9cab64ec43703b8fc82cbccfbc17a7bfd6b1&scene=21#wechat_redirect)
CVPR 2018 最佳论文解读：探秘任务迁移学习
[深度学习模型复现难？句子对模型复现论文](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489955&idx=1&sn=cabe28465e40ba2b2bc0d1aab0c752ec&chksm=96e9c623a19e4f3526303ea05db1b1d6e9c2cf50a4815568e6c074fe76285888b6ab1a2b9b39&scene=21#wechat_redirect)


**关于PaperWeekly**

PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgl9qrwuXS7D8F2ZLyZNmqfWibCVlSbGBVCrd80blia0iaiaKuVk5p1tWP8tCaIiaYxiaQwiacIOlu9yOw6Mg/640?)
▽ 点击 |阅读原文| 查看原论文


