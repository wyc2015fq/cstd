# 综述：图像风格化算法最全盘点 | 内附大量扩展应用 - Paper weekly - CSDN博客





2018年05月14日 00:00:00[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1507














作者丨静永程

学校 | 浙江大学硕士生

导师 | 宋明黎教授、俞益洲教授

研究方向丨纹理合成、计算摄影




# 前言




这篇文章可以看做是我之前放在 arXiv 上的一篇综述文章 **Neural Style Transfer: A Review**（v4 最新版更新于 2018 年 4 月 26 日）的中文精简版，对应的项目主页详见下方，有兴趣的同学建议去看英文的完整版。 




■ 论文 | Neural Style Transfer: A Review

■ 链接 | https://www.paperweekly.site/papers/1926

■ 主页 | https://github.com/ycjing/Neural-Style-Transfer-Papers




非常感谢在准备这篇综述文章的过程中大牛 @Xun Huang @张航 @爱可可-爱生活 老师的帮助，以及感谢 @机器之心 的翻译总结[神经风格迁移研究概述：从当前研究到未来方向](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650726632&idx=1&sn=bb9e6eedba961cad5e935295d9f379e1&scene=21#wechat_redirect)，和淘宝AI Team的 @黄真川 哥在项目落地上的支持和帮助。 希望这篇文章能够对后面打算做风格化相关研究的同学有所帮助。




下面我将以风格化**简介—前传—起源—发展—应用**的顺序依次对相关内容进行介绍。

# 简介




首先，第一个问题，**什么是图像风格化（Neural Style Transfer）？**




简单来讲，**图像风格化是将一张照片渲染成有艺术风格的画作**。图像风格化算法的输入有二，分别是内容图和风格图，输出有一个，为风格迁移后的结果图。举个例子：









上面这张表示图像风格化算法将输入的一张长城的照片作为内容图，将富春山居图作为风格图，将长城的内容保留，风格替换成中国山水画的风格，创作出一张新的艺术作品。




ok，基本的概念有了，下面我们来唠一唠风格化前传。




# 前传




图像风格化算法说白了其实就是一个图像渲染的过程。看到上面长城那张图，第一反应，图形学的一大分支——**非真实感图形学**，不也是干这件事的嘛。




没错，大体来说，在非真实感图形学领域，图像艺术风格渲染技术可以大体分为基于笔触渲染的方法（Stroke-based Rendering）、基于图像类比的方法（Image Analogy）、基于图像滤波的方法（Image Filtering）。




但为啥这些基于图形学的技术没有大规模落地，出现 Prisma，Ostagram 之类风靡全球的应用呢？咱们一个一个分析一下。




首先，基于笔触渲染的方法，他在算法设计的之前首先会确定某一种风格，也就是说每一个基于笔触渲染的方法一般只对应于一种风格，而不能简单地扩展到其他风格的迁移；图像类比方法需要很多成对儿的原图和风格结果图作为训练集，然而对所有风格图找到这些成对儿的数据集貌似不太现实。




而通过图像滤波的方法速度快、效果稳定，可满足工业界落地的需求（之前很火的时光相册出的“你的名字”滤镜估计就是算法工程师用这种方式不断调整出来的），但是基于图像滤波的方法能模拟出来的风格种类很有限。 




而在基于统计学的计算机视觉领域，图像艺术风格迁移也有很多大牛前辈研究过了。在视觉领域，图像艺术风格渲染一般被认为是**纹理合成**的一个扩展问题。纹理合成是给定一个源纹理图，然后去合成更多的类似的纹理结构，最终组成一个大的纹理图（关于纹理合成的相关工作这里就不详细说了，有兴趣的同学可以去看这个博客）。




风格迁移中的风格图其实可以看成是一种纹理，由此，假如我们在合成纹理图的时候去刻意保留一些语义信息，那不就是风格迁移的结果嘛。




没错，但那个年代管这个问题不叫风格迁移，而叫**纹理迁移（texture transfer）**，换汤不换药，意思是差不多的。




但是呢，为啥纹理迁移在那会儿没有流行起来呢？因为那会儿纹理迁移是基于低层次的图像特征来做的，没有考虑语义信息，所以很多结果不那么得尽如人意。 




是时候祭出下面这张图了：









# 起源




由前传中的介绍，我们可以发现现有的图像艺术风格渲染方法照着大规模落地还有一段距离。而正好在这个时代，有着很多前人所做的扎实的基础研究。很多新的研究都是站在巨人的肩膀上开拓和完成的，Neural Style Transfer 也不例外。 




譬如咱们说**纹理建模方法（Visual Texture Modelling）**，主要研究如何表示一种纹理，是纹理合成技术的核心。




以往的纹理合成方法可以分为两大类：**基于统计分布的参数化纹理建模方法（Parametric Texture Modelling with Summary Statistics）**和**基于 MRF 的非参数化纹理建模方法（Non-parametric Texture Modelling with MRFs）**。




基于统计分布的参数化方法主要将纹理建模为 N 阶统计量，而基于 MRF 的方法一个经典套路是用 patch 相似度匹配进行逐点合成。而咱们之前提到过，风格可以看成一种纹理。




**那我们能不能直接把纹理建模这一套方法用于风格图中的风格建模？**Bingo，这就是图像风格化迁移算法（Neural Style Transfer）的起源和基石之一（到后面风格化发展章节中的现有风格化算法分类时大家对此会有更深的体会）。




纹理建模方法的相关研究解决了图像风格化迁移的第一个大问题：**如何对风格图中的风格特征进行建模和提取**。






**▲ **Image credited to Kaijian Gao from Brown University




好了，成功把风格图中的风格抽出来后，如何和内容混合然后还原成一个相应的风格化结果呢？这就到了另一个领域——**图像重建（Image Reconstruction）**了。




图像重建的输入是特征表达，输出是特征表达对应的图像。他其实和咱们通常的输入图像提特征的过程相反，是把某个特征逆向重建为原来的图像（重建结果不唯一）。




那这有啥用呢？以往图像重建主要是用来理解某些特征表达的。就比如说 CNN 特征，CNN 是黑盒子，某个 CNN 特征具体包含哪些特征，这谁也不敢下定论。




然后图像重建提供了一个可以加深特征理解的途径。假如说给定一张猴子的某个图像分类特征，我们重建出来的不同结果中猴子的五官位置均正确保留，而其他的比如颜色等不同结果不太一样，那么我们可以理解成此分类网络在分类猴子这个类别的图像的时候，会参考五官的位置来与其他类别进行区分。




下面附上我两年前在课题组讲 seminar 时候画的一张图以帮助大家理解：









而通过重建预训练的分类网络（如 VGG）中的高层特征，发现重建结果保留了_高层语义信息_而摒弃了低层的颜色等信息。




此时此刻，结合咱们刚说的_风格建模_，拍脑门一想，假如我在图像重建时候加上个保留给定风格信息的约束，那不就可以让重建出的结果既有我们想要的内容图的_高层语义信息_，又有给定风格图中包含的_风格信息_吗？




一个崭新的领域——**图像风格化迁移（Neural Style Transfer）由此诞生**。围绕图像风格化迁移技术，学术界和工业界风起云涌，英雄倍出。









# 三部曲之二




上文主要和大家讨论了**图像风格化迁移的简介、前传和起源**，目的是想让大家对风格化这个领域有一个初步的认识。**其中起源这一部分比较重要**，这里再做一个简单回顾以及补充。 




图像风格化迁移源于两个其他领域：**纹理建模（Visual Texture Modelling）和图像重建**。 




**其中纹理建模（Visual Texture Modelling）又分为两类： **




(a) 基于统计分布的参数化纹理建模方法（Parametric Texture Modelling with Summary Statistics）




(b) 基于MRF的非参数化纹理建模方法（Non-parametric Texture Modelling with MRFs） 




纹理建模解决了如何对风格特征进行提取的问题。 




而**图像重建解决的则是如何将给定的特征表达重建还原为一张图像**。上篇文章中没有提到的一点是**图像重建算法其实也可以分为两类：**




(a) 基于在线图像优化的慢速图像重建方法（Slow Image Reconstruction based on Online Image Optimisation）



(b) 基于离线模型优化的快速图像重建方法（Fast Image Reconstruction based on Offline Model Optimisation） 




由名字也可以看出这种分类方式的主要依据是图像重建的速度。第一类图像重建的方法 (a) 是在图像像素空间做梯度下降来最小化目标函数。




这一类算法的过程可以理解为：由随机噪声作为起始图，然后不断迭代改变图片的所有像素值来寻找一个目标结果图 x' ，这个目标结果图的特征表达和我们作为重建目标的目标特征表达 Φ(x) 相似，即像素迭代的目标为 Φ(x')≈Φ(x) 。




由于每个重建结果都需要在像素空间进行迭代优化很多次，这种方式是很耗时的（几百乘几百的图需要几分钟），尤其是当需要的重建结果是高清图的时候，占用的计算资源以及需要的时间开销很大。




为了加速这一过程，一个直接的想法是我们能不能设计一个前向网络，用数据驱动的方式，喂给它很多训练数据去提前训练它，训练的目标就是给定一个特征表达作为输入，这个训练好的网络只需要一次前向就能输出一张重建结果图像。




Bingo，这种方式最终被一些德国研究者在一篇 CVPR 论文中证明了其有效性，即第二类方法 (b) 。后续这些德国研究者又融入了生成对抗网络（Generative Adversarial Network）的思想，进一步提升了效果，这个工作发表在了一篇 NIPS 上。 




下面用这张图总结一下**图像风格化算法的起源和基石**，本文剩下部分全是建立在这些内容之上的：






**▲ **图1：图像风格化迁移之起源




ok，下面开始这篇文章的重头戏——**图像风格化迁移（Neural Style Transfer）的发展**。




# 发展




前面提到，**图像风格化迁移算法=图像重建算法+纹理建模算法**，而图像重建和纹理建模又各自可以分为两类方法。于是乎，我们可以很自然地想到，如果将两类图像重建算法和两类纹理建模方法进行排列组合，不就可以得到一系列的图像风格化迁移算法嘛？




事实上，粗略来讲，在图像风格化这一领域大家也确实都是这么做的。**下面我先给出一个现有图像风格化迁移的分类方法**，大家可以和前面的**图 1 **放在一块看，图像风格化迁移这一领域的研究脉络和套路即可了然于胸。






**▲ **图2：图像风格化迁移算法分类方法




下面咱们一起对每一类图像风格化算法做具体介绍和优缺点分析。为了讨论方便，涉及具体算法时直接采用上面这张图中的**引用序号**进行表示。




**1. 基于在线图像优化的慢速图像风格化迁移算法（Slow Neural Method Based On Online Image Optimisation）**



**1.1. 基于统计分布的参数化慢速风格化迁移算法（Parametric Slow Neural Method with Summary Statistics）**




 通过名字就可以看出，这一类风格化算法是由基于在线图像优化的慢速图像重建方法和基 于统计分布的参数化纹理建模方法结合而来。




其中，图像风格化迁移这一领域的祖师爷 Gatys 的开山大作 [4] 就是属于这一类方法的。在此之前，祖师爷自己先发了一篇 NIPS 提出了一个新的基于 CNN 的纹理建模方法[27]***Texture Synthesis Using Convolutional Neural Networks***。




[27] 的核心思想是在图像经过预训练的 VGG 网络时的特征表达（feature map）上计算 Gram 矩阵，利用得到的 Gram 矩阵来表示一种纹理。




Gram 矩阵的计算方式是先将预训练 VGG 某一层的特征表达由reshape 成，然后用 reshape 后的特征表达和其转置矩阵相乘，最后得到的 Gram 矩阵维度为。




祖师爷发现这个 Gram 矩阵可以很好地表示大多数纹理。结合咱们在上一篇博文讨论的内容，这个 Gram 矩阵的纹理表示方法其实是利用了二阶统计量来对纹理进行建模。




祖师爷之后用 Gram 矩阵来对图像中的风格进行建模和提取，再利用慢速图像重建方法，让重建后的图像以梯度下降的方式更新像素值，使其 Gram 矩阵接近风格图的 Gram 矩阵（即风格相似），然后其 VGG 网络的高层特征表达接近内容图的特征表达（即内容相似），实际应用时候经常再加个总变分 TV 项来对结果进行平滑，最终重建出来的结果图就既拥有风格图的风格，又有内容图的内容 ***Image Style Transfer Using Convolutional Neural Networks***。




除了 Gram 矩阵外，还有一些其他方式对风格进行建模。[37] 里 @Naiyan Wang 老师在 IJCAI2017 的文章中从 Domain Adaption 的角度对风格化迁移进行解释和分析，并提出了一些其他的用于风格建模的方法 ***Demystifying Neural Style Transfer***。




这里简单解释一下，Domain Adaption 指的是当训练数据和测试数据属于不同的域时，我们通过某种手段利用源域有标签的训练数据训练得到的模型，去预测无标签的测试数据所在的目标域中的数据。




Domain Adaption 其中一个套路就是以最小化统计分布差异度量 MMD 的方式，让目标域中的数据和源域中的数据建立起一种映射转换关系。




@Naiyan Wang 老师通过公式推导发现，最小化重建结果图和风格图的 Gram 统计量差异其实等价于最小化两个域统计分布之间的基于二阶核函数的 MMD。换言之，风格迁移的过程其实可以看做是让目标风格化结果图的特征表达二阶统计分布去尽可能地逼近风格图的特征表达二阶统计分布。




由此可以很自然地想到，既然是衡量统计分布差异，除了有二阶核函数的 MMD 外，其他的 MMD 核函数例如一阶线性核函数、高阶核函数、高斯核函数，也可能达到和 Gram 统计量类似的效果。实验证明也确实如此。这些计算风格特征的方式其实都是在特征表达（feature map）的所有 channel 上进行计算的。




除了这些贡献外， @Naiyan Wang 老师还提出了一个新的用 channel-wise 的 BN 统计量去对风格进行建模的方法，即利用 VGG 某些层的特征表达的每一个channel的均值和方差（channel-wise）来表示风格。表示 VGG 中第 l 层的 feature map 的第 c 个 channel，并取得了很好的效果，后续有很多研究者 follow 了这种风格建模的方式。




看到这里大家可以发现一点的是，以往风格化算法在提取特征的时候都是在高层次的 CNN 特征空间（feature space）中完成的，虽然这样做的效果在感知效果上（perceptually）优于利用传统的在像素空间（pixel space）计算的特征，但由于特征空间是对图像的一种抽象表达，会不可避免丢失一些低层次的如边缘等的图形信息这会导致风格化结果图中有一些不是很漂亮的变形等。 




@李韶华 老师为解决这一问题，在 ACM MM2017 的一篇文章中 [40]***Laplacian-Steered Neural Style Transfer***，提出在风格化迁移的过程中**同时考虑像素空间和特征空间**。




具体做法为在像素空间中将内容图的**拉普拉斯算子**的滤波结果和风格化重建结果图的滤波结果之间的差异作为一个新的 loss，加到祖师爷 Gatys 提出的损失函数上面。这样的话就弥补了抽象特征空间丢失低层次图像信息的缺点。




**1.2. 基于MRF的非参数化慢速风格化迁移算法（Non-parametric Slow Neural Method with MRFs）**




另外一类慢速风格化算法就是利用**基于 MRF 的非参数化纹理建模方法**对风格信息进行建模了。代表性工作由浙大出身、德国美茵茨大学 Postdoc Chuan Li 学长完成并发表于 CVPR 2016 [41]***Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis***。




其核心思想是提出了一个取代 Gram 损失的新的 MRF 损失。思路与传统的 MRF 非参数化纹理建模方法相似，即先将风格图和重建风格化结果图分成若干 patch，然后对于每个重建结果图中的 patch，去寻找并逼近与其最接近的风格 patch。




与传统 MRF 建模方法不同之处在于，以上操作是在 CNN 特征空间中完成的。另外还需要加一个祖师爷 Gatys 提出的内容损失来保证不丢失内容图中的高层语义信息。




这种基于 patch 的风格建模方法相比较以往基于统计分布的方法的一个明显优势在于，当风格图不是一幅艺术画作，而是和内容图内容相近的一张摄影照片（**Photorealistic Style**），这种基于 patch 匹配（**patch matching**）的方式可以很好地保留图像中的局部结构等信息。




**2. 基于离线模型优化的快速图像风格化迁移算法（Fast Neural Method Based On Offline Model Optimisation）**




ok，咱们前面介绍的都是用慢速图像重建方法对风格化结果进行重建的，所以速度肯定是比较慢的，而且很吃资源，在工业界落地的成本肯定是很高的。




所以**另外一个大的图像风格化迁移算法分支——快速图像风格化迁移算法主要解决速度问题**，核心思想就是利用**基于离线模型优化的快速图像重建方法**对风格化结果进行重建，基预先训练前向网络来解决计算量大、速度慢的问题。




根据一个训练好的前向网络能够学习到多少个风格作为分类依据，我们将快速图像风格化迁移算法分为**单模型单风格（PSPM）**、**单模型多风格（MSPM）**和**单模型任意风格（ASPM）**的快速风格化迁移算法。




**下面先放一张不同快速风格化方法的对比图：**














**2.1. 单模型单风格的快速风格化迁移算法（Per-Style-Per-Model Fast Neural Method）**




单模型单风格的快速风格化迁移算法是最早的一类快速风格化算法。主要想法是针对每一个风格图，我们去训练一个特定（style specific）的前向模型，这样当测试的时候，我们只需要向前向模型扔进去一张内容图，就可以前向出一个风格化结果了。




如此这般，工业化落地就非常方便了，直接将模型打包，做成一个 API，用户上传数据后直接把数据扔进去返回结果就好了（模型大小也不大，在淘宝 AI Team 的 @黄真川 哥指导下对模型进行优化并打包，最终用的时候一个 tf 模型才 0.99MB）。 




这一类算法（简称PSPM）其实可以再分成两类：**（1）一类是基于统计分布的参数化快速风格化PSPM算法以及（2）基于MRF的非参数化PSPM算法**。




 （1）这一小类算法代表性工作主要有两个，一个由斯坦福的 Justin Johnson 提出（CS231n 的大神 lecture）[42]***Perceptual Losses for Real-Time Style Transfer and Super-Resolution***，另一个由俄罗斯成立不久的 Skolkovo 科技研究所的 Ulyanov 提出（Deep Image Prior 的作者）[43]***Texture Networks: Feed-forward Synthesis of Textures and Stylized Images***。




这两个工作的思想相同，都是用一个前向网路求学一个风格。训练数据可以用 COCO 的 8 万张图，损失函数和祖师爷 Gatys 的慢速风格化算法相同，用 Gram 统计量来进行风格建模。




不同之处在于两个工作的**具体网络框架设计不同**，一个基于当时最新的残差网络设计的，一个是设计了多尺度的网络（我实验发现基于残差网络的设计能更好地最小化风格化损失函数，感兴趣的同学可以去看我 arXiv 上 Review 的实验部分）。




Ulyanov 后来又在 CVPR 2017 上对其之前的工作做了改进，他们发现 **Instance Normalization** 比 **Batch Normalization** 能够更快、更好地使模型达到收敛（其实就是把 batch normalization 的 batch size 设成 1），但其实 Instance Normalization 的 idea 最早是由 @Naiyan Wang 老师在 ***[1603.04779] Revisiting Batch Normalization For Practical Domain Adaptation ***中提出的 ，即文章中的 **Adaptive Batch Normalization (AdaBN)**。




 （2）第二小类基于 MRF 的快速 PSPM 风格化算法也是由 Chuan Li 学长提出的 [47]***Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks***，他们将自己之前提出的基于 patch 的慢速风格化算法进行了加速。同样是训练一个前向网络，Chuan Li 学长进一步利用 **GAN 中的判别网络**的想法来取代他们慢速风格化算法中的 patch 匹配（patch matching）过程。




这一工作的最终效果虽然不是特别理想（由我个人的审美来看），但这篇文章的理论价值很大，其想法在另一篇具有开创意义的文章，清华出身、BAIR PhD 的大牛朱俊彦的 ***Image-to-Image Translation with Conditional Adversarial Networks*** 中被进一步延伸和发展。




**2.2. 单模型多风格的快速风格化迁移算法（Multiple-Style-Per-Model Fast Neural Method）**




楼上的单模型单风格的快速风格化迁移算法对于每一个风格都要训练一个模型，这个就很不便利了，而且如果在工业落地的时候，有几百个风格的话还好说，要是有上万、百万的风格的话，所有模型占用的巨大空间开销就肯定会被产品经理拎出来说了。于是乎，学术圈的很多研究者开始研究咋**利用一个模型去学习很多个风格**，即**单模型多风格**的快速风格化迁移算法（下面简称 **MSPM**）。




大家可以设身处地想一下这个问题，要是需要咱们自己去解决这个问题，可以怎么去想。




首先把多个风格整合到一个模型中，理论上是合理的。比如咱们就说中国山水画，有很多著名山水画作品，但不同山水画虽然风格不尽相同，但是还是有很多相似的地方的（相似特征），所以对每一幅山水画训练得到的网络之间理论上是有**共享的部分**的。




于是乎在这种情况下，对每个风格都学习一个网络本来就是一件很冗余、浪费资源的事情。沿着这个思路想，我们能不能**发掘出不同风格网络之间共享的部分****，然后对于新的风格只去改变其有差别的部分，共享的部分保持不变呢？**




Bingo，这个就是 Google Brain 的众大佬们研究出来的一个 MSPM 算法的基本思路[48]***A Learned Representation for Artistic Style***。他们发现在训练好的一个风格化网络基础上，只通过在 Instance Norlization 层上做一个仿射变换（他们起了个名字叫 **Conditional Instance Normalization**，简称 **CIN**），就可以得到一个具有完全不同风格的结果。




这下好了，我们只需要把 CIN 层中仿射变换的很少的参数与每一个风格进行绑定，每个新风格只需要去训练这些参数，其余部分保持不变就 ok 了。




最后实验效果显著。但说实话这个有点类似科学发现的意味，CIN 层 work 的理由现在也没有严格的推导证明。一个大概的解释是 CIN 能够进行一种 style normalization，能够将图像中的风格直接 normalize 成另外一种风格。




另一个由 @微软亚洲研究院 与中科大的联合培养博士生 @陈冬冬 哥提出的 MSPM 法 [49]***StyleBank: An Explicit Representation for Neural Image Style Transfer*** 与 Google Brain 这篇思路有异曲同工之妙，核心思想为把风格化网络中间的几层单独拎出来（文章中起了个名字叫 **StyleBank 层**），与每个风格进行绑定，对于每个新风格只去训练中间那几层，其余部分保持不变。这里也有一篇 @微软亚洲研究院 的介绍文章：**AI 创造艺术风格化：从图片到视频 **[1]。 **其中特别感谢 @陈冬冬 哥帮我 forwarding 算法结果**。




ok，上面俩工作的共同点都是把网络的一部分拿出来与每个风格进行绑定，从而实现 MSPM，虽然随着风格的增加，模型大小不会大很多，但总归还是会跟着变大。所以呢，另外有一些研究者想，咱能不能试试完全用一个网络，看它能不能学到多个风格。




这时候需要考虑的问题是既然只用一个网络，那就需要给网络一个信号，我们需要风格化成哪一个风格。这一思路最早由 Amazon AI 的 @张航 在 2017 年 3 月提出 [51]***Multi-style Generative Network for Real-time Transfer***。 




该算法的核心思想是把通过 VGG 网络提取到的风格特征与风格化网络中的多个尺度的中间层的 feature map 通过提出的** Inspiration Layer** 结合在一起，相当于将风格特征作为信号输入到网络中来决定要风格化成哪一个风格。




最终算法的效果非常显著，由我自己实验结果来看 [51] 是质量上最接近 PSPM 算法结果的 MSPM 方法。在 @张航 的博文里有对这个方法更详细的介绍：**多风格生成网络——实时风格转换**[2]。**在此非常感谢 @张航 哥在训练模型上给予我的帮助**。




另外除了把风格特征作为信号外，另一个选择是把图像像素作为信号输入进去风格化网络。这一想法的可行性在浙大李一君学长的工作 [50] 中得到了证明 ***Diversified Texture Synthesis With Feed-Forward Networks***。李一君学长首先将每一张风格图与一个随机产生的噪声图进行绑定，然后将噪声图与风格化网络中间层的 feature map 进行 concat，作为网络进行风格选择的信号。




**2.3. 单模型任意风格的快速风格化迁移算法（Arbitrary-Style-Per-Model Fast Neural Method）**




有了上面的介绍，现在咱们已经能够做到用一个网络迁移多个风格了，虽然模型大小的问题一定程度上解决了，不过对于新的一组风格，我们仍然有额外训练时间的开销。于是有学者开始想，我们能不能搞一个模型出来，做到 **Zero-shot Fast Style Transfer**，即来一个新风格不需要训练，我们就可以很快速地把风格化结果输出来？（这里我们称之为单模型任意风格，简称 **ASPM**）。




最早的 ASPM 算法由多伦多大学的 Tian Qi Chen（不是 UW 的明星博士生陈天奇）提出的 [52]***Fast Patch-based Style Transfer of Arbitrary Style***。这个算法是基于 patch 的，可以归到**基于 MRF 的非参数化 ASPM 算法**。




基本思想是在 CNN 特征空间中，找到与内容 patch 匹配的风格 patch 后，进行内容 patch 与风格 patch 的交换（**Style Swap**），之后用快速图像重建算法的思想对交换得到的 feature map 进行快速重建。但由于 style swap 需要一定的时间开销，[52] 没有达到实时。




第一篇能达到实时的 ASPM 算法由康奈尔的大牛 @Xun Huang 提出 [46]***Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization***。[46] 中的工作主要受到 MSPM 的 CIN 层启发，提出一个 **Adaptive Instance Normalization (AdaIN)**。




AdaIN 的输入是通过 VGG 提取的风格和内容特征，用数据驱动的方式，通过在大规模风格和内容图上进行训练，让 AdaIN 能够直接将图像中的内容 normalise 成不同的风格。这一工作录用为 ICCV 2017 的 Oral。**在此非常感谢 @Xun Huang 哥在实验过程中给予的帮助**。




另外一个数据驱动的 ASPM 方法由 CIN 的提出者——Google Brain 提出 [53] ***Exploring the Structure of a Real-time, Arbitrary Neural Artistic Stylization Network***。




[53] 可以看做是 [48] 的一个 follow-up 的工作，既然通过改变 CIN 层中仿射变换的参数，就可以得到不同的 style，换言之，只要任意给一个风格，我们只需要知道他的 CIN 层中的仿射变换的参数就可以了。




沿着这个思路，Google Brain 的研究者设计和训练了一个 **style prediction network** 去专门预测每个 style 的**仿射变换的参数**，style prediction network 需要大规模 style 和 content 图来进行训练。




这个方法的缺点也很明显，数据驱动的方式不可避免地导致风格化效果与训练数据集中 style 的种类和数量非常相关。




由以上数据驱动 ASPM 算法的局限性，李一君学长进一步思考能不能用一种不需要学习训练的方式（**style learning-free**），而是单纯使用一系列特征变换来进行 ASPM 风格迁移 [54]***Universal Style Transfer via Feature Transforms***。




李一君学长发现在 VGG 提取的特征上用 **ZCA whitening transform** 能够把一张图片的风格信息抹去，而保留原有高级语义信息，之后应用 **coloring transform** 将风格图的颜色进行迁移，即可重建出效果不错的风格化结果。这一工作发表于 NIPS 2017 上，也是很少见的一篇 NIPS 上发表的 application 类的文章，足见学术界对 Neural Style Transfer 的关注。




# 结语




由以上讨论，大家可以发现，其实在图像风格化迁移这一领域的很多代表性工作中都有着中国研究者的身影。这些工作经过进一步地传承与发展，最终形成了这一全新的领域。前一阵子看到 @周博磊 老师的谈华人学者 leadership 问题的文章时正好在准备这篇 Review，当时很有感触，在此贴上 @周博磊 老师的文章链接与大家共勉：**从 CVPR‘18 谈谈华人研究者的 Leadership**[3]。




# 有趣的扩展应用









还是主要围绕这张框架图，下面主要对这里面的 Extensions 中的每一个部分上图展示（以下所有图 Copyright 由论文作者所有，相应论文详见 arXiv）。




**语义风格迁移**






**▲ **Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis







**▲ **Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork 







**▲ **Decoder Network Over Lightweight Reconstructed Feature for Fast Semantic Style Transfer




**涂鸦变油画**






**▲ **Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork




**肖像风格迁移**






**▲ **Painting Style Transfer for Head Portraits Using Convolutional Neural Networks




**字符风格迁移**






**▲ **Awesome Typography: Statistics-based Text Effects Transfer






**▲ **Multi-content GAN for Few-shot Font Style Transfer



**属性风格迁移**





**▲ **Visual Attribute Transfer through Deep Image Analogy




**保留深度信息的图像风格化迁移**






**▲ **Depth-Preserving Style Transfer




**图像颜色风格迁移**






****▲ ****Deep Photo Style Transfer






****▲ ****A Closed-form Solution to Photorealistic Image Stylization




**基于图像风格化的样稿上色**






****▲ ****Style Transfer for Anime Sketches with Enhanced Residual U-net and Auxiliary Classifier GAN




**立体风格迁移**






****▲ ****Stereoscopic Neural Style Transfer




**基于风格化迁移的面部特征变换**






****▲ ****Deep Feature Interpolation for Image Content Changes




**艺术品补丁和谐化**










****▲ ****Deep Painterly Harmonization




**服饰风格化迁移**






****▲ ****Fashion Style Generator




最后，用几组效果很好的风格化迁移结果图来为这篇文章拉下帷幕。
























# 相关链接




[1]. https://zhuanlan.zhihu.com/p/27512619

[2]. https://zhuanlan.zhihu.com/p/25892708

[3]. https://zhuanlan.zhihu.com/p/36169261

[4]. L.A. Gatys, A.S.Ecker, and M.Bethge,“Image style transfer using convolutional neural networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2414–2423.

[27]. L.A. Gatys, A.S. Ecker, and M. Bethge, “Texture synthesis using convolutional neural networks,” in Advances in Neural Information Processing Systems, 2015, pp. 262–270.

[37]. Y. Li, N. Wang, J. Liu, and X. Hou, “Demystifying neural style transfer,” in Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, 2017, pp. 2230–2236.

[40]. S. Li, X. Xu, L. Nie, and T.-S. Chua, “Laplacian-steered neural style transfer,” in Proceedings of the 2017 ACM on Multimedia Conference. ACM, 2017, pp. 1716–1724.

[41]. C. Li and M. Wand, “Combining markov random fields and convolutional neural networks for image synthesis,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2479–2486.

[42]. J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time style transfer and super-resolution,” in European Conference on Computer Vision, 2016, pp. 694–711.

[43]. D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky, “Texture networks: Feed-forward synthesis of textures and stylized images,” in International Conference on Machine Learning, 2016, pp. 1349–1357.

[46]. X. Huang and S. Belongie,“Arbitrary style transfer in real-time with adaptive instance normalization,” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1501–1510.

[47]. C. Li and M. Wand, “Precomputed real-time texture synthesis with markovian generative adversarial networks,” in European Conference on Computer Vision, 2016, pp. 702– 716.

[48]. V. Dumoulin, J. Shlens, and M. Kudlur, “A learned representation for artistic style,” in International Conference on Learning Representations, 2017.

[49]. D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua, “Stylebank: An explicit representation for neural image style transfer,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1897–1906.

[50]. Y. Li, F. Chen, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Diversified texture synthesis with feed-forward networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 3920–3928.

[51]. H. Zhang and K. Dana, “Multi-style generative network for real-time transfer,” arXiv preprint arXiv:1703.06953, 2017.

[52]. T. Q. Chen and M. Schmidt,“Fast patch-based style transfer of arbitrary style,” in Proceedings of the NIPS Workshop on Constructive Machine Learning, 2016.

[53]. G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens, “Exploring the structure of a real-time, arbitrary neural artistic stylization network,” in Proceedings of the British Machine Vision Conference, 2017.

[54]. Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Universal style transfer via feature transforms,” in Advances in Neural Information Processing Systems, 2017, pp. 385–395.









**点击以下标题查看其他文章：**




- 
[自适应注意力机制在Image Caption中的应用](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489065&idx=1&sn=ea3a885b18e07e457169f5e5685266a9&chksm=96e9c9a9a19e40bf8b802f452855b92d59e426d165004510fd99c9ff77fc5859ac712074002c&scene=21#wechat_redirect)

- 
[CVPR 2018值得一看的25篇论文，都在这里了](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488760&idx=1&sn=eda8baba99e3e5dfdcbaf6a2bf26f747&chksm=96e9cb78a19e426e70106410fa8ca1b48b75de5c2c3f94ea8af2603efadb9760f6bb895dd1f1&scene=21#wechat_redirect)

- 
[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487949&idx=1&sn=e09391933f3c4493cfb737b0ea2cf0af&chksm=96e9ce4da19e475b0c789088d403a0f49449b8ba0c43734aa835c5d2a7cb69c3d839c7ce056c&scene=21#wechat_redirect)[进击的YOLOv3，目标检测网络的巅峰之作](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489099&idx=1&sn=d2aa0e49cc72c9957c411afe7fb72e22&chksm=96e9c9cba19e40dda2281d2dd535ad5b7ad718aada74bd726290a1cbc9b1af36e92d5dfe52f5&scene=21#wechat_redirect)


- 
[Wasserstein距离在生成模型中的应用](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488555&idx=1&sn=d24b8b306c7e854ec7b2d3225a49aaa0&chksm=96e9cbaba19e42bdc1309a9ecade04d03e41d68935c710b50bbdcfef80632e502258401a3c4e&scene=21#wechat_redirect)


- 
[深度学习在CTR预估中的应用](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488284&idx=1&sn=53bb819e7403360cafb41853d60f98f2&chksm=96e9cc9ca19e458a30e40aa1540a82dac61528b13fd82e0c3a493e77494f512806103a715c68&scene=21#wechat_redirect)










[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)

**▲**戳我查看招募详情




**#****作 者 招 募#**



****[让你的文字被很多很多人看到，喜欢我们不如加入我们](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)****









**关于PaperWeekly**





PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。






▽ 点击 | 阅读原文| 进入作者知乎专栏




