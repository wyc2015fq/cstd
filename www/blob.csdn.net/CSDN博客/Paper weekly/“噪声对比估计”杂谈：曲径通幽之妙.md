# “噪声对比估计”杂谈：曲径通幽之妙 - Paper weekly - CSDN博客





2018年06月15日 12:10:24[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1161









![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhglryG74dIr2B1019Yibv9PAGsWGGYBiaoSGbK2kzUnbIsicCEiazKMticicR0MPtmr1ynDovFe2kGicSydcg/640)




作者丨苏剑林

单位丨广州火焰信息科技有限公司

研究方向丨NLP，神经网络

个人主页丨kexue.fm




说到噪声对比估计，或者“负采样”，大家可能立马就想到了 Word2Vec。事实上，它的含义远不止于此，噪音对比估计（NCE, Noise Contrastive Estimation）是一个迂回但却异常精美的技巧，它使得我们**在没法直接完成归一化因子（也叫配分函数）的计算时，就能够去估算出概率分布的参数**。本文就让我们来欣赏一下 NCE 的曲径通幽般的美妙。 




注：由于出发点不同，本文所介绍的“噪声对比估计”实际上更偏向于所谓的“负采样”技巧，但两者本质上是一样的，在此不作区分。




# 问题起源




问题的根源是难分难舍的指数概率分布。




**指数族分布**




在很多问题中都会出现指数族分布，即对于某个变量 x 的概率 p(x)，我们将其写成：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcR7N905II0S1URVbeZ4UVjZ9mlSbmLsTgBrX3ZA6ibTREmbTeP0jbsUOA/640)




其中 G(x) 是 x 的某个“能量”函数，而![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRL6JFuq9oGLnLDzjHEvojSHiaBDAr7TzAfXt4CTdsJH93uNDggNWmeSA/640)则是归一化常数，也叫配分函数。这种分布也称为“玻尔兹曼分布”。




在机器学习中，指数族分布的主要来源有两个。**第一个来源是 softmax**：我们做分类预测时，通常最后都会将全连接层的结果用 softmax 激活，这就是一个离散的、有限个点的玻尔兹曼分布了；**第二个则是来源于最大熵原理**：当我们引入某个特征并且已经能估算出特征的期望时，最大熵模型告诉我们其分布应该是特征的指数形式。




**难算的配分函数**




总的来说，指数族分布是非常实用的一类分布，不论是机器学习、数学还是物理领域，都能够碰见它。然而，它却有一个比较大的问题：不容易算，准确来说是配分函数不容易算。




具体来说，不好算的原因可能有两个。一个是计算量太大，比如语言模型（包括 Word2Vec）的场景，因为要通过上下文来预测当前词的分布情况，这就需要对几十万甚至几百万项（取决于词表大小）进行求和来算归一化因子，这种情况下不是不能算，而是计算量大到难以承受了。




另一种情况是根本算不出来，比如假设![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRV6syiaMVHHt9lib0MUl43UIhzt12jZQIfPF6iaWygJArLCduToOtuI0cA/640)，那么就有：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRsUC57Z4cw9CudDcAiciaKoG509f8W6rvGsjRXnwWOEHAytm7iaiczd64Kw/640)




这积分根本就没法简单地算出来呀，更不用说更加复杂的函数了。**现在我们也许能从这个角度感受到为什么高斯分布那么常用了，因为，因为，因为，换个分布就没法算下去了**……




在机器学习中，如果只是分类、预测，那么归一化因子算不算出来都无所谓，因为我们只要相对比较取出最大的那个。但是在预测之前，我们还面临着训练的问题，也就是参数估计，具体来说，G(x) 其实是含有一些位置参数 θ 的，准确来说要写成 G(x;θ)，那么概率分布就是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRKMsfp5zzsEWBOCSWw5NkAXzXZRia83YAVDkDhybWZPnGHnQqelic6lVQ/640)




我们要从 x 的样本中推算出 θ 来，通常我们会用最大似然，但是不算出 Z(θ) 来我们就没法算似然函数，也就没法做下去了。

# NCE登场




非常幸运的是，NCE 诞生了，它成功地绕开了这个困难。对于配分函数算不出来的情形，它提供了一种算下去的可能性；对于配分函数计算量太大的情形，它还提供了一种降低计算量的方案。 




**变成二分类问题**




NCE 的思想很简单，它希望我们将真实的样本和一批“噪声样本”进行对比，从中发现真实样本的规律出来。 




具体来说，能量还是原来的能量 G(x;θ)，但这时候我们不直接算概率 p(x) 了，因为归一化因子很难算。我们去算：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRrO5icPDjcIGlz5w9fcNlhcMbe60lJCfK0ApgDQkcdJFZO8kicVuNnbkw/640)




这里的 θ 还是原来的待优化参数，而 γ 则是新引入的要优化的参数。




然后，NCE 的损失函数变为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRTKia2qdrMChu8iabmbkZoBOyKVK61IVoV86kb3tYicXRYNXsgBlMKicqicw/640)




其中 p̃(x) 是真实样本，U(x) 是某个“均匀”分布或者其他的、确定的、方便采样的分布。 




说白了，**NCE 的做法就是将它转化为二分类问题，将真实样本判为 1，从另一个分布采样的样本判为 0**。




**等价于原来分布 **




现在的问题是，从 (5) 式估算出来的 θ，跟直接从 (3) 式的最大似然估计（理论上是可行的）出来的结果是不是一样的。 




**答案是基本一样的**。我们将 (5) 式中的 loss 改写为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRFhpn0Hyvz0oribWrT9hsj2Sp1gf0Iqkb1icQ8CmgLeasw9U0Yx3Cvp0w/640)




因为 p̃(x) 和 U(x) 都跟参数 θ，γ 没关，因此将 loss 改为下面的形式，不会影响优化结果：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRjaPAI6BZia5lEYicjRBW0uShI26CPib5XdBgHpQYoNsx6ab79phvqdpUA/640)




其中：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRkIlEgyiaw3rI59Vlsa0S3nOLcMGAWx4XSpFLuLcPycEb71QXKZrR5Fw/640)




(7) 式是 KL 散度的积分，而 KL 散度非负，那么当“假设的分布形式是满足的、并且充分优化”时，(7) 式应该为 0，从而我们有 p̃(y|x)=p(y|x)，也就是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRGGv3yVMma4Oiaeg8UQnVdfJLtKQJm99oK6TfTpaekBIP76080o8e5XA/640)




从中可以解得：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcR1oQ4hR2QibGjiaZTN7vMSbVlH5nDDDpicOdFkw6iaF6slZTjsH8mc0Owsw/640)




如果 U(x) 取均匀分布，那么 U(x) 就只是一个常数，所以最终的效果表明 γ−logU(x) 起到了 logZ 的作用，而分布还是原来的分布 (3)，θ 还是原来的 θ。




这就表明了 NCE 就是一种间接优化 (3) 式的巧妙方案：看似迂回，实则结果等价，并且 (5) 式的计算量也大大减少，因为计算量就只取决于采样的数目了。




# 一些插曲




一些跟 NCE 相关的话题，就都放在这里了。 




**NCE与负采样简述**




NCE 的系统提出是在 2010 年的论文 **Noise-contrastive estimation: A new estimation principle for unnormalized statistical models **中，后面训练大规模的神经语言模型基本上都采用 NCE 或者类似的 loss 了。



论文的标题其实就表明了 NCE 的要点：**它是“非归一化模型”的一个“参数估计原理”，专门应对归一化因子难算的场景**。 




但事实上，“负采样”的思想其实早就被使用了，比如就在 2008 年的 ICML 上，Ronan Collobert 和 Jason Weston 在发表的 **A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning **中已经用到了负采样的方法来训练词向量。




要知道，那时候距离 Word2Vec 发布还有四五年。关于词向量和语言模型的故事，请参考 licstar 的**《词向量和语言模型》**[1]。 




基于同样的为了降低计算量的需求，后来Google的Word2Vec也用上了负采样技巧，在很多任务下，它还比基于Huffman Softmax的效果要好，尤其是那个“词类比（word analogy）”实验。这里边的奥妙，我们马上就来分析。 




**Word2Vec**




现在我们落实到 Word2Vec 来分析一些事情。以 Skip Gram 模型为例，Word2Vec 的目标是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRcSyr1C6csauGH18OzANpcallDBMv2QohxICaD2sibRbDa84ov3ueVBw/640)




其中 ui，vj 都是待优化参数，代表着上下文和中心词的两套不同的词向量空间。显然地，这里的问题就是归一化因子计算量大，其中应对方案有 Huffman Softmax 和负采样。




这里我们不关心 Huffman Softmax，只需要知道它就是原来标准 Softmax 的一种近似就行了。我们来看负采样的，Word2Vec 将优化目标变为了：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRnCh42FuTngqRxQb6OmQx32881GEjCBq7LcTvZY5GkcswQgomTFoxFA/640)




这个式子看着有点眼花，总之它就是表达了**“语料出现的 Skip Gram 视为正样本，随机采样的词作为负样本”**的意思。 




首先最明显的是，(12) 式相比 (4)，(5) 式，少引入了 γ 这个训练参数，或者就是说默认了 γ=0，这允许吗？据说确实有人做过对比实验，结果显示训练出来的 γ 确实在 0 上下浮动，因此这个默认操作基本上是合理的。 




其次，对于负样本，Word2Vec 可不是“均匀地采样每一个词”，而是**按照每个词本身的总词频来采样**的。这样一来，(10) 式就变成了：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRU96Y5HYu2gibJWs0O9OsPXxnMR64rVMIJrNhX084MzqU7iajLc899hfg/640)



也就是说，最终的拟合效果是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRsBP9ib6LRXXycz6JYZEKWNyt8ZVYhE3nXoT8AG0QtEB9UibnCt8jbNJg/640)




大家可以看到，左边就是两个词的互信息。**本来我们的拟合目标是两个词的内积等于条件概率 p̃(wj|wi)（的对数），现在经过负采样的 Word2Vec，两个词的内积就是两个词的互信息**。 




现在大概就可以解释为什么 Word2Vec 的负采样会比 Huffman Softmax 效果要好些了。Huffman Softmax 只是对 Softmax 做了近似，它本质上还是在拟合 p̃(wj|wi)，而负采样技巧则是在拟合互信息![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRwZibAAOlsiajRIABXFdrUqRwdYTBbGSicrIKg29VOq8vymzRbmR5UibBEw/640)。




我们之后，Word2Vec 是靠词的共现来反应词义的，互信息比条件概率 p̃(wj|wi)更能反映词与词之间“真正的”共现关系。换言之，p̃(wj|wi) 反映的可能是“我认识周杰伦，周杰伦却不认识我”的关系，而互信息反映的是“你认识我，我也认识你”的关系，后者更能体现出语义关系。 




我之前构造的另一个词向量模型**《更别致的词向量模型（三）：描述相关的模型》**[2] 中也表明了，基于互信息出发构造的模型，能理论上解释“词类比（word analogy）”等很多实验结果，这也间接证实了，基于互信息的“Skip Gram + 负采样”组合，是 Word2Vec 的一个绝佳组合。




所以，根本原因不是 Huffman Softmax 和负采样本身谁更优的问题，而是它们的优化目标就已经不同。




# 列车已到终点站




本文的目的是介绍 NCE 这种精致的参数估算技巧，指出它可以在难以为完成归一化时来估算概率分布中的参数，原则上这是一种通用的方法，而且很可能，在某些场景下它是唯一可能的方案。




最后我们以 Word2Vec 为具体例子进行简单的分析，谈及了使用 NCE 时的一些细节问题，并且顺带解释了负采样为什么好的这个问题。




# 相关链接




**[1]. 词向量和语言模型**

http://licstar.net/archives/328

**[2]. 更别致的词向量模型（三）：描述相关的模型**

https://kexue.fm/archives/4671





![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)




**点击以下标题查看作者其他文章：**




- 
[从无监督构建词库看「最小熵原理」](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488802&idx=1&sn=eb35229374ee283d5c54d58ae277b9f0&chksm=96e9caa2a19e43b4f624eac3d56532cb9dc7ca017c9e0eaf96387e20e5f985e37da833fbddfd&scene=21#wechat_redirect)

- 
[基于CNN的阅读理解式问答模型：DGCNN](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488480&idx=1&sn=7bb9a4dd60680df5013670287a9e0cc2&chksm=96e9cc60a19e457618b2ffdea2a7e13ba172ea3fdfbfda07de53aae57126047f1b71a6969d76&scene=21#wechat_redirect)

- 
[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488480&idx=1&sn=7bb9a4dd60680df5013670287a9e0cc2&chksm=96e9cc60a19e457618b2ffdea2a7e13ba172ea3fdfbfda07de53aae57126047f1b71a6969d76&scene=21#wechat_redirect)[再谈最小熵原理：飞象过河之句模版和语言结构](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489564&idx=1&sn=a5d2191dd978e7917f06f66022ce8358&chksm=96e9c79ca19e4e8ac874f0fa5808b99b7323149d42a78fade690c6d60c0337ecabe043f616d1&scene=21#wechat_redirect)

- 
[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487949&idx=1&sn=e09391933f3c4493cfb737b0ea2cf0af&chksm=96e9ce4da19e475b0c789088d403a0f49449b8ba0c43734aa835c5d2a7cb69c3d839c7ce056c&scene=21#wechat_redirect)[再谈变分自编码器VAE：从贝叶斯观点出发](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488093&idx=1&sn=08a77550c0cc7309c34a0a38bad0bcba&chksm=96e9cddda19e44cb7ce6143a7990eb4fc47d114b55b564e727a014538402f7218fc89bf1f3c0&scene=21#wechat_redirect)


- 
[变分自编码器VAE：这样做为什么能成？](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488238&idx=1&sn=06ffb033332a54279e600c511e1c5c5f&chksm=96e9cd6ea19e44781ee1313b349e0e77631781a2a163e2fd845c841dc2200d988424bd73c4c7&scene=21#wechat_redirect)


- 
[简明条件随机场CRF介绍 | 附带纯Keras实现](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489378&idx=1&sn=0e0ed4424bb336022f36d8e2236f96cc&chksm=96e9c8e2a19e41f4d1fb67254ee3c057ce66a4eaa4084db89d53f314c833b73fb79b8ee3c0dd&scene=21#wechat_redirect)







![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmsvubgibQtWV5t7M3ETKt3bbXiaAothCErMicibic9QCUBpxkuibuht62MGcCTcLyAxqGrsUXbv254InDA/640?)

**▲**戳我查看招募详情




**![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)****#****作 者 招 募#**



****[让你的文字被很多很多人看到，喜欢我们不如加入我们](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)****








**关于PaperWeekly**





PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgkXb8A1kiafKxib8NXiaPMU8mQvRWVBtFNic4G5b5GDD7YdwrsCAicOc8kp5tdEOU3x7ufnleSbKkiaj5Dg/640?)

▽ 点击 | 阅读原文| 进入作者博客




