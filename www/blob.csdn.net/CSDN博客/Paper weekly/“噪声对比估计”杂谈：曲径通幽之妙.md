
# “噪声对比估计”杂谈：曲径通幽之妙 - Paper weekly - CSDN博客


2018年06月15日 12:10:24[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1136


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhglryG74dIr2B1019Yibv9PAGsWGGYBiaoSGbK2kzUnbIsicCEiazKMticicR0MPtmr1ynDovFe2kGicSydcg/640)

作者丨苏剑林
单位丨广州火焰信息科技有限公司
研究方向丨NLP，神经网络
个人主页丨kexue.fm

说到噪声对比估计，或者“负采样”，大家可能立马就想到了 Word2Vec。事实上，它的含义远不止于此，噪音对比估计（NCE, Noise Contrastive Estimation）是一个迂回但却异常精美的技巧，它使得我们**在没法直接完成归一化因子（也叫配分函数）的计算时，就能够去估算出概率分布的参数**。本文就让我们来欣赏一下 NCE 的曲径通幽般的美妙。

注：由于出发点不同，本文所介绍的“噪声对比估计”实际上更偏向于所谓的“负采样”技巧，但两者本质上是一样的，在此不作区分。

# 问题起源

问题的根源是难分难舍的指数概率分布。

**指数族分布**

在很多问题中都会出现指数族分布，即对于某个变量 x 的概率 p(x)，我们将其写成：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcR7N905II0S1URVbeZ4UVjZ9mlSbmLsTgBrX3ZA6ibTREmbTeP0jbsUOA/640)

其中 G(x) 是 x 的某个“能量”函数，而![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRL6JFuq9oGLnLDzjHEvojSHiaBDAr7TzAfXt4CTdsJH93uNDggNWmeSA/640)则是归一化常数，也叫配分函数。这种分布也称为“玻尔兹曼分布”。

在机器学习中，指数族分布的主要来源有两个。**第一个来源是 softmax**：我们做分类预测时，通常最后都会将全连接层的结果用 softmax 激活，这就是一个离散的、有限个点的玻尔兹曼分布了；**第二个则是来源于最大熵原理**：当我们引入某个特征并且已经能估算出特征的期望时，最大熵模型告诉我们其分布应该是特征的指数形式。

**难算的配分函数**

总的来说，指数族分布是非常实用的一类分布，不论是机器学习、数学还是物理领域，都能够碰见它。然而，它却有一个比较大的问题：不容易算，准确来说是配分函数不容易算。

具体来说，不好算的原因可能有两个。一个是计算量太大，比如语言模型（包括 Word2Vec）的场景，因为要通过上下文来预测当前词的分布情况，这就需要对几十万甚至几百万项（取决于词表大小）进行求和来算归一化因子，这种情况下不是不能算，而是计算量大到难以承受了。

另一种情况是根本算不出来，比如假设![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRV6syiaMVHHt9lib0MUl43UIhzt12jZQIfPF6iaWygJArLCduToOtuI0cA/640)，那么就有：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRsUC57Z4cw9CudDcAiciaKoG509f8W6rvGsjRXnwWOEHAytm7iaiczd64Kw/640)

这积分根本就没法简单地算出来呀，更不用说更加复杂的函数了。**现在我们也许能从这个角度感受到为什么高斯分布那么常用了，因为，因为，因为，换个分布就没法算下去了**……

在机器学习中，如果只是分类、预测，那么归一化因子算不算出来都无所谓，因为我们只要相对比较取出最大的那个。但是在预测之前，我们还面临着训练的问题，也就是参数估计，具体来说，G(x) 其实是含有一些位置参数 θ 的，准确来说要写成 G(x;θ)，那么概率分布就是：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRKMsfp5zzsEWBOCSWw5NkAXzXZRia83YAVDkDhybWZPnGHnQqelic6lVQ/640)

我们要从 x 的样本中推算出 θ 来，通常我们会用最大似然，但是不算出 Z(θ) 来我们就没法算似然函数，也就没法做下去了。
# NCE登场

非常幸运的是，NCE 诞生了，它成功地绕开了这个困难。对于配分函数算不出来的情形，它提供了一种算下去的可能性；对于配分函数计算量太大的情形，它还提供了一种降低计算量的方案。

**变成二分类问题**

NCE 的思想很简单，它希望我们将真实的样本和一批“噪声样本”进行对比，从中发现真实样本的规律出来。

具体来说，能量还是原来的能量 G(x;θ)，但这时候我们不直接算概率 p(x) 了，因为归一化因子很难算。我们去算：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRrO5icPDjcIGlz5w9fcNlhcMbe60lJCfK0ApgDQkcdJFZO8kicVuNnbkw/640)

这里的 θ 还是原来的待优化参数，而 γ 则是新引入的要优化的参数。

然后，NCE 的损失函数变为：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRTKia2qdrMChu8iabmbkZoBOyKVK61IVoV86kb3tYicXRYNXsgBlMKicqicw/640)

其中 p̃(x) 是真实样本，U(x) 是某个“均匀”分布或者其他的、确定的、方便采样的分布。

说白了，**NCE 的做法就是将它转化为二分类问题，将真实样本判为 1，从另一个分布采样的样本判为 0**。

**等价于原来分布**

现在的问题是，从 (5) 式估算出来的 θ，跟直接从 (3) 式的最大似然估计（理论上是可行的）出来的结果是不是一样的。

**答案是基本一样的**。我们将 (5) 式中的 loss 改写为：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRFhpn0Hyvz0oribWrT9hsj2Sp1gf0Iqkb1icQ8CmgLeasw9U0Yx3Cvp0w/640)

因为 p̃(x) 和 U(x) 都跟参数 θ，γ 没关，因此将 loss 改为下面的形式，不会影响优化结果：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRjaPAI6BZia5lEYicjRBW0uShI26CPib5XdBgHpQYoNsx6ab79phvqdpUA/640)

其中：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRkIlEgyiaw3rI59Vlsa0S3nOLcMGAWx4XSpFLuLcPycEb71QXKZrR5Fw/640)

(7) 式是 KL 散度的积分，而 KL 散度非负，那么当“假设的分布形式是满足的、并且充分优化”时，(7) 式应该为 0，从而我们有 p̃(y|x)=p(y|x)，也就是：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRGGv3yVMma4Oiaeg8UQnVdfJLtKQJm99oK6TfTpaekBIP76080o8e5XA/640)

从中可以解得：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcR1oQ4hR2QibGjiaZTN7vMSbVlH5nDDDpicOdFkw6iaF6slZTjsH8mc0Owsw/640)

如果 U(x) 取均匀分布，那么 U(x) 就只是一个常数，所以最终的效果表明 γ−logU(x) 起到了 logZ 的作用，而分布还是原来的分布 (3)，θ 还是原来的 θ。

这就表明了 NCE 就是一种间接优化 (3) 式的巧妙方案：看似迂回，实则结果等价，并且 (5) 式的计算量也大大减少，因为计算量就只取决于采样的数目了。

# 一些插曲

一些跟 NCE 相关的话题，就都放在这里了。

**NCE与负采样简述**

NCE 的系统提出是在 2010 年的论文**Noise-contrastive estimation: A new estimation principle for unnormalized statistical models**中，后面训练大规模的神经语言模型基本上都采用 NCE 或者类似的 loss 了。

论文的标题其实就表明了 NCE 的要点：**它是“非归一化模型”的一个“参数估计原理”，专门应对归一化因子难算的场景**。

但事实上，“负采样”的思想其实早就被使用了，比如就在 2008 年的 ICML 上，Ronan Collobert 和 Jason Weston 在发表的**A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning**中已经用到了负采样的方法来训练词向量。

要知道，那时候距离 Word2Vec 发布还有四五年。关于词向量和语言模型的故事，请参考 licstar 的**《词向量和语言模型》**[1]。

基于同样的为了降低计算量的需求，后来Google的Word2Vec也用上了负采样技巧，在很多任务下，它还比基于Huffman Softmax的效果要好，尤其是那个“词类比（word analogy）”实验。这里边的奥妙，我们马上就来分析。

**Word2Vec**

现在我们落实到 Word2Vec 来分析一些事情。以 Skip Gram 模型为例，Word2Vec 的目标是：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRcSyr1C6csauGH18OzANpcallDBMv2QohxICaD2sibRbDa84ov3ueVBw/640)

其中 ui，vj 都是待优化参数，代表着上下文和中心词的两套不同的词向量空间。显然地，这里的问题就是归一化因子计算量大，其中应对方案有 Huffman Softmax 和负采样。

这里我们不关心 Huffman Softmax，只需要知道它就是原来标准 Softmax 的一种近似就行了。我们来看负采样的，Word2Vec 将优化目标变为了：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRnCh42FuTngqRxQb6OmQx32881GEjCBq7LcTvZY5GkcswQgomTFoxFA/640)

这个式子看着有点眼花，总之它就是表达了**“语料出现的 Skip Gram 视为正样本，随机采样的词作为负样本”**的意思。

首先最明显的是，(12) 式相比 (4)，(5) 式，少引入了 γ 这个训练参数，或者就是说默认了 γ=0，这允许吗？据说确实有人做过对比实验，结果显示训练出来的 γ 确实在 0 上下浮动，因此这个默认操作基本上是合理的。

其次，对于负样本，Word2Vec 可不是“均匀地采样每一个词”，而是**按照每个词本身的总词频来采样**的。这样一来，(10) 式就变成了：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRU96Y5HYu2gibJWs0O9OsPXxnMR64rVMIJrNhX084MzqU7iajLc899hfg/640)

也就是说，最终的拟合效果是：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRsBP9ib6LRXXycz6JYZEKWNyt8ZVYhE3nXoT8AG0QtEB9UibnCt8jbNJg/640)

大家可以看到，左边就是两个词的互信息。**本来我们的拟合目标是两个词的内积等于条件概率 p̃(wj|wi)（的对数），现在经过负采样的 Word2Vec，两个词的内积就是两个词的互信息**。

现在大概就可以解释为什么 Word2Vec 的负采样会比 Huffman Softmax 效果要好些了。Huffman Softmax 只是对 Softmax 做了近似，它本质上还是在拟合 p̃(wj|wi)，而负采样技巧则是在拟合互信息![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPsgibRLHPEPTH8XMTKPrcRwZibAAOlsiajRIABXFdrUqRwdYTBbGSicrIKg29VOq8vymzRbmR5UibBEw/640)。

我们之后，Word2Vec 是靠词的共现来反应词义的，互信息比条件概率 p̃(wj|wi)更能反映词与词之间“真正的”共现关系。换言之，p̃(wj|wi) 反映的可能是“我认识周杰伦，周杰伦却不认识我”的关系，而互信息反映的是“你认识我，我也认识你”的关系，后者更能体现出语义关系。

我之前构造的另一个词向量模型**《更别致的词向量模型（三）：描述相关的模型》**[2]中也表明了，基于互信息出发构造的模型，能理论上解释“词类比（word analogy）”等很多实验结果，这也间接证实了，基于互信息的“Skip Gram + 负采样”组合，是 Word2Vec 的一个绝佳组合。

所以，根本原因不是 Huffman Softmax 和负采样本身谁更优的问题，而是它们的优化目标就已经不同。

# 列车已到终点站

本文的目的是介绍 NCE 这种精致的参数估算技巧，指出它可以在难以为完成归一化时来估算概率分布中的参数，原则上这是一种通用的方法，而且很可能，在某些场景下它是唯一可能的方案。

最后我们以 Word2Vec 为具体例子进行简单的分析，谈及了使用 NCE 时的一些细节问题，并且顺带解释了负采样为什么好的这个问题。

# 相关链接

**[1]. 词向量和语言模型**
http://licstar.net/archives/328
**[2]. 更别致的词向量模型（三）：描述相关的模型**
https://kexue.fm/archives/4671

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)

**点击以下标题查看作者其他文章：**

[从无监督构建词库看「最小熵原理」](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488802&idx=1&sn=eb35229374ee283d5c54d58ae277b9f0&chksm=96e9caa2a19e43b4f624eac3d56532cb9dc7ca017c9e0eaf96387e20e5f985e37da833fbddfd&scene=21#wechat_redirect)
[基于CNN的阅读理解式问答模型：DGCNN](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488480&idx=1&sn=7bb9a4dd60680df5013670287a9e0cc2&chksm=96e9cc60a19e457618b2ffdea2a7e13ba172ea3fdfbfda07de53aae57126047f1b71a6969d76&scene=21#wechat_redirect)
再谈最小熵原理：飞象过河之句模版和语言结构
[再谈变分自编码器VAE：从贝叶斯观点出发](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488093&idx=1&sn=08a77550c0cc7309c34a0a38bad0bcba&chksm=96e9cddda19e44cb7ce6143a7990eb4fc47d114b55b564e727a014538402f7218fc89bf1f3c0&scene=21#wechat_redirect)

[变分自编码器VAE：这样做为什么能成？](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488238&idx=1&sn=06ffb033332a54279e600c511e1c5c5f&chksm=96e9cd6ea19e44781ee1313b349e0e77631781a2a163e2fd845c841dc2200d988424bd73c4c7&scene=21#wechat_redirect)

[简明条件随机场CRF介绍 | 附带纯Keras实现](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489378&idx=1&sn=0e0ed4424bb336022f36d8e2236f96cc&chksm=96e9c8e2a19e41f4d1fb67254ee3c057ce66a4eaa4084db89d53f314c833b73fb79b8ee3c0dd&scene=21#wechat_redirect)


![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmsvubgibQtWV5t7M3ETKt3bbXiaAothCErMicibic9QCUBpxkuibuht62MGcCTcLyAxqGrsUXbv254InDA/640?)
**▲**戳我查看招募详情

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)**\#****作 者 招 募****\#**

**[让你的文字被很多很多人看到，喜欢我们不如加入我们](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)**


**关于PaperWeekly**

PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgkXb8A1kiafKxib8NXiaPMU8mQvRWVBtFNic4G5b5GDD7YdwrsCAicOc8kp5tdEOU3x7ufnleSbKkiaj5Dg/640?)
▽ 点击 |阅读原文| 进入作者博客


