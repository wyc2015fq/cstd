
# 经典论文复现 | 基于深度学习的图像超分辨率重建 - Paper weekly - CSDN博客


2018年09月27日 12:21:19[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1002


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgm9RFr5icmiaj0bibJxUeIGdAFHNM4G6PJEiccw293RuVnOiadQ4zcdibdJa5FFfn0ZMgpbKib4AAKD8dm2w/640)

过去几年发表于各大 AI 顶会论文提出的 400 多种算法中，公开算法代码的仅占 6%，其中三分之一的论文作者分享了测试数据，约 54% 的分享包含“伪代码”。这是今年 AAAI 会议上一个严峻的报告。人工智能这个蓬勃发展的领域正面临着实验重现的危机，就像实验重现问题过去十年来一直困扰着心理学、医学以及其他领域一样。**最根本的问题是研究人员通常不共享他们的源代码。**

可验证的知识是科学的基础，它事关理解。随着人工智能领域的发展，打破不可复现性将是必要的。为此，**PaperWeekly 联手百度 PaddlePaddle 共同发起了****本次论文有奖复现**，我们希望和来自学界、工业界的研究者一起接力，为 AI 行业带来良性循环。

作者丨Molly
学校丨北京航天航空大学
研究方向丨计算机视觉

笔者本次选择复现的是汤晓鸥组 Chao Dong 的作品，这篇论文也是**深度学习应用在超分辨率重构上的开山之作**。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNnwwXNbdQ772RQQYAmfYLqqrC2Mx04JO4ibgLWFpDzmPRAgjQfsWiadlg/640)

**论文复现代码：**

http://aistudio.baidu.com/\#/projectdetail/23978

# 超分辨率重构

**单图像超分辨率重构（SR）可以从一张较小的图像生成一张高分辨率的图像。**显然，这种恢复的结果是不唯一的。可以这样直观地理解：远远看到一个模糊的身影，看不清脸，既可以认为对面走来的是个男生，也可以认为这是个女生。那么，当我想象对面人的长相时，会如何脑补呢？

**这就依赖于我们的先验知识。**假如我认为，一个穿着裙子的人肯定是个女生，而对面那个人穿着裙子，所以我认为那是个女生，脑补了一张女神脸。然而，如果我知道穿裙子的人不一定是女生，还可能是女装大佬。迎面走来那个人瘦瘦高高，所以我认为十有八九是个男孩子，就会脑补一个……

也就是说，**不同的先验知识，会指向不同的结果。我们的任务，就是学习这些先验知识。**目前效果最好的办法都是基于样本的（example-based）。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNlUDSeXGEfXbcODvhMY9YnJxIKNOW2BgicRzYnq8jWcF7pszibK6Py2Pw/640)
▲超分辨率重构的结果。SRCNN所示为论文提出的模型的结果，可以看出，边缘更加清晰。

**论文提出一种有趣的视角：****CNN 所构造的模型和稀疏编码方法（sparse coding based）是等价的。**稀疏编码方法的流程如下：

1. 从原始图片中切割出一个个小块，并进行预处理（归一化）。这种切割是密集的，也就是块与块之间有重叠；

2. 使用低维词典（low-resolution dictionary）编码，得到一个稀疏参数；

3. 使用高维词典（high-resolution dictionary）结合稀疏参数进行重建（换了个密码本）；

4. 将多个小块拼接起来，重合部分使用加权和拼接。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNG1sjlh0lan3PaYZiaXiaF3JbYLUDNHORsKVOH5uCV4yDbZv8vOiatB1Hg/640)

上图是卷积神经网络对应于稀疏编码的结构。对于一个低分辨率图像 Y，第一个卷积层提取 feature maps。第二个卷积层将 feature maps 进行非线性变换，变换为高分辨率图像的表示。最后一层恢复出高分辨率图像。

相比于稀疏编码，论文提出的模型是 end-to-end 的，便于优化。并且，不需要求最小二乘的解，运算速度更快。

# 模型构造和训练

**模型的结构**

这是一个 base-line 模型。如下图，f1=9,f2=1,f3=5,n1=64,n2=32，前两层使用 relu 作为激活函数。输入为图像的 Y 通道。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqN4tJhhVjQvodd2Kskz8q9WEurzplc6nbqGlEWWia31TwgglIOQ0slyicA/640)

为了减轻边界带来的影响，论文使用 valid 方式处理卷积的边界。所以模型输出的结果是比输入要小一点点的。 显然，这是一个 FCN 的网络。我们使用图像的一个个 patch 进行训练，在测试时输入为一整张图片。由于没有全连接层，输入图像的大小可以是任意的。

**训练数据**

为了使模型更好地收敛，我们在原始的训练数据集上面切出一系列 33 X 33 大小的图像进行训练，切割的步长为 14。也就是说，我们使用的训练集图像，是有互相重合的部分的。

**我们使用的是 timofte 数据集，共 91 张图片。**论文中进行对比试验的时候，使用的都是 ImageNet 数据集。相比于 timofte，ImageNet 数据集可以提供更丰富的样本，得到更好的训练结果。但是 91 张图片给出的样本已经很丰富了，并且模型本身参数也不多，还不至于过拟合。所以使用 ImageNet 对结果的提升比较有限。

我们进行论文复现的时候，考虑到计算资源限制，使用 timofte 数据集，可以得到相似的结果。 我们使用 set5 作为验证集，使用 set14 可以得到类似的结论。

```python
def
```
```python
read_data
```
```python
(self, data_path)
```
```python
:
```

```python
def
```
```python
data_reader
```
```python
()
```
```python
:
```

```python
for
```
```python
image
```
```python
in
```
```python
os.listdir(data_path):
```

```python
if
```
```python
image.endswith(
```
```python
'.bmp'
```
```python
):
```

```python
img = cv2.imread(os.path.join(data_path, image))
```

```python
yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)
```

```python
img_y, img_u, img_v = cv2.split(yuv)
```

```python
# 下面是切图的步骤
```

```python
j =
```
```python
0
```

```python
count =
```
```python
0
```

```python
while
```
```python
j+
```
```python
33
```
```python
< len(img_y):
```

```python
i =
```
```python
0
```

```python
while
```
```python
i+
```
```python
33
```
```python
< len(img_y[
```
```python
0
```
```python
]):
```

```python
img_patch = img_y[j:j+
```
```python
33
```
```python
, i:i+
```
```python
33
```
```python
]
```

```python
img_gth = img_patch[
```
```python
6
```
```python
:
```
```python
27
```
```python
,
```
```python
6
```
```python
:
```
```python
27
```
```python
].copy()
```

```python
img_blur = cv2.GaussianBlur(img_patch, (
```
```python
5
```
```python
,
```
```python
5
```
```python
),
```
```python
0
```
```python
)
```

```python
img_sumsample = cv2.resize(img_blur, (
```
```python
11
```
```python
,
```
```python
11
```
```python
))
```

```python
img_input = cv2.resize(img_blur, (
```
```python
33
```
```python
,
```
```python
33
```
```python
), interpolation=cv2.INTER_CUBIC)
```

```python
yield
```
```python
img_input, img_gth
```

```python
i+=
```
```python
14
```

```python
j+=
```
```python
14
```

```python
return
```
```python
data_reader
```
▲数据读取代码展示

**损失函数和模型评估**

我们使用 MSE 作为损失函数，即：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNKTwSGesV4wAtfwV3PEEKsu6z02F4pdU7xgmcr9Tm1Kic57rLwvU08jw/640)

其中，是模型中的所有参数。 如上文所述，我们使用 valid 方式处理边界，所以输出的图像比输入图像略小。计算损失值时，只使用输入图像中间和输出图像对应位置的部分进行计算。

由于超分辨率重建的结果是不唯一的，所以其结果的评估往往比较困难。**论文使用峰值信噪比（PSNR）作为模型的评价指标。**它和人眼的感受并不完全一致。可能会出现指标很高，但是人眼感受不太好的情况。但是，它仍然是广为接受的指标。 PSNR 的计算公式如下：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNKIpNpYyMSaziaicI8BCOz6rNbEGgEibeCMkW3S4dmibG8sIrAISjcWuUXA/640)

其中，n 为每像素的比特数，一般取 8。

**模型训练结果**

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqN3clD9z6aQWjjdSZd7OEicg5uG3f7kOia419Ez86IRDpjsOVKzERqSiciag/640)

当训练的 backprops 数达到![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNT5IRvDkvAmPG1aNiaLaHOO31FD3EKxmSCxt2rBlj884ltZOeaMjLtUw/640)时，模型 PSNR 值达到 32.39dB。而同样的迭代次数，如果使用 ImageNet 数据集，可以达到 32.52dB。也就是说，使用更大的数据集，可以取得更好的结果。

**使用更大的模型**

论文从卷积核个数、卷积核大小、卷积层数三个方面增大模型的复杂度，在 ImageNet 上面对比可以看出，更加复杂的模型，可以取得更优的结果。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNXjpEicEJficmRoibP7riaauVxicqa4hibL7xze5AsaUU8da4vYvUiaE3vl0HA/640)

**彩色图像上的实验**

在前面的实验中，论文使用图像的 Y 通道进行重建，其它通道使用双三次插值（bicubic）得到。下面进行彩色图像上的探索，包括：baseline（只使用 Y 通道），bicubic，YCbCr 格式的三通道直接输入，预训练 Y 再三个通道一起训练，预训练 CbCr 再三个通道一起训练，RGB 格式的三通道直接输入。对比这些实验，得到了一些非常有意思的结论。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNZLWGoDg7Qo2MtnWVVIxvXKGjhV3R8A7sp13O2do1XMorCn8P0GAJiaQ/640)

最令人瞩目的结论就是，**直接把 YCbCr 格式的三通道图像输入模型，结果竟然连 Bicubic 都不如**。作者认为这是由于 Y 和 CbCr 的特征差别较大，导致模型陷入局部最小值。

另外，使用预训练的结果继续训练，结果会比直接训练 YCbCr 三个通道好些，但是还是不如只用 Y 的。最好的结果出现在使用 RGB 的。也比较好理解，因为 RGB 三个通道图像相关性比较高。 需要注意的一点是，加上 CbCr 的结果只比只用 Y 通道的结果好一点点，可见色相和饱和度两个通道对超分辨率重建的帮助不大。

# 论文总结

论文提出了一种 FCN 模型，并指出它和基于稀疏编码的超分辨率重构方法是等价的，并进行了一些改进，对比结果。

但是笔者认为，有一些分析是比较牵强的。例如彩色图像的超分辨率重构，其 PSNR 除了直接使用 YCbCr 训练效果很差之外，其它相差都在 0.1 左右，甚至小于 0.1。这么小的差距，在 set5 验证集上面没有太多的说服力（set5 只有 5 张图片，而且这个结果是用 timofte 数据集训练的，也是一个比较小的数据集）。

**论文的主要意义其实还是在于开拓了一个新的方法，构造了一个新的超分辨率重构的框架。**

# 论文复现结果

**Baseline模型复现结果**

考虑到算力限制，使用 timofte 数据集进行训练。Scale Factor 为 3 。为了加快收敛速度，我们使用 AdamOptimizer，前两层学习率为![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNQoF7ta6wTOSCic86y1pWQNdJQs84Tne8dV3ibJ0upPkORjdKfZII5AuQ/640)， 最后一层为![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNsncWE49yga3gF7nM4WWsD46rX1mgQ0MkXVo01UEL5b1sypBnUiaXrzQ/640)。相比于 SGD，AdamOptimizer 收敛到一个更好的结果。

**这里有一个 trick，就是最后一层的学习率和前两层不同。**这个设置是非常重要的，在实际测试中发现，使用这样的设置，模型收敛更加稳定。而所有层使用相同的学习率时，很容易出现 model collapse。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNq8JVcpK756W6YfgD1icuZxUNgiaRNQCnhCSeIMjM61WSib8RPiaBSRS2pA/640)

图中横轴是反向传播次数（batchsize * batchnum）。纵轴是 PSNR 值（dB）。可见，收敛速度比论文中快了很多，效果也更好。经过 150 个 epoch 的迭代，最终在 set5 测试集上 PSNR 达到了 35.25dB。 看一下恢复出来的图片是什么样的。Y 通道使用 SRCNN 恢复，Cr,Cb 通道使用 bicubic 插值。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNfDFPj5UvNia5HLk7xS3N3cbqNxKP4nfjwF2jNwAEUibkDZv9etWyqicmQ/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNb3rqWeoZJh7bh5oUuEYGjvgNGGIBZnb7F101hd9fHr0Tub1fFkXF9g/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNUxfficegrk33JNDv1ncVfRYACkltchD7gpUFjfJxaeOFicZGSQFeBibFw/640)
![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNvotsa8xIHKiaoXYIicHxCAA8t27blVxguvgIgu5HBKrCLS4zh7YQLpHA/640)

另一张图片：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNAZsYItczDYKtZ21C6uG6knSYsTMs9oG05WC4GwybQ6nupSbV0oySnQ/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNLQRodArYNKyKvoM5tpDbgPHnVWtRYIANnibfb5ZnzmZembTdrnAD8EA/640)

对比一下高频细节，可见重构的效果还是不错的：上图为输入，下图为输出。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNxrVRPmzMccACwZ1XGu216gwHwSgMGvBayJ27fem6o45moxcmeQPibMA/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqN8TNbnD3ianenD6Bs5p1LXEO1R3wtoicct5k2wEDZbNGgwdFiaMbOIF0icw/640)

**构造更深的网络模型**

论文给出来的结果大同小异，结论是类似的。我们这里复现 filter size 改变的结果。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNjiauOa5Pdk3kKsAUO7IVP8QLxFglUJhRGTDLqTbEyXBU4JUNuMogX7A/640)

PSNR 曲线为：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNzibnibZm0UrAicLvkP2LyzhncPnKWAViau49TGhXWM0UQu5C8nOqX9sqDg/640)

使用 9-3-1 的模型结构，150 个 epoch 之后的 PSNR 值为 35.82。我们做一个对比图片，可以看出，虽然收敛速度、最终收敛结果不同（因为用的是不同的 Optimizor），但是得到的结论是一致的。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNdialFYhqCQkK5oEQyp7qvA79Uq5h7l1NqfxDUtelvNgGLCz9YnNhnIA/640)

论文中的结果为：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNqYicHwmXFuz5ROxupZnJGkYCXicdZPJ7LPCd4icEArLW2icxyFLKNtQcYw/640)

**三通道 RGB 训练结果**

输入图像为 RGB 三个通道。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNGx63ZicHAd5ARfx3k9ichemG4aFlndB3icJUapicjibZ4WRWvqbEZhe3SlQ/640)

模型训练比较困难，很容易出现 model collapse 。使用单通道的参数作为预训练参数，再此基础上进行训练。经过更长的迭代次数（200 个 epoch），PSNR 达到了 35.92。

看一下 3 个通道训练结果：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqN5pwkiaKhogwibVWRc1E0icWza13dggT3DPlNahCVP5I9th7dJEaSbpYuw/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNSgAxm2vhCCKscGg1pS4nab62p7kCTic0Giamn6aYHOhWvVicdnDTkboZw/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm8aVplc32163WjbCpChGqNcNzrQG44shOgicIHoKhHpLVzTl8W3ms7bDdlKEnSKSwcQOv4iaX1T4xQ/640)

对比一下高频细节，左图是输入，右图是输出：

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm8aVplc32163WjbCpChGqNTpByltbicdIZ9UsRad5kwWBTibhicq1RYbTibukcyy6TICrBnuCbCKUjqg/640)

# 总结

**SRCNN 网络是 CNN 应用在超分辨率重建领域的开山之作。**虽然论文尝试了更深的网络，但是相比于后来的神经网络，如 DRCN 中的网络，算是很小的模型了。受限于模型的表达能力，最终训练的结果还有很大的提升空间。

另外，虽然相比于 sparse coding 方法，SRCNN 可以算是 end to end 方法了。但是仍然需要将图片进行 bicubic 差值到同样大小。此后的 ESPCN 使用 sub-pixel convolutional layer，减少了卷积的运算量，大大提高了超分辨率重建的速度。

在复现的过程中，笔者发现 SGD 收敛速度相当慢，论文中曲线横轴都是数量级。使用 Adam 优化器，收敛速度更快，并且几个模型的 PSNR 值更高。说明使用 SGD 训练时候，很容易陷入局部最优了。

# 关于PaddlePaddle

笔者目前只用到 PaddlePaddle 一些较为基础的功能，看介绍说 program 是特色但是本人在复现过程中并没有用到。

整体使用感受跟 TensorFlow 较为相似，数据读取那个部分较之 TensorFlow 更为方便好用，超赞！另外，可能PaddlePaddle目前是在进行版本更替，本人看到很多函数有重复和不兼容的，略感迷茫。

**（小道消息：版本更替大动作即将现身）**

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)


**点击标题查看更多论文解读：**

[ECCV 2018最佳论文：基于解剖结构的面部表情生成](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491543&idx=1&sn=e40163badcbeb30cc079c659677b04b5&chksm=96e9c057a19e4941dbf6fea886067f10e7ba4a62b8ea684e2d8852045125f201c0d90d63207e&scene=21#wechat_redirect)
[神经网络架构搜索（NAS）综述](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491248&idx=1&sn=d2946d8a37f7c6567b1a767a497006fb&chksm=96e9c130a19e48267f72ad32c527ec4a1697741e409d865d9233c5d7035a1f66a59b5e40792d&scene=21#wechat_redirect)
ECCV 2018 | 基于深度学习的视频显著性方法

[ECCV 2018 | 从单帧RGB图像生成三维网格模型](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491465&idx=1&sn=a6871b72d460debf90d2daa0bed719c8&chksm=96e9c009a19e491f8c247f36a53bad0a6812c462f3fb5b2d9ae74b38be673b946b82a4b44330&scene=21#wechat_redirect)
[ECCV 2018 | 基于三维重建的全新相机姿态估计方法](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491494&idx=1&sn=2922261ba0775f00ad67042dda355b52&chksm=96e9c026a19e493039bf5a90a1d523a46167df4058ee16f7f0797f10f6d392e0c98ef510acf2&scene=21#wechat_redirect)
[ECCV 2018 | 腾讯AI Lab提出视频再定位任务](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491347&idx=1&sn=e1f2cc16c9fcfcc5d2935118f09ee094&chksm=96e9c093a19e49855931cf621ec7f715c1d2dd5041e3343bc311aea2e5069ae03aaa1367e8f9&scene=21#wechat_redirect)
[杜伦大学提出GANomaly：无负例样本实现异常检测](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491630&idx=1&sn=394ffec2969cff23f63022526684f259&chksm=96ea3faea19db6b830104036bd176201ec17f3539608009bf2b17c4628331f4e4ea569b26eb9&scene=21#wechat_redirect)



![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)**\#****投 稿 通 道****\#**
**让你的论文被更多人看到**

如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？**答案就是：你不认识的人。**

总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。

PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是**最新论文解读**，也可以是**学习心得**或**技术干货**。我们的目的只有一个，让知识真正流动起来。

📝**来稿标准：**
• 稿件确系个人**原创作品**，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向）
• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接
• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志

**📬 投稿邮箱：**
• 投稿邮箱：hr@paperweekly.site
• 所有文章配图，请单独在附件中发送
• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通



🔍

现在，在**「知乎」**也能找到我们了
进入知乎首页搜索**「PaperWeekly」**
点击**「关注」**订阅我们的专栏吧


**关于PaperWeekly**

PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgkXb8A1kiafKxib8NXiaPMU8mQvRWVBtFNic4G5b5GDD7YdwrsCAicOc8kp5tdEOU3x7ufnleSbKkiaj5Dg/640?)
▽ 点击 |阅读原文| 收藏复现代码


