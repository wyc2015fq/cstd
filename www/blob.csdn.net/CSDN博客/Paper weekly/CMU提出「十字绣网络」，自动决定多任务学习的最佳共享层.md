# CMU提出「十字绣网络」，自动决定多任务学习的最佳共享层 - Paper weekly - CSDN博客





2018年05月29日 12:47:58[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：272

















在碎片化阅读充斥眼球的时代，越来越少的人会去关注每篇论文背后的探索和思考。





在这个栏目里，你会快速 get 每篇精选论文的亮点和痛点，时刻紧跟 AI 前沿成果。




点击本文底部的「**阅读原文**」即刻加入社区，查看更多最新论文推荐。
这是 PaperDaily 的第 **77** 篇文章


本期推荐的论文笔记来自 PaperWeekly 社区用户 **@Cratial**。多任务学习是机器学习的重要组成部分，但是对于应用深度学习进行多任务学习时会遇到一个棘手的问题，那就是我们**该如何确定网络的共享部分**。针对不同的任务，其最佳共享层往往不同。




此外，针对不同的多任务学习，我们需要根据任务需求设计不同的共享层，并没有统一的标准。因此，**本文针对这一问题设计了“十字绣”单元，通过端对端的学习来自动决定共享层**。

 如果你对本文工作感兴趣，点击底部**阅读原文**即可查看原论文。

# 关于作者：吴仕超，东北大学硕士生，研究方向为脑机接口、驾驶疲劳检测和机器学习。

■ 论文 | Cross-stitch Networks for Multi-task Learning

■ 链接 | https://www.paperweekly.site/papers/1969

■ 作者 | Ishan Misra / Abhinav Shrivastava / Abhinav Gupta / Martial Hebert




# 引出主题




多任务学习是机器学习的重要组成部分，但是在应用深度学习进行多任务学习时会遇到一个棘手的问题，那就是我们**该如何确定网络的共享部分**。针对不同的任务，其最佳共享层往往不同。 




**本文使用 AlexNet 网络分别进行图像检测、图像属性生成及图像语义分割、图像平面法向量生成（Surface Normal）等多任务学习**。针对两个任务，作者分别从 fc8 层进行共享，一直测试到两者完全不进行共享的为止。






**▲**图1




实验结果如图 1 所示，其中，图 1（b）是不同共享层完全独立时的效果对比，可以看出对于任务一来说，共享部分在 conv4 时所能达到的效果，针对于任务二来说，在 fc7 层的效果最好。




此外，针对不同的多任务学习，我们需要根据任务需求设计不同的共享层，并没有统一的标准。因此，**本文针对这一问题设计了“十字绣”单元，通过端对端的学习来自动决定共享层**。




# “十字绣”单元与网络设计




**Cross-stitch Unit**




本文的核心就在这里，设计“十字绣”单元的思想如图 2 所示，通过在两个网络的特征层之间增加“十字绣”单元可以使网络自动学习到需要共享的特征。






**▲**图2




其中的“十字绣”单元就是一个系数矩阵。其表达式如式（1）所示。









针对这个“十字绣”单元如何进行端对端学习呢？作者给了具体的计算公式：









在下文中，为了方便，将 αAB 及 αBA 统称为 αD 表示不同任务的权重值，αAA 及 αBB 统称为 αS 表示相同任务的权重值。




**“十字绣”网络设计**




图 3 是作者通过对网络添加“十字绣”单元设计的网络。






**▲**图3




网络设计好之后，作者又针对网络提出了以下几个问题： 




**1. 如何对“十字绣”单元进行初始化，及如何设置“十字绣”单元的学习速率？ **




作者认为初始值设置应该在 [0,1] 之间，此外，作者还针对初始值及学习速率的设计进行了实验，以决定如何对两者进行设计能够达到最优效果。




**2. 如何对网络 A 和 B 进行初始化？**



如何对 A、B 进行初始化呢？作者提出了两种方案，第一种就是网络全用由 ImageNet 训练出的参数进行初始化，然后对整个“十字绣”网络进行训练。第二种是针对一种任务在网络上进行微调，然后再添加“十字绣”单元，并对网络进行训练。 




**参数选取 **




针对 2.2 中提到的两个问题，作者分别尝试了不同的策略来对参数进行选择。实验结果如表 1 所示：






**▲**表1




在对“十字绣”单元进行训练时作者发现，用来更新网络参数的学习速率相对于“十字绣”单元而言太小了，以至于无法得到好的效果，实验结果如表 2 所示：






**▲**表2




关于如何对网络 A、B 进行初始化，作者同样进行了实验，实验结果如表 3 所示：






**▲**表3




表 4 展示了在 αD 及 αS 取不同的初始值时，最终网络各层各个通道的 αD 及 αS 的值的分布情况。其中 αD 越大说明共享程度越大，αS 越大，说明特征的特殊性越强。






**▲**表4




# 实验及结果分析




为验证本文算法的有效性，作者分别在 NYU-v2 数据集进行了图像语义分割及图片平面法向量生成实验，在 PASCAL VOC 2008 数据集上进行了物体检测和属性预测实验。 




作者分别针对单任务网络、结合两个单任务的网络（文中称为“ensemble”）、多任务网络（如图 1 中那样尝试在各个特征层共享特征）进行了实验。平面法向量生成及图像语义分割的实验结果如表 5 所示：






**▲**表5




从表 5 可以看出，使用“十字绣”单元的网络均能在两个任务上得到较好的结果。此外，作者分析在图像语义分割任务中，存在严重的数据匮乏问，如图 5 中黑线所示，wall、floor 的数据数量远超出其余的类别数，其中，蓝色柱状表示采用“十字绣”单元的多任务学习相对于单任务学习在效果上的增益，从图中可以看出，数量越少的类别所获得增益越多。






**▲**图5




图像检测及属性预测的实验结果及增益情况分别如表 6 及图 6 所示：






**▲**表6






**▲**图6





# 总结与分析




虽然本文作者提到他们的方法相对于传统方法不需要去依次尝试如何选取所要共享的特征层，但是增加的“十字绣”单元同样带来了很多麻烦的东西，例如，如何进行初始化，如何设置学习速率。最终的实验结果表明，使用该方法对多任务学习的性能有一定的提升。

**本文由 AI 学术社区 PaperWeekly 精选推荐，社区目前已覆盖自然语言处理、计算机视觉、人工智能、机器学习、数据挖掘和信息检索等研究方向，点击「阅读原文」即刻加入社区！**






**点击标题查看更多论文解读：**




- 
[Tree-CNN：一招解决深度学习中的灾难性遗忘](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488661&idx=1&sn=cf7fd1fbfdf347163ea056f0b31bbeba&chksm=96e9cb15a19e420333818d6c6b787ef02e85e44122fee5546f078cc0dfb35b8e82c7bcaa254b&scene=21#wechat_redirect)

- 
[深度神经网络模型压缩和加速都有哪些方法？](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488630&idx=1&sn=894b06c31b37ccdad3e9bfdd7323a33f&chksm=96e9cbf6a19e42e0c666d6727430a39fe4e09db047c3cfc0465a34923b87a36dfbe7585fe339&scene=21#wechat_redirect)

- 
[深度强化学习在指代消解中的一种尝试](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489287&idx=1&sn=fa4fe655a657a917f387e4b7008adc74&chksm=96e9c887a19e4191bf92d5462663c625691da3a6157449af220a45aa143c6339fca45702700c&scene=21#wechat_redirect)

- 
[综述：图像风格化算法最全盘点](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489172&idx=1&sn=42f567fb57d2886da71a07dd16388022&chksm=96e9c914a19e40025bf88e89514d5c6f575ee94545bd5d854c01de2ca333d4738b433d37d1f5&scene=21#wechat_redirect)

- 
[5 篇 AAAI 2018 论文看「应答生成」](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489246&idx=1&sn=a5180ae78bb1a072d4106794dbfb389a&chksm=96e9c95ea19e40485402a274cd4751b10e282e86ff8d582d2b000ce8665662d07872afc0e561&scene=21#wechat_redirect)

- 
[深度协同过滤：用神经网络取代内积建模](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489455&idx=1&sn=a4800237744e79aee2caced488e8e772&chksm=96e9c82fa19e413987a8f2bf5216c95e3df879bef5c66f579687fb0b094240c7ec16d93d2d63&scene=21#wechat_redirect)





[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488603&idx=2&sn=7320cb23efba3e7b5a381be83b7fe3ad&chksm=96e9cbdba19e42cd5840d3d51e86da4709b3d5273b2cf2512c32d84ab2b42ac4e7f13bf9ba63&scene=21#wechat_redirect)

[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)

**▲**戳我查看招募详情




**#****作 者 招 募#**



****[让你的文字被很多很多人看到，喜欢我们不如加入我们](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487954&idx=1&sn=d247e5b99ecb2c37e85d962d7f93d7d7&chksm=96e9ce52a19e474457e04affae41dc6b6fe521154f95ae7122260b46ec91f55ae7c8fb472c3c&scene=21#wechat_redirect)****







******关于PaperWeekly******




PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。






▽ 点击 | 阅读原文| 查看原论文




