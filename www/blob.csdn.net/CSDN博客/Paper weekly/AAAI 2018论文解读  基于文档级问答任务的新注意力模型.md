# AAAI 2018论文解读 | 基于文档级问答任务的新注意力模型 - Paper weekly - CSDN博客





2018年02月06日 00:00:00[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：1203












![640?wxfrom=5&wx_lazy=1](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgl7VHx00TkzicBMAfz1dFT8icD4HwmJZpt0Jiccw6ns7c3co7MpZslIia8VAuZicUTSuoPaq6hE4KbxWPg/640?wxfrom=5&wx_lazy=1)





在碎片化阅读充斥眼球的时代，越来越少的人会去关注每篇论文背后的探索和思考。





在这个栏目里，你会快速 get 每篇精选论文的亮点和痛点，时刻紧跟 AI 前沿成果。




点击本文底部的「**阅读原文**」即刻加入社区，查看更多最新论文推荐。
这是 PaperDaily 的第 **40** 篇文章


 本期推荐的论文笔记来自 PaperWeekly 社区用户** @duinodu**。**本文提出了一种端到端的、从问题出发的、多因素注意力网络，用来完成基于文档的问题回答任务。**这个模型可以从多个句子中收集分散的证据，用于答案的生成。

 如果你对本文工作感兴趣，点击底部的**阅读原文**即可查看原论文。

# 关于作者：杜敏，华中科技大学硕士生，研究方向为模式识别与智能系统。




■ 论文 | A Question-Focused Multi-Factor Attention Network for Question Answering

■ 链接 | https://www.paperweekly.site/papers/1597

■ 源码 | https://github.com/nusnlp/amanda




# 研究背景




基于阅读理解的回答问题系统中，机器需要通过理解一段文本，回答给定的一个问题。 从 2013 年到 2017 年出现了各种问题回答的数据集（NewsQA，TriviaQA, SearchQA, SQuAD…）。




大多数已有的解决方法，关注在问题和段落的关系（passage-question interaction），通过寻找相似的上下文来抽取文本作为答案。




**这类方法有两点不足：**




1. 不能通过多个句子合成答案所需要的材料，所以在很多开放 QA 数据集上表现不好。 




2. 没有显式地关注问题-答案的类型信息，而实际上，问题-答案的类型在 QA 中很重要。




**本文提出了一种端到端的、从问题出发的、多因素注意力网络，用来完成基于文档的问题回答任务**。这个模型可以从多个句子中收集分散的证据，用于答案的生成。




# 问题的数学描述



 QA 任务描述如下： 给定一组（文本 P，问题 Q），需要从文本 P 中抽取一个文本块（text span），作为问题的回答。




文本 P 表示为 (*P1,P2,...PT*)，问题 Q 表示为 (*Q1,Q2,...QU*)，T 和 U 分别是文本和问题的单词数。要回答问题，也就是要找到 *b,e*，其中 *1*≤*b*≤*e*≤*T*，输出的回答也就是 (*Pb,Pb+1,...Pe*)。




从上面的数学描述可以发现，这种 QA 给出的回答，只能是文本中出现的单词和句子，不能产生新的单词作为回答。




# 论文模型




模型有一些复杂，我们一点一点理解。 




**1. Word-level Embedding**




几乎所有的 NLP 问题，第一步都是做 embedding。**本文采取两种 embedding 方法：**



- 
**GloVe（Word）**



- 
**CNN-based（Char）**：把两种方式产生的 embedding vector 拼接起来，产生单词级的 embedding。图中有两列，分别表示对文本 P 和问题 Q 都进行相同处理。





![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmljGZZB0nBKVGn3C8tsT92kib1mIxnCIB6uOAYh6XWzkJ7Arucld3OLMA/640)




这一部分的输出是 TxH 和 UxH 的矩阵，T 和 U 表示文本和问题的长度，H 表示单词对应的向量长度。 




**2. Sequence-level Encoding**




第一步仅仅是对单个单词的处理，还需要在句子级进行处理。对序列数据建模的常见工具：LSTM，所以这部分主要是 LSTM，而且是 BiLSTM。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlHOcHo30GnDgAGgiaZpenVibw0MOaNkuicA0Xneu02YJRUROvtUwiceiaqdQ/640)




输出是分别是![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlcOeJSUqE875piaUoMiam1utNibClYCWED34hLwvibIRvngFw1KopFco5YA/640)和![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlxXtJWtDwwmcRIItfIPzvEHeMbBbbj15560A3yoicpKS6wYdfxYrboaA/640)，H 也是 BiLSTM 中隐含节点的个数。




**3. Cartesian Similarity-based Attention Layer**




本质上就是一个点积操作，目的是寻找 P 和 Q 中相似的部分。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlU1dbcmOe7Dfm7plGDghl4gKeuAcQjATBALOib2GSaMhSp9Plzic5LxoA/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmljBzGDKxgr1lCfjboOeRthdkgavF8VrWLjViceHib9QwLCTjKtbXoRwYg/640),![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlYLJNRZvQFlGiarjq4lXIicv95qcziaFBdprrJqmypbiaT2j8yHOv7rzibQQ/640)表示，文本的第 *i* 个单词，和问题的第 *j *个单词的相关性。能这样做的原因是因为第一步做了 embedding。




这个矩阵从列看过去，就能找到文本中哪些地方最可能出现答案，也就是应该注意的地方，所以* A* 也叫注意力矩阵（attention matrix）。当然这个矩阵也可以从行看过去，就能找到问题中哪些词是关键词，第 4 步就是这样做的。




**4. Question-dependent Passage Encoding**




用 *A* 从 *Q* 中取出问题中值得关注的词 *G*，把 *G* 加入到 *P* 中再做一次 LSTM 得到 *V*。由于 *V* 的计算过程考虑了 *Q*，所以把 *V* 叫做 question-dependent passage word encoding vector。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlhj7iaVIQwxrPKdEGs7TAE1tMPbE4CibVVibVNs7qPUntAlk7buAB0TEeQ/640)




**5. Multi-factor Attentive Encoding**




这部分的目标，是从文本中，把和问题相关的部分突显出来。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlsIUMicroNOSsDnrZMcqibEYXTv9ZMDfWJRZfxr8YnLFia9icIjUA33JeRw/640)




输入是![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmliaOAfDgTicIibD7eXXicPE9fJ72hc4XUHqibKKyk34TvVKplpSyfdXlpCgA/640)，输出![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGml0TeRicAx0UvhicAtMaH6dTInpSQ0jtQ3BzJQ8CukkQdFl3xo0iaSd0UlQ/640)。现在要对整个文本建模，这个文本的特点是很长（long context），一般的 RNN 或者 LSTM 难以胜任这样的任务，这里采用一种基于张量变换的方法。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlMd3zfWRmFlGiaoDSJLOHayDen14VqUXHGHDEfDmSRLyRamYqd6ufoTg/640)




如果把 *W* 去掉，这个式子很好理解，*V* 的每一行代表一个的单词，![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlmJDicPM4jkDGSlzohvZWVw3Pr8icGnZV2tYEQsJ1OibOj4ENaoQ9bxJ6Q/640)可以计算出两个单词的相关程度，然后再引入一个可训练的参数 *W*，使得这个相关性的计算可训练。还不够，再让这个计算重复多次，取最大值，最后再归一化。后面的实验找到最佳 *m* 取值为 3 或 4。**本文标题中的 Multi-factor 就是指这个 *m***。




F̃ 是注意力权重，用它和 *V* 相乘，就能挑出需要注意哪些单词了。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlU0ZmxIkLGJAeKT559Y99qks271YhiattZwGp8Er6dkS2JicTicicTN0CKA/640)




这里可以认为是一种残差结构，concat(*x*, *f*(*x*))。在网络结构设计时使用这种设计的好处是，如果 *f* 损害了 *x* 的表达性，旁路的 *x* 依然可以使用，这样的网络更健壮。 




还差一步，文中说，为了控制 M̃ 的影响，用 M̃ 和控制因子按元素相乘。这个控制因子是用一个一层的前馈神经网络计算得到。这里面又有类似残差的想法。 




**6. Question-focused Attentional Pointing**




前面做了那么多工作，都是为了得到![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlxNWZC16wrIs6ibnQRXOSQwC6refVYBsFYSybFl9ow5Cw5X2CbqZTNeA/640)，它表示文本每个单词和问题以及和文本中其他词的相关程度。最后要做的，是如何得到答案。这一步，再次使用了 question，所以叫做 question-focused。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlQGmNicibendSu482euWBRmeWTVVIwprOzGF7IvmNQCubPwrVfvJico2Xw/640)




用注意力权重 *A* 找到 *Q* 中的关键词 *q*ma。再把问题的问句类型考虑进来，所谓的问句类型，指的是（what, who, how, when, which, where, why）里面的哪个，如果都没有，就取前两个单词，这样计算得到 *q*f，然后把两个 *q* 拼接，用神经元融合一下，这里的神经元融合指的是 sigmoid 或 tanh(xW+b)。 




最后，用两个 BiLSTM 寻找最终要找的 *b* 和 *e*。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlvqkm60yhEX4pKIWr5zN7s5B8xT1A4BlYOujh1nAXpibS7ibia1hGzulvA/640)




# 实验




**对比实验**




本文在 NewsQA，TriviaQA 和 SearchQA 数据集上进行对比实验，实验证明本文模型均比其他方法的效果好。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGml8PV6XXWBtoVNHoLUF2hRskYUcZgzcmfJxYmTibYzlHyjYbXarvkW7Gg/640)




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlUveQpTKbgMFKBWYtHJ4WbAlQrGdxUebM2LN4PV4tD8Ugcp7mmRO3Iw/640)




**分析实验**




用实验分析并验证模型的每一个部分对结果的影响。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlOAny2M9Jb13ibss01iaYAP9jXtYDhFbib5shVjuMUlhaszufZ4zqrFr8g/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlIv2YzhO4rQbQD6cMlPD5oBMrlKWNic49TqEAfaDIXBicHJ5xibmGQSMfA/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGml75RgwZqTib6muMhOxvsyt0PTu21Mgw7dJicWTHNM22Xpvkz6Ea713OBA/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuzrdagib7Mia00TbiaOBCGmlOVLfVQ9TnGsuSiaOxVOGXybnmQFuECQJlJhqQT8ibeq89kHKAkfy97Cw/640)




# 个人评价




本文模型真的有些复杂，有很多网络设计方法是值得学习的： 



- 
concat + 神经元融合 



- 
反复使用“残差”来设计网络 



- 
网络中引入张量





**本文由 AI 学术社区 PaperWeekly 精选推荐，社区目前已覆盖自然语言处理、计算机视觉、人工智能、机器学习、数据挖掘和信息检索等研究方向，点击「阅读原文」即刻加入社区！**




我是彩蛋



**解锁新功能：热门职位推荐！**




PaperWeekly小程序升级啦




**今日arXiv√猜你喜欢√**热门职位****√****




找全职找实习都不是问题



** 解锁方式 **

1. 识别下方二维码打开小程序

2. 用PaperWeekly社区账号进行登陆

3. 登陆后即可解锁所有功能




** 职位发布 **

请添加小助手微信（**pwbot01**）进行咨询




**长按识别二维码，使用小程序**

*点击阅读原文即可注册




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnwLopkg177jgoQCbq2j2UJqSZOScYnsaSZf7ibXORdFOUEicycYycARG6V9pvHMyY7jYpdZFKpxcSQ/640?)








******关于PaperWeekly******




PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgl9qrwuXS7D8F2ZLyZNmqfWibCVlSbGBVCrd80blia0iaiaKuVk5p1tWP8tCaIiaYxiaQwiacIOlu9yOw6Mg/640?)

▽ 点击 | 阅读原文| 查看原论文




