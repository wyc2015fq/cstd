# 最小熵原理：“物以类聚”之从图书馆到词向量 - Paper weekly - CSDN博客





2018年12月19日 12:15:30[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：118









![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgm9RFr5icmiaj0bibJxUeIGdAFHNM4G6PJEiccw293RuVnOiadQ4zcdibdJa5FFfn0ZMgpbKib4AAKD8dm2w/640)




作者丨苏剑林

单位丨广州火焰信息科技有限公司

研究方向丨NLP，神经网络

个人主页丨kexue.fm




从第一篇看下来到这里，我们知道**所谓“最小熵原理”就是致力于降低学习成本，试图用最小的成本完成同样的事情。**所以整个系列就是一个“偷懒攻略”。那偷懒的秘诀是什么呢？答案是“套路”，所以本系列又称为“套路宝典”。




**“最小熵系列”前文回顾：**




[从无监督构建词库看最小熵原理，套路是如何炼成的](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488802&idx=1&sn=eb35229374ee283d5c54d58ae277b9f0&chksm=96e9caa2a19e43b4f624eac3d56532cb9dc7ca017c9e0eaf96387e20e5f985e37da833fbddfd&scene=21#wechat_redirect)


[再谈最小熵原理：“飞象过河”之句模版和语言结构](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489564&idx=1&sn=a5d2191dd978e7917f06f66022ce8358&chksm=96e9c79ca19e4e8ac874f0fa5808b99b7323149d42a78fade690c6d60c0337ecabe043f616d1&scene=21#wechat_redirect)





本篇我们介绍图书馆里边的套路。




先抛出一个问题：**词向量出现在什么时候？**是 2013 年 Mikolov 的 Word2Vec？还是 2003 年 Bengio 大神的神经语言模型？都不是，其实词向量可以追溯到千年以前，在那古老的图书馆中。




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm3IVG232BxxYricq5jdrO9ywQPiaNBADg9gbXNoHYWpic6FxPdZPO8VU6ZM6GfW0htH8nJicgic5r7Xug/640)

▲ 图书馆一角（图片来源于百度搜索）




# 走进图书馆




图书馆里有词向量？还是千年以前？在哪本书？我去借来看看。 




**放书的套路**




其实不是哪本书，而是放书的套路。 很明显，图书馆中书的摆放是有“套路”的：它们不是随机摆放的，而是分门别类地放置的，比如数学类放一个区，文学类放一个区，计算机类也放一个区；同一个类也有很多子类，比如数学类中，数学分析放一个子区，代数放一个子区，几何放一个子区，等等。**读者是否思考过，为什么要这么分类放置？分类放置有什么好处？跟最小熵又有什么关系？ **




有的读者可能觉得很简单：不就是为了便于查找吗？这个答案其实不大准确。如果只是为了方便找书，那很简单，只要在数据库上记录好每一本书的坐标，然后在地面上也注明当前坐标，这样需要借哪本书，在数据库一查坐标，然后就可以去找到那本书了，整个过程不需要用到“图书分类”这一点。所以，如果单纯考虑找书的难易程度，是无法很好的解释这个现象。




**省力地借书**




其实原因的核心在于：**我们通常不只是借一本书。**




前面说了，只要建好索引，在图书馆里找一本书是不难的，问题是：如果找两本呢？一般情况下，每个人的兴趣和研究是比较集中的，因此，如果我要到图书馆借两本书，那么可以合理地假设你要借的这两本书是相近的，比如借了一本《神经网络》，那么再借一本《深度学习》的概率是挺大的，但再借一本《红楼梦》的概率就很小了。




借助于数据库，我可以很快找到《神经网络》，那么《深度学习》呢？如果这本书在附近，那么我只需要再走几步就可以找到它了，如果图书是随机打乱放置的，我可能要从东南角走到西北角，才找到我想要的另一本书《深度学习》，再多借几本，我不是要在图书馆里跑几圈我才能借齐我要的书？ 




这样一来，图书分类的作用就很明显了。**图书分类就是把相近的书放在一起，而每个人同一次要借的书也会相近的，所以图书分类会让大多数人的找书、借书过程更加省力。**这又是一个“偷懒攻略”。




也就是说，将我们要处理的东西分类放好，相近的放在一起，这也是满足最小熵原理的。生活中我们会将常用的东西分类放在触手可及的地方，也是基于同样的原理。




# 图书馆规划




下面我们再来从数学角度，更仔细地考察这个过程。 




**简化的借书模型**




假如我们到图书馆去借两本书，分别记为 i,j，假设借第一本书的成本是 d(i)，两本书之间的成本函数为 d(i,j)，这也就是说，找到第一本书 i 后，我就要再花 d(i,j) 那么多力气才能找到第二本书 j。我们可以考虑这个过程对所有人的平均，即：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9ySYBkqDkerfDY0V9C1OnJvMpZI9pz1pJdVL2shuCPFPKkRyFu6kVB7g/640)




其中 p(i) 是 i 这本书被借的概率，p(j|i) 就是借了 i 之后还会再借 j 的概率。图书馆的要把书放好，那么就要使得 S 最小化。




现在我们以图书馆入口为原点，在图书馆建立一个三维坐标系，那么每本书的位置都可以用一个向量 v 来表示，不失一般性，我们可以简单考虑 d(i) 为这本书到图书馆原点的欧氏距离，d(i,j) 为两本书的欧氏距离，那么 S 的表达式变为：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yF2CPOHJfxFKOgaupwenJU7fAnDjuw2rormv78JDboTkSg9d9Z175RA/640)




让我们再来解释一下各项的含义，其中 (i,j) 代表着一种借书习惯，即借了书 i 还借书 j，p(i,j) 代表着这种借书习惯出现的概率，实际生活中可以通过图书馆的借书记录去估算它；‖vi‖＋‖vi−vj‖ 则代表着先借 i 再借 j 的总成本。其中 ‖vi‖ 这一项要尽量小，意味着我们要将热门的书放在靠近出口（原点）的地方；而 ‖vi−vj‖ 要尽量小，则告诉我们要把相近的书放在一起。




**约束优化规划**




假如我们拿到了图书馆的借书记录，也就是说已知 p(i,j) 了，那么是不是可以通过最小化 (2) 来得到图书馆的“最佳排书方案”了呢？思想对了，但还不完整，因为很显然式 (2) 的最小值是 0，只需要让所有的 v 都等于 0，也就是说，所有的书都挤在出口的位置。 




显然这是不可能的，因为实际上书不是无穷小的，两本书之间有一个最小间距 dmin>0，所以完整的提法应该是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yC28GE0wuToEMHGj6aG3hpnqLXaYG4nXzqczQgKib84zz6OSicnkfibnBA/640)




也就是说，这是一个带约束的极值问题，解决了这个问题，我们就可以得到图书馆对图书的最合理安排了（理论上）。当然，如果真的去给图书馆做规划，我们还要根据图书馆的实际情况引入更多的约束，比如图书馆的形状、过道的设置等，但 (3) 已经不妨碍我们理解其中的根本思想了。




# 一般成本最小化




现在我们再将问题一般化，从更抽象的视角来观察问题，能得到更深刻的认识。




**均匀化与去约束**




我们先将成本函数 ‖vi‖＋‖vi−vj‖ 代换为一般的 f(vi,vj)，即考虑：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yrwtia3yaxISHGW2Ru9aDicibcmLVTOErhLmania0vRb6M2RKtUyGsTibibHw/640)




同时 v 可以不再局限为 3 维向量，可以是一般的 n 维向量。我们依旧是希望成本最低，但是我们不喜欢诸如 ‖vi−vj‖≥dmin 的约束条件，因为带约束的优化问题往往不容易求解，所以如果能把这个约束直接体现在 f 的选择中，那么就是一个漂亮的“去约束”方案了。




怎么实现这个目的呢？回到图书馆的问题上，如果没有约束的话，理论最优解就是把所有图书都挤在出口的位置，为了防止这个不合理的解的出现，我们加了个约束“两本书之间有一个最小间距 dmin>0”，防止了解的坍缩。其实有很多其他约束可以考虑，**比如可以要求所有图书必须尽量均匀地放满图书馆，在这个希望之下，也能够得到合理的解。**




“尽量均匀”其实可以理解为某种归一化约束，因为归一，所以不能全部集中在一点，因为只有一点就不归一了。“归一”启发我们可以往概率的方向想，也就是说，先构造概率分布，然后作为成本函数的度量。在这里就不做太多牵强的引导了，直接给出其中一个选择：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9y9MueRJsJFiazyQvaNnk34b2K1Hj3Qr1Mux8zfOIWLricBNM5w26gVxbA/640)




**最小熵=最大似然**




让我们来分步理解一下这个式子。首先如果不看分母 Zi，那么结果就是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yKicvAfcpQia5iby6AaIib3CQxFbgenBeQMkhiaKR0x5RTmibm1mCb6qJiaMdw/640)




也就是说，这个 f 相当于成本函数为![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yViciaAGxzgmribh0lxq7EMIv6MAtw7aibu7gsuY6homJTk7fnhRgWdWsVQ/640)。然后，由于分母的存在，我们知道：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9y6FiboITjficxxynL49icib5KE6SsVOo5QtyibbsMuLCvapPvWcKdtxNVb9A/640)




所以![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9ybzyySkSdjyX9z1Uew4MRkYtAM7WJV8vo0n1uDmeB5dN8eol60tbLYw/640)实际上定义了一个待定的条件概率分布 q(j|i)，说白了，这实际上就是对![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yYAzzAOQFV0bbfs0FSTNydFwQ2cfySGceicoVnXseKKW0jerh1uF9EeA/640)的一个 softmax 操作，而此时 (4) 实际上就是：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yfwxtEUCowBC1X7CmVsrkCYFgoBsIMMz07sXwiauEZygsa25JVCKSQQg/640)




对于固定的 i 而言，最小化上式这不就是相当于最大对数似然了吗？所以结果就是 q(j|i) 会尽量接近 p(j|i)，从而全部取 0 不一定就是最优解的，因为全部取 0 对应着均匀分布，而真实的 p(j|i) 却不一定是均匀分布。




**现在再来想想，我们从最小成本的思想出发，设计了一个具有概率的负对数形式的 f(vi,vj)，然后发现最后的结果是最大似然。**这个结果可以说是意料之外、情理之中，因为 −logq(j|i) 的含义就是熵，我们说要最大似然，就是要最小化式 (8)，其含义就是最小熵了。最大似然跟最小熵其实具有相同的含义。




# Word2Vec




只要稍微将对象一转变，Word2Vec 就出来了，甚至 everything2vec。




**多样的度量**




纯粹形式地看，式 (5) 的选择虽然很直观，但并不是唯一的，可取的选择还有：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yNXAtCm8IQshwOFNwYcBqUoJgoVBQlMCrGWCAU6TleiavQNyEOFDP3Ug/640)




这以内积为距离度量，希望相近的对象内积越小越好。 




**Skip Gram**




事实上，如果 i,j 分别代表句子窗口里边的一个词，那么式 (9) 就对应了著名的词向量模型——Word2Vec 的 Skip Gram 模型了，也就是说，最小化：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9y9IjiblGyn8cWOoswmk4JsnJibH34s54PUY0aXDeZHiac9VtpsApWnNMmA/640)




这正好是 Word2Vec 的 Skip Gram 模型的优化目标。 




注：Word2Vec 实际上对上下文向量和中心词向量做了区分，也就是用了两套词向量，但这里为了直观理解其中的思想，我们就不区别这一点。




**原理类比分析**




等等，怎么突然就出来词向量了？




我们再重新捋一下思路：是这样的，我们把每个词当作一本书，每个句子都可以看成每个人的“借书记录”，这样我们就能知道哪两本“书”经常被一起借了是吧？



**按照我们前面讨论了一大通的图书馆最佳放书方案，我们就可以把“书”的最佳位置找出来，理论上用 (3),(5) 或 (9) 都可以，这就是词向量了。**如果用式 (9)，就是 Word2Vec 了。 




反过来，找出一个最佳放书方案也就简单了，把图书馆的每个人的借书记录都当成一个句子，每本书当成一个词，设置词向量维度为 3，送入 Word2Vec 训练一下，出来的词向量，就是最佳放书方案了。那些 doc2vec、node2vec、everything2vec，基本上都是这样做的。 




所以，开始的问题就很清晰了：**将图书馆的每本书的三维坐标记录下来，这不就是一个实实在在的“book embedding”？相近的书的向量也相近呀，跟词向量的特性完美对应。**所以，自从有了图书馆，就有了 embedding，尽管那时候还没有坐标系，当然也没有计算机。





# 再来看看t-SNE




有了“借书记录”，也就是 p(j|i),p(i)，我们就可以照搬上述过程，得到一个“最佳位置规划”，这就是向量化的过程。




如果没有呢？




**SNE**




那就造一个出来呀！比如我们已经有了一堆高维样本 x1,x2,…,xN，它们可以是一堆图像数据集，我们想要得到一个低维表示 z1,z2,…,zN。我们构造一个：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yyuEtNmeG5tbadFjH4Rnae8XJ2RzfscwzjzTJ3Tjd1V3IzNvrK7dWicw/640)



然后还是用式 (5) 作为成本函数（假设 p(i) 是常数，即均匀分布，同时求和不对自身进行），去优化 (4)，即：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yC82gic54PxJjvU8X4CtuHlM2UEJQp36p9fBZUr2dvtUzkfK5jia5wysA/640)




这便是称为 SNE 的降维方法了。一般来说它还有一些变种，我们就不细抠了，这也不是本文的重点，我们只需要理解基本思想。




SNE 本质上就是尽量保持相对距离的一种降维方案。因为它保持的是相对距离，保持了基本的形状不变，所以降维效果比 PCA 等方法要好。原因是 PCA 等方法仅仅保留主成分，只适用于比较规则的数据（比如具有中心聚拢特性、各向同性的），SNE 的思想可以适用于任意连通形状。




**t-SNE**




前面说得 SNE 已经体现出降维思想了。但是它会有一些问题，主要的就是“Crowding 问题”。




这个“Crowding 问题”，简单来看，就是因为低维分布 (5) 也是距离的负指数形式，负指数的问题就是在远处迅速衰减到 0，而 (5) 中的 v 是我们要求解的目标，这样一来优化结果是所有的点几乎都拥挤（Crowding）在某处附近（因为指数衰减，距离较远的点几乎不会出现），效果就不够好了。 




为了解决这个问题，我们可以把式 (5) 换成衰减没那么快的函数，比如说简单的分式：




![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm3IVG232BxxYricq5jdrO9yAVp5ex4kJdicUkG0DFMnEpzdKay7wdyu3taqD87SMwuQewfR2DCxrjQ/640)




这称为 t 分布。




式 (13)、式 (11) 和式 (4) 结合，就是称为 t-SNE 的降维方法，相比 SNE，它改善了 Crowding 问题。




当然，t-SNE 与 SNE 的差别，其实已经不是本文的重点了，本文的重点是揭示 SNE 这类降维算法与 Word2Vec 的异曲同工之处。




虽然在深度学习中，我们直接用 t-SNE 这类降维手段的场景并不多，哪怕降维、聚类都有很多更漂亮的方案了，比如降维可以看这篇[深度学习中的互信息：无监督提取特征](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492040&idx=1&sn=f90a6b899e62748c4db489ce06276869&chksm=96ea3e48a19db75e8c07d942a4772bb6c784fac7bcb117da2023186546cfe1876b121a8121cc&scene=21#wechat_redirect)、聚类可以看这个[变分自编码器VAE：一步到位的聚类方案](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491609&idx=1&sn=6f8448342445dfda5be9f1b1ddfc5734&chksm=96ea3f99a19db68f88b7a258217b28a5bcdd993e5cb83624d010cd8f9995bad50756d366ecd0&scene=21#wechat_redirect)。但是 t-SNE 的本质思想在很多场景都有体现，所以挖掘并体味其中的原理，并与其它知识点联系起来，融汇成自己的知识体系，是一件值得去做的事情。




# 本文总结




本文基于最小成本的思想，构建了一个比较理想化的模型来分析图书馆的图书安排原理，进而联系到了最小熵原理，并且思考了它跟 Word2Vec、t-SNE 之间的联系。




就这样，又构成了最小熵原理的一个个鲜活例子：**物以类聚、分门别类，都能降低成本。**比如我们现在可以理解为什么预训练词向量能够加快 NLP 任务的收敛、有时还能提升最终效果了，因为词向量事先将词摆在了适合的位置，它的构造原理本身就是为了降低成本。




同时，将很多看似没有关联的东西联系在一起，能够相互促进各自的理解，达到尽可能融会贯通的效果，其妙不言而喻。




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)







**点击以下标题查看作者其他文章：**




- 
[变分自编码器VAE：原来是这么一回事 | 附开源代码](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487949&idx=1&sn=e09391933f3c4493cfb737b0ea2cf0af&chksm=96e9ce4da19e475b0c789088d403a0f49449b8ba0c43734aa835c5d2a7cb69c3d839c7ce056c&scene=21#wechat_redirect)

- 
[](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488480&idx=1&sn=7bb9a4dd60680df5013670287a9e0cc2&chksm=96e9cc60a19e457618b2ffdea2a7e13ba172ea3fdfbfda07de53aae57126047f1b71a6969d76&scene=21#wechat_redirect)[再谈变分自编码器VAE：从贝叶斯观点出发](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488093&idx=1&sn=08a77550c0cc7309c34a0a38bad0bcba&chksm=96e9cddda19e44cb7ce6143a7990eb4fc47d114b55b564e727a014538402f7218fc89bf1f3c0&scene=21#wechat_redirect)

- 
[变分自编码器VAE：这样做为什么能成？](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488238&idx=1&sn=06ffb033332a54279e600c511e1c5c5f&chksm=96e9cd6ea19e44781ee1313b349e0e77631781a2a163e2fd845c841dc2200d988424bd73c4c7&scene=21#wechat_redirect)

- 
[从变分编码、信息瓶颈到正态分布：论遗忘的重要性](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247493326&idx=1&sn=7ba19fe14ee11bff0e1c865adcb52ca1&chksm=96ea394ea19db0587dc096898730f7522a8e3a7bb3b55bac576422eea63a987ea97ad5886bca&scene=21#wechat_redirect)


- 
[深度学习中的互信息：无监督提取特征](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492040&idx=1&sn=f90a6b899e62748c4db489ce06276869&chksm=96ea3e48a19db75e8c07d942a4772bb6c784fac7bcb117da2023186546cfe1876b121a8121cc&scene=21#wechat_redirect)

- 
[全新视角：用变分推断统一理解生成模型](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490514&idx=1&sn=c066be4f8d2ac3afa8378d180864eed0&chksm=96e9c452a19e4d44eb6a879c5eb4a1426d6de370a0f3c5b6a27c6b8dfc6a938a3851baa258e5&scene=21#wechat_redirect)

- 
[细水长flow之NICE：流模型的基本概念与实现](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490842&idx=1&sn=840d5d8038cd923af827eef497e71404&chksm=96e9c29aa19e4b8c45980b39eb28d80408632c8f9a570c9413748b2b5699260190e0d7b4ed16&scene=21#wechat_redirect)

- 
[细水长flow之f-VAEs：Glow与VAEs的联姻](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491695&idx=1&sn=21c5ffecfd6ef87cd4f1f754795d2d63&chksm=96ea3fefa19db6f92fe093e914ac517bd118e80e94ae61b581079023c4d29cedaaa559cb376e&scene=21#wechat_redirect)

- 
[深度学习中的Lipschitz约束：泛化与生成模型](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492180&idx=1&sn=3ea92a3a9f1306efde89ce1777b80da6&chksm=96ea3dd4a19db4c20dcbc9627b0eb307672b4d61008a93c42814fa6728ca7b6f7c293cff1d80&scene=21#wechat_redirect)












**![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)****#****投 稿 通 道#**

** 让你的论文被更多人看到 **





如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ **答案就是：你不认识的人。**




总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 




PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是**最新论文解读**，也可以是**学习心得**或**技术干货**。我们的目的只有一个，让知识真正流动起来。




📝 **来稿标准：**

• 稿件确系个人**原创作品**，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向） 

• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接 

• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志




**📬 投稿邮箱：**

• 投稿邮箱：hr@paperweekly.site

• 所有文章配图，请单独在附件中发送 

• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通










🔍




现在，在**「知乎」**也能找到我们了

进入知乎首页搜索**「PaperWeekly」**

点击**「关注」**订阅我们的专栏吧







**关于PaperWeekly**





PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgkXb8A1kiafKxib8NXiaPMU8mQvRWVBtFNic4G5b5GDD7YdwrsCAicOc8kp5tdEOU3x7ufnleSbKkiaj5Dg/640?)

▽ 点击 | 阅读原文| 查看作者博客




