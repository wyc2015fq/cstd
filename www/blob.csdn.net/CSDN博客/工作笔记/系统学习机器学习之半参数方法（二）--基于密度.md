# 系统学习机器学习之半参数方法（二）--基于密度 - 工作笔记 - CSDN博客





2018年12月08日 21:39:31[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：5590








转自：[https://www.cnblogs.com/pinard/p/6208966.html](https://www.cnblogs.com/pinard/p/6208966.html)

# 基于密度聚类

DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。下面我们就对DBSCAN算法的原理做一个总结。

# 1. 密度聚类原理

　　　　DBSCAN是一种基于密度的聚类算法，这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。

　　　　通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。

# 2. DBSCAN密度定义

　　　　在上一节我们定性描述了密度聚类的基本思想，本节我们就看看DBSCAN是如何描述密度聚类的。DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数(ϵ, MinPts)用来描述邻域的样本分布紧密程度。其中，ϵ描述了某一样本的邻域距离阈值，MinPts描述了某一样本的距离为ϵ的邻域中样本个数的阈值。

　　　　假设我的样本集是D=(x1,x2,...,xm),则DBSCAN具体的密度描述定义如下：

　　　　1） ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D|distance(xi,xj)≤ϵ}, 这个子样本集的个数记为|Nϵ(xj)|

　　　　2) 核心对象：对于任一样本xj∈D，如果其ϵ-邻域对应的Nϵ(xj)至少包含MinPts个样本，即如果|Nϵ(xj)|≥MinPts，则xj是核心对象。　

　　　　3）密度直达：如果xi位于xj的ϵ-邻域中，且xj是核心对象，则称xi由xj密度直达。注意反之不一定成立，即此时不能说xj由xi密度直达, 除非且xi也是核心对象。

　　　　4）密度可达：对于xi和xj,如果存在样本样本序列p1,p2,...,pT,满足p1=xi,pT=xj, 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,...,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。注意密度可达也不满足对称性，这个可以由密度直达的不对称性得出。

　　　　5）密度相连：对于xi和xj,如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。

　　　　从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的ϵ-邻域内所有的样本相互都是密度相连的。



![](https://img-blog.csdnimg.cn/20181208214712876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==,size_16,color_FFFFFF,t_70)

　　　　有了上述定义，DBSCAN的聚类定义就简单了。

# 3. DBSCAN密度聚类思想

　　　　DBSCAN的聚类定义很简单：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。

　　　　这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的ϵ-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的ϵ-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的ϵ-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。

　　　　那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。

　　　　基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。

　　　　第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。

　　　　第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。如果大家对于最近邻的思想，距离度量，KD树和球树不熟悉，建议参考之前写的另一篇文章[K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)。

　　　　第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于ϵ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说BDSCAN的算法不是完全稳定的算法。

# 4. DBSCAN聚类算法

　　　　下面我们对DBSCAN聚类算法的流程做一个总结。

　　　　输入：样本集D=(x1,x2,...,xm)，邻域参数(ϵ,MinPts)， 样本距离度量方式

　　　　输出： 簇划分C.　

　　　　1）初始化核心对象集合Ω=∅, 初始化聚类簇数k=0，初始化未访问样本集合Γ = D,  簇划分C = ∅

　　　　2) 对于j=1,2,...m, 按下面的步骤找出所有的核心对象：

　　　　　　a) 通过距离度量方式，找到样本xj的ϵ-邻域子样本集Nϵ(xj)

　　　　　　b) 如果子样本集样本个数满足|Nϵ(xj)|≥MinPts， 将样本xj加入核心对象样本集合：Ω=Ω∪{xj}

　　　　3）如果核心对象集合Ω=∅，则算法结束，否则转入步骤4.

　　　　4）在核心对象集合Ω中，随机选择一个核心对象o，初始化当前簇核心对象队列Ωcur={o}， 初始化类别序号k=k+1，初始化当前簇样本集合Ck={o}, 更新未访问样本集合Γ=Γ−{o}

　　　　5）如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck生成完毕, 更新簇划分C={C1,C2,...,Ck}, 更新核心对象集合Ω=Ω−Ck， 转入步骤3。

　　　　6）在当前簇核心对象队列Ωcur中取出一个核心对象o′,通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ， 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−Δ,  更新Ωcur=Ωcur∪(Δ∩Ω)−o′，转入步骤5.

　　　　输出结果为： 簇划分C={C1,C2,...,Ck}

# 5. DBSCAN小结

　　　　和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。

　　　　那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。

　　　　下面对DBSCAN算法的优缺点做一个总结。

　　　　DBSCAN的主要优点有：

　　　　1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。

　　　　2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。

　　　　3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。

　　　　DBSCAN的主要缺点有：

　　　　1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。

　　　　2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。

　　　　3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。

# 使用网格

基于距离的聚类算法只能发现球状簇、处理大数据集以及高维数据集时，不够有效。另一方面发现的聚类个数往往依赖于用户参数的指定。这对用户来说是非常困难的。基于网格的聚类算法将空间量化为有限数目的单元，形成一个网格结构，所有聚类都在网格上进行。

　　基于网格的聚类方法采用空间驱动的方法，把嵌入空间划分成独立于输入对象分布的单元。基于网格的聚类方法使用一种多分辨率的网络数据结构。它将对象空间量化成有限数目的单元，这些网格形成了网格结构，所有的聚类结构都在该结构上进行。这种方法的主要优点是处理速度快，其处理时间独立于数据对象数，而仅依赖于量化空间中的每一维的单元数。

　　总结一下就是：将对象空间量化为有限数目的单元，形成一个网状结构，所有聚类都在这个网状结构上进行。

　　基本思想就是将每个属性的可能值分割成许多相邻的区间，创建网格单元的集合（我们假设属性值是连续的，序数的，区间的）。每个对象落入一个网格单元，网格单元对应的属性空间包含该对象的值。

1.STING：统计信息网格

　　STING是一种基于网格的多分辨率的聚类技术，它将输入对象的空间区域划分成矩形单元，空间可以用分层和递归方法进行划分。这种多层矩形单元对应不同的分辨率，并且形成了一个层次结构：每个高层单元被划分成低一层的单元。关于每个网格单元的属性的统计信息（如均值，最大值和最小值）被作为统计参数预先计算和存储。对于查询处理和其他数据分析任务，这些统计参数是有效的。

　　STING算法：

　　（1）      针对不同的分辨率，通常有多个级别的矩形单元。

　　（2）      这些单元形成了一个层次结构，高层的每个单元被划分成多个底一层的单元。

　　（3）      关于每个网格单元属性的统计信息（例如平均值，max,min）被预先计算和存储，这些统计信息用于回答查询。（统计信息是进行查询使用的）

　　网格中常用的参数：

　　（1）      count 网格中对象数目

　　（2）      mean网格中所有值的平均值

　　（3）      stdev网格中属性值的标准偏差

　　（4）      min 网格中属性值的最小值

　　（5）      max 网格中属性值的最大值

　　（6）      distribution 网格中属性值符合的分布类型。如正态分布，均匀分布

　　STING聚类的层次结构:

![](https://img-blog.csdnimg.cn/20181208221332936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/20181208221348382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==,size_16,color_FFFFFF,t_70)

　　通过上面两幅图，我们可以清晰的理解，STING的层次结构，上一层与下一层的关系。

　　注意：当数据加载到数据库时。最底层的单元参数直接由数据计算，若分布类型知道，可以用户直接指定。而较高层的单元的分布类型可以基于它对应的低层单元多数的分布类型，用一个阈值过滤过程的合取来计算，若底层分布类型彼此不同，那么高层分布类型为none

　　STING查询算法步骤：

　　（1） 从一个层次开始

　　（2） 对于这一个层次的每个单元格，我们计算查询相关的属性值。

　　（3） 从计算的属性值以及约束条件下，我们将每一个单元格标记成相关或者不想关。(不相关的单元格不再考虑，下一个较低层的处理就只检查剩余的相关单元)

　　（4） 如果这一层是底层，那么转（6），否则转（5）

　　（5） 我们由层次结构转到下一层，依照步骤2进行

　　（6） 查询结果得到满足，转到步骤8，否则（7）

　　（7） 恢复数据到相关的单元格进一步处理以得到满意的结果，转到步骤（8）

　　（8） 停止

　　到这儿，STING算法应该基本就差不多了，其核心思想就是：根据属性的相关统计信息进行划分网格，而且网格是分层次的，下一层是上一层的继续划分。在一个网格内的数据点即为一个簇。

　　同时，STING聚类算法有一个性质：如果粒度趋向于0（即朝向非常底层的数据），则聚类结果趋向于DBSCAN聚类结果。即使用计数count和大小信息，使用STING可以近似的识别稠密的簇。

　　STING算法的优点：

　（1）     基于网格的计算是独立于查询的，因为存储在每个单元的统计信息提供了单元中数据汇总信息，不依赖于查询。

　（2）     网格结构有利于增量更新和并行处理。

　（3）     效率高。STING扫描数据库一次开计算单元的统计信息，因此产生聚类的时间复杂度为O(n)，在层次结构建立之后，查询处理时间为）O(g),其中g为最底层网格单元的数目，通常远远小于n。

　缺点：

　（1） 由于STING采用了一种多分辨率的方法来进行聚类分析，因此STING的聚类质量取决于网格结构的最底层的粒度。如果最底层的粒度很细，则处理的代价会显著增加。然而如果粒度太粗，聚类质量难以得到保证。

　（2） STING在构建一个父亲单元时没有考虑到子女单元和其他相邻单元之间的联系。所有的簇边界不是水平的，就是竖直的，没有斜的分界线。降低了聚类质量。

2.CLIQUE

        一种类似于Apriori的子空间聚类算法

　　CLIQUE算法是基于网格的空间聚类算法，但它同时也非常好的结合了基于密度的聚类算法，因此既能够发现任意形状的簇，又可以像基于网格的算法一样处理较大的多维数据。

　　CLIQUE算法把每个维划分成不重叠的社区，从而把数据对象的整个嵌入空间划分成单元，它使用一个密度阈值来识别稠密单位，一个单元是稠密的，如果映射到它的对象超过密度阈值。

　　总结之就是：CLIQUE算法是一种基于网格的聚类算法，用于发现子空间中基于密度的簇。

　　算法概述：

　　算法需要两个参数：一个是网格的步长，第二个是密度的阈值。

　　网格步长确定了空间的划分，而密度阈值用来定义密集网格

　  聚类思想：

　　（1）     首先扫描所有网格。当发现第一个密集网格时，便以该网格开始扩展，扩展原则是若一个网格与已知密集区域内的网格邻接并且其其自身也是密集的，则将该网格加入到该秘籍区域中，知道不再有这样的网格被发现为止。（密集网格合并）

　　（2）     算法再继续扫描网格并重复上述过程，知道所有网格被遍历。以自动地发现最高维的子空间，高密度聚类存在于这些子空间中，并且对元组的输入顺序不敏感，无需假设任何规范的数据分布，它随输入数据的大小线性地扩展，当数据的维数增加时具有良好的可伸缩性。

![](https://img-blog.csdnimg.cn/20181208222748181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==,size_16,color_FFFFFF,t_70)

　　聚类算法如上图所示，总结之就是：首先判断是不是密集网格，如果是密集网格。那么对其相邻的网格进行遍历，看是否是密集网格，如果是的话，那么属于同一个簇。

 　　CLIQUE优点：

　　（1）      给定每个属性的划分，单遍数据扫描就可以确定每个对象的网格单元和网格单元的计数。

　　（2）      尽管潜在的网格单元数量可能很高，但是只需要为非空单元创建网格。

　　（3）      将每个对象指派到一个单元并计算每个单元的密度的时间复杂度和空间复杂度为O(m)，整个聚类过程是非常高效的

　　 缺点：

　　（1）      像大多数基于密度的聚类算法一样，基于网格的聚类非常依赖于密度阈值的选择。（太高，簇可能丢失。太低，本应分开的簇可能被合并）

　　（2）      如果存在不同密度的簇和噪声，则也许不可能找到适合于数据空间所有部分的值。

　　（3）      随着维度的增加，网格单元个数迅速增加（指数增长）。即对于高维数据，基于网格的聚类倾向于效果很差。



