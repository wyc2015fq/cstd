# 系统学习机器学习之非参数方法 - 工作笔记 - CSDN博客





2015年12月18日 13:21:35[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：6420








                
前面的章节中，我们介绍了参数和半参数方法，这两种方法在实际训练前都需要对数据遵从的模型进行一个假定，这个假定可以是一个已知的概率分布或混合分布。参数方法的优点是把估计概率密度、判别式或回归函数问题归结为估计少量参数值，缺点则是模型假定并非总成立，当不成立时就会出现很大的误差。

这时我们就需要使用非参数方法，其中我们只需要假定一个事实：即相似的输入具有相似的输出。因为我们一般都认为世界的变化时平稳、量变到质变的，因此无论是密度、判别式还是回归函数都应当缓慢地变化。在这样的非参数估计（nonparamitric estimation）中，局部实例对于密度的影响就显得颇为重要，而较远的实例影响则较小。本节要点如下：
- 非参数密度估计
- 直方图形式的估计
- 核估计
- k-最近邻估计

#### 一、非参数密度估计

这里我们假设一个具有N个实例的样本集X，为了简单，我们先只考虑自变量是标量的情况，多维的情况可以容易地进行公式扩展。那么我们可以定义累积分布函数F(x) 和密度函数的非参数估计P(x)： ![](http://i.imgur.com/Qa9XInM.png)、

其中（1）式表示概率分布，而（2）式表示概率密度，可以看出概率分布其实是密度的积分，密度使用一个邻近的区间来表示实例x附近的概念，即实例x邻近实例占样本的比例。

### 二、直方图形式的估计

最古老最流行的方法莫过于直方图（histogram）了，直方图最大的优势是直观，而且一旦计算和存放了直方图，我们就不再需要保留训练集。

直方图的方法是将输入空间划分成被称作“箱”的相等区间，我们可以给出估计：

![](http://i.imgur.com/qHdhT4H.png)

（1）式就是非参数方法的直方图估计，（2）式是质朴估计法（naive estimator），这种方法不需要像（1)一样先设定一个原点作为划分box的起点，因为它等于实例x总是落在宽度为h的box中心的直方图估计。

### 三、核估计

采用直方图得到的估计是不光滑的，如果我们想得到一个光滑的估计，那么我们可以使用一个光滑的权重函数，称作核函数（Kernel Function），现在最流行的莫过于高斯核函数：

![](http://i.imgur.com/jODKLCg.png)

（1）式就是著名的高斯核函数，它可以用来做一个权重函数，因为距离x越近的点K越大，反之越小。将直方图估计的(2)式变形，用高斯核代替权重函数后就是当前的（2）式。核函数K决定影响的形状，窗口宽度h决定影响的宽度。质朴估计是box的和，核估计是“凸块”的和。所有的实例都对x估计有影响，并且影响随距离增大而减小。

### 四、k-最近邻估计

估计的最近邻方法调整光滑量使得适应数据的局部密度。近邻数k远小于样本规模N，我们定义一连串的针对实例x的距离：

![](http://i.imgur.com/yvyZ8wB.png)

（1）式表示实例x距离k个近邻的距离的排序，（2）式则给出了k-近邻的密度估计，注意这里采用的是k个近邻中最远的距离作为参照，我们不是固定h并检查有多少样本落入box中，而是固定落入box中的观测数k，来计算box的大小。密度高的box小，密度低的地方box大。

### 五、补充

在结束今天的章节前，还是要说说非参数方法的缺点。虽然非参数方法不依赖于模型的假定，但是却需要更多的时间和空间复杂度。因为非参数方法的本质是使用合适的距离度量从训练集中找出相似的实例，然后由它们的插值得到正确的输出，因此实际运算中需要存储所有的N个实例，即O(N)，实际计算时，对每个实例也需要时间复杂度O(N)，因此这是一种耗费存储和计算量的方法。

一种解决的思路当然是减少需要计算存储的样本了，已经提出了类似的一些精简方法，以减少实际存放的实例数来提高性能。其基本思想时选择X的最小子集Z来替代X，同时误差不增加。


Refer： 《机器学习导论》，Ethen Alpaydin（土耳其），机械工业出版社



