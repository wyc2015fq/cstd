# 系统学习NLP（十四）--句子向量与篇章向量 - 工作笔记 - CSDN博客





2019年03月11日 19:08:17[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：90








转自：[https://blog.csdn.net/qq_35082030/article/details/72582103](https://blog.csdn.net/qq_35082030/article/details/72582103)

Doc2Vec 或者叫做 paragraph2vec, sentence embeddings，是一种非监督式算法，可以获得 sentences/paragraphs/documents 的向量表达，是 word2vec 的拓展。

# 摘要

许多机器翻译的算法都需要使用固定长度的词向量特征。在到达文本层面时，我们最常用的一个固定长度的特征是词袋模型。尽管他们很流行，但是词袋模型有两大缺点：1、失去了词序特征；2、忽略了语义特征，例如，powerful与strong和Paris距离都是非常远的。在本文中，我们提出了一个段落向量，它是一个无监督算法，从变长的文本（句子、段落、文章）中学习到固定长度的词向量特征。我们的算法使用一个密集向量来表示每个文本，而这个是通过预测词出现在文本来训练的。它的结构给了我们可以克服词袋模型缺点的可能。实验结果显示，段落向量优于词袋模型，也同样优于其他文本向量表示。最终我们在几个文本分类和情感分析任务中取得了最优值。

# 1 引言

文本分类和聚类在许多应用中扮演重要作用，例如文本检索、网页搜索和垃圾邮件过滤。而这些应用的核心都是像逻辑斯蒂回归或者K-means聚类这样的机器学习算法。这些算法通常需要使用一个定长的向量来表示输入文本。由于词袋模型和n-graims模型的简单、有效和经常去的不错的准确度，因此他们是最常用文本定长向量表达方式。

但是，词袋模型有许多缺点。词序特征会丢失，因此不同的句子，如果使用相同的单词，可能得到的是相同的向量表达。即使是n-gram模型，它也只是在较短的上下文中考虑词序，这就受到了稀疏数据和高维的约束。词袋模型和n-gram模型对于词义都是不敏感的，它们更多的会考虑词与词之间的距离。就像摘要中说的那样，powerful与strong和Paris距离都是非常远的,事实上，我们知道，powerful与strong更近。

本文中，我们提出了段落向量，一种无监督框架来学习为每篇文本分配向量。文本可以是变长的，无论是句子还是篇章。之所以叫做段落向量是为了强调这个方法可以应用到变长文本中，无论是句子、段落还是更大的篇章单元。

在我们的模型中，向量表达是由在段落中预测有用的词来训练的。更确切来讲，我们通过几个从段落中获取的词向量来连接成段落向量，并且对在给出的上下文中预测接下来的词。词向量和段落向量都是基于SGD和BP算法的。不同的是，词向量是共享的，而段落向量是段落间相互独立的。在预测时，段落向量通过固定的词向量来进行预测，并且训练一个新的段落向量直到收敛。

我们的技术主要受到了最近使用神经网络学习词向量表示的工作的激励。在他们的前期工作中，每一个词都会由一个向量表示，而这个向量来源于上下文中其他词向量的拼接或者平均。例如，在2006年的Bengio的神经网络语言模型中，它使用前几个词向量的拼接来构建神经网络的输入，并且尝试去预测接下来的词。其结果就是，当模型训练完成后，词向量被映射到了一个向量空间，在这个向量空间中，具有相似语义的词会有相似的向量表示。

接着这些成功的技术，研究者们又试图把模型扩展以从词层面到达短语级或者句子级层面。例如，一个简单的方法就是使用文本的所有词向量加权平均。一个更复杂的方法是使用矩阵操作，根据句子的短语结构分析树给出的顺序组合词向量。这两种方法都有缺陷，第一种方法和词袋模型一样丢失了词序特征，第二种方法则只能在句子层面上做，因为它依赖于短语句法分析。

段落向量能够组合序列化变长输入的表达。不像之前的方法，它具有普适性而且可以应用于各种长度的文本：句子、段落和篇章。他不需要对特定任务进行词权重的调整，它也不需要依赖于句法分析树。在接下来的部分中，我们将会展现一些在基准数据集上的实验，这些实验充分展示了段落向量的优势。例如，在情感分析中，我们取得了最优的结果，比之前复杂的方法还要减少16%的错误率。在文本分类任务中，我们方法明显打败了词袋模型，取得了大约30%的性能提升。

# 2算法

我们从之前学习词向量的方法开始讲起。这些方法都对我们的段落向量方法有促进作用。

## 2.1 词向量表示学习方法

这一部分，我们介绍词向量的相关概念。一个著名的词向量学习框架如图1所示，其任务就是通过给出的上下文中的词可以预测当前词的向量。 
![这里写图片描述](https://img-blog.csdn.net/20170520165638797?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在这个框架中，每个词都会被映射成一个[独一无二](https://www.baidu.com/s?wd=%E7%8B%AC%E4%B8%80%E6%97%A0%E4%BA%8C&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的词向量，通过一个一维矩阵W（译者注：onehot编码），这一列表示着这个词在词典中的位置。通过求和或者求平均的方法来获得矩阵，这个矩阵被用来预测下一个词的词向量的特征。
更正式的，给定一串单词w1,w2,w3,…wT,词向量模型的目标就是使得平均似然估计最大：

预测任务通常是使用一个多类分类器，例如Softmax函数。这里，我们使用： 
![这里写图片描述](https://img-blog.csdn.net/20170520170325980?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

每一个yi都是一个非归一化的似然概率，对应的是每一个输出的词i，其中y的计算如下： 
![这里写图片描述](https://img-blog.csdn.net/20170520170333042?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

这里，U，b是softmax的参数。h是由W中抽取出的词向量的平均值或者求和构成。 
![这里写图片描述](https://img-blog.csdn.net/20170520170346505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

实际应用中，层次化的Softmax（Morin&Bengio,2005;Mnih &Hinton,2008;Mikolov et all,2013c）比普通Softmax更适合快速训练。在我们的工作中，层次化的Softmax是一个二叉哈夫曼树。短编码被分配给了频率高的词。这时一个很好的加速训练的小技巧因为这样一般的词都可以得到快速训练。其代码和（Mikolov et all,2013c）用的是一样的。
基于词向量的神经网络模型大多使用SGD方法训练，其梯度通过BP算法获得。这种模型在神经语言模型中非常常见。一个关于词向量的训练算法的具体实现在这里可以找到：code.google.com/p/word2vec/( Mikolov et all,2013a)

当训练收敛后，拥有类似语义的单词会被映射到向量空间里相似的位置上。还是“powerful”和“strong”，他们离的更近。而且词向量间的不同，也表示着不同的意思。这就意味着距离差距也有语义差距。King-man=Queen-woman。大概就是这意思，这个可以被用来学习一个线性矩阵来翻译不同语言间的单词和短语。

这些特性使得词向量对于一些自然语言处理的任务特别适合，例如语言模型（Bengio et al 2006;Mikolov et all 2012），自然语言理解（Collobert & Weston 2008；Zhila et all,2013），统计机器翻译（Mikolov et all 2013b;Zou et al.,2013），图像理解（Frome et al,2013）和相关性抽取（Socher et al,2013a）。

## 2.2 段落向量

我们的学习段落向量的方法是受到了学习词向量的启发。这个启发就是，我们使用词向量来预测句子中下一个单词。所以，尽管这些词向量初始化时是随机的，但是他们最终还是捕获了语义信息作为预测结果的副产品。我们将使用同样的这种方法应用到我们的段落向量中。段落向量也被用来在段落里给出上下文预测下一个词。

在我们的段落向量框架中（图2），每一个段落也被映射成一个独立的向量，使用一个矩阵D来表示。而每个单词也被映射为一个独立的向量，使用矩阵W来表示。段落向量和词向量都被平均或者求和在一个上下文中用来预测下一个词。在实验中，我们使用平均的方法来组合这些向量。 
![这里写图片描述](https://img-blog.csdn.net/20170520165712204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

更重要的，模型中唯一改变的只有式1中的h，从只是D表示变成了D和W共同表示。
段落块可以被看成是另一个词，这个词里记载着当前上下文所缺失的信息，或者说是段落的主题。正因为这个原因，我们经常叫这个模型为分布式记忆模型——段落向量（PV-DM）

上下文信息是按照固定长度在段落上根据滑动窗口不断采样，段落向量会被该段落产生的所有上下文窗口所共同拥有，但是不跨越段落，也就是说，不同段落的段落向量是不同的。但是不同段落的词向量是相同的。

段落向量和词向量使用随机梯度下降方法训练，这个梯度使用的是BP算法获得。每一步的随机梯度下降，都是从一个随机段落里采样获得固定长度的上下文，通过图2种的网络计算梯度误差并且使用该梯度更新模型中的参数。

在预测的时候，需要一个实现一个预测的步骤来进行计算一个新段落的段落向量，这也是通过梯度下降获得。在这步骤中，模型的剩余参数，包括词向量W和Softmax的权重都是固定的。

假设这里有N个段落在语料库中，有M个词在词典里，我们想去学习段落向量。因此每段都被映射到p维里，每个词被映射到q维里，这样，模型总共拥有N*p+M*q参数，包括softmax的参数。即使当N非常大时，这些参数也有可能非常大，但是整个训练过程也是稀疏的并且高效的。

经过训练后，段落向量就可以被用来作为段落的特征。例如可以替代词袋特征等。我们可以把这个特征直接用到机器学习算法中，例如逻辑斯蒂回归、支持向量机或者K-means聚类。

总的来说，这个算法有2个主要步骤：1）使用无监督方法训练词向量W（译者注：和Word2vec一样）；2）推测阶段获取段落向量D。第三步骤是使用D在一个标准分类器上进行标签预测，例如逻辑斯蒂分类或者支持向量机。

段落向量的优势：段落向量的一个最主要的优势在于它不需要标注的语料。

段落向量也克服了一些词袋模型的缺点。首先它隐含了词向量模型的最重要的特点，词的语义。也就是说，相似的语义的词会有相似的位置（译者注：意思是，相似的语义的段落也有相似的位置）。第二个优势就是它考虑了词序，就像在较短的上下文中n-gram模型所做的那样。这是非常重要的，因为n-gram模型提供了大量的段落信息，包括词序。我们的模型就有可能优于n-gram模型，因为n-gram模型可能创建出一个高维的但却稀疏的矩阵。

## 2.3无词序的段落向量：分布的词袋模型

以上的方法都是考虑了在一个文本窗口中使用词向量和段落向量的链接来预测下一个单词。另一个方法是在输入中忽略上下文单词，但是在输出中强制模型对段落中随机采样的单词进行预测。事实上，SGD的每一次迭代中，我们都会从一个文本窗口中采样，然后从这个文本窗口中随机采样一个单词并且构建一个基于段落向量的分类任务。这项技术见图三。我们叫做这种方法为PV-DBOW，与PV-DM相对应。

除了概念上简单外，这个模型也存储更少的数据。我们只需要存储Softmax权重，而之前的模型需要存储Softmax权重和词向量。这个模型更像是词向量模型中的Skip-gram模型。 
![这里写图片描述](https://img-blog.csdn.net/20170520170714673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在我们的试验中，每个段落向量由2部分组成：一个是通过标准段落向量（PV-DM）另一个是(PV-DBOW)。PV-DM通常可以取得很好的成绩在很多任务上，但是如果和PV-DBOW搭配的话，能对多个系统都取得更连续的好的成绩，因此我们强烈推荐。
# 3实验

我们做了实验来更好的理解段落向量的表现。为此，我们在两个文本理解问题上做了基准的段落向量：情感分析和信息检索。

对于情感分析任务，我们使用了2个数据集[斯坦福](https://www.baidu.com/s?wd=%E6%96%AF%E5%9D%A6%E7%A6%8F&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)情感分析树库数据集（Socher ,2013b）和IMDB数据集（Mass ,2011）。在这两个数据集中的文本长度是非常不同的，Socher每个例子都是一个单独的句子，而Mass的数据集中的例子都是好几个句子连在一起的。

我们也使用了我们的方法在信息检索的任务上，这个任务是当给出一个查询时，判断一个文档是否应当被检索到。

## 3.1 斯坦福情感树库数据集上的情感分析

数据集：这个数据集首先被（Pang & Lee ,2005）提出来，并且被（Socher et all. 2013c）扩展作为情感分析的基准系统。它包含了11855个从烂番茄上的影视评论的句子。

这个数据集包涵一下集合：8544个训练集，2210个测试集，1101个验证集。 

每一个句子都有一个标签，这个标签从0-1分别表示最消极到最积极。这些标签都是Amazon Mechanical Turk上由人工标记得到的。

这个数据集附带有每个句子的具体标签以及子句法结构树，前面两个做了很多工作（译者注：此处省略），最终这个数据集可以在[http://nlp.Stanford.edu/sentiment/](http://nlp.Stanford.edu/sentiment/)获取。

任务和基准线：在（Socher et all.2013）文章中，任务被分成了2个基准系统，一个细准系统使用5分类，一个基准系统分为2分类。而且既可以对整个句子进行标注也可以对所有的短语进行标注。这里使用的是对整个句子标注。

(Socher et al. 2013b)使用和好几种方法在这个数据及上，并且发现递归神经张量网络要比词袋模型好很多。这可以被认为是影评经常非常短并且结构在判断是否是积极还是消极上具有重要作用，就像在很小的数据集上给出单词后。

实验草案： 

我们按照(Socher et al. 2013b)的方法实现了一下。为了保证我们能够充分使用已提供的数据集，在我们的模型中，我们把每个短语[都看](https://www.baidu.com/s?wd=%E9%83%BD%E7%9C%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)成是一个独立的句子，因此我们是在所有的短语中训练的。

在学习完短语和句子的向量表达后，我们使用这些来学习一个影评打分的预测器。

在测试的时候，我们固定每个词的向量表达并且学习这个句子的向量表达，使用的是梯度下降方法。一旦句子向量表达学习完成后，我们就使用它用一个逻辑斯蒂回归来预测影评打分。

在我们的实验中，我们使用验证集交叉验证了窗口的大小，最优的窗口大小为8.因此向量表达的分类器有两个向量组成，一个是PV-DBOW，另一个是PV-DM。在这两个中，所有的词和段落向量都是400维的。我们为了预测第8个词，我们使用的是7个词向量和一个段落向量。而特殊符号（。？！）等，我们也视作是一个普通的字符。如果这个句子小于9个词，我们使用NULL来填充。

结果： 

我们把不同方法的错误率都放在了表1种。首先应当注意的就是传统的词袋模型（贝叶斯、SVM、二元贝叶斯）的表现非常差。这是因为词袋模型没有考虑到句子的组成，例如词序。因此有很多复杂的语言现象不能够被识别，例如讽刺。这个结果也显示递归神经网络模型这种更先进的方法，使用的句法分析而考虑了句子组成，因此表现的更好。 
![这里写图片描述](https://img-blog.csdn.net/20170520165830298?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

但是我们的方法由于以上所有基准系统，包括递归神经网络。而且我们也不需要句法分析。在粗颗粒度上我们降低了2.4%的错误率，相比较最好的基准系统提升了16%。
## 3.2 真正的段落向量：在IMDB数据集上的情感分析

之前都是一个句子，下面是多个句子，即使是（Socher,2013b）使用的RNTN，也是需要依靠句法分析的。我们段落是没有句法分析的，但是我们仍然可以做，因为我们不需要句法分析。（译者注：这个实验可以见[http://blog.csdn.net/lenbow/article/details/52120230](http://blog.csdn.net/lenbow/article/details/52120230)，但是效果没有论文的报告上的好）实验表明，我们的方法优于其他方法。

数据集：这里同样的选取100000条来自IMDB的影评，这个工作在（Maas 2011）里讲过了。主要是25000个已标注的训练实例、25000个已标注的测试集实例和50000个未标注的实例。总共有2种标签：积极和消极。这些标签在训练集和测试集里是平均分布的。数据集可以从[http://ai.Stanford.edu/amaas/data/sentiment/index.html](http://ai.Stanford.edu/amaas/data/sentiment/index.html)获得。

实验草案; 

我们使用了75000个训练样例（25000标注的50000未标注的）。然后获取已标注的实例的段落向量并把它们放到一个含有一层一藏层50个单元的神经网络并且使用一个逻辑斯蒂分类器来学习预测分类器。

在测试时，给出一个实力句子，我们固定好其他的网络并且使用梯度下降方法学习测试样例中的段落向量。一旦向量学习完毕后，我们通过给神经网络输入这些向量来预测这些评论的情感。

我们段落向量模型的超参数使用的和之前的那个任务一样，只是窗口大小变成了10个词。其他都没变。

结果：实验结果如表2所示。从表中我们可以看出，词袋模型在长句中的表现很非常好，但是很难有很大的提升知道使用了词向量。最显著的改进是在2012年的（Dahl et al. 2012）的工作中，他们在词袋模型中使用了一种限制型波兹曼机。组合了这两个模型也只改进了1.5%的错误率。

另一个重要改进工作来源于（Wang & Manning,2012）.在他们尝试的方法中，NBSVM的2元特征取得了最好成绩并且提升了2%的错误率。 
![这里写图片描述](https://img-blog.csdn.net/20170520165854519?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在本文中，我们提到的方法显著的低于10%的错误率，达到了7.42%，比最好的还低了1.3%，如果算相对改进的话，则改进了15%。
## 3.3使用段落向量进行信息抽取

我们开始把注意力转移到了另一个任务上，在这个任务上，我们使用定长的段落向量表达。其主要任务就是给出最热门的1000000查询，然后每个查询选取前10个结果。然后每次使用3个段落，这三个段落中，有2个是来自同一查询，另一个是随机选取的。目的是能预测出哪两个段落来自同一查询。判断的方法为段落向量化并测量之间的距离，距离近的为来自同一个查询。（译者注：这里我们不详细举例，这是Google的特权） 
![这里写图片描述](https://img-blog.csdn.net/20170520170020131?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzUwODIwMzA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
# 4 相关工作

这里主要介绍了从向量表达到神经网络语言模型，以及从词袋模型到词向量表达，并且到达短语向量表达和段落向量表达的整个过程。具体可见原文。

# 5 结论

我们描述了一种无监督学习算法来从变长的文本（句子、段落）中学习到向量表达，我们称为段落向量。这些向量表达是根据上下文来预测段落中采样获得的单词。

我们在几个文本分类任务上都做了实验，例如在斯坦福树库和IMDB数据集上的情感分析任务。在这些任务中，这种方法都是最优的。这些好的表现也表明了段落向量可以对段落的语义有表征能力。事实上，段落向量确实可以克服很多词袋模型的缺点。

尽管现在的工作都是聚焦于文本表达，但我们的方法是可以被应用于序列化数据的表达。在无文本领域，句法分析是没有提供的，我们认为段落向量是可以取代词袋模型和n-gram模型的。
补充：


## 篇章表示

如果处理的对象是比句子更长的文本序列（比如篇章），为了降低模型复杂度，一般采用层次化的方法，先得到句子编码，然后以句子编码为输入，进一步得到篇章的表示。具体的层次化可以采用以下几种方法：

（1）层次化的卷积神经网络

即用卷积神经网络对每个句子进行建模，然后以句子为单位再进行一次卷积和池化操作，得到篇章表示。

（2）层次化的循环神经网络

即用循环神经网络对每个句子进行建模，然后再用一个循环神经网络建模以句子为单位的序列，得到篇章表示。

（3）混合模型

先用循环神经网络对每个句子进行建模，然后以句子为单位再进行一次卷积和池化操作，得到篇章表示。在上述模型中，循环神经网络因为非常适合处理文本序列，因此被广泛应用在很多自然语言处理任务上。










