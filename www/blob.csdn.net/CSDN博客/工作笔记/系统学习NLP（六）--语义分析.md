# 系统学习NLP（六）--语义分析 - 工作笔记 - CSDN博客





2018年10月15日 20:25:58[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：5498
个人分类：[自然语言](https://blog.csdn.net/App_12062011/article/category/8102623)









转自：https://www.jianshu.com/p/7463267b0106

对于不同的语言单位，语义分析的任务各不相同。在词的层次上，语义分析的基本任务是进行**词义消歧（WSD）**，在句子层面上是**语义角色标注（SRL）**，在篇章层面上是**指代消歧**，也称共指消解。

**词义消歧**

由于词是能够独立运用的最小语言单位，句子中的每个词的含义及其在特定语境下的相互作用构成了整个句子的含义，因此，词义消歧是句子和篇章语义理解的基础，词义消歧有时也称为词义标注，其任务就是确定一个多义词在给定上下文语境中的具体含义。

词义消歧的方法也分为有监督的消歧方法和无监督的消歧方法，在有监督的消歧方法中，训练数据是已知的，即每个词的词义是被标注了的；而在无监督的消歧方法中，训练数据是未经标注的。

多义词的词义识别问题实际上就是该词的上下文分类问题，还记得词性一致性识别的过程吗，同样也是根据词的上下文来判断词的词性。

有监督词义消歧根据上下文和标注结果完成分类任务。而无监督词义消歧通常被称为聚类任务，使用聚类算法对同一个多义词的所有上下文进行等价类划分，在词义识别的时候，将该词的上下文与各个词义对应上下文的等价类进行比较，通过上下文对应的等价类来确定词的词义。此外，除了有监督和无监督的词义消歧，还有一种基于词典的消歧方法。

在词义消歧方法研究中，我们需要大量测试数据，为了避免手工标注的困难，我们通过人工制造数据的方法来获得大规模训练数据和测试数据。其基本思路是将两个自然词汇合并，创建一个伪词来替代所有出现在语料中的原词汇。带有伪词的文本作为歧义原文本，最初的文本作为消歧后的文本。

**有监督的词义消歧方法**

有监督的词义消歧方法通过建立分类器，用划分多义词上下文类别的方法来区分多义词的词义。

基于互信息的消歧方法

基于互信息的消歧方法基本思路是，对每个需要消歧的多义词寻找一个上下文特征，这个特征能够可靠地指示该多义词在特定上下文语境中使用的是哪种语义。

互信息是两个随机变量X和Y之间的相关性，X与Y关联越大，越相关，则互信息越大。

这里简单介绍用在机器翻译中的Flip-Flop算法，这种算法适用于这样的条件，A语言中有一个词，它本身有两种意思，到B语言之后，有两种以上的翻译。

我们现在有的，是B语言中该词的多种翻译，以及每种翻译所对应的上下文特征。

我们需要得到的，是B语言中的哪些翻译对应义项1，哪些对应义项2。

这个问题复杂的地方在于，对于普通的词义消歧，比如有两个义项的多义词，词都是同一个，上下文有很多，我们把这些上下文划分为两个等价类；而这种跨语言的，不仅要解决上下文的划分，在这之前还要解决两个义项多种词翻译的划分。

这里面最麻烦的就是要先找到两种义项分别对应的词翻译，和这两种义项分别对应的词翻译所对应的上下文特征，以及他们之间的对应关系。

想象一下，地上有两个圈，代表两个义项；这两个圈里，分别有若干个球，代表了每个义项对应的词翻译；然后这两个圈里还有若干个方块，代表了每个义项在该语言中对应的上下文。然后球和方块之间有线连着（球与球，方块与方块之间没有），随便连接，球可以连多个方块，方块也可以连多个球。然后，圈没了，两个圈里的球和方块都混在了一起，乱七八糟的，你该怎么把属于这两个圈的球和方块分开。

Flip-Flop算法给出的方法是，试试。把方块分成两个集合，球也分成两个集合，然后看看情况怎么样，如果情况不好就继续试，找到最好的划分。然后需要解决的问题就是，怎么判定分的好不好？用互信息。

如果两个上下文集（方块集）和两个词翻译集（球集）之间的互信息大，那我们就认为他们的之间相关关系大，也就与原来两个义项完美划分更接近。

实际上，基于互信息的这种方法直接把词翻译的义项划分也做好了。

**基于贝叶斯分类器的消歧方法**

基于贝叶斯分类器的消歧方法的思想与《浅谈机器学习基础》中讲的朴素贝叶斯分类算法相同，当时是用来判定垃圾邮件和正常邮件，这里则是用来判定不同义项（义项数可以大于2），我们只需要计算给定上下文语境下，概率最大的词义就好了。

根据贝叶斯公式，两种情况下，分母都可以忽略，所要计算的就是分子，找最大的分子，在垃圾邮件识别中，分子是P(当前邮件所出现的词语|垃圾邮件)P(垃圾邮件)，那么乘起来就是垃圾邮件和当前邮件词语出现的联合分布概率，正常邮件同理；而在这里分子是P(当前词语所存在的上下文|某一义项)P(某一义项)，这样计算出来的就是某一义项和上下文的联合分布概率，再除以分母P(当前词语所存在的上下文)，计算出来的结果就是P(某一义项|当前词语所存在的上下文)，就能根据上下文去求得概率最大的义项了。

**基于最大熵的词义消歧方法**

利用最大熵模型进行词义消歧的基本思想也是把词义消歧看做一个分类问题，即对于某个多义词根据其特定的上下文条件（用特征表示）确定该词的义项。

**基于词典的词义消歧方法**

**基于词典语义定义的消歧方法**

M. Lesk 认为词典中的词条本身的定义就可以作为判断其词义的一个很好的条件，就比如英文中的core，在词典中有两个定义，一个是『松树的球果』，另一个是指『用于盛放其它东西的锥形物，比如盛放冰激凌的锥形薄饼』。如果在文本中，出现了『树』、或者出现了『冰』，那么这个core的词义就可以确定了。

我们可以计算词典中不同义项的定义和词语在文本中上下文的相似度，就可以选择最相关的词义了。

**基于义类词典的消歧方法**

和前面基于词典语义的消歧方法相似，只是采用的不是词典里义项的定义文本，而是采用的整个义项所属的义类，比如ANMINAL、MACHINERY等，不同的上下文语义类有不同的共现词，依靠这个来对多义词的义项进行消歧。

**无监督的词义消歧方法**

严格地讲，利用完全无监督的消歧方法进行词义标注是不可能的，因为词义标注毕竟需要提供一些关于语义特征的描述信息，但是，词义辨识可以利用完全无监督的机器学习方法实现。

其关键思想在于上下文聚类，计算多义词所出现的语境向量的相似性就可以实现上下文聚类，从而实现词义区分。

**语义角色标注概述**

语义角色标注是一种浅层语义分析技术，它以句子为单位，不对句子所包含的予以信息进行深入分析，而只是分析句子的谓词-论元结构。具体一点讲，语义角色标注的任务就是以句子的谓词为中心，研究句子中各成分与谓词之间的关系，并且用语义角色来描述它们之间的关系。比如：

![](https://img-blog.csdn.net/20181015202458270?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

实际上就是填槽吧，找到句子中的时间、地点、施事者、受事者和核心谓词。

目前语义角色标注方法过于依赖句法分析的结果，而且领域适应性也太差。

自动语义角色标注是在句法分析的基础上进行的，而句法分析包括短语结构分析、浅层句法分析和依存关系分析，因此，语义角色标注方法也分为**基于短语结构树**的语义角色标注方法、**基于浅层句法分析结果**的语义角色标注方法和**基于依存句法分析结果**的语义角色标注方法三种。

它们的基本流程类似，在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元，也就是说任务是确定的，找出这个任务所需的各个槽位的值。其流程一般都由4个阶段组成：

![](https://img-blog.csdn.net/20181015202519820?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

候选论元剪除的目的就是要从大量的候选项中剪除掉那些不可能成为论元的项，从而减少候选项的数目。

论元辨识阶段的任务是从剪除后的候选项中识别出哪些是真正的论元。论元识别通常被作为一个二值分类问题来解决，即判断一个候选项是否是真正的论元。该阶段不需要对论元的语义角色进行标注。

论元标注阶段要为前一阶段识别出来的论元标注语义角色。论元标注通常被作为一个多值分类问题来解决，其类别集合就是所有的语义角色标签。

最终，后处理阶段的作用是对前面得到的语义角色标注结果进行处理，包括删除语义角色重复的论元等。

**基于短语结构树的语义角色标注方法**

首先是第一步，候选论元剪除，具体方法如下：

将谓词作为当前结点，依次考察它的兄弟结点：如果一个兄弟结点和当前结点在句法结构上不是并列的关系，则将它作为候选项。如果该兄弟结点的句法标签是介词短语，则将它的所有子节点都作为候选项。

将当前结点的父结点设为当前结点，重复上一个步骤，直至当前结点是句法树的根结点。

举个例子，候选论元就是图上画圈的：

![](https://img-blog.csdn.net/20181015202529717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

经过剪除得到候选论元之后，进入论元识别阶段，为分类器选择有效的特征。人们总结出了一些常见的有效特征，比如谓词本身、路径、短语类型、位置、语态、中心词、从属类别、论元的第一个词和最后一个词、组合特征等等。

然后进行论元标注，这里也需要找一些对应的特征。然后后处理并不是必须的。

**基于依存关系树的语义角色标注方法**

该语义角色标注方法是基于依存分析树进行的。由于短语结构树与依存结构树不同，所以基于二者的语义角色标注方法也有不同。

在基于短语结构树的语义角色标方法中，论元被表示为连续的几个词和一个语义角色标签，比如上面图给的『事故 原因』，这两个词一起作为论元A1；而在基于依存关系树的语义角色标注方法中，一个论元被表示为一个中心词和一个语义角色标签，就比如在依存关系树中，『原因』是『事故』的中心词，那只要标注出『原因』是A1论元就可以了，也即谓词-论元关系可以表示为谓词和论元中心词之间的关系。

下面给一个例子：

![](https://img-blog.csdn.net/20181015202541978?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FwcF8xMjA2MjAxMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

句子上方的是原来的依存关系树，句子下方的则是谓词『调查』和它的各个论元之间的关系。

第一步仍然是论元剪除，具体方法如下：

将谓词作为当前结点

将当前结点的所有子结点都作为候选项

将当前结点的父结点设为当前结点，如果新当前结点是依存句法树的根结点，剪除过程结束，如果不是，执行上一步

论元识别和论元标注仍然是基于特征的分类问题，也有一些人们总结出来的常见特征。这里不详述。

基于语块的语义角色标注方法

我们前面知道，浅层语法分析的结果是base NP标注序列，采用的方法之一是IOB表示法，I表示base NP词中，O表示词外，B表示词首。

基于语块的语义角色标注方法将语义角色标注作为一个序列标注问题来解决。

基于语块的语义角色标注方法一般没有论元剪除这个过程，因为O相当于已经剪除了大量非base NP，也即不可能为论元的内容。论元辨识通常也不需要了，base NP就可以认为是论元。

我们需要做的就是论元标注，为所有的base NP标注好语义角色。与基于短语结构树或依存关系树的语义角色标注方法相比，基于语块的语义角色标注是一个相对简单的过程。

当然，因为没有了树形结构，只是普通序列的话，与前两种结构相比，丢失掉了一部分信息，比如从属关系等。

**语义角色标注的融合方法**

由于语义角色标注对句法分析的结果有严重的依赖，句法分析产生的错误会直接影响语义角色标注的结果，而进行语义角色标注系统融合是减轻句法分析错误对语义角色标注影响的有效方法。

这里所说的系统融合是将多个语义角色标注系统的结果进行融合，利用不同语义角色标注结果之间的差异性和互补性，综合获得一个最好的结果。

在这种方法中，一般首先根据多个不同语义角色标注结果进行语义角色标注，得到多个语义角色标注结果，然后通过融合技术将每个语义角色标注结果中正确的部分组合起来，获得一个全部正确的语义角色标注结果。

融合方法这里简单说一种基于整数线性规划模型的语义角色标注融合方法，该方法需要被融合的系统输出每个论元的概率，其基本思想是将融合过程作为一个推断问题处理，建立一个带约束的最优化模型，优化目标一般就是让最终语义角色标注结果中所有论元的概率之和最大了，而模型的约束条件则一般来源于人们根据语言学规律和知识所总结出来的经验。

除了基于整数线性规划模型的融合方法之外，人们还研究了若干种其他融合方法，比如最小错误加权的系统融合方法。其基本思想是认为，不应该对所有融合的标注结果都一视同仁，我们在进行融合时应当更多的信赖总体结果较好的系统。



