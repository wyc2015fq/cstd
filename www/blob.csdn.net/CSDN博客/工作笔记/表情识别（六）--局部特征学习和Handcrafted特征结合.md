# 表情识别（六）--局部特征学习和Handcrafted特征结合 - 工作笔记 - CSDN博客





2018年05月28日 16:34:17[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：6916
所属专栏：[机器视觉](https://blog.csdn.net/column/details/33959.html)









论文《Local Learning with Deep and Handcrafted Features for Facial Expression Recognition》

作者基于局部特征学习和手工特征，集成模型（BOVW）的方式，在FER2013上达到75.42%的精度，在FER+上达到86.71%的精度。超过所有比赛结果，目前最好。

主要包含，多CNN模型特征提取，手工特征提取，两者模型集成，混合两种类型的特征之后，作者设计了一个局部学习框架，分如下三步：

    1.采用KNN模型，针对每张测试图片，选择与其最类似的训练样本（若干）

    2.利用上面1得到的样本数据，训练出1：N的多分类器SVM，

    3.该SVM用来预测每张测试图片的类别。

下面具体说明：

1.多CNN模型特征提取

作者采用了VGG-Face,VGG-13,VGG-f三种CNN网络模型，其中，VGG-f,VGG-Face作者采用预训练结果进行，之后三种模型训练，采用SGD，batch 512，动量0.9，所有模型基于数据水平翻转做了增强。

**VGG-Face**,16层，是作者采用的最深的一个网络结构，因为该网络结构预训练的网络权重是用作人脸识别，因此，作者固定了卷积层参数，只训练全连接层参数来适应面部表示识别，并修改了最后的softmax输出类别数，作者采用一个0均值单位方差的高斯分布随机初始化该层。学习速率0.0001，学习策略为验证误差不在下降超过10个epoch时下降10%。作者采用DSD方式fineturn了VGG-Face网络模型，在FER2013数据集上，第一次训练采用Dense方式训练110epoch，然后采用 sparse方式训练40个epoch，稀疏比0.6，之后，再用Dense方式，训练了40个epoch，接着又做了一次sparse训练10次epoch,还是0.6稀疏比。同样，作者也在FER+数据集上训练了FC层，8类，DSD训练过程为：250个Dense epoch->60个Sparse  epoch->30个Dense epoch->60个Sparse epoch。

**VGG-F**，8层，预训练为物体分类网络，作者fineturn全层，学习速率0.0001，学习策略为验证误差不在下降超过10个epoch时下降10%。作者在每个FC后面，增加一个dropout（0.5）,同时也在最后两个卷积层后增加了dropout（0.35）。（话说，都18年的论文了，怎么还有人改这种模型，用这种思路），同样，也采用DSD的方式 ，训练。但是，不同的地方在于，作者对VGG-F的前两个卷积层，在Sparse阶段，做了部分抑制，因为这两层对验证误差，有较高的负面影响，如下图：

![](https://img-blog.csdn.net/20180529091654550)

因此，作者采用分段稀疏比的方式，每层的最高稀疏比，在{0.3，0.4，0.5，0.6}。这种方式，在FER2013上，对验证集的精度影响不超过0.5%。最终的DSD训练方式:300个Dense epoch->50个Sparse  epoch->50个Dense epoch->50个Sparse epoch->100个Dense epoch->150个Sparse epoch->50个Dense epoch，共750个epoch。在FER+上，学习速率0.001,同样采用DSD，但是没有使用权重抑制，采用一样的dropout机制，但是卷积层后面的占比下降到0.25，DSD过程为：60个Dense epoch->20个Sparse  epoch->20个Dense epoch。

**   VGG-13,**该网络结构，只在FER+数据集上重新设计，主要的参数，不在多说了，作者也没啥提升，尤其是DSD没有帮助，因此，没有使用DSD方式fineturn。有兴趣的可以看论文。

2.手动特征提取

    作者的BOVW模型，分测试和训练两个阶段。在训练阶段，作者对所有训练图片，提取了稠密SIFT特征，然后用K-mean聚类的方式，量化这些描述子，成为visual word（VW），视觉词汇，个人认为主要是针对bag这种投票集成算法，针对视觉图像定义的。这些VW存储在k-d树形成的随机森林里以减少搜索代价。在建立好词汇表后，训练和测试等价，意思就是训练和测试没有之前离线分类那种明显的区分了，后面有用到KNN，所以，训练和测试没有明显区分。

    对每张测试/训练图像，存储每张图片上，是否存在每个VW元素的状态，形成二值特征值向量。标准的BOVW模型，没有考虑VW元素之间的空间信息，所以作者通过将图片分块，针对每块提取上面的二值向量特征，然后把每个块得到的二值特征联合起来，形成最终的特征表示。这么做的原因，通俗的解释就是人脸特征表示，尤其是表示的特征，实际上只跟五官等关键区域有关。例如AUs，所以，作者，通过这种方式建立起二值特征之间的空间联系。

3.特征融合与学习

    模型融合，作者在学习阶段之前，通过联合自动和手动特征，实现模型融合。其中，自动特征，采用多个CNN网络模型生成，去除了softmax层，即最后的FC层特征，作为输入图片的自动特征表示，并用L2-norm归一化。BOVW只有手动特征，即上面生成的二值向量，也用L2-norm归一化。

    全局学习，作者采用线性SVM进行1：N分类。

    局部学习，局部学习的目的在于，针对每个类别下的分类效果，进行局部调整，即相对于全部类别一起分，作者分解成针对每个类别，与其他类别的分类训练独立的分类器，以此提高性能。局部学习算法如下：

    1.针对每张测试图片，选择与其特征类似的训练样本，这里采用KNN实现。

    2.针对1选择的这些图片，训练一个线性SVM分类器，1：N。

    3.根据上面2得到的SVM，预测测试图片的类别。





需要注意的是，相似度度量，不是欧式距离，而是余弦距离。

巧合的是，这些每个输入空间的线性SVM分类器，形成的局部学习框架，在整体上来看，却变成了非线性的。这种结构的好处如下：

![](https://img-blog.csdn.net/20180529142411277)



4.FER结果





从结果可以看出：

1.VGG-Face，原始版本，单模型加上局部SVM，已经到达68.96%。作者fineturn后，增加数据增强，结果达到72.11%，但是移除softmax,用局部SVM代替后，精度反而下降了0.61%。

2.VGG-F，原始版本，实际效果只有59.38%，在此基础上增加SVM，也只有66.09%，这应该是局部SVM的作用。作者fineturn后，达到70.30%。移除softmax后，增加SVM，70.60%。

3.VGG-13,66.87%,比其他都低。

其他结果如下图：

![](https://img-blog.csdn.net/20180529144114515)

![](https://img-blog.csdn.net/20180529144138119)

最终的最佳效果，是多个模型融合出来的特征组合，即BOVW特征向量，预训练VGG-Face, Fineturn VGG-Face,fineturn VGG-F,VGG-13四个CNN网络结构提取出的深度特征，最终采用局部SVM的方式，达到75.42.



其他基于FER+的，不在赘述。有兴趣的可以看论文。



