# 系统学习机器学习之神经网络（九） --Hopfield网络 - 工作笔记 - CSDN博客





2017年01月09日 16:52:07[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：10650








转自：http://blog.csdn.net/lg1259156776/article/details/47323889

一、前言

经过一段时间的积累，对于神经网络，已经基本掌握了感知器、BP[算法](http://lib.csdn.net/base/datastructure)及其改进、AdaLine等最为简单和基础的前馈型神经网络知识，下面开启的是基于反馈型的神经网络Hopfiled神经网络。前馈型神经网络通过引入隐层及非线性转移函数（激活函数）使得网络具有复杂的非线性映射能力。前馈网络的输出仅由当前输入和权矩阵决定，而与网络先前的输出状态无关。J.J.
 Hopfield教授在反馈神经网络中引入了能量函数的概念，使得反馈型神经网络运行稳定性的判断有了可靠依据，1985年Hopfield和Tank共同用模拟电子线路实现了Hopfield网，并成功的求解了优化组合问题中最具有代表性的旅行商TSP问题，从而开辟了神经网络用于智能信息处理的新途径。

前馈网络中，不论是离散还是连续，一般都不考虑输入和输出之间在时间上的滞后性，而只是表达两者间的映射关系，但在Hopfield网络中，需考虑输入输出间的延迟因素，因此需要通过微分方程或差分方程描述网络的动态数学模型。

神经网络的学习方式包括三种：监督学习、非监督学习、灌输式学习。对于Hopfield网络的权值不是经过反复学习获得的，而是按照一定的实现规则计算出来，在改变的是网络的状态，直到网络状态稳定时输出的就是问题的解。

Hopfield网络分为连续性和离散型，分别记为CHNN和DHNN。这里主要讲解DHNN。

二、DHNN

1. 网络结构与工作方式

DHNN的特点是任一神经元的输出xi均通过链接权wij反馈至所有神经元xj作为输入，目的是为了让输出能够受到所有神经元的输出的控制，从而使得各个神经元的输出相互制约。每个神经元均设有一个阈值Tj，以反映对输入噪声的控制。DHNN可简记为N=（W,T）。

（1） 网络的状态

所有神经元的状态集合构成了反馈网络状态X=（x1,x2,x3,...,xn），反馈网络的输入就是网络的状态初始值，X（0） = （x1(0),x2(0),x3(0),...,xn(0)）。反馈网络在外界输入的激发下，从初始状态进入动态演变过程，其间每个神经元的状态不断变化，变化规律为：xj = f（net-j），f为转移函数，常采用符号函数，则神经元j的净输入net-j = sum(wji*xi
 - Tj)，对于DHNN网，一般有wii = 0， wji= wij。即权矩阵的对角线元素为0，且为对阵矩阵。表示神经元i的输出不反馈到神经元i，而是反馈到神经元i以外的所有神经元的输入端。

反馈网络稳定时每个神经元的状态都不再改变，即X(t) = X(t+1) = ... = X(∞)。

（2） 网络的异步工作方式

串行，网络每次只对一个神经元的状态进行调整计算，其他均不变。这样调整的顺序就有一定的影响了。可以随机选定或者按照固定的顺序。本次调整的结果会在下一个神经元的净输入中发挥作用。

（3） 网络的同步工作方式

并行，所有神经元同时进行状态调整计算。

2. 网络的稳定性与吸引子

（1） 稳定性

反馈网络是一种能够存储若干预先设置的稳定点的网络，作为非线性动力学系统，具有丰富的动态特性，如稳定性、有限环状态和混沌状态等；

稳定性指的是经过有限次的递归后，状态不再发生改变；

有限环状态指的是限幅的自持震荡；

混沌状态指的是网络状态的轨迹在某个确定的范围内变迁，既不重复也不停止，状态变化无穷多个，轨迹也不发散到无穷远。

对于DHNN，由于网络状态是有限的，不可能出现混沌状态。

利用Hopfield网络可实现联想记忆功能：用网络的稳态表示一种记忆模式，初始状态朝着稳态收敛的过程便是网络寻找记忆模式的过程，初态可视为记忆模式的部分信息，网络演变可视为从部分信息回忆起全部信息的过程，从而实现联想记忆。

可实现优化求解问题：将带求解问的目标函数设置为网络能量函数，当能量函数趋于最小时，网络状态的输出就是问题的最优解。网络的初态视为问题的初始解，而网络从初始状态向稳态的收敛过程便是优化计算过程，这种寻优搜索是在网络演变过程中自动完成的。

（2） 吸引子与能量函数

网络的稳定状态X就是网络的吸引子，用于存储记忆信息。网络的演变过程就是从部分信息寻找全部信息，即联想回忆过程。吸引子有以下的性质：

X=f(WX-T)，则X为网络的吸引子；

对于DHNN，若按异步方式调整，且权矩阵W为对称，则对于任意初态，网络都最终收敛到一个吸引子；

对于DHNN，若按同步方式调整，且权矩阵W为非负定对称，则对于任意初态，网络都最终收敛到一个吸引子；

X为网络吸引子，且阈值T=0，在sign(0)处，xj(t+1) = xj(t)，则-X也一定是该网络的吸引子；

吸引子的线性组合，也是吸引子；

能使网络稳定在同一吸引子的所有初态的集合，称为该吸引子的吸引域；

对于异步方式，若存在一个调整次序，使网络可以从状态X演变为Xa，则称X弱吸引到Xa；若对于任意调整次序，网络都可以从X演变为Xa，则称X强吸引到Xa。则对应弱吸引域和强吸引域。

若使反馈网络具有联想能力，每个吸引子都应该具有一定的吸引域，只有这样，对于带有一定噪声或缺损的初始样本，网络才能经过动态演变而稳定到某一个吸引子状态，从而实现正确联想。反馈网络设计的目的就是要使网络能落到期望的稳定点上，并且还要具有尽可能大的吸引域，以增强联想功能。

3. 网络的权值设计

吸引子的分布是由网络权值包括阈值决定的，设计吸引子的核心就是如何设计一组合适的权值，为了使得所设计的权值满足要求，权值矩阵应符合以下要求：

（1） 为保证异步方式网络收敛，W为对称矩阵；

（2） 为保证同步方式网络收敛，W为非负定对称矩阵；

（3） 保证给定的样本是网络的吸引子，并且要有一定的吸引域。

根据应用所要求的吸引子数量，可以采用以下不同的方法：

（1） 联立方程法

对于吸引子较少时，可采用该方法。

（2） 外积和法

对于吸引子较多时，可采用该方法。采用Hebb规律的外积和法。

补充：



根据其提出者，John Joseph Hopfield 命名。Hopfield 在 1982 年提出的划时代的：[Neural networks and physical systems with emergent collective computational abilities](https://bi.snu.ac.kr/Courses/g-ai09-2/hopfield82.pdf) 一文。顾名思义，从论文的名字中我们就可看出，Hopfield
 神经网络是将物理学的相关思想（动力学）引入到神经网络的构造当中，事实上，Hopfield 本人正是一位物理学家。

## 1. DHNN
- DHNN，Discrete Hopfield Neural Networks，存在离散型 HNN，自然也少不了 CHNN，Continuous HNN，连续型网络。



![](https://img-blog.csdn.net/20161107121218138)


- 从其对应的网状结构可以清晰地看出，DHNN 和其他神经网络不同的是，DHNN 并**没有层（Layer）的概念**，也没有**前向和后向**的区别。
- ，称为每一个神经元（neuron）的门槛值，因为最终是要用加权值和减去该值，又可称其为**截断值**，

## 2. 两个定理

吸引子（attractor）：，
 既是输入也是输出，即表明达到稳态；
- 
定理之一：对于 DHNN 网，若按异步方式调整网络状态，且连接矩阵 
 为对称阵，则对于任意初态，网络都最终收敛到一个吸引子；

此时我们引入**能量函数**（energy function）的定义，







令能量函数的改变量为 ，网络状态的改变量为
，则有：







将相关变量的定义代入进 ，可得：







由于该定理是规定按照异步工作方式，第 
 个时刻只有一个神经元调整状态，


参考资料：

韩力群，人工神经网络教程，北京邮电大学出版社，2006年12月



