# 系统学习深度学习（十二）--池化 - 工作笔记 - CSDN博客





2017年02月25日 11:49:19[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：11610









转自：http://blog.csdn.net/danieljianfeng/article/details/42433475


在卷积神经网络中，我们经常会碰到池化操作，而池化层往往在卷积层后面，通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。

为什么可以通过降低维度呢？

因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值 (或最大值)来代表这个区域的特征。[1]




## **1.  ****一般池化（General Pooling）**

池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。

![](http://deeplearning.stanford.edu/wiki/images/0/08/Pooling_schematic.gif)

我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。

![](https://img-blog.csdn.net/20150105213214237?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGFuaWVsamlhbmZlbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

最常见的池化操作为平均池化mean pooling和最大池化max pooling：

平均池化：计算图像区域的平均值作为该区域池化后的值。

最大池化：选图像区域的最大值作为该区域池化后的值。

随机池化：只需对feature map中的元素按照其概率值大小随机选择，即元素值大的被选中的概率也大

随机池化理解，转自：http://blog.csdn.net/maxiemei/article/details/17355047


补充(转自：http://blog.yinfupai.com/2400.html)


在提取信息的时候，在[池化](http://blog.yinfupai.com/tag/%E6%B1%A0%E5%8C%96)的时候，如果取区域均值（mean-pooling），往往能保留整体数据的特征，能凸出背景的信息，而如果取区域最大值（max-pooling），则能更好保留纹理上的特征，但这些应该都不如小波变换那样，可以保留更多的细节特征，整体上也应该更加细微。

　　在ICLR2013上，Zeiler提出了stochastic pooling，元素值大的被选中的概率也大，但不是像max-pooling那样总是取最大值，这种方法的优势是，一方面最大化保证了Max值的取值，一方面又部分确保不会所有元素都被max值给忽悠住，造成过度失真。

　　这种方式想来还是有缺陷的，因为这种随机行挑选尽管有概率倾向，但它是人为叠加上的，无法总是保证一定随机的概率选择中能够选择到更好的结果，所以也会出现更糟糕的结果的时候，不过加入概率算法好处是，它为产生更好的结果产生了可能，所以总的来说，还是有可能得到更好的结果的。

　　假设目标总是容易被命中的，而有那么个正态分布与目标的分布是近似重合的，如何保证这种分布比较能吻合目标？平均值与最大值都会产生偏移，因为毕竟太暴力了，而概率算法加入无疑是比较理想的，能减少这种偏移的可能，如果运气足够好，收敛会非常好，那么还有可能得到更加贴近的结果，于是这个又扯到了运气上来了。

　　只是，术数里的收敛为何能那么准确，这个从数学角度实在是难解，究竟是什么没有考虑到？目前有一种隐隐地思路，需要探索以术数的模型套上去，只是还是没有找到桥梁在哪里，第一是要找出，信息是如何演算并折叠在卦中的，第二是要找出如何还能够把信息进行还原。

　　在尝试计算了近十万个图形与随机起卦之间的联系后，发现要建立这个联系，是极难完成的任务，制作自动编码器运算到一定程度收敛越来越慢，随便估计也是要花上个几个月的（还不一定最后算得出来），想来还是思路有问题。


## 2. 重叠池化（OverlappingPooling）[2]
重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域，此时sizeX>stride。
论文中[2]中，作者使用了重叠池化，其他的设置都不变的情况下， top-1和top-5 的错误率分别减少了0.4% 和0.3%。







## **3. 空金字塔池化（Spatial Pyramid Pooling）[3]**

空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。



一般的CNN都需要输入图像的大小是固定的，这是因为全连接层的输入需要固定输入维度，但在卷积操作是没有对图像尺度有限制，所有作者提出了空间金字塔池化，先让图像进行卷积操作，然后转化成维度相同的特征输入到全连接层，这个可以把CNN扩展到任意大小的图像。
![](https://img-blog.csdn.net/20150105213450046?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGFuaWVsamlhbmZlbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

空间金字塔池化的思想来自于Spatial Pyramid Model，它一个pooling变成了多个scale的pooling。用不同大小池化窗口作用于卷积特征，我们可以得到1X1,2X2,4X4的池化结果，由于conv5中共有256个过滤器，所以得到1个256维的特征，4个256个特征，以及16个256维的特征，然后把这21个256维特征链接起来输入全连接层，通过这种方式把不同大小的图像转化成相同维度的特征。
![](https://img-blog.csdn.net/20150105213522578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGFuaWVsamlhbmZlbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

对于不同的图像要得到相同大小的pooling结果，就需要根据图像的大小动态的计算池化窗口的大小和步长。假设conv5输出的大小为a*a，需要得到n*n大小的池化结果，可以让窗口大小sizeX为![](https://img-blog.csdn.net/20150105213736284)，步长为![](https://img-blog.csdn.net/20150105213741105) 。下图以conv5输出的大小为13*13为例。


![](https://img-blog.csdn.net/20150105213813531)


疑问：如果conv5输出的大小为14*14，[pool1*1]的sizeX=stride=14，[pool2*2]的sizeX=stride=7，这些都没有问题，但是，[pool4*4]的sizeX=5，stride=4，最后一列和最后一行特征没有被池化操作计算在内。




**SPP其实就是一种多个scale的pooling，可以获取图像中的多尺度信息；在CNN中加入SPP后，可以让CNN处理任意大小的输入，这让模型变得更加的flexible。**



**4.        Reference**

[1]    [UFLDL_Tutorial](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)

[2]    Krizhevsky, I. Sutskever, andG. Hinton, “Imagenet classification with deep convolutional neural networks,”in NIPS,2012.

[3]    Kaiming  He, Xiangyu Zhang, Shaoqing Ren, Jian Su,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,LSVRC-2014 contest










