# 系统学习机器学习之组合多分类器 - 工作笔记 - CSDN博客





2015年12月29日 11:01:55[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：9365







现在为止我们也了解了不少机器学习相关的算法，实际上，每个算法都有自己的假设条件，不同的归纳偏倚会有不同的数据符合度。一般来说，我们可以针对同一个数据集使用多个机器学习算法，然后比较算法的契合度，基本就是准确率，然后选择最合适的一个。但是也存在一些情况，所有单个算法都不能达到我们预期的准确率，此时我们就要考虑组合学习了。
组合学习，顾名思义，就是对于数据集同时使用多个机器学习算法，从而获得算法优劣互补的一种复合算法。近来随着计算和存储变得更为廉价，组合多个学习器的系统也随之流行。对于组合学习来说，有两个基本的问题：
- 如何产生互补的基学习器；
- 为了最大化准确率，如何组合基学习器？

下面我们围绕这两个基本的问题对组合学习策略做一个简单的介绍。
- 产生差异基学习器
- 模型组合方案
- 投票法
- 装袋
- 层叠泛华

#### 一、产生差异基学习器

利用我们已知的算法，我们可以产生有差异的不同的学习器，主要有以下几种方法：
- 算法差异：主要针对同一个数据集采用不同的学习算法，比如参数学习与非参数学习，以此帮助我们摆脱单一决策；
- 超参数差异：在实际的归纳偏倚中差异化超参数，比如多层感知器中的隐藏单元数目、K-最近邻中的K值、决策树中的误差阈值等；
- 输入表示差异化：不同输入表示的一个例子是语音识别单词，我们可以采用声学输入，也可以采用视频捕捉发音时的口形输入，这样就获得了两种不同的基学习器；
- 训练集差异化：可以使用训练集的不同子集来训练不同的基学习器，这可以通过在给定的样本上随机抽取训练集来实现，称作装袋（bagging）；

需要注意的是，组合学习注重的是组合后的算法的准确率，而不是单个基学习器自身的准确率。

#### 二、模型组合方案

在获得差异化的基学习器后，有多种方案来组合这些学习器。主要有两种：
- 多专家组合（multiexpert combination）- 全局方法（Global）：并行架构，给定一个输入，所有的基学习器产生一个输出，然后使用所有的输出获得一个判断，如投票和层叠；
- 局部方法（Local）：与全局方法不同的是，存在一个模型考察输入，并选择一个或几个学习器来产生输出，比如混合专家；

- 多级组合（multistage combination）：这是一种顺序方法，即一个学习器只有在前一个学习器预测不够准确的实例上进行训练和检验。其基本思想是差异化的基学习器按照复杂度递增排序，使得除非前一个更简单的基学习器的结果置信度不够，否则不使用复杂度更高的学习器。这样的一个例子是级联，如果所有前驱学习器的结果置信度都不够，我们才使用下一个学习器，并且使用所有使得前一个学习器不够好的实例作为新的训练集。根本思想时以尽可能低的代价解释大部分实例，并将其余实例作为异常存储。比如英语动词大多情况下都是d/ed，但是也存在不规则的，比如go/went，对于不规则的我们只需要投入最小的计算量即可。

#### 三、投票法

组合多个学习器最常用的方法就是投票，原理就是多个学习器并行处理相同的输入，然后对每个输出“求和”，这里的“求和”只是代表一种运算，并不限于加法。基本的框架如图：（W1、W2、W3代表学习器的权重）

![](http://i.imgur.com/7xF1i0E.png)

常用的分类器“加法”有以下几种：
- 求和，Di的和；
- 加权和：这是最为常用的，但是每个学习器的权重需要事先指定或者训练学习得到，形式为DiWi的和；
- 中位数：对离群点更鲁棒；
- 最值：最小和最大分别是悲观和乐观的估计；
- 乘积：每个学习器都有否决权；

#### 四、装袋

装袋（bagging）也是一种投票方法，其中基学习器通过在稍有差异的训练集上训练而有所不同。这里主要采取“有放回”的自助法从同一个训练集中随机抽取L个训练集，然后使用不稳定的学习过程训练L个基学习器，并在检验时取平均值。

#### 五、层叠泛华

层叠泛化（stacked generalization）其实也是投票的一种扩展，因为投票系统的组合方式大多是线性的，而层叠泛化则不一定如此，组合方式可以是一个单独的复杂的学习器。比如：

![](http://i.imgur.com/cC3WiEq.png)

这里的组合系统F（|φ* 可以是线性的，也可是非线性的，也可以是一个单独的学习器，比如是一个多层感知器，参数φ是连接权重。

在层叠泛化中，我们希望基学习器尽可能不同，使得它们可以互相补充。因此最好的基学习器都基于不同的学习算法。层叠泛化相较于传统的投票模型，训练过的规则更灵活并且具有更小的偏倚，但是却增加了额外的参数，有引入方差的风险。




