# 决策树（五）--OpenCV决策树 - 工作笔记 - CSDN博客





2016年08月06日 13:50:10[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：6011








﻿﻿

OpenCV中的CvDTree类实现了单一的决策树，可以作为基类用在Boosting 和 Random Trees中。决策树是一个二叉树，回归和分类中都可以用。在分类中，每个树叶都被标记了一个类别，多个叶子也可以是相同的类别，在回归中，一个常量也可以赋值给一个叶子，所以逼近函数是分段的。在机器学习中，基尼纯度被用来作为分类的准则，平方和误差被用来作为回归的评价。在以下四种情况下，分割会终止。

1 结构树的深度已经达到了指定的最大值

2 在没有统计数据表征应该继续分割的情况下，节点中的训练数据小于指定的阈值

3 节点中的所有样本属于同一类，或者在回归中，变化很小

4 找到的最好的分割与随机选择并没有明显的提高

当树完成创建后，可能会用到交叉验证来做剪枝处理。有些导致模型过拟合的枝将被剪掉。正常来说，这个过程只应用在独立的决策树中。通常，此法创建的树已经够小了并且它们拥有防止过拟合的机制。



OpenCV实现的是CART分类和回归树。算法的要点是给树的每个节点定义一个衡量标准。比如：当我们拟合一个函数的时候，我们使用真实值和预测值差的平方和，这是衡量标准，算法的目的是使差的平方和最小。对于分类问题，我们定义一个度量，使得当一个节点的大多数值都属于同一类时，这个度量最小。三个最常用度量是entropy，Gini index， misclassification。

一旦我们定义了度量，二叉数搜寻整个特征向量，搜寻哪个特征和那个阈值可以正确分类数据或正确拟合数据。根据惯例，我们说特征值大于这个阈值的数据为”真“，被分配到左分支，其他的点放到右分支。从二叉数的每个节点递归使用这个过程直到数据都纯净了，或者节点里的数据样本数达到最小值。



Gini不纯度

![image](http://img.aiuxian.com/tech/000/001/073/438_06e_263.jpg)

熵（Entropy）

![image](http://img.aiuxian.com/tech/000/001/073/439_706_869.jpg)

错误率

![image](http://img.aiuxian.com/tech/000/001/073/440_77b_85c.jpg)

上面的三个公式均是值越大，表示越 “不纯”，越小表示越“纯”。三种公式只需要取一种即可，实践证明三种公式的选择对最终分类准确率的影响并不大，

纯度差，也称为信息增益（Information Gain），公式如下：

![image](http://img.aiuxian.com/tech/000/001/073/441_296_eb5.jpg)

其中，I代表不纯度（也就是上面三个公式的任意一种），K代表分割的节点数，一般K = 2。vj表示子节点中的记录数目。上面公式实际上就是当前节点的不纯度减去子节点不纯度的加权平均数，权重由子节点记录数与当前节点记录数的比例决定。

**停止条件**

决策树的构建过程是一个递归的过程，所以需要确定停止条件，否则过程将不会结束。一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。另一种可行的方法是当前节点中的记录数低于一个最小的阀值，那么就停止分割，将max(P(i))对应的分类作为当前叶节点的分类。

**过度拟合**

采用上面算法生成的决策树在事件中往往会导致过度拟合。也就是该决策树对训练数据可以得到很低的错误率，但是运用到测试数据上却得到非常高的错误率。原因有以下几点：
- 噪音数据：训练数据中存在噪音数据，决策树的某些节点有噪音数据作为分割标准，导致决策树无法代表真实数据。
- 缺少代表性数据：训练数据没有包含所有具有代表性的数据，导致某一类数据无法很好的匹配，这一点可以通过观察混淆矩阵（Confusion Matrix）分析得出。
- 多重比较（Mulitple Comparition）：举个列子，股票分析师预测股票涨或跌。假设分析师都是靠随机猜测，也就是他们正确的概率是0.5。每一个人预测10次，那么预测正确的次数在8次或8次以上的概率为![image](http://img.aiuxian.com/tech/000/001/073/442_ff2_bdc.jpg)，只有5%左右，比较低。但是如果50个分析师，每个人预测10次，选择至少一个人得到8次或以上的人作为代表，那么概率为![image](http://img.aiuxian.com/tech/000/001/073/443_aec_0d5.jpg)，概率十分大，随着分析师人数的增加，概率无限接近1。但是，选出来的分析师其实是打酱油的，他对未来的预测不能做任何保证。上面这个例子就是**多重比较**。这一情况和决策树选取分割点类似，需要在每个变量的每一个值中选取一个作为分割的代表，所以选出一个噪音分割标准的概率是很大的。

**优化方案1：修剪枝叶**

决策树过度拟合往往是因为太过“茂盛”，也就是节点过多，所以需要裁剪（Prune Tree）枝叶。裁剪枝叶的策略对决策树正确率的影响很大。主要有两种裁剪策略。

**前置裁剪 **在构建决策树的过程时，提前停止。那么，会将切分节点的条件设置的很苛刻，导致决策树很短小。结果就是决策树无法达到最优。实践证明这中策略无法得到较好的结果。

**后置裁剪 **决策树构建好后，然后才开始裁剪。采用两种方法：

1）用单一叶节点代替整个子树，叶节点的分类采用子树中最主要的分类；

2）将一个子树完全替代另外一子树。后置裁剪有个问题就是计算效率，有些节点计算后就被裁剪了，有点浪费。

**优化方案2：K-Fold Cross Validation**

首先计算出整体的决策树T，叶节点个数记作N，设i属于[1,N]。对每个i，使用[K-Fold Validataion]()方法计算决策树，并裁剪到i个节点，计算错误率，最后求出平均错误率。这样可以用具有最小错误率对应的i作为最终决策树的大小，对原始决策树进行裁剪，得到最优决策树。

**优化方案3：Random Forest**

[Random Forest]()是用训练数据随机的计算出许多决策树，形成了一个森林。然后用这个森林对未知数据进行预测，选取投票最多的分类。实践证明，此算法的错误率得到了经一步的降低。一颗树预测正确的概率可能不高，但是集体预测正确的概率却很高。



