# 系统学习机器学习之监督学习 - 工作笔记 - CSDN博客





2015年12月07日 17:46:17[Eason.wxd](https://me.csdn.net/App_12062011)阅读数：6689








```
监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。
```

监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情况下形成。

## 基本定义：

利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。 

监督学习是从标记的训练数据来推断一个功能的机器学习任务。 

一点总结： 

1.样本噪声类型： 

    1>.记录输入属性可能不准确，这会导致数据在输入空间上移动 

    2>.标记数据点可能有错，比如正负标反，这种情况称为指导噪声 

    3>.可能存在我们没有考虑到的其他属性，而他们影响实例的标注，可能是隐藏的，或者潜在的，属于随机成分。 

2.在模型选择上，简单模型会复杂模型的泛化能力好，即奥克姆剃刀规则。同时，我们把为了使得学习成为可能所做的假设集称为学习算法的归纳偏倚。 

3.利用三元权衡得到在所有的由实例数据训练的学习算法中，存在在以下三种因素中平衡： 

1>.拟合数据的假设的复杂性，即假设类的能力 

2>.训练数据的总量 

3>.在新的实例上的泛化误差 

监督学习中需要注意的问题： 

1、偏置方差权衡 

第一个问题就是偏见和方差之间的权衡。假设我们有几种不同的,但同样好的演算数据集。一种学习算法是基于一个未知数的输入，在经过这些数据集的计算时,系统会无误的预测到并将正确的未知数输出。一个学习算法在不同的演算集演算时如果预测到不同的输出值会对特定的输入有较高的方差。一个预测误差学习分类器是与学习算法中的偏差和方差有关的。一般来说,偏差和方差之间有一个权衡。较低的学习算法偏差必须“灵活”,这样就可以很好的匹配数据。但如果学习算法过于灵活,它将匹配每个不同的训练数据集,因此有很高的方差。许多监督学习方法的一个关键方面是他们能够调整这个偏差和方差之间的权衡(通过提供一个偏见/方差参数,用户可以调整)。 

2、功能的复杂性和数量的训练数据 

第二个问题是训练数据可相对于“真正的”功能（分类或回归函数）的复杂度的量。如果真正的功能是简单的，则一个“不灵活的”学习算法具有高偏压和低的方差将能够从一个小数据量的学习。但是，如果真功能是非常复杂的（例如，因为它涉及在许多不同的输入要素的复杂的相互作用，并且行为与在输入空间的不同部分），则该函数将只从一个非常大的数量的训练数据，并使用可学习“灵活”的学习算法具有低偏置和高方差。因此，良好的学习算法来自动调整的基础上可用的数据量和该函数的明显的复杂性要学习的偏压/方差权衡。 

3、输入空间的维数 

第三个问题是输入空间的维数。如果输入特征向量具有非常高的维数，学习问题是很困难的，即使真函数仅依赖于一个小数目的那些特征。这是因为许多“额外”的尺寸可混淆的学习算法，并使其具有高方差。因此，高的输入维数通常需要调整分类器具有低方差和高偏置。在实践中，如果工程师能够从输入数据手动删除不相关的特征，这是有可能改善该学习功能的准确性。此外，还有许多算法的特征选择，设法确定相关特征，并丢弃不相关的。这是维数降低，其目的是将输入数据映射到较低维空间中运行的监督学习算法之前的更一般的策略的一个实例。 

4、噪声中的输出值 

第四个问题是在所需要的输出值（监控目标变量）的噪声的程度。如果所希望的输出值，通常是不正确的（因为人为错误或传感器的错误），则学习算法不应试图找到一个函数完全匹配的训练示例。试图以适应数据过于谨慎导致过度拟合。当没有测量误差（随机噪声），如果你正在努力学习功能，是您学习模式太复杂，你甚至可以过度拟合。在这种情况下的目标函数，该函数不能被模拟“腐化”你的训练数据的那部分-这一现象被称为确定性的噪声。当任一类型的噪声存在时，最好是去一个更高的偏见，低方差估计。
## 实例分析：

正如人们通过已知病例学习诊断技术那样，计算机要通过学习才能具有识别各种事物和现象的能力。用来进行学习的材料就是与被识别对象属于同类的有限数量样本。监督学习中在给予计算机学习样本的同时，还告诉计算各个样本所属的类别。若所给的学习样本不带有类别信息,就是无监督学习。任何一种学习都有一定的目的,对于模式识别来说，就是要通过有限数量样本的学习，使分类器在对无限多个模式进行分类时所产生的错误概率最小。 

不同设计方法的分类器有不同的学习算法。对于贝叶斯分类器来说，就是用学习样本估计特征向量的类条件概率密度函数。在已知类条件概率密度函数形式的条件下，用给定的独立和随机获取的样本集，根据最大似然法或贝叶斯学习估计出类条件概率密度函数的参数。例如，假定模式的特征向量服从正态分布，样本的平均特征向量和样本协方差矩阵就是正态分布的均值向量和协方差矩阵的最大似然估计。在类条件概率密度函数的形式未知的情况下，有各种非参数方法，用学习样本对类条件概率密度函数进行估计。在分类决策规则用判别函数表示的一般情况下,可以确定一个学习目标,例如使分类器对所给样本进行分类的结果尽可能与“教师”所给的类别一致，然后用迭代优化算法求取判别函数中的参数值。 

在无监督学习的情况下，用全部学习样本可以估计混合概率密度函数，若认为每一模式类的概率密度函数只有一个极大值，则可以根据混合概率密度函数的形状求出用来把各类分开的分界面。 

监督学习方法是目前研究较为广泛的一种机器学习方法，例如神经网络传播算法、决策树学习算法等已在许多领域中得到成功的应用，但是，监督学习需要给出不同环境状态下的期望输出（即导师信号），完成的是与环境没有交互的记忆和知识重组的功能，因此限制了该方法在复杂的优化控制问题中的应用。 

总结： 

在监督学习算法（不管是分类，还是K维分类，或者回归）中，我们选择一个足够容量的模型及归纳偏倚（即在包含噪声的情况产生数据的未知函数），足够大的训练数据支撑，定义损失函数，计算出逼近误差，求解参数的最小化近似误差（即有好的优化方法，以便找出正确的假设）。 

不同的机器学习算法之间的区别在于或者他们的假设模型不同，或者他们所使用的损失度量不同，或者他们所使用的最优化过程不同。




