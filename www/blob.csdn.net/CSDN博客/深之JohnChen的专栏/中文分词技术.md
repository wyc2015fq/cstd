# 中文分词技术 - 深之JohnChen的专栏 - CSDN博客

2010年08月16日 14:37:00[byxdaz](https://me.csdn.net/byxdaz)阅读数：3923标签：[算法																[lucene																[文档																[搜索引擎																[中文分词库																[classification](https://so.csdn.net/so/search/s.do?q=classification&t=blog)](https://so.csdn.net/so/search/s.do?q=中文分词库&t=blog)](https://so.csdn.net/so/search/s.do?q=搜索引擎&t=blog)](https://so.csdn.net/so/search/s.do?q=文档&t=blog)](https://so.csdn.net/so/search/s.do?q=lucene&t=blog)](https://so.csdn.net/so/search/s.do?q=算法&t=blog)
个人分类：[技术资料](https://blog.csdn.net/byxdaz/article/category/144574)


**中文分词技术**

一、为什么要进行中文分词？

词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文信息处理的基础与关键。

Lucene中对中文的处理是基于自动切分的单字切分，或者二元切分。除此之外，还有最大切分（包括向前、向后、以及前后相结合）、最少切分、全切分等等。

二、中文分词技术的分类

我们讨论的分词算法可分为三大类：基于字典、词库匹配的分词方法；基于词频度统计的分词方法和基于知识理解的分词方法。

第一类方法应用词典匹配、汉语词法或其它汉语语言知识进行分词，如：最大匹配法、最小分词方法等。这类方法简单、分词效率较高,但汉语语言现象复杂丰富，词典的完备性、规则的一致性等问题使其难以适应开放的大规模文本的分词处理。第二类基于统计的分词方法则基于字和词的统计信息，如把相邻字间的信息、词频及相应的共现信息等应用于分词，由于这些信息是通过调查真实语料而取得的，因而基于统计的分词方法具有较好的实用性。

下面简要介绍几种常用方法:

1）．逐词遍历法。

逐词遍历法将词典中的所有词按由长到短的顺序在文章中逐字搜索,直至文章结束。也就是说,不管文章有多短,词典有多大,都要将词典遍历一遍。这种方法效率比较低，大一点的系统一般都不使用。

2）．基于字典、词库匹配的分词方法（机械分词法）

这种方法按照一定策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。识别出一个词，根据扫描方向的不同分为正向匹配和逆向匹配。根据不同长度优先匹配的情况，分为最大（最长）匹配和最小（最短）匹配。根据与词性标注过程是否相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的方法如下：

（一）最大正向匹配法 (ＭａｘｉｍｕｍＭａｔｃｈｉｎｇＭｅｔｈｏｄ)通常简称为ＭＭ法。其基本思想为：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理……如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。

其算法描述如下：

(1)初始化当前位置计数器，置为0；

(2)从当前计数器开始，取前2i个字符作为匹配字段，直到文档结束；

(3)如果匹配字段长度不为0，则查找词典中与之等长的作匹配处理。

如果匹配成功，

则，

a)把这个匹配字段作为一个词切分出来，放入分词统计表中；

b)把当前位置计数器的值加上匹配字段的长度；

c)跳转到步骤2);

否则

a) 如果匹配字段的最后一个字符为汉字字符，

则

①把匹配字段的最后一个字去掉；

②匹配字段长度减2；

否则

①把匹配字段的最后一个字节去掉；

②匹配字段长度减1；

b)跳转至步骤3）；

否则

a)如果匹配字段的最后一个字符为汉字字符，

则当前位置计数器的值加2；

否则当前位置计数器的值加1；

b)跳转到步骤2)。

（二）逆向最大匹配法 (ＲｅｖｅｒｓｅＭａｘｉｍｕｍＭａｔｃｉｎｇＭｅｔｈｏｄ)通常简称为ＲＭＭ法。ＲＭＭ法的基本原理与ＭＭ法相同 ,不同的是分词切分的方向与MM法相反，而且使用的分词辞典也不同。逆向最大匹配法从被处理文档的末端开始匹配扫描，每次取最末端的2i个字符（i字字串）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。

由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。所以，逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明 ,单纯使用正向最大匹配的错误率为 1/16 9,单纯使用逆向最大匹配的错误率为 1/245。例如切分字段“硕士研究生产”，正向最大匹配法的结果会是“硕士研究生 / 产”，而逆向最大匹配法利用逆向扫描，可得到正确的分词结果“硕士 / 研究 / 生产”。

当然，最大匹配算法是一种基于分词词典的机械分词法，不能根据文档上下文的语义特征来切分词语，对词典的依赖性较大，所以在实际使用时，难免会造成一些分词错误，为了提高系统分词的准确度，可以采用正向最大匹配法和逆向最大匹配法相结合的分词方案（即双向匹配法，见（四）。）

（三）最少切分法：使每一句中切出的词数最小。

（四）双向匹配法：将正向最大匹配法与逆向最大匹配法组合。先根据标点对文档进行粗切分，把文档分解成若干个句子，然后再对这些句子用正向最大匹配法和逆向最大匹配法进行扫描切分。如果两种分词方法得到的匹配结果相同，则认为分词正确，否则，按最小集处理。

3). 全切分和基于词的频度统计的分词方法

基于词的频度统计的分词方法是一种全切分方法。在讨论这个方法之前我们先要明白有关全切分的相关内容。

全切分

全切分要求获得输入序列的所有可接受的切分形式，而部分切分只取得一种或几种可接受的切分形式，由于部分切分忽略了可能的其他切分形式，所以建立在部分切分基础上的分词方法不管采取何种歧义纠正策略，都可能会遗漏正确的切分，造成分词错误或失败。而建立在全切分基础上的分词方法，由于全切分取得了所有可能的切分形式，因而从根本上避免了可能切分形式的遗漏，克服了部分切分方法的缺陷。

全切分算法能取得所有可能的切分形式，它的句子覆盖率和分词覆盖率均为100%，但全切分分词并没有在文本处理中广泛地采用，原因有以下几点：

1)全切分算法只是能获得正确分词的前提，因为全切分不具有歧义检测功能，最终分词结果的正确性和完全性依赖于独立的歧义处理方法，如果评测有误，也会造成错误的结果。

2)全切分的切分结果个数随句子长度的增长呈指数增长，一方面将导致庞大的无用数据充斥于存储数据库；另一方面当句长达到一定长度后，由于切分形式过多,造成分词效率严重下降。

基于词的频度统计的分词方法：

这是一种全切分方法。它不依靠词典,而是将文章中任意两个字同时出现的频率进行统计,次数越高的就可能是一个词。它首先切分出与词表匹配的所有可能的词,运用统计语言模型和决策算法决定最优的切分结果。它的优点在于可以发现所有的切分歧义并且容易将新词提取出来。

4)．基于知识理解的分词方法。

该方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界，它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断。这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式。因此目前基于知识的分词系统还处在试验阶段。

5)．一种新的分词方法

并行分词方法：这种分词方法借助于一个含有分词词库的管道进行 ,比较匹配过程是分步进行的 ,每一步可以对进入管道中的词同时与词库中相应的词进行比较 ,由于同时有多个词进行比较匹配 ,因而分词速度可以大幅度提高。这种方法涉及到多级内码理论和管道的词典数据结构。（详细算法可以参考吴胜远的《并行分词方法的研究》。）

**常用中文分词包**

1. 庖丁解牛分词包，适用于与Lucene整合。http://www.oschina.net/p/paoding

庖丁中文分词库是一个使用Java开发的，可结合到Lucene应用中的，为互联网、企业内部网使用的中文搜索引擎分词组件。

Paoding填补了国内中文分词方面开源组件的空白，致力于此并希翼成为互联网网站首选的中文分词开源组件。 Paoding中文分词追求分词的高效率和用户良好体验。

Paoding's Knives 中文分词具有极 高效率 和 高扩展性 。引入隐喻，采用完全的面向对象设计，构思先进。

高效率：在PIII 1G内存个人机器上，1秒 可准确分词 100万 汉字。

采用基于 不限制个数的词典文件对文章进行有效切分，使能够将对词汇分类定义。

能够对未知的词汇进行合理解析

2. LingPipe，开源自然语言处理的Java开源工具包。http:/alias-i.com/lingpipe/

功能非常强大，最重要的是文档超级详细，每个模型甚至连参考论文都列出来了，不仅使用方便，也非常适合模型的学习。

主题分类（Top Classification）、命名实体识别（Named Entity Recognition）、词性标注（Part-of Speech Tagging）、句题检测（Sentence Detection）、查询拼写检查（Query Spell Checking）、兴趣短语检测（Interseting Phrase Detection）、聚类（Clustering）、字符语言建模（Character Language Modeling）、医学文献下载/解析/索引（MEDLINE Download, Parsing and Indexing）、数据库文本挖掘（Database Text Mining）、中文分词（Chinese Word Segmentation）、情感分析（Sentiment Analysis）、语言辨别（Language Identification）等

3. JE分词包

4. LibMMSeg http://www.oschina.net/p/libmmseg

采用C++开发，同时支持Linux平台和Windows平台，切分速度大约在300K/s（PM-1.2G），截至当前版本（0.7.1）。

LibMMSeg没有为速度仔细优化过，进一步的提升切分速度应仍有空间。

LibMMSeg 简介

LibMMSeg 是Coreseek.com为[Sphinx](http://www.open001.com/p/sphinx)全文搜索引擎设计的中文分词软件包，其在GPL协议下发行的中文分词法，采用[Chih-Hao Tsai](http://chtsai.org/)的MMSEG算法。

MMSEG: A Word Identification System for Mandarin Chinese Text Based on Two Variants of the Maximum Matching Algorithm

Published: 1996-04-29
Updated: 1998-03-06
Document updated: 2000-03-12
License: Free for noncommercial use
Copyright   1996-2006 [Chih-Hao Tsai](http://chtsai.org/) (Email: hao520 at yahoo.com )

您可以在[Chih-Hao Tsai's Technology Page](http://technology.chtsai.org/)找到算法的原文。

LibMMSeg 采用C++开发，同时支持Linux平台和Windows平台，切分速度大约在300K/s（PM-1.2G），截至当前版本（0.7.1）LibMMSeg没有为速度仔细优化过，进一步的提升切分速度应仍有空间。

**下载**

下载[MMSeg 0.7.3](http://www.coreseek.com/uploads/sources/mmseg-0.7.3.tar.gz)

**修订记录**

0.7.3
- 2008.05.27 修正 Makefile 无法安装csr_typedefs.h的问题
- 2008.05.27 修正 x64系统上编译无法作为动态库的一部分编译的问题

0.7.2
- 2008.05.19 修正指定的目录中无词典不提示错误的问题
- 2008.05.19 新增 Ruby 的调用API

0.7.1
- 2008.04.23 修正了在类似 “english 中文 english" 的句子，切分不正确的问题

0.7
- 第一次发行

**安装**

Window平台

打开源码包中src/win32 子目录下的对应的工程文件，目前LibMMSeg内置了VS2003和VS2005的工程文件。

Linux平台

在源码包根目录下执行：

./configure && make && make install

**使用**

**词典的构造**

mmseg -u unigram.txt

该命令执行后，将会产生一个名为unigram.txt.uni的文件，将该文件改名为uni.lib，完成词典的构造。需要注意的是，unigram.txt 必须为UTF-8编码。

**词典文件格式：**

....
河 187
x:187
造假者 1
x:1
台北队 1
x:1
湖边 1
......

其中，每条记录分两行。其中，第一行为词项，其格式为：[词条]/t[词频率]。需要注意的是，对于单个字后面跟这个字作单字成词的频率，这个频率需要在大量的预先切分好的语料库中进行统计，用户增加或删除词时，一般不需要修改这个数值；对于非单字词，词频率处必须为1。第二行为占位项，是由于 LibMMSeg库的代码是从Coreseek其他的分词算法库（N-gram模型）中改造而来的，在原来的应用中，第二行为该词在各种词性下的分布频率。LibMMSeg的用户只需要简单的在第二行处填"x:1"即可。

用户可以通过修改词典文件增加自己的自定义词，以提高分词法在某一具体领域的切分精度，系统默认的词典文件在data/unigram.txt中。

**分词**

mmseg -d tobe_segment.txt

其中，命令使用‘-d’开关指定词库文件所在的位置，参数dict_dir为词库文件（uni.lib ）所在的目录；tobe_segment.txt 为待切分的文本文件，必须为UTF-8编码。如果一切正确，mmseg会将切分结果以及所花费的时间显示到标准输出上。

**对特殊短语的支持**

由于LibMMSeg是为Sphinx全文搜索引擎设计的，因此其内置了部分搜索引擎切分算法的特性，主要表现在对特殊短语的支持上。

在搜索引擎中，需要处理C++时，如果分词器中没有词组C++，则将被切分为C/x +/x +/x，在进一步的检索中，可能每个词会由于出现的过于频繁而被过滤掉，导致搜索的结果与C++相关度不高不说，也严重影响的全文搜索的速度。在 LibMMSeg中，内置对特殊短语的支持。

其输入文件格式如下

// test commit 
.net => dotnet
c# => csharp
c++ => cplusplus

其中左侧是待支持的特殊短语，右侧是左侧的特殊短语需要被转换为的短语。这一转换在分词前进行。

可以在行的开头加入'//'作为注释符号，发现符号'//'后，整行将被忽略。

特殊短语词库构造命令：

mmseg -b exceptions.txt

其中, 开关'-b'指示mmseg是要构造特殊短语词库；exceptions.txt是用户编辑的特殊短语转换规则。

该命令执行后，将在当前目录下产生一个名为"synonyms.dat"的文件，将该文件放在"uni.lib"同一目录下，分词系统将自动启动特殊短语转换功能。

注意：

1、在启用了该功能后，如果分词系统发现了一个特殊短语，将直接输出其在右侧对应的替换的值；

2、右侧被替换的值，请保证不会被分词器进行切分。（eg. C++ => C# 这个转换的意义不大，并且可能导致C++这个短语永远无法被检索到！）

附录：

**MMSeg****算法说明**

首先来理解一下chunk，它是MMSeg分词算法中一个关键的概念。Chunk中包含依据上下文分出的一组词和相关的属性，包括长度 (Length)、平均长度(Average Length)、标准差的平方(Variance)和自由语素度(Degree Of Morphemic Freedom)。下面列出了这4个属性：

|属性|含义|
|----|----|
|长度(Length)|chuck中各个词的长度之和|
|平均长度(Average Length)|长度(Length)/词数|
|标准差的平方(Variance)|同数学中的定义|
|自由语素度(Degree Of Morphemic Freedom)|各单字词词频的对数之和|

Chunk中的4个属性只有在需要该属性的值时才进行计算，而且只计算一次。

其次来理解一下规则(Rule)，它是MMSeg分词算法中的又一个关键的概念。实际上我们可以将规则理解为一个过滤器(Filter)，过滤掉不符合要求的chunk。MMSeg分词算法中涉及了4个规则：

·规则1：取最大匹配的chunk (Rule 1: Maximum matching) 

·规则2：取平均词长最大的chunk (Rule 2: Largest average word length) 

·规则3：取词长标准差最小的chunk (Rule 3: Smallest variance of word lengths) 

·规则4：取单字词自由语素度之和最大的chunk (Rule 4: Largest sum of degree of morphemic freedom of one-character words) 

这4个规则符合汉语成词的基本习惯。

再来理解一下匹配方式复杂最大匹配(Complex maximum matching)：

复杂最大匹配先使用规则1来过滤chunks，如果过滤后的结果多于或等于2，则使用规则2继续过滤，否则终止过滤过程。如果使用规则2得到的过滤结果多于或等于2，则使用规则3继续过滤，否则终止过滤过程。如果使用规则3得到的过滤结果多于或等于2，则使用规则4继续过滤，否则终止过滤过程。如果使用规则 4得到的过滤结果多于或等于2，则抛出一个表示歧义的异常，否则终止过滤过程。

最后通过一个例句--“研究生命起源来简述”一下复杂最大匹配的分词过程。MMSeg分词算法会得到7个chunk，分别为：

|编号|chunk|长度|
|----|----|----|
|0|研_究_生|3|
|1|研_究_生命|4|
|2|研究_生_命|4|
|3|研究_生命_起|5|
|4|研究_生命_起源|6|
|5|研究生_命_起|5|
|6|研究生_命_起源|6|

使用规则1过滤后得到2个chunk，如下：

|编号|chunk|长度|
|----|----|----|
|4|研究_生命_起源|6|
|6|研究生_命_起源|6|

计算平均长度后为：

|编号|chunk|长度|平均长度|
|----|----|----|----|
|4|研究_生命_起源|6|2|
|6|研究生_命_起源|6|2|

使用规则2过滤后得到2个chunk，如下：

|编号|chunk|长度|平均长度|
|----|----|----|----|
|4|研究_生命_起源|6|2|
|6|研究生_命_起源|6|2|

计算标准差的平方后为：

|编号|chunk|长度|平均长度|标准差的平方|
|----|----|----|----|----|
|4|研究_生命_起源|6|2|0|
|6|研究生_命_起源|6|2|4/9|

使用规则3过滤后得到1个chunk，如下：

|编号|chunk|长度|平均长度|标准差的平方|
|----|----|----|----|----|
|4|研究_生命_起源|6|2|0|

匹配过程终止。最终取“研究”成词，以相同的方法继续处理“生命起源”。

分词效果:

研究_生命_起源_
研究生_教育_

coreseek 中文分词核心配置：

请参考：[中文分词核心配置](http://www.coreseek.cn/products/products-install/coreseek_mmseg/)

mmseg.ini配置：（请将其放置到词典文件uni.lib所在的目录，并在文件结尾空两行）

[mmseg] 

merge_number_and_ascii=0

number_and_ascii_joint=

compress_space=1

seperate_number_ascii=0

配置说明：【因为Sphinx-0.9.9发生变化，导致mmseg.ini的字母和数字切分规则无法正常应用，故暂停使用；下一版本将彻底解决该问题】

merge_number_and_ascii : ;合并英文和数字 abc123/x；如果0，则abc123切分为abc、123；如果1，则abc123为一个整体

number_and_ascii_joint : 定义可以连接英文和数字的字符；该字符将把英文和数字作为一个整体处理；如果设置为-，则abc-123将被切分为abc123

compress_space : 预留暂不支持

seperate_number_ascii : 将数字打散；如果0，则123为一个整体；如果1，则123切分为1、2、3


使用中如果出现问题，可以查看[常见问题解答](http://www.coreseek.cn/products-install/faq/)。

[http://www.coreseek.com/opensource/mmseg/](http://www.coreseek.com/opensource/mmseg/)



5. IKAnalyzer http://www.oschina.net/p/ikanalyzer

IKAnalyzer基于lucene2.0版本API开发，实现了以词典分词为基础的正反向全切分算法，是LuceneAnalyzer接口的实现。

该算法适合与互联网用户的搜索习惯和企业知识库检索，用户可以用句子中涵盖的中文词汇搜索，如用"人民"搜索含"人民币"的文章，这是大部分用户的搜索思维；

不适合用于知识挖掘和网络爬虫技术，全切分法容易造成知识歧义，因为在语义学上"人民"和"人民币"是完全搭不上关系的。

6. PHPCWS http://www.oschina.net/p/phpcws

PHPCWS 是一款开源的PHP中文分词扩展，目前仅支持Linux/Unix系统。

PHPCWS 先使用“ICTCLAS 3.0 共享版中文分词算法”的API进行初次分词处理，再使用自行编写的“逆向最大匹配算法”对分词和进行词语合并处理，并增加标点符号过滤功能，得出分词结果。

ICTCLAS（Institute of Computing Technology, Chinese Lexical Analysis System）是中国科学院计算技术研究所在多年研究工作积累的基础上，基于多层隐马模型研制出的汉语词法分析系统，主要功能包括中文分词；词性标注；命名实体识别；新词识别；同时支持用户词典。ICTCLAS经过五年精心打造，内核升级6次，目前已经升级到了ICTCLAS3.0，分词精度 98.45%，各种词典数据压缩后不到3M。ICTCLAS在国内973专家组组织的评测中活动获得了第一名，在第一届国际中文处理研究机构SigHan 组织的评测中都获得了多项第一名，是当前世界上最好的汉语词法分析器。

ICTCLAS 3.0 商业版是收费的，而免费提供的 ICTCLAS 3.0 共享版不开源，词库是根据人民日报一个月的语料得出的，很多词语不存在。所以本人对ICTCLAS分词后的结果，再采用逆向最大匹配算法，根据自己补充的一个9万条词语的自定义词库（与ICTCLAS词库中的词语不重复），对ICTCLAS分词结果进行合并处理，输出最终分词结果。

由于 ICTCLAS 3.0 共享版只支持GBK编码，因此，如果是UTF-8编码的字符串，可以先用PHP的iconv函数转换成GBK编码，再用phpcws_split函数进行分词处理，最后转换回UTF-8编码。

7、KTDictSeg 一个C#.net做的简单快速准确的开源中文分词组件(这个分词算法效果也不错)
[http://www.cnblogs.com/eaglet/archive/2007/05/24/758833.html](http://www.cnblogs.com/eaglet/archive/2007/05/24/758833.html)

代码下载：[http://download.csdn.net/source/521857](http://download.csdn.net/source/521857)

本文来自CSDN博客，转载请标明出处：http://blog.csdn.net/allenshi_szl/archive/2009/11/24/4864698.aspx



