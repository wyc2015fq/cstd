# 人工智能系统研究的9大挑战和4大趋势 - 算法与数学之美 - CSDN博客
2017年12月02日 00:00:00[算法与数学之美](https://me.csdn.net/FnqTyr45)阅读数：350

摘要：近年来，随着计算机视觉、语音识别、机器翻译的技术的发展和商业化，及诸如数字广告和智能基础设施等基于机器学习的后台技术的普遍部署，人工智能已经从实验室的研究项目变成了实际生产系统不可或缺的关键技术。正是因为积累的海量数据、计算能力前所未有的发展高度、机器学习方法的不断进展、系统软件和架构的持续创新、及方便这些技术落地的开源项目和云计算平台，促使了人工智能技术的广泛应用。
>>>> 
下一代人工智能系统将更广泛地影响我们的生活，人工智能将会通过与环境交互替人类进行更关键的和更加个性化的决策。若想要人工智能发挥更大的作用，我们将面临诸多极具挑战性的问题：我们需要人工智能系统可以在各种极端情况下及时做出安全的决策，比如在各种恶意攻击情况下具备鲁棒性，在保证隐私的情况下具备处理跨多组织多个人的共享数据的能力。随着摩尔定律的终结，存储和处理数据的能力将受限，这些挑战也将变得更加难以解决。在这篇文章里，我们将总结在系统领域、体系结构领域、安全领域等方面的具体研究方向。
**四大趋势：**
- 
关键性任务的人工智能（Mission-critical AI）
- 
个性化人工智能（Personalized AI）
- 
跨多组织机构的人工智能（AI across organizations）
- 
后摩尔定律时期的人工智能（AI demands outpacing the Moore's Law
**九大挑战：**
- 
持续学习（Continual learning）
- 
鲁棒决策（Robust decisions）
- 
可解读的决策（Explainable decisions）
- 
安全飞地（Secure enclaves）
- 
对抗学习（Adversarial learning）
- 
在保密数据上的共享学习（Shared learning on confidential data）
- 
特定领域定制的硬件（Domain specific hardware）
- 
组件化的AI系统（Composable AI systems）
- 
跨云端和边缘的系统（Cloud-edge systems）
**1、引言**
自二十世纪60年代最初提出模拟人类智能的构想以来，人工智能已经成为一种被广泛应用的工程技术，它利用算法和数据可以解决包括模式识别、学习、决策等多种问题，被工程和科学中越来越多的学科所使用，同时也需要其他多种学科的研究所支持，成为计算领域一门交叉学科技术。
计算机系统近年来推动了人工智能技术的发展，并行计算设备[31, 58, 90]和高扩展性软件系统[32, 46, 114]的进步促进了机器学习框架[14, 31, 98]和算法[18, 56, 62, 91]的发展，使人工智能可以处理真实世界的大规模问题；存储设备、众包、移动APP、物联网、数据采集成本的迅速降低[1,
 40, 80]促使了数据处理系统和人工智能技术的进一步发展[87]。在很多实际任务中，人工智能已经接近甚至超过了人类，成熟的人工智能技术不仅大大提高了网络搜索和电子商务等主流产品的服务质量，也促进了物联网、增强现实、生物技术、自动驾驶汽车等新兴产业的发展。
许多应用需要人工智能系统与现实世界的交互来进行决策，例如无人驾驶飞机、机器人手术、医疗诊断治疗、虚拟助手等。由于现实世界是不断变化的，有时甚至是意料之外的变化，这些应用需要持续（continual learning）学习、终身学习（life-long learning）[96, 109]和永动学习（never-ending
 learning）[76]。终身学习通过高效地转化和利用已经学过的知识来完成新的任务，并且要最大程度降低突发性遗忘带来的问题[71]。永动学习每次迭代处理一个任务集合，随着这个任务集合的不断变大，处理结果的质量每次迭代后越来越好。
为了满足以上这些需求，我们要面临诸多艰巨的挑战，比如如何主动探索不断动态变化的环境、如何在恶意攻击和噪音输入情况下做出安全稳定的决策、如何提高决策的可解读能力、如何设计模块化架构以简化应用系统构建等。另外，由于摩尔定律的终结，我们也不能寄希望于计算和存储能力的增强来解决这些下一代人工智能系统的问题。
解决这些难题需要体系结构、软件和算法的协同创新。这篇文章并不是解决人工智能算法和技术上的某些特定问题，而是分析系统方面的研究对人工智能技术发展的重要性，提出若干有意义的系统方面的研究方向。
**2、人工智能成功背后的原因**
人工智能在过去二十年飞速发展的原因归结于三点：1）大数据，2）高扩展性的计算机和软件系统，3）开源软件（Spark、TensorFlow、MXNet、Caffe、PyTorch、BigDL）及公有云服务（Amazon AWS、Google Cloud、MS Azure）的兴起和流行，这使研究人员可以很容易的租用GPU服务器或者FPGA服务器来验证他们的算法。
** 3、趋势和挑战**
虽然人工智能已经应用到了众多应用领域，但是人类希望在更多领域发挥人工智能的作用，包括健康医疗、交通运输、工业制造、国防、娱乐、能源、农业、销售业等等领域。大规模系统和机器学习框架已经帮助人工智能取得了一定程度的成功，我们期待计算机系统能够可以更进一步地促进人工智能的发展。我们需要考虑如下几个人工智能发展的趋势来应对挑战。
### **3.1    关键性任务的人工智能（Mission-critical AI）**
从银行交易到自动驾驶，再到机器人手术和家居自动化，人工智能开始涉及到一些关键性任务，这些应用往往与人们的生命安全息息相关。如果人工智能要在动态变化的环境中部署，人工智能系统必须能够不断地适应新环境并且学习新技能。例如，自动驾驶汽车应该快速适应各种无法预料的危险路况（如事故或冰面道路），这可以通过观察其它汽车处理这些危险的行为进行实时学习；还有基于人工智能的入侵检测系统必须在入侵行为发生后立刻迅速地检测到新的攻击行为。另外，这些关键性任务也必须能够处理各种噪声数据及防范各种恶意的人为攻击。
挑战：通过与动态变化的环境不断交互，设计可以不断学习和自适应的人工智能系统，使其可以做出及时、稳定、安全的决策。
### **3.2    个性化人工智能（Personalized AI）**
从虚拟助理到自动驾驶和政治竞选，考虑用户行为（如虚拟助理要学习用户的口音）和用户偏好（如自动驾驶系统要学习用户的驾驶习惯和偏好）的个性化决策越来越重要。这就需要采集大量敏感的用户个人信息，对这些敏感信息的滥用有可能会反过来泄漏用户的隐私。
挑战：设计支持个性化服务的系统，同时要保护用户的隐私和保证用户的安全。
### **3.3    跨多组织机构的人工智能（AI across organizations）**
各大公司利用第三方数据来提升他们自己的人工智能服务的质量[27]，许多医院开始共享他们的数据来防止疫情暴发，金融机构也会共享他们的数据来提升各自的欺诈检测能力。以前是一个公司利用自己业务收集的数据进行处理分析并提供服务，而未来将是多个公司共享数据来提供服务，这种趋势将导致数据垄断到数据生态系统的变革。
挑战：设计多组织机构数据的共享机制，支持跨多组织机构的人工智能系统，同时要保障各组织机构自己数据的保密性，甚至是共享给竞争对手的数据也要保证数据的隐私信息不被泄露。
### **3.4    后摩尔定律时期的人工智能（AI demands outpacing the Moore’s Law）**
处理和存储大数据的能力是近年来人工智能成功的关键因素，然而匹配人工智能进步需求的大数据处理能力将变得越来越困难，主要有以下两点原因：
> 
第一，数据量持续以指数级规模增长。2015年思科白皮书[25]声称，万物网（Internetof Everything）设备采集的数据量到2018年将达到400ZB，几乎是2015年估计数据量的50倍；近期研究[100]预测，到2025年，为了处理人类基因组，我们需要计算机处理能力有3到4个数量级的增长，这就需要计算机处理能力每年至少以2倍的速度增长。
第二，相对于数据爆炸，计算硬件设备处理能力的增长遇到了瓶颈[53]。DRAM内存和磁盘容量预计在未来十年才能翻倍，而CPU性能预计在未来二十年才能翻倍，这种不匹配的增长速度意味着，在未来，存储和处理大数据将变得异常困难。
挑战：开发针对特定用途（domain-specific）的架构和软件系统，以适应后摩尔定律时期人工智能应用的需要，这包括针对特定人工智能应用的定制芯片、以提高数据处理效率为目的的边缘-云联合计算系统（edge-cloud systems）、以及数据抽象技术和数据采样技术。
** 4、研究方向**
这一部分讨论如何利用系统、安全、体系结构领域的创新成果来解决之前提出的若干挑战和问题。我们总结了9个研究方向，可以分类为三大主题，包括：动态环境下的处理技术、安全的人工智能、人工智能定制的体系结构。下图总结了人工智能的4大趋势和9大研究方向的关联关系。
![0?wx_fmt=jpeg](https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/47ibaBJyUH47YgUH3GDAzj964fUlttrSxfjZExnoS64eL6nN5N7ialfbHmQm2UmOao3ElZnP7mgYHIvglIoVbYJA/0?wx_fmt=jpeg)
### **4.1    动态环境下的处理技术（Acting in dynamic environments）**
相对于目前主流的针对静态数据进行学习，未来的人工智能应用将会在动态性更强的环境下进行，这种动态性体现为突发性、不可预期性、不可重复性等方面。例如，一队机器人负责维护一座办公楼的安全，当其中一个机器人坏掉或者一个新机器人加入后，其它机器人能够统一地更新各自的巡逻路径、巡逻目的、协同控制机制。或者由于某机器人自己的异常行为（比如被恶意控制）或者外部环境变化（比如电梯失灵）导致的环境突发性变化，所有机器人必须要重新迅速调整自己的策略。这就要求人工智能系统即使没有相关处理经验情况下也能快速响应的能力。
研究课题1：持续学习（Continuallearning）。目前许多人工智能系统，包括电影推荐、图片识别、自动翻译等，都是通过离线训练和在线预测完成的。也就是说，这些任务不是基于动态数据的持续学习来完成的，而是通过对某个时间段的静态数据进行定期学习来完成的，利用定期学习得到的模型来预测未来。通常这种定期学习每一天进行一次，通过学习前一天的数据来更新模型，最好的情况也是每小时进行一次，但是预测和决策是需要每分每秒都发生的，定期学习有可能是利用过时的数据学习，这就导致了定期学习无法适应持续动态变化的环境，特别是对于关键性任务定期学习就会更加危险了。甚至于某些任务需要人工智能能够持续地学习和适应异步的变化，这就使持续学习变得更加困难了。
适应动态环境的学习在某些方面可以应用在线学习（online learning）[17]解决，在线学习基于随时到来的数据更新模型，但是传统在线学习只能简单地应对数据的变化，不能应对环境变化（比如机器人的例子），另外传统在线学习需要对它的动作及时进行收益打分反馈以更新自己的模型，不能适应反馈延迟的复杂情况（比如下棋时的收益反馈只能在整局棋结束才知道，即输或者赢）。
这些情况可以利用增强学习（Reinforcementlearninig）来解决，增强学习的核心任务是学习一个策略函数，它以最大化某长远收益为目标，建立一个观察值到输出行为的映射关系。比如在自动驾驶中以避免碰撞为目标，建立一个汽车摄像头拍摄的图像到减速动作的映射，或者在推荐系统中以增加销售量为目标，建立一个用户访问网页请求到显示某广告动作的映射。增强学习算法根据用户动作对环境的影响变化来更新模型策略，如果由于环境的变化又导致了收益的变化，它也会相应地更新模型策略。增强学习其实在某些领域已经取得了很大的成功，包括在十五子棋[108]、学习行走[105]、基本运动技能的学习[86]等领域都可以达到很好的效果。但是，它需要对每一个应用进行有针对性的调整。近期将深度神经网络和增强学习结合的方法（深度增强学习 Deep
 RL）可以达到更加稳定学习效果并适用于不同应用领域，包括近期Google的AlphaGo [95]，另外在医疗诊断[104]和资源管理[33]等方面都取得了成功。
支持增强学习的系统（Systems forRL）：现在许多增强学习应用依赖于模拟现实世界的反馈来解决复杂任务，通常需要万亿次的模拟来搜索可行解空间，例如在智能游戏中尝试不同变种的游戏设置，或者在机器人模拟中试验不同的控制策略。每次模拟尝试可能仅需要几毫秒，但是每次模拟需要的时间极不稳定，比如可能仅需要走几步棋就输了，也可能需要走几百步棋赢了。现实世界部署增强学习系统需要处理来自众多不同接收器观察到的环境变化数据，对应不同接收器的处理任务在处理时间、计算量、资源需求方面可能有很大不同，系统也要具有在固定时间内处理这些异构任务的能力。比如大规模集群系统要能够在一秒钟内完成上百万次的模拟，而现有的系统还远不能达到这个需求，流行的数据并行系统[59,79,114]每秒仅能处理几万或几千次的模拟，而高性能计算系统和分布式深度学习系统[2,23,82]并不能处理异构的任务，所以我们需要新的支持高效增强学习应用的系统出现。
模拟现实（Simulatedreality, SR）：与外部环境交互的能力是增强学习成功的关键，然而与现实世界的环境交互可能很久才能得到反馈（甚至几十秒几百秒），另外与现实世界环境的交互也可能会造成不可逆的物理伤害，而我们往往需要上百万次的交互才能学到一个比较好的策略模型，这就使与现实世界的交互变得不太可行。有一些算法可以减少与现实世界交互的次数[99,111,112]，更通用的方法是利用模拟现实，这样就可以在真正做出交互动作之前，利用模拟现实环境不断进行模拟和预测，执行收益最高代价最小的动作。
模拟现实使学习不但更快而且更安全，想象一个机器人在打扫房间，突然发现一个以前从没见过的新手机，如果机器人在真实世界中进行一系列的尝试来学习如果抓起这个手机的话，它可能需要很长时间的尝试，也可能由于一次用力过猛的尝试直接捏碎手机。而如果机器人能够提取手机的形状信息，并在虚拟现实环境中尝试不同的动作，学习到手机的硬度、质地、重量等信息，然后在真实环境中使用一个合理的姿势和力度抓起手机，就可以避免手机被搞坏了。
模拟现实不同于虚拟现实（virtualreality, VR），虚拟现实是模拟一个假想的环境（例如《我的世界》这个游戏，玩家可以在一个随机生成的3D世界内，以带材质贴图的立方体为基础进行游戏）或者是利用过去的真实世界场景（例如飞行模拟器），而模拟现实是模拟人工智能实体正在交互的那个真实环境。模拟现实也不同于增强现实（augmented
 reality, AR），增强现实是在真实世界场景中加入虚拟物体。
模拟现实系统最大的挑战是，为了模拟不断变化的真实世界环境，需要不断更新模拟器的参数，同时，要在做出一个动作之前执行很多次的模拟尝试。因为学习算法与真实世界交互，它可以获得很多知识来提高模拟准确度，在每次与真实环境交互后都要更新模拟器参数，并在做下一个动作之前完成很多很多次类似“如果这么做结果会怎样”的尝试，所以模拟尝试一定要很快很快。
研究内容：（1）构建支持增强学习的系统，它需要充分利用并行能力，支持动态任务图（dynamic task graphs），达到毫秒级的反应速度，并且能够在异构硬件环境中保持反应速度；（2）构建模拟现实系统，可以完全模拟（动态变化的，不可预期的）真实世界环境，并且需要实时的反应速度。
研究课题2：鲁棒决策（Robustdecisions）。人工智能替人类做出决策，特别是在关键性任务上，它应该能够在获得各种不确定的或者是错误的输入和反馈时，能够做出具有鲁棒性的决策。在统计和机器学习领域，防噪声干扰和鲁棒学习是一个核心问题，增加系统层面的支持将会显著提升传统方法的性能。例如构建可以追踪数据来源的系统，对输出不稳定的数据源特殊照顾，避免不确定性带来的影响，我们也可以利用从其它数据源获得的信息来帮助构建基于每个数据源的噪音模型（例如发现遮挡的摄像头），这些能力要求数据存储系统具有对数据源检查和噪音建模的能力。有两种鲁棒性对于人工智能系统尤为重要：（1）在噪音输入的情况下和恶意虚假反馈情况下的鲁棒学习能力；（2）在存在意外输入（unforeseen
 inputs）和对抗输入（adversarial inputs，对抗输入是扮演攻击角色，试图用来引发模型出错的机器学习模型的输入）的情况下的鲁棒决策能力。
学习系统使用从不可靠的数据源获得的数据，这些数据可能是打了不正确的标签，有些时候可能是故意的。例如微软的Tay聊天机器人就过于依赖与人类的交流来提高对话能力了，当被放在Twitter上与人交流一段时间后，Tay就学坏了[16] 。（在Tay推出一天之后，因为Tay开始有一些种族歧视之类的偏激言论，因此微软暂时关闭了Tay的Twitter账号，这些言论明显的是和网络上一些有偏激言论的人互动后，被刻意教导而出现的）
除了处理噪声数据，另外一个研究问题是应对与训练数据分布完全不同的输入，我们希望系统能够判断出这些反常数据并做出安全的反应动作，比如在自动驾驶中的安全动作就是减速停车，或者是如果有人在旁边的话，系统能够把控制权交给人类。最好是设计一个模型可以明确拒绝对其不确信的输入进行反应，或者是执行一个默认安全的动作，这样可以大大降低计算开销并且执行准确可靠的动作。
研究内容：（1）构建具有精确追踪数据来源能力的人工智能系统，可以将收益变化与每个数据来源进行联系，能够自动学习基于每个数据源的噪音模型；（2）设计可以指定决策置信区间的编程接口和语言，允许用户根据实际应用的安全程度需要指定置信区间，并且能够标识反常的数据输入。
研究课题3：可解读的决策（Explainabledecisions）。除了黑盒预测和决策，人工智能系统往往需要向人类解释他们的决策，这往往在一些监管性的任务，还有安全和医疗等需要负法律责任的应用上尤为重要。这里的可解读性并不是可理解性（interpretable），可理解性只是强调人工智能算法的输出对于某领域的专家是可以理解的，而可解读性的意思是能够指出输入数据的那些属性导致了这个输出结果，并且能够回答反事实问题（counterfactual
 questions，虽然没有实际发生，但是假设发生了会怎样）或者回答“如果XX会怎样？”的问题。例如在医疗诊断中，我想要知道X射线检查出来某个器官的哪些指标（如大小、颜色、位置、形式）导致了这个诊断结果，如果那些指标稍微变化一点的话结果会有什么样的变化，或者是我想知道是否有其他指标组合也会导致同样的诊断，哪些指标组合最有可能导致这个诊断。我们不仅想要解释这个输出结果，还要知道哪些其他的输入也会导致这个结果，这种因果推断（causal inference）是未来许多人工智能任务的必备功能。
实际上，支持决策可解读性的关键一点是，记录和重现导致某一决策结果的计算过程的能力，这就需要系统层面的支持，系统根据过去导致某决策输出的输入数据可以重现计算，或者根据随机的或者对抗性的输入，或者根据反事实的输入，如果系统能够具有这些根据不同输入重现计算的能力，就可以帮助人工智能系统分析输入和输出的因果关系，提高决策的可解读能力。例如基于视频的安全警报系统，它想要找出什么原因导致了一个错误警报，可以通过扰动输入视频数据（比如遮挡视频图像的某些区域），或者是通过用近期相似的历史数据来尝试，看这些尝试是否会导致同样的错误警报，或者是看对警报发生概率的影响。这样的系统支持也能帮助提高新模型的统计判断能力和训练测试效果，例如设计一些有解读能力的新模型。
研究内容：构建具有交互诊断分析能力的AI系统，它可以完全重现执行过程，并可以帮助分析那些对结果起关键作用的输入，这可以是通过尝试各种扰动的输入来尝试重现决策结果，甚至是使系统具有因果推断能力。
### **4.2    安全的人工智能（Secure AI）**
安全是个广泛的课题，人工智能应用普及和发展的关键往往都是安全相关的问题。例如，执行关键性任务的人工智能应用，个性化学习，跨组织结构的学习，这些都需要系统具有很强的安全性。安全问题的涉及面很广，我们这里只关注两大类安全问题。第一类是攻击者影响决策的正确性：攻击者可以通过破坏和控制AI系统本身，或者通过特意改变输入来使系统不知不觉地做出攻击者想要的决定。第二类是攻击者获取AI系统训练的保密数据，或者破解加密模型。接下来，我们讨论三个有前途的研究方向来抵御这种攻击。
研究课题4：安全飞地（Secureenclaves）。（飞地：某国家拥有一块与本国主体分离开来的领土，该领土被其他国家包围，则该领土被称为飞地。比如在西德与东德尚未合并前，原本柏林境内属于美英法占领区所合并的西柏林市，四周皆被苏联控制的东德领土包围，是最出名的一块飞地）公共云的迅速崛起以及软件栈的复杂性日益增加，AI应用程序受到攻击的风险大大增加。二十年前，大多数应用程序都运行在商业操作系统（如Windows或SunOS）之上，位于企业防火墙后部署的单个服务器上。今天，各企业公司可能在公共云上的分布式服务器上运行AI应用程序，这些租用的服务器是他们无法控制的，很可能与其竞争对手共享的一个复杂的软件栈，操作系统本身运行在虚拟机管理程序之上或在容器内。而且这些应用程序直接或间接地共享着其他系统，如日志摄取系统，存储系统和数据处理框架。如果这些软件组件中的任何一个受到危害，AI应用程序本身可能会受到影响。
处理这些攻击的一般方法是提供一个“安全飞地”抽象，就是一个安全的硬件执行环境，它保护飞地内运行的应用程序免受在飞地外运行的恶意代码的影响。最近的例子是英特尔的软件防护扩展（SGX）[5]，它提供了一个硬件隔离的执行环境。SGX内部的代码可以根据输入数据进行计算，即使是受损的操作系统或管理程序（在飞地之外运行）也无法看到这些代码或数据。SGX还提供了远程认证[6]，一个协议使远程客户端能够验证该飞地是否正在运行预期的代码。ARM的TrustZone是另一个硬件飞地的例子。另一方面，云服务提供商开始提供物理保护的特殊裸机实例，它们部署在安全的“保险柜”中，只有授权人员通过指纹或虹膜扫描进行身份验证才有权访问。
一般来说，使用任何飞地技术，应用程序开发人员必须信任飞地内运行的所有软件。而事实上，即使在硬件飞地里，如果在飞地内运行的代码受到入侵，也可能泄露解密的数据或影响决策。由于小型代码库通常更容易保护，所以一个研究的方向是将AI系统的代码拆分成在飞地内运行的代码，并且让其尽可能少，然后在不可信环境下通过利用密码技术运行另一部分代码。另一种确保飞地内的代码不会泄露敏感信息的方法是开发静态和动态验证工具以及沙盒方法（sandboxing）[9,12,93]。
请注意，除了最小化可信计算区域之外，分割应用程序代码还有两个额外好处：增加功能性和降低成本。首先，某些功能可能在飞地内不可用，例如用于运行深度学习（DL）算法的GPU处理，或未经审查/移植以在安全飞地内运行的服务和应用程序。其次，由云提供商提供的安全实例可能比常规实例贵得多。
研究内容：建立利用安全飞地的AI系统，以确保数据的保密性、用户隐私和决策正确性，将AI系统的代码拆分为在飞地内运行的最小代码库和在飞地外运行的代码，保证该飞地不泄露信息和不损害决策的正确性。
研究课题5：对抗学习（Adversariallearning）。机器学习算法的自适应特性使学习系统面临新型的攻击，比如通过恶意地改变训练数据或决策输入来影响决策的正确性。有两种广泛的攻击类型：闪避攻击（evasion attacks）和药饵攻击（data poisoning attacks）。
闪避攻击发生在推理阶段，攻击者试图制作被学习系统错误分类的数据[47,103]。比如略微改变一个停车标志的形象，虽然人类仍然认为它是一个停车标志，但自动驾驶汽车可能视为一个避让标志。
药饵攻击发生在训练阶段，对手将药饵数据（例如，具有错误标签的数据）注入训练数据集中，导致学习系统学习错误的模型，从而使攻击者具有了导致学习器错误分类的输入数据 [73,74,113]。如果用于再训练的弱标记数据是从不可信或不可靠的来源收集的，定期进行再训练的学习系统特别容易受到这种攻击。随着新的AI系统不断地与动态环境交互来学习，处理药饵攻击变得越来越重要。
现在还没有什么有效的解决方案来防范闪避攻击，所以有一些研究挑战：解释为什么对抗攻击往往容易发现，发现可以有效地防御攻击者的方法，评估防御措施的防御能力。对于药饵攻击，研究挑战包括如何检测药饵输入数据，以及如何建立适应不同类型药饵攻击的学习系统。另外，因为数据来源被认定为具有欺诈性或因监管原因被明确撤回的数据源，我们可以利用重现技术（参见研究课题3：可解读的决策）和增量计算来有效地消除这些来源对学习模型的影响。正如前面所指出的，这种能力是通过在数据存储系统中将建模与数据来源和有效计算结合起来实现的。
研究内容：构建具有对抗学习能力的AI系统，在训练和预测期间，通过设计新的机器学习模型和网络体系结构追踪欺诈数据源，在去掉欺诈数据源后重现或重做计算以获取新的正确的决策。
研究课题6：在保密数据上的共享学习（Sharedlearning on confidential data）。如今，每家公司通常都会收集数据，分析数据，并使用这些数据来实现新的功能和产品。然而，并不是所有的企业都拥有像Google，Facebook，微软和亚马逊这样的大型AI公司所拥有的大量数据。展望未来，我们期待越来越多的公司会收集有价值的数据，会出现更多的第三方数据服务公司，并从多个公司组织的数据中获取更多的好处（参见第3节）。
事实上，根据我们与工业界的合作经历，我们发现这种情况越来越多。一家大银行为我们提供了一个场景，他们和其他银行希望将他们的数据汇集在一起，并使用共享的学习来改进他们的合作欺诈检测算法。虽然这些银行在金融服务方面是竞争对手，但这种“合作”对于减少由于欺诈活动而造成的损失对他们来说至关重要。另外，一个非常大的医疗保健提供商描述了一个类似的情景，其中有竞争关系的多家医院希望共享数据来训练一个预测流感暴发的共享模型，但是分享的数据不能用作其他目的。这将使他们能够提高对流行病的反应速度并控制疾病暴发，在关键地点迅速部署流动疫苗接种车。同时，每家医院都要保护自己医院数据中病人的隐私信息。
共享学习的关键挑战是如何利用属于不同（可能是竞争关系的）组织的数据学习模型，但同时不会在训练过程中泄漏这些数据的隐私信息。一种可能的解决方案是将所有数据集中在硬件飞地上学习这个模型，但是因为硬件飞地还没有被广泛部署，在某些情况下，由于监管约束或者数据量太大，数据无法复制到硬件飞地上。
另一个比较有前途的方法是使用安全多方计算（MPC）[13,45,70]。MPC允许n方（每方都有私人输入）计算输入的联合功能，而没有任何一方知道其他方的输入。但是，虽然MPC对于简单的计算是有效的，但是对于复杂的计算，比如模型训练来说，它有一个非常大的开销。一个有趣的研究方向是研究如何将模型训练分成（1）局部计算和（2）MPC计算，这样我们就可以最小化MPC的计算复杂度。
虽然在不影响数据保密的情况下训练模型是实现共享学习的重要一步，但是还有其他问题。模型服务，即基于模型的推断，仍然可能泄露数据的隐私信息[42,94]。应对这一挑战的一个方法是使用差分隐私（differential privacy）技术[36,37,39]，这是一种在统计数据库中的流行技术。差分隐私为每个查询增加了噪声，以保护数据隐私[35]。差分隐私的一个核心概念是隐私预算（privacy
 budgets），该隐私预算限制了提供隐私保证的查询数量。
在将差分隐私应用于模型服务时，有三个有趣的研究方向：首先，利用模型和预测的固有统计特性，应用差分隐私处理复杂模型和推理；其次，尽管理论研究很多，但目前实际应用的差异性隐私系统很少，一个重要的研究方向是构建工具和系统，以便为实际的应用程序提供差分隐私的保护能力，包括智能地选择哪个隐私机制用于给定的应用程序，并自动将非差分隐私计算转换为差分隐私计算；最后，在持续学习中数据隐私是时间相关的，即新数据的隐私远比旧数据的隐私更重要。例如股票市场和在线投标，新数据的隐私是最重要的，而历史数据是不重要的甚至有时是公开的，可以开发具有自适应隐私预算的差分隐私系统，只为最新的数据的进行差分隐私保护，另一个研究方向是在数据公开后进一步发展差分隐私[21,38
 ]。
即使我们能够在训练和决策过程中保护数据隐私，但是这还不够。事实上，即使数据隐私得到保证，组织和公司也可能拒绝分享其数据，因为这些数据可能改进竞争对手的服务质量。因此，我们需要研究激励机制以鼓励组织和公司共享其数据或其数据的副产品。具体而言，我们需要制定一些方法，让这些组织相信通过共享数据可以得到比不共享数据更好的服务（即更好的决策）。这就要求确定某个组织提供的数据的质量，这个问题可以通过排除法来解决，不论组织的数据是否包含在训练集中，都可以比较其性能，然后提供与组织提供的数据质量成反比的噪声来破坏决策，这可以激励组织提供更高质量的数据。总体而言，这种激励机制需要置于机制设计的框架内，以便组织机构制定个人数据共享策略。
研究内容：构建具有如下两个功能的人工智能系统（1）可以跨多个数据源进行学习，而不会在训练或决策期间泄漏数据的隐私信息；（2）提供激励策略，以促使潜在竞争组织共享其数据。
### **4.3    AI定制的体系结构（AI-speci!carchitectures）**
对AI的需求将会带来系统和硬件架构的双重革新。这些新式架构既可以提升性能，同时也会通过提供易于组合的丰富的模块化库来简化下一代AI应用的开发。
研究课题7：特定领域定制的硬件（Domainspecific hardware）。处理和存储巨量的数据的能力是AI成功的关键因素之一（见2.1节），但是维持这种处理存储能力增长的速度将会越来越具有挑战性。正如第3部分所说，数据持续仍然呈指数级地增长，但40多年来支撑计算机工业发展的性能、成本、能耗方面的改进速度将放缓：
摩尔定律的终结导致晶体管不会变得太小，
登纳德缩放定律（Dennardscaling）的失效使得电能限制了芯片的承载规模，
我们已经从一个低效的处理器/芯片过渡到了一个芯片装载多个高效处理器，但是阿姆达尔定律（Amdahl’s Law）说明了并行处理的极限.
设计专用领域的处理器是保持处理器消耗能源带来性能上提升的方法之一。这些处理器只会做少量的特定任务但是会将它们做到极致。因此摩尔定律下的处理器性能的快速提升一定是基于计算机架构的革新而不是半导体工艺的改进。未来的服务将会涉及到比过去更加多元化的处理器。一个具有开拓性的专用领域处理器的例子就是谷歌的张量处理单元（Tensor
 Processing Unit, TPU），它在2015被部署在谷歌的数据中心并由数十亿人使用。相较于同期的CPU和GPU，它将深度神经网络的前馈阶段的执行速度提升了15到30倍，而且每瓦特的性能也提升了30到80倍。此外，微软已宣布在他的Azure云平台上部署了FPGA[88]。一系列的包括英特尔、IBM以及一些例如Cerebras和Graphcore这种初创公司在内的公司都在开发针对AI的特殊硬件，这会保持超越当前主流处理器的巨大性能提升的规律[19,48,54,78]。
考虑到DRAM也显露出了同样的极限，人们正在开发一些新奇的技术成为它的继任者。来自英特尔和镁光的3D XPoint旨在保持类DRAM访存性能的基础上提供10倍的存储能力。STTMRAM想要发展有类似于DRAM的扩展限制的闪存技术。因此云端可能会有更多级的存储和记忆能力，包含更广泛的技术。有了这些越来越多样化的处理、记忆和存储的设备之后，如何让服务匹配到硬件资源便成为一个挑战性更大的难题。相比于包括一个机柜顶部交换机和数十台服务器每个配备两个CPU、1TB的DRAM和4TB的闪存的经典标准机架设计，这些快速的变化要求我们建设更加灵活的云计算平台。
例如，UCBerkeley Firebox项目[41]提出了一种多机架的超级计算机，它可以通过光纤将数以千计的处理器芯片与DRAM芯片和非易失性的存储介质链接起来，提供低延时、高带宽和长距离的传输能力。像这样的硬件系统可以支撑系统软件基于恰当比例和类型的专用领域的处理器、DRAM和NVRAM来提供计算服务。这种大规模分离式的资源可以极大提升愈加多样化的任务到与之匹配的多样化资源的分配。这对AI的工作负载十分有价值，它可以从大规模存储中获得显著的性能提升，并且适合应对多样化的资源需求。
除了性能提升之外，新的硬件架构也会带来其他额外功能，例如对安全的支持。尽管英特尔的SGX和ARM的TrustZone正在涉足硬件安全环境，但在它们可以完全应用于AI应用之前还有很多工作要做。特别是现存的安全环境显示出包括定址存储在内的多种资源极限，它们仅是服务于一般目的的CPU。打破这种限制，提供这些包括GPU和TPU在内的专用处理器之上的通用硬件抽象便是未来的研究方向。此外，像RISC-V的开放指令集处理器代表着一种令人激动的开发新安全特性的大环境趋势。
研究内容：（1）设计专用领域的硬件架构来以数量级为单位提升性能降低AI应用消耗的能源，或者加强这些应用的安全性。（2）设计AI软件系统，利用这些专用领域的架构、资源分离式的结构和未来的非易失性存储技术。
研究课题8：组件化的AI系统（Composable AI systems）。模块化和组件化技术在软件系统快速更新中扮演着重要的角色，他们使开发人员能够基于现有组件快速构建产生新的系统。这样的例子包括微内核操作系统，LAMP栈，微服务架构和网络。与此相反的是，现有的AI系统则是一整块的，这便导致系统很难开发测试和更新。
与此类似，模块化和组件化将会成为提升AI系统开发速度和应用度的关键，这会使在复杂系统中集成AI更加地容易。接下来，我们探讨几个关于模型和动作组合的研究问题。
模型组合（modelcomposition）对于开发更加复杂强大的AI系统至关重要。在一个模型服务系统中组合多种模型并以不同模式应用它们可以取得决策精度、延迟和吞吐量之间的折中效果。例如，我们可以序列化的查询模型，每一个模型可以反馈一个高准确度的决策或者说“我不知道”，然后决策会被传递到下一个模型。按照从最高到最低的“我不知道”比率和从最低到最高的延迟度来对模型加以排序，我们就可以同时优化延迟度和精确度了。
要想充分应用模型组合，仍然有很多难题亟待解决。例如（1）需要设计一种声明式语言（declarative language）来描述这些组件之间的拓扑结构和应用的性能目标，（2）为每个组件提供包括资源需求量、延时和产能在内的精确的性能模型，（3）通过调度和优化算法来计算出这些组件执行的计划，以及以最低消耗将组件匹配到相应的资源上来满足延时和吞吐量要求。
动作组合（actioncomposition）是将基本的决策/动作序列组织成低级原语，也称为选项（options）。例如对于无人驾驶汽车，某一个选项可以是当在高速公路上行驶时变更车道，执行的动作包括了加速减速，左右转向，打开转向灯等。对于机器人，某一个原语可以是抓取物体，执行的工作包括转动机器人的关节。选项已经在层级学习中被广泛的研究[30,34,84,97,102,110]，它可以通过让代理选择一系列既存的选项来完成给定的任务而不是从更长的低级动作列表中选择，以此来极大地加速对新环境的学习或适应性。
丰富的选项库会使得新AI应用的开发就像当前的web开发人员通过调用强大的web接口以几行代码开发出应用一样，通过简单的组合恰当的选项来实现。另外，选项可以提升响应能力，因为基于选项来选择下一个动作要比在原始动作空间中选择一个动作简单得多。
研究内容：设计AI系统和接口，使得模型和动作以一种模块化和灵活的方式进行组合，同时应用这些接口来开发丰富的模型和选项库以此极大简化AI应用的开发。
研究课题9：跨云端和边缘的系统（Cloud-edgesystems）。当今很多AI应用例如语音识别、自然语言翻译是部署在云上的。接下来我们预计跨边缘设备和云端的AI系统将快速增加。一方面，将当前仅在云上部署的AI系统例如用户推荐系统，迁移他们的部分功能到边缘设备上，这样可以提高安全性、保护隐私和降低时延（包括解决无法连接网络的问题）。另一方面，当前部署在边缘设备上的AI系统例如自动驾驶汽车、无人机和家用机器人都需要与云端共享数据且利用云端的计算资源来更新模型和策略[6]。
然而由于多种原因，要开发跨云和边缘的系统富有挑战。首先，边缘设备和云端数据中心服务器之间的计算能力相差很大。未来这种差距会更大，因为包括智能手机和输入板在内的边缘设备相较于数据中心的服务器有严格的电量和体积大小的极限。第二，各边缘设备之间在计算资源和能力上存在异构性，从低级的ARM或支持物联网设备的RISC-V CPU到自动驾驶汽车和软件平台的高性能GPU。这种异构性导致应用开发的难度加大。第三，边缘设备的硬件和软件更新周期要远远慢于数据中心。第四，随着存储能力提升速度的放缓而数据产生速度的持续加快，再去存储这些海量数据可能不再可行或者变得低效。
有两种方法可以解决云和边缘设备的融合。一个是通过多目标软件设计和编译技术将代码重定义到多样化的平台上面。为了解决边缘设备多样化的情况和升级运行在这些设备上的应用的困难，我们需要定义新的软件栈来抽象多种设备，将硬件能力以通用API的形式暴露给应用。另一个可能的方向是开发编译器和及时编译技术从而有效的编译正在运行的复杂算法，使它们能够在边缘设备上运行。这可以使用近期的代码生成工具，例如TensorFlow和XLA、Halide和Weld。
第二个通用方法是设计适应于在云和边缘云上分割执行的AI系统。例如，模型组合（见4.3）可以是在边缘设备上运行轻量级低精度的模型而在云上运行计算密集型高精度的模型。这种架构可以在不损失精确度的情况下降低延时，而且已经在最近的视频推荐系统中被采用[59,115]。再比如，动作组合可以是将对层级选项的学习放在云端，而这些选项的执行放在边缘设备上。
机器人是另一个可以利用边缘云架构的领域。当前很是缺乏机器人应用的开源平台。作为当今广泛使用的这种平台的ROS被限制只在本地运行而且缺少实时应用所需要的性能优化。为了利用AI研究的新成果，例如共享学习和持续学习，我们需要跨云和边缘云的系统，他可以允许开发者在机器人和云之间无缝地迁移功能，从而优化决策延时和学习收敛。云平台可以通过利用来自实时分布式的机器人收集到的信息运行复杂算法持续更新模型，而机器人可以基于之前下载的模型策略在本地持续地执行动作。
为了解决从边缘设备收集到的大量数据，可以采用适应于学习的压缩方法来减少处理开销，例如通过采样（sampling）和梗概（sketching），这些方法都已经成功的应用在分析工作负载的任务上了[4,10,28,51,81]。一个研究方向就是以系统化的方式利用采样和梗概的方法来支持多种学习算法和预测任务。一个更大的挑战是减小存储消耗，这可能需要删除数据。关键是我们不知道未来数据会如何被使用。这是一个压缩问题，而且是针对于机器学习算法的压缩。此外，基于采样和梗概的分布式方法可以帮助解决该问题，就像机器学习方法在特征选择或模型选择策略上的应用一样。
研究方向：设计跨云端和边缘的AI系统，（1）利用边缘设备减小延时、提升安全性并实现智能化的数据记忆技术，（2）利用云平台来共享各边缘设备的数据和模型，训练复杂的计算密集型的模型和采取高质量的决策。
**5、结论**人工智能在过去十年中取得的惊人进展，使其从研究实验室的研究项目成功转化为目前可以取代大量人力的商业应用核心技术。人工智能系统和机器人不但取代了部分人类工作者，而且有挖掘人类潜力和促进新形式合作的作用[44]。
若想使人工智能更好地服务我们，要克服许多艰巨的挑战，其中许多挑战与系统和基础设施有关。人工智能系统需要做出更快、更安全和更易于解读的决策，确保这些决策在针对多种攻击类型的学习过程中得到准确的结果，在摩尔定律终结的前提下不断提高计算能力，以及构建易于整合到现有应用程序中的可组合系统，并且具有跨越云端和边缘的处理能力。
本文总结了几个系统、体系结构和安全方面的研究课题，我们希望这些问题能够启发新的研究来推动人工智能的发展，使其计算能力更强，具有可解释性、安全性和可靠性。
来源：全球人工智能
![0?wx_fmt=gif](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/0?wx_fmt=gif)
**算法数学之美微信公众号欢迎赐稿**
**稿件涉及数学、物理、算法、计算机、编程等相关领域。**
**稿件一经采用，我们将奉上稿酬。**
**投稿邮箱：math_alg@163.com**
