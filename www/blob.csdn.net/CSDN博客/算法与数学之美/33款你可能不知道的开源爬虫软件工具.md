# 33款你可能不知道的开源爬虫软件工具 - 算法与数学之美 - CSDN博客
2018年09月17日 21:36:40[算法与数学之美](https://me.csdn.net/FnqTyr45)阅读数：99
要玩大数据，没有数据怎么玩？这里推荐一些33款开源爬虫软件给大家。
2
爬虫，即网络爬虫，是一种自动获取网页内容的程序。是搜索引擎的重要组成部分，因此搜索引擎优化很大程度上就是针对爬虫而做出的优化。
网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重复上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索；对于聚焦爬虫来说，这一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。
世界上已经成型的爬虫软件多达上百种，本文对较为知名及常见的开源爬虫软件进行梳理，按开发语言进行汇总。虽然搜索引擎也有爬虫，但本次我汇总的只是爬虫软件，而非大型、复杂的搜索引擎，因为很多兄弟只是想爬取数据，而非运营一个搜索引擎。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxRpNmhjWruVDIrfp6TqicGzC7azCfZUKkXwTEdDicojnBCO8k0NcER1RYFrZoTulDaBibMxZxN2RNeQ/640?wx_fmt=png)
## ** Java爬虫 **
2.crawlzilla
crawlzilla 是一个帮你轻松建立搜索引擎的自由软件，有了它，你就不用依靠商业公司的搜索引擎，也不用再烦恼公司內部网站资料索引的问题。
由 nutch 专案为核心，并整合更多相关套件，并卡发设计安装与管理UI，让使用者更方便上手。
crawlzilla 除了爬取基本的 html 外，还能分析网页上的文件，如（ doc、pdf、ppt、ooo、rss ）等多种文件格式，让你的搜索引擎不只是网页搜索引擎，而是网站的完整资料索引库。
拥有中文分词能力，让你的搜索更精准。
crawlzilla的特色与目标，最主要就是提供使用者一个方便好用易安裝的搜索平台。
授权协议： Apache License 2
开发语言： Java JavaScript SHELL
操作系统： Linux
> 
**项目主页： https://github.com/shunfa/cra...**
**下载地址： http://sourceforge.net/projec...**
特点：安装简易，拥有中文分词功能
#### 4.Heritrix
Heritrix 是一个由 java 开发的、开源的网络爬虫，用户可以使用它来从网上抓取想要的资源。其最出色之处在于它良好的可扩展性，方便用户实现自己的抓取逻辑。
Heritrix采用的是模块化的设计，各个模块由一个控制器类（CrawlController类）来协调，控制器是整体的核心。
**代码托管：https://github.com/internetar...**
授权协议： Apache
开发语言： Java
操作系统： 跨平台
特点：严格遵照robots文件的排除指示和META robots标签
#### 6.ItSucks
ItSucks是一个java web spider（web机器人，爬虫）开源项目。支持通过下载模板和正则表达式来定义下载规则。提供一个swing GUI操作界面。
特点：提供swing GUI操作界面
#### 8.JSpider
JSpider是一个用Java实现的WebSpider，JSpider的执行格式如下：
jspider [URL] [ConfigName]
URL一定要加上协议名称，如：http://，否则会报错。如果省掉ConfigName，则采用默认配置。
JSpider 的行为是由配置文件具体配置的，比如采用什么插件，结果存储方式等等都在conf[ConfigName]目录下设置。JSpider默认的配置种类 很少，用途也不大。但是JSpider非常容易扩展，可以利用它开发强大的网页抓取与数据分析工具。要做到这些，需要对JSpider的原理有深入的了 解，然后根据自己的需求开发插件，撰写配置文件。
授权协议： LGPL
开发语言： Java
操作系统： 跨平台
特点：功能强大，容易扩展
#### 10.MetaSeeker
是一套完整的网页内容抓取、格式化、数据集成、存储管理和搜索解决方案。
网络爬虫有多种实现方法，如果按照部署在哪里分，可以分成：
1、服务器侧：一般是一个多线程程序，同时下载多个目标HTML，可以用PHP， Java, Python(当前很流行）等做，可以速度做得很快，一般综合搜索引擎的爬虫这样做。但是，如果对方讨厌爬虫，很可能封掉你的IP，服务器IP又不容易 改，另外耗用的带宽也是挺贵的。建议看一下Beautiful soap。
2、客户端：一般实现定题爬虫，或者是聚焦爬虫，做综合搜索引擎不容易成功，而垂直搜诉或者比价服务或者推荐引擎，相对容易很多，这类爬虫不是什么页面都 取的，而是只取你关系的页面，而且只取页面上关心的内容，例如提取黄页信息，商品价格信息，还有提取竞争对手广告信息的，搜一下Spyfu，很有趣。这类 爬虫可以部署很多，而且可以很有侵略性，对方很难封锁。
MetaSeeker中的网络爬虫就属于后者。
MetaSeeker工具包利用Mozilla平台的能力，只要是Firefox看到的东西，它都能提取。
MetaSeeker工具包是免费使用的，下载地址：**www.gooseeker.com/cn/node/download/front**
特点：网页抓取、信息提取、数据抽取工具包，操作简单
#### 12.Spiderman
Spiderman 是一个基于微内核+插件式架构的网络蜘蛛，它的目标是通过简单的方法就能将复杂的目标网页信息抓取并解析为自己所需要的业务数据。
怎么使用？
首先，确定好你的目标网站以及目标网页（即某一类你想要获取数据的网页，例如网易新闻的新闻页面）
然后，打开目标页面，分析页面的HTML结构，得到你想要数据的XPath，具体XPath怎么获取请看下文。
最后，在一个xml配置文件里填写好参数，运行Spiderman吧！
授权协议： Apache
开发语言： Java
操作系统： 跨平台
特点：灵活、扩展性强，微内核+插件式架构，通过简单的配置就可以完成数据抓取，无需编写一句代码
#### 14.Web-Harvest
Web-Harvest是一个Java开源Web数据抽取工具。它能够收集指定的Web页面并从这些页面中提取有用的数据。Web-Harvest主要是运用了像XSLT,XQuery,正则表达式等这些技术来实现对text/xml的操作。
其实现原理是，根据预先定义的配置文件用httpclient获取页面的全部内容（关于httpclient的内容，本博有些文章已介绍），然后运用XPath、XQuery、正则表达式等这些技术来实现对text/xml的内容筛选操作，选取精确的数据。前两年比较火的垂直搜索（比如：酷讯等）也是采用类似的原理实现的。Web-Harvest应用，关键就是理解和定义配置文件，其他的就是考虑怎么处理数据的Java代码。当然在爬虫开始前，也可以把Java变量填充到配置文件中，实现动态的配置。
授权协议： BSD
开发语言： Java
特点：运用XSLT、XQuery、正则表达式等技术来实现对Text或XML的操作，具有可视化的界面
#### 16.YaCy
YaCy基于p2p的分布式Web搜索引擎.同时也是一个Http缓存代理服务器.这个项目是构建基于p2p Web索引网络的一个新方法.它可以搜索你自己的或全局的索引,也可以Crawl自己的网页或启动分布式Crawling等.
授权协议： GPL
开发语言： Java Perl
操作系统： 跨平台
特点：基于P2P的分布式Web搜索引擎
## ** Python爬虫 **
#### 18.PyRailgun
这是一个非常简单易用的抓取工具。支持抓取javascript渲染的页面的简单实用高效的python网页爬虫抓取模块
授权协议： MIT
开发语言： Python
操作系统： 跨平台 Windows Linux OS X
特点：简洁、轻量、高效的网页抓取框架
备注：此软件也是由国人开放
**github下载：https://github.com/princehaku...**
#### 20.hispider
HiSpider is a fast and high performance spider with high speed
严格说只能是一个spider系统的框架, 没有细化需求, 目前只是能提取URL, URL排重, 异步DNS解析, 队列化任务, 支持N机分布式下载, 支持网站定向下载(需要配置hispiderd.ini whitelist).
特征和用法:
- 
基于unix/linux系统的开发
- 
异步DNS解析
- 
URL排重
- 
支持HTTP 压缩编码传输 gzip/deflate
- 
字符集判断自动转换成UTF-8编码
- 
文档压缩存储
- 
支持多下载节点分布式下载
- 
支持网站定向下载(需要配置 hispiderd.ini whitelist )
- 
可通过 http://127.0.0.1:3721/ 查看下载情况统计,下载任务控制(可停止和恢复任务)
- 
依赖基本通信库libevbase 和 libsbase (安装的时候需要先安装这个两个库)
工作流程:
- 
从中心节点取URL(包括URL对应的任务号, IP和port,也可能需要自己解析)
- 
连接服务器发送请求
- 
等待数据头判断是否需要的数据(目前主要取text类型的数据)
- 
等待完成数据(有length头的直接等待说明长度的数据否则等待比较大的数字然后设置超时)
- 
数据完成或者超时, zlib压缩数据返回给中心服务器,数据可能包括自己解析DNS信息, 压缩后数据长度+压缩后数据,如果出错就直接返回任务号以及相关信息
- 
中心服务器收到带有任务号的数据, 查看是否包括数据, 如果没有数据直接置任务号对应的状态为错误, 如果有数据提取数据种link然后存储数据到文档文件
- 
完成后返回一个新的任务
授权协议： BSD
开发语言： C/C++
操作系统： Linux
特点：支持多机分布式下载, 支持网站定向下载
#### 22.Methabot
Methabot 是一个经过速度优化的高可配置的 WEB、FTP、本地文件系统的爬虫软件。
授权协议： 未知
开发语言： C/C++
操作系统： Windows Linux
特点：过速度优化、可抓取WEB、FTP及本地文件系统
**源代码：http://www.oschina.net/code/t...**
## C#爬虫
#### 24.Sinawler
国内第一个针对微博数据的爬虫程序！原名“新浪微博爬虫”。
登录后，可以指定用户为起点，以该用户的关注人、粉丝为线索，延人脉关系搜集用户基本信息、微博数据、评论数据。
该应用获取的数据可作为科研、与新浪微博相关的研发等的数据支持，但请勿用于商业用途。该应用基于.NET2.0框架，需SQL SERVER作为后台数据库，并提供了针对SQL Server的数据库脚本文件。
另外，由于新浪微博API的限制，爬取的数据可能不够完整（如获取粉丝数量的限制、获取微博数量的限制等）
本程序版权归作者所有。你可以免费: 拷贝、分发、呈现和表演当前作品,制作派生作品。 你不可将当前作品用于商业目的。
5.x版本已经发布！ 该版本共有6个后台工作线程：爬取用户基本信息的机器人、爬取用户关系的机器人、爬取用户标签的机器人、爬取微博内容的机器人、爬取微博评论的机器人，以及调节请求频率的机器人。更高的性能！最大限度挖掘爬虫潜力！ 以现在测试的结果看，已经能够满足自用。
本程序的特点：
> 
***6个后台工作线程，最大限度挖掘爬虫性能潜力！***
***界面上提供参数设置，灵活方便***
***抛弃app.config配置文件，自己实现配置信息的加密存储，保护数据库帐号信息***
***自动调整请求频率，防止超限，也避免过慢，降低效率***
***任意对爬虫控制，可随时暂停、继续、停止爬虫***
***良好的用户体验***
授权协议： GPLv3
开发语言： C# .NET
操作系统： Windows
#### 26.Web Crawler
mart and Simple Web Crawler是一个Web爬虫框架。集成Lucene支持。该爬虫可以从单个链接或一个链接数组开始，提供两种遍历模式：最大迭代和最大深度。可以设置 过滤器限制爬回来的链接，默认提供三个过滤器ServerFilter、BeginningPathFilter和 RegularExpressionFilter，这三个过滤器可用AND、OR和NOT联合。在解析过程或页面加载前后都可以加监听器。介绍内容来自Open-Open
开发语言： Java
操作系统： 跨平台
授权协议： LGPL
特点：多线程，支持抓取PDF/DOC/EXCEL等文档来源
#### 28.OpenWebSpider
OpenWebSpider是一个开源多线程Web Spider（robot：机器人，crawler：爬虫)和包含许多有趣功能的搜索引擎。
授权协议： 未知
开发语言： PHP
操作系统： 跨平台
特点：开源多线程网络爬虫，有许多有趣的功能
#### 30.ThinkUp
ThinkUp 是一个可以采集推特，facebook等社交网络数据的社会媒体视角引擎。通过采集个人的社交网络账号中的数据，对其存档以及处理的交互分析工具，并将数据图形化以便更直观的查看。
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxRpNmhjWruVDIrfp6TqicGzYhIgZeZ1wicubqFO5A8IZ7hYwvxkHG8HjZbgLWYfglKYIYhnZc1BL3w/640?wx_fmt=png)
授权协议： GPL
开发语言： PHP
操作系统： 跨平台
**github源码：https://github.com/ThinkUpLLC...**
特点：采集推特、脸谱等社交网络数据的社会媒体视角引擎，可进行交互分析并将结果以可视化形式展现
32.Ebot
Ebot 是一个用 ErLang 语言开发的可伸缩的分布式网页爬虫，URLs 被保存在数据库中可通过 RESTful 的 HTTP 请求来查询。
授权协议： GPLv3
开发语言： ErLang
操作系统： 跨平台
**github源代码：https://github.com/matteoreda...**
**项目主页： http://www.redaelli.org/matte...**
特点：可伸缩的分布式网页爬虫
## ** Ruby爬虫 **
#### 33.Spidr
Spidr 是一个Ruby 的网页爬虫库，可以将整个网站、多个网站、某个链接完全抓取到本地。
开发语言： Ruby
授权协议：MIT
特点：可将一个或多个网站、某个链接完全抓取到本地
以上内容来自36大数据（36dsj.com)：36大数据：33款可用来抓数据的开源爬虫软件工具
∑编辑 | Gemini
来源 | 知乎 数据分析师
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png)
算法数学之美微信公众号欢迎赐稿
稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。
投稿邮箱：math_alg@163.com
