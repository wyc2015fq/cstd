# AI算法的进步超越了摩尔定律！ - 算法与数学之美 - CSDN博客
2018年10月12日 21:38:17[算法与数学之美](https://me.csdn.net/FnqTyr45)阅读数：116
![640?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkyLmgmcjgCpItmIm4VBw19j9ONNrdAHeAMLy2pDKdWkJ9GI1mnvsJJJQZmEgYKbibmJZXnqCwMf88Q/640?wx_fmt=png)
**“摩尔定律将继续改变世界，但算法的进步对推动电子技术的发展越来越重要。”**
现有的半导体芯片或正在开发的新计算架构仍然适合未来的算法，我们对此有多大信心？随着算法的进步超过硬件的发展，即使是最先进的深度学习模型都可以部署在小到5美元的Raspberry Pi上。
在20世纪80年代的处理器上运行最先进的现代算法，和在最先进的处理器上运行20世纪80年代的算法，哪个算得更快？答案令人惊讶，通常都是在旧处理器上跑新算法更快些。
虽然摩尔定律作为电子行业快速发展的驱动力而备受关注，但它只是一个驱动因素而已。我们经常忘记算法的进步其实在很多情况下超过了摩尔定律。
Martin Groetschel教授观察到，在1988年需要花费82年才能解算的一个线性编程问题，在2003年只需要一分钟。其中硬件加速占1000倍，而算法进步占43,000倍。同样，麻省理工学院教授Dimitris Bertsimas的研究表明，1991年至2013年间，混合整数求解器的算法加速是58万倍，而峰值超级计算机的硬件加速仅增加了32万倍。据说，类似的结果也发生在其他类型的约束优化问题和素数因子分解中。
###  这对AI意味着什么？ 
过去五年来，无论学术界、工业界还是创业界，都见证了人工智能（AI）的爆发。可能最大的拐点发生在2012年，当时来自多伦多大学的AlexNet团队，使用深度学习一举赢得了ImageNet大规模视觉识别挑战赛（ILSVRC）的冠军。从那以后，深度学习成为了人工智能实现的关键配方。
计算机视觉的进步开始蔓延到自然语言处理和其他AI领域。智能音箱、实时计算机翻译、机器人对冲基金，以及网络参考引擎，不再让我们感到惊讶了。
AI也成为了交通运输行业的驱动力（这也是Autotech Ventures公司的投资领域）。我们预见到，高级驾驶辅助系统（ADAS）、自动驾驶、车队检查、制造质量控制，以及车载人机界面等细分市场，具有巨大的发展潜力。到目前为止，Autotech Ventures已经对几家专注于开发该领域AI解决方案的初创公司进行了投资，包括ADAS和自动驾驶、视觉检测和边缘计算。在分析这些商业机会时，算法和硬件之间的相互作用是其投资决策中的一个关键考虑因素。
###  公众对AI硬件的关注 
基于深度学习的AI在其拐点之后出现了对图形处理单元（GPU）的强劲需求。由于具有很强的并行计算能力，GPU对于深度学习算法所采用的逻辑恰好具有惊人的运行效率。GPU的主要供应商英伟达（NVIDIA)从竞争中脱颖而出，其股价从2013年到2018年上涨了20倍。
当然，英伟达的竞争对手正在努力追赶。高通、Arm和其他公司将注意力集中在了AI芯片的设计上，而英特尔则收购了AI芯片初创公司Nervana Systems。谷歌、Facebook、苹果和亚马逊都已纷纷为各自的数据中心及其他项目开发他们的AI处理器。也有一些初创公司（例如Graphcore、Mythic、Wave Computing、Cerebras和SambaNova）看到机会加入进来，试图搭建设计得更好的图灵机系统。像D-wave Systems和IBM等其他一些公司也在积极探索后图灵时代的架构。大多数芯片开发的目标是赶上或超过英伟达。然而，据我们所知，大多数处理器都是针对今天的AI算法而设计的。
尽管需要巨大的前期开发成本，各种AI芯片设计的发展仍会进入寒武纪式的大爆炸。人工智能的前景是如此诱人，行业玩家愿意投入巨资开发硬件，以便与基础数学算法相匹配。但是，现有的半导体芯片或正在开发的新计算架构仍然适合未来的算法，我们对此有多大信心？
考虑到算法演变的速度和幅度变化是如此之快，许多替代AI芯片设计可能还没有正式投放市场就已经过时了。我们推测明天的AI算法可能需要不同的计算架构、内存资源，以及数据传输能力等。
尽管深度学习框架已经出现很长时间了，但直到最近才真正付诸实践，这要感谢摩尔定律所预测的硬件的快速发展。最初的数学不一定是为工程实践而设计的，因为早期的研究人员无法想象今天用1000美元就可以得到那么大的算力。现今的许多AI实现都是使用最初的数学模型，朝着更加准确、简单且更深层的方向发展，或者添加更多数据。这样做很快就会消耗掉GPU的计算容量。只有一小部分研究人员专注于改进基础数学和算法框架的难题。
还是有很多机会认识并利用这些创新的的数学进步的。我们了解到的方法包括精简冗余的数学运算而减少计算时间，将卷积压缩到较小的矩阵而减少内存需求，或者对加权矩阵进行二值化而简化数学运算。这些是进入算法进步的第一次尝试，其发展之快已经开始超过硬件的进步。
例如，从加州大学伯克利分校的研究项目剥离出来的DeepScale 就是将用于高级驾驶辅助系统（ADAS）和自动驾驶的AI“挤压”到汽车级芯片中（而不是GPU）。与仅使用算法的物体检测模型相比，他们的神经网络模型的运算速度要快30倍，同时在能耗和内存占用方面也有很大的提升，足以在现有硬件上运行。
另一个算法跨越式进步的例子来自艾伦人工智能研究所（Allen Institute of Artificial Intelligence）的研究人员。他们使用一种采用神经网络二值化的新颖数学方法，已经证明可以大幅提高速度，同时降低功耗和内存要求。这样就可能让最先进的深度学习模型部署在售价仅5美元的Raspberry Pi上。其研究人员最近将这种算法和处理工具独立出来成立专门的公司XNOR.ai，以便在边缘设备上部署AI，并进一步推动AI算法的进步。
有趣的是，新的二值化框架从根本上改变了最佳处理逻辑的类型。它们不再需要解决神经网络所需的32位浮点卷积，而只需要进行位计数操作——将功率平衡从GPU移开。此外，如果这些算法与专门设计的芯片相匹配，则可以进一步降低计算资源需求。
算法的进步不会停止。有时需要数年甚至数十年才能发明（或者可能发现）新的算法。这些突破无法以与摩尔定律推动的计算进步所相同的方式来预测。它们本质上是非确定性的。但是当它们发生时，整个格局的变化往往会使现有的主导者变成脆弱的猎物。
###  黑天鹅 
Nassim Nicolas Taleb在他的畅销书《The Black Swan: The Impact of the Highly Improbable（黑天鹅：如何应对不可预知的未来）》中阐明说，最佳决策在很大程度上取决于分析过程是不可预测还是不确定。换句话说，我们是在处理“已知的未知数”还是“未知的未知数”？算法创新从根本上是未知的未知数。投注到这些发展上需要持续关注，因为它们具有不确定的发现时间和不可预测的影响。
然而，在过去的二十年中，在应用数学领域，尤其是人工智能方面，出现了几个颠覆性的算法发现。它们与GPU一起，将AI从一个不起眼的研究领域带到了商业化的最前沿。
我们认识到这些计算领域“黑天鹅”的潜力，它们将使现有芯片架构成为过去，或者一夜之间重新洗牌。对我们来说，这些黑天鹅可能会带来更为安全的自动驾驶汽车，以及许多其他未知的应用。
*- Alexei Andreev博士是Autotech Ventures投资公司的执行董事，Jeff Peters博士是公司首席研究员，Autotech Ventures是一家专注于交通相关技术的风险投资公司。（声明：DeepScale和XNOR.ai是Autotech Ventures投资的初创公司）*
来源 | EDN电子技术设计
算法数学之美微信公众号欢迎赐稿
投稿邮箱：math_alg@163.com
