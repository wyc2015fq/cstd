# 几种常见DRL(深度强化学习)方法总结与对比之前提基本概念 - 一个跳popping的quant的博客 - CSDN博客





2017年12月16日 20:31:42[敲代码的quant](https://me.csdn.net/FrankieHello)阅读数：4159标签：[深度学习																[机器学习																[人工智能](https://so.csdn.net/so/search/s.do?q=人工智能&t=blog)
个人分类：[reinforcement learning](https://blog.csdn.net/FrankieHello/article/category/7349479)





从今年的九月份到现在，接触机器学习、深度学习再到现在的深度强化学习已经有三个月的时间了。从java web开发到人工智能的领域转变的过程中，学到了很多很杂的东西，感觉这才是我以后要研究的东西。然而，在这个转变的过程中，老是急于求成，虽然代码写过很多，论文看了不少，但是总是觉得基础不够牢固，所以想写下博客来沉淀一下。 
**前提概念**

 1、离散动作和连续动作 

    根据动作的不同类型选择和构造的模型影响很大，之前在研究各种方法的时候，经常会遇到关于动作的连续还是离散的问题。 

    所谓离散动作就是不连续的动作，好比在进行交易的时候，用-1、0、1分别代表这卖、持仓和买，这几个动作就是离散的，动作的空间也就是｛-1，0，1｝。而在操控汽车的自动驾驶的时候，方向盘要转动多少角度，这就是个连续动作问题了，因为这个动作是从[-X度，X度]的选择，动作的空间是一个区间[-X度，X度]。
2、Q函数和策略函数（Value-based Function&Policy-based Function） 

   Q函数也就是经常用到的价值函数，用来估计一个（s，a）状态动作对的价值，而策略函数则是根据状态来输出动作或者动作的概率。 

   两者的区别就是，Q函数输入状态和动作，输出价值。策略函数就是输入状态，输出动作或者动作的概率。在深度强化学习中，就是用神经网络进行function approximation（函数逼近），来模拟这几种函数。 

   如下图所示： 
![这里写图片描述](https://img-blog.csdn.net/20171216201932565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRnJhbmtpZUhlbGxv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
3、单步更新与回合更新（Monte-carlo update&Temporal-Difference update） 

    单步更新，顾名思义，也就是每次执行一次action后，进行一次更新；而回合更新，就是在一次训练的epoch中，结束后才进行更新。 

    用我现在在研究的量化交易与人工智能来举个例子，就好比每次执行一次买或卖的action后，更新一下模型中的参数，这就是单步更新；而从开盘到结束作为一次的训练epoch，从开始到结束的这一回合训练过程中，所有动作都是根据目前的参数进行选择，直到结束这一回合的训练后，才进行更新参数。 

    个人目前觉得，单步更新容易陷入局部最小，而回合更新则更全局一些，但是回合更新要等到这一回合的训练结束才进行参数更新，所以学习的效率不高。
4、基于概率和基于值（Policy-based&Value-based） 

    基于概率的学习方法就是根据每个动作的概率来进行选择，那么每个动作都有可能被选中，只是概率不同；而基于值的则是为每一个动作都评一个分数，选择时直接选择那个分数最高的动作。 

    如果动作是离散的，那么通过基于值的方法就可以很好的解决，当动作是在一个连续的区间上进行选择，那么就只能通过基于概率的方法了。因为，如果把一个区间的动作进行离散化，那么动作将很多，而基于概率的方法可以直接输出一个概率分布。](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)




