# 卷积神经网络的通俗解释和经典模型介绍 - happyhorizon的算法天空 - CSDN博客
2017年09月08日 14:09:48[lxy_Alex](https://me.csdn.net/happyhorizion)阅读数：13756

神经网络是在传统多项式回归的基础上，受到了生物神经网络”激活“现象的启发，引入了激活函数而构建起来的机器学习模型。
在图像处理领域，由于图像的数据量非常大，伴随着产生的问题是网络参数量非常大，而卷积神经网络引入卷积核巧妙地优化了这个问题。卷积核对图像进行局部扫描，提取其中的特征。对于小卷积核无法获取全局特征的问题，通过增加网络层数，前面多层小卷积核的感受野逐渐叠加后，后面小卷积核的感受野也会逐渐扩大。而且随着网络层数的增加，每次完成卷积后都会引入ReLU激活函数，对模型引入了更多的非线性，增强了网络的拟合能力。
# 卷积
关于卷积的含义，有很多种解释。知乎上最经典的解释：
## 降维打击
卷积就是把二元函数 U(x,y) = f(x)g(y) 卷成一元函数 V(t) 嘛，俗称降维打击。
![1240](https://upload-images.jianshu.io/upload_images/4685306-4b6e9265d039b9d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
### 1）怎么卷？
考虑到函数 f 和 g 应该地位平等，或者说变量 x 和 y 应该地位平等，一种可取的办法就是沿直线 x+y = t 卷起来：
### 2）卷了有什么用？
可以用来做多位数乘法呀，比如：
![1240](https://upload-images.jianshu.io/upload_images/4685306-75d1e3218202d384.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
注意第二个等号右边每个括号里的系数构成的序列 (14,34,14,4)，实际上就是序列 (2,4) 和 (7,3,1) 的卷积。
这里的“乘法转加法”的运算还可以有更加直观的一个解释：
![1240](https://upload-images.jianshu.io/upload_images/4685306-9430527b7176ea9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
左图序列保持不动，右图序列顺序反转然后开始旋转，每旋转一次两序列重合位置相乘求和，得到循环卷积序列。拿刚才的卷积为例，
![1240](https://upload-images.jianshu.io/upload_images/4685306-5a9e5193884d1a58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
（不知道为什么此处想到了机械计算器，不知道会不会是类似的原理。）
### 信号卷积的通俗理解
有了刚才的认识，就可以从非时变系统的角度去理解卷积，以离散信号为例，连续信号同理。
已知x[0]=a, x[1]=b, x[2]=c
![1240](https://upload-images.jianshu.io/upload_images/4685306-42ad3a2a7e276aa2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
已知y[0]=i, y[1]=j, y[2]=k
![1240](https://upload-images.jianshu.io/upload_images/4685306-478b5fe0948e4af7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
下面通过演示求x[n]*y[n]的过程，揭示卷积的物理意义。
第一步，x[n]乘以y[0]并平移到位置0：
![1240](https://upload-images.jianshu.io/upload_images/4685306-a5fde6f64f148785.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
第二步，x[n]乘以y[1]并平移到位置1：
![1240](https://upload-images.jianshu.io/upload_images/4685306-49a24df8ded73b0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
第三步，x[n]乘以y[2]并平移到位置2：
![1240](https://upload-images.jianshu.io/upload_images/4685306-da90285e70555c51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
最后，把上面三个图叠加，就得到了：
![1240](https://upload-images.jianshu.io/upload_images/4685306-5aac68a0489772d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
从这里可以看到卷积的重要的物理意义是：一个函数（如：单位响应）在另一个函数（如：输入信号）上的加权叠加。
对于线性时不变系统，如果知道该系统的单位响应，那么将单位响应和输入信号求卷积，就相当于把输入信号的各个时间点的单位响应$$加权叠加$$，就直接得到了输出信号。通俗的说：
**在输入信号的每个位置，叠加一个单位响应，就得到了输出信号。**
这也正是单位响应是如此重要的原因。
# 卷积神经网络
用在图像识别领域，卷积神经网络中的卷积核（滤镜）要实现的就是，将图像中的特征提取出来。
具体怎么提取？以猫为例，显著特征是圆眼睛三角耳朵尖下巴，但是总体来说狐狸也长这个样子，只是耳朵更大，下巴更尖而已。这些细小的区别很难描述。
## 卷积核的引入
图像识别首先要解决的问题是：特征是什么？第二要解决的问题是：参数规模可控。
由于图像可以看住是数据矩阵，输入传统的神经网络后，将网络的输出和标签求交叉熵，代入sigmoid函数，或者求对数放入softmax函数，就可以得到图像属于某个类别的概率。然后用梯度下降方法训练网络参数即可。
但是根据图像矩阵的数据分布特点，对于一张1000*1000的图片而言，如果用传统的神经网络，以隐层节点为10^6为例，参数的数据量为10^3×10^3×10^6=10^12。如下图中左图。
![1240](https://upload-images.jianshu.io/upload_images/4685306-81eb3760912c62f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
但是，如果用卷积核来提取特征，以10×10的卷积核为例，如右图，参数量瞬间降到了10×10×10^6=10^8！参数降维的效果还是很明显的。
## 参数共享
另外还有一个参数降维的好办法就是参数共享。
怎么理解权值共享呢？卷积核的这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。一个形象化的展示如下图。相同的卷积核在整个图片上扫描，得到的结果取值越大，说明该区域的数据特征与卷积核越一致，通过这种方式，就提取出了图像的局部特征。
![1240](https://upload-images.jianshu.io/upload_images/4685306-ae5566618e3c0689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
从图上可以看到，一个有着固定取值的卷积核对图像矩阵进行扫描，具体的运算过程见下图。就是卷积核和被扫描区域的数据相乘后直接相加，最后相加的结果作为卷积后该位置的取值。这也是为何卷积网络的特征提取可以保留其位置特征的原因。
![1240](https://upload-images.jianshu.io/upload_images/4685306-1367b34906fcb115.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
很显然，图像矩阵的数据分布与卷积核越接近，取值就会越大。这样，卷积核的取值会逐渐接近图像中数据的分布，而类似的图像（被打成同样标签）中的相同特征被逐渐提取出来。
## 多个卷积核
在有多个卷积核时，如下图所示：
![1240](https://upload-images.jianshu.io/upload_images/4685306-4f5a925bc75aa8cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
上图右，不同颜色表明不同的卷积核。每个卷积核都会将图像生成为另一幅图像。比如两个卷积核就可以将生成两幅图像，这两幅图像可以看做是一张图像的不同的通道。
## 池化
在通过卷积获得了特征 (features) 之后，下一步我们希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如 softmax 分类器，但这样做面临计算量的挑战。例如：对于一个 96X96 像素的图像，假设我们已经学习得到了400个定义在8X8输入上的特征，每一个特征和图像卷积都会得到一个 (96 − 8 + 1) × (96 − 8 + 1) = 7921 维的卷积特征，由于有 400 个特征，所以每个样例 (example) 都会得到一个7921× 400 = 3,168,400 维的卷积特征向量。学习一个拥有超过 3 百万特征输入的分类器十分不便，并且容易出现过拟合 (over-fitting)。
为了解决这个问题，首先回忆一下，我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值 (或最大值)。这些概要统计特征不仅具有低得多的维度 (相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。这种聚合的操作就叫做池化 (pooling)，有时也称为平均池化或者最大池化 (取决于计算池化的方法)。
![1240](https://upload-images.jianshu.io/upload_images/4685306-beeac0b1f6ca2f28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
## 全连接层
全连接就是个矩阵乘法，相当于一个特征空间变换，可以把有用的信息提取整合。全连接层一般在卷积网络的最后，前面的卷积层、池化层和激活函数将原始数据映射到隐层特征空间，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间。或者说，全连接层的主要目的就是维度变换，把高维的数据（分布式特征表示）变成低维（样本标记）。在这个过程中，有用的信息保留下来，但会损失特征的位置信息。另外，全连接层可以用(h×w)个1×1的卷积核来代替，h和w分别为前层卷积结果的高和宽。
由于全连接层的引入带来了大量参数，近来人们逐渐发现全连接层的存在与否对结果的影响并不明显，另一方面，全局平均池化（global averaging pooling，AGP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程的方式在ResNet和GoogleNet上都取得了非常好的预测结果。
另一方面，魏秀参等（见参考）近期的研究发现，FC可在模型表示能力迁移过程中充当“防火墙”的作用。
## 经典卷积网络模型
卷积神经网络的发展过程中，先后出现了很多里程碑意义的网络模型，例如AlexNet，VGG和GoogleNet。
### AlexNet
AlexNet由7层隐层构成。其中1~5是卷积层，6-7是全连接层。
下图即为Alex的CNN结构图。需要注意的是，该模型采用了2-GPU并行结构，即第1、2、4、5卷积层都是将模型参数分为2部分进行训练的。在这里，更进一步，并行结构分为数据并行与模型并行。数据并行是指在不同的GPU上，模型结构相同，但将训练数据进行切分，分别训练得到不同的模型，然后再将模型进行融合。而模型并行则是，将若干层的模型参数进行切分，不同的GPU上使用相同的数据进行训练，得到的结果直接连接作为下一层的输入。
![1240](https://upload-images.jianshu.io/upload_images/4685306-02cd529dd1573afb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
上图模型的基本参数为：
输入：224×224大小的图片，3通道
第一层卷积：5×5大小的卷积核96个，每个GPU上48个。
第一层max-pooling：2×2的核。
第二层卷积：3×3卷积核256个，每个GPU上128个。
第二层max-pooling：2×2的核。
第三层卷积：与上一层是全连接，3×3的卷积核384个。分到两个GPU上个192个。
第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。
第五层卷积：3×3的卷积核256个，两个GPU上个128个。
第五层max-pooling：2×2的核。
第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。
第二层全连接：4096维
Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。
### VGG
VGG最大的贡献是成功证明了小卷积核+小池化层+深度网络的潜力。通过反复堆叠3*3的小型卷积核和2*2的池化层，VGG将卷积神经网络的深度拓展到了19层。更重要的是，VGG的拓展性能非常好，迁移到其他图片数据上的泛化性非常好。
下图中展示了VGGNet的网络结构。作者从A方案11层卷积神经网络开始，逐渐增加卷积的层数到E方案的19层。
![1240](https://upload-images.jianshu.io/upload_images/4685306-cb194ef883dae7d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
从图上可以看出，VGGNet有5段卷积，每段中都有1~3个3×3的小卷积核，每段卷积核的数量一样，段数越大，卷积核数量增加：64-128-256-512-512。后期层数增加后，会有两个小卷积核堆叠在一起的情况，也就是多个小感受野堆叠后实际感受野扩大。例如2层3×3的小卷积核叠在一起，相当于1个5×5的卷积核，但是卷积核的参数比较小。
![1240](https://upload-images.jianshu.io/upload_images/4685306-bb76518c9d5ed329.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
image.png
此外，每次做完卷积，都要带入到ReLU激活函数中去，也增加了模型的非线性表达能力。
未完待续。。。
最后，感谢知乎各位大神对卷积的精彩解释！水平有限，在内容上表述上的疏忽，还有一些理解不到位不正确的地方，欢迎留言指出～谢谢！
参考：
[卷积解释]
[https://www.zhihu.com/question/54677157/answer/141316355](https://www.zhihu.com/question/54677157/answer/141316355)
[https://www.zhihu.com/question/22298352/answer/34267457](https://www.zhihu.com/question/22298352/answer/34267457)
卷积神经网络模型：[http://www.36dsj.com/archives/24006](http://www.36dsj.com/archives/24006)
全连接层：[https://www.zhihu.com/question/41037974/answer/150522307](https://www.zhihu.com/question/41037974/answer/150522307)
深度学习Deep Learning：[https://github.com/exacity/deeplearningbook-chinese](https://github.com/exacity/deeplearningbook-chinese)
