# 贝叶斯网络和概率推理（一）:理性决策与朴素贝叶斯 - happyhorizon的算法天空 - CSDN博客
2018年02月21日 02:28:14[lxy_Alex](https://me.csdn.net/happyhorizion)阅读数：940
在实际问题中，理性决策（rational decision）就意味着必须对结果出现的相关因素及其重要性，以及目标实现的可能性进行合理评估。由于未知和惰性，让我们对问题中的每个“因果关系”不能给出确定性的衡量，最多给出“因果”之间的信念度（degree of belief），也就是事情发生的概率。与此同时，不同决策间的偏好（preference）也是理性决策过程中不可避免的组成部分，而对于“效用更高”状态的这种偏好，也被称为效用理论（utility theory）。可以说：
`决策理论=概率理论+效用理论`
在理性状态下，所有的决策都应该使得对应的效用最大化，也就是所谓的期望效用最大化（Maximun Expected Utility, MEU）原则。
首先来看决策理论中的概率部分。这部分决定了决策的走向，也就是说，事情往往会朝着概率最大的方向发展。对于一个随机事件，如果不考虑其它信息的情况下，考察其发生的信念度，就称为先验概率（prior probabilities）或者无条件概率（unconditional probabilities）。一旦考虑其它信息，比如某种已知的因素，这时的概率称为后验概率（posterior probabilities）或者条件概率（conditional probabilities）。注意后验概率是一种条件上的蕴含关系（conditioning implication），而不是逻辑上的蕴含关系（logical implication）。也就是说，条件概率p(A|B)是指如果仅有条件B已知的情况下，事件A发生的概率，并不是如果B发生，A一定会发生的概率。理解了这一点，也就很容易理解条件概率定义的乘法规则（product rule）形式： 
$ P(a\wedge b)=P(a|b)P(b) $
从条件概率的乘法规则形式可以很容易地推导出贝叶斯条件概率公式： 
$ P(a\wedge b)=P(a|b)P(b)=P(b|a)P(a) $
进而得到： 
$ P(b|a)=\frac{P(a|b)P(b)}{P(a)} $
其中，P(a)是事件a发生的先验概率，与$P(a|b)$， $P(b|a)$和$P(b)$都无关，可以认为是常数$\alpha$。乍一看，贝叶斯条件概率公式似乎只是做了个数学上的把戏，但是如果将a认为是effect, b认为是cause， 那么上述公式可以整理为： 
$ P(cause|effect)=\alpha P(effect|cause)P(cause) $
就刚好可以描述医生诊断疾病时候遇到的问题模型（也是实际应用中最常见的模型）。在医学书籍上，疾病（cause）和生理指征（effect） 的关系通常是$P(effect|cause)$，而诊断疾病的过程，就恰好是由“果”推“因”的过程，即$P(cause|effect)$。
如果要表示事件a和b发生的所有可能组合的概率，就需要定义其联合概率分布（joint probability distribution）： 
$ P(a, b)=P(a|b)P(b) $
上文中逻辑上的蕴含关系，就是联合概率分布中，a=true, b=true的情况， 
$ P(a=true \wedge b=true)=P(a=true|b=true)P(b=true) $
显然，逻辑蕴含只是条件蕴含的特例。
所有随机变量的联合分布可以定义一个完整的概率模型，也就是完全联合概率分布（full joint probability distribution）。完全联合概率分布同样满足概率的基本定义，即任意随机样本（命题）发生的概率<1, 总的概率之和=1. 
将命题与其否命题的关系$P(-a)=1-P(a)$稍作推广，就可以得到包含-排除原理（inclusion-exclusion principle）: 
$ P(a \vee b)=P(a)+P(b)-P(a \wedge b) $
符号$\vee$表示的是一种“或”的关系，而符号$\wedge$代表的是“且”的关系。描述联合概率分布时，就是这种“且”的关系。 
如果已知完全联合概率分布，求具体某个变量的分布，就需要将其它所有变量在所有可能取值下的概率相加，已知$P(a,b)$，求$P(a)$， 
$ P(a=true)=P(a=true,b=true)+P(a=true,b=false) $
$ P(a=false)=P(a=false,b=true)+P(a=false,b=false) $
这个过程称为边缘化（marginalization）或者求和消元（summing out）, 对应a的概率称为边缘概率。求和消元的称呼更容易理解，边缘化的说法则来自于保险精算师们常常将已知的概率加和起来写在保险计算时的概率表格边上以提取某个因素的发生概率。 
可以写出如下更加通用的边缘化概率计算形式： 
$ P(A)= \sum_{b\in B}P(A,b) = \sum_{b\in B}P(A|b)P(b) $
上式的后半部分展示了另外一个常用的技巧，称为条件化（conditioning）。另外，还有一个常用的技巧，就是归一化（利于概率之和=1的特点）。如果A和B是独立的，那么$P(A|b)$与$P(b)$无关，可以认为是一个常数，从累加符号中提出来，就可以得到 
$ P(A)=  P(A|b)\sum_{b\in B}P(b) =  P(A|b) $
这种特性被称为独立性或者边缘独立性（marginal independence）或者绝对独立性（absolute independence）。另外一种独立性的定义为： 
$ P(A,B|C)=P(A|C)P(B|C) $
也就是所谓的条件独立性。是指在给定已知随机变量C的情况下，随机变量A和B的联合概率密度等于给定C发生A的概率与给定C发生B的概率之乘积。
条件独立性极大地简化了在给定条件下多个随机变量联合概率分布的计算和表达，将大的概率问题转化为多个小概率问题，是人工智能领域中近年来最大的进展之一，其衍生的概率分布被称为朴素贝叶斯（naive Bayes）, 是贝叶斯网络的基本假设。回到医生的问题，用朴素贝叶斯将多中生理指征下某个具体疾病的概率表示为： 
$ P(cause, effect_1, effect_2, …, effect_n)=P(cause) \prod_{i} P(effect_i|cause)$
自此，我们也就可以回答朴素贝叶斯为何被称为“朴素”的了，因为这里使用了条件独立假设，认为在给定单一的原因变量之后，所有的结果变量都是条件独立的。
参考： 
《人工智能：一种现代方法》，清华大学出版社
