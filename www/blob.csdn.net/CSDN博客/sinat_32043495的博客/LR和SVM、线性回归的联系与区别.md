# LR和SVM、线性回归的联系与区别 - sinat_32043495的博客 - CSDN博客





2018年03月24日 20:45:16[LZXandTM](https://me.csdn.net/sinat_32043495)阅读数：3317








**LR和SVM的联系**：


- 都是监督的分类算法
- 都是线性分类方法 **(不考虑核函数时）**

- 都是判别模型
判别模型和生成模型是两个相对应的模型。 
判别模型是直接生成一个表示P(Y|X)或者Y=f(X)的判别函数（或预测模型） 
生成模型是先计算联合概率分布P(Y,X)然后通过贝叶斯公式转化为条件概率。 
SVM和LR，KNN，决策树都是判别模型，而朴素贝叶斯，隐马尔可夫模型是生成模型。 

****LR和SVM的**不同**

1、损失函数的不同

LR是cross entropy

![](https://img-blog.csdn.net/20180324202626766)


SVM的损失函数是最大化间隔距离


![](https://img-blog.csdn.net/20180324202652469)


​逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值


支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面


2、**SVM不能产生概率，LR可以产生概率**

LR本身就是基于概率的，所以它产生的结果代表了分成某一类的概率，而SVM则因为优化的目标不含有概率因素，所以其不能直接产生概率。


3、**SVM自带结构风险最小化，LR则是经验风险最小化**

在假设空间、损失函数和训练集确定的情况下，经验风险最小化即最小化损失函数

结构最小化是为了防止过拟合，在经验风险的基础上加上表示模型复杂度的正则项

4、**SVM会用核函数而LR一般不用核函数**

SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。 而LR则每个点都需要两两计算核函数，计算量太过庞大


5、**LR和SVM在实际应用的区别**

根据经验来看，对于小规模数据集，SVM的效果要好于LR，但是大数据中，SVM的计算复杂度受到限制，而LR因为训练简单，可以在线训练，所以经常会被大量采用


6、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

参考：https://blog.csdn.net/haolexiao/article/details/70191667

@AntZ: LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。
@nishizhen：个人感觉逻辑回归和线性回归首先都是广义的线性回归，
其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，
另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
@乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。







