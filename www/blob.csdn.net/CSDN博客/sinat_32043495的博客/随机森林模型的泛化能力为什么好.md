# 随机森林模型的泛化能力为什么好 - sinat_32043495的博客 - CSDN博客





2017年12月06日 11:30:24[LZXandTM](https://me.csdn.net/sinat_32043495)阅读数：6099








决策树是广泛应用的一种分类算法，它是一种树状分类器，在每个内部节点选择最优的分裂属性进行分类，每个叶节点是具有同一个类别的数据。当输入待分类样本时，决策树确定一条由根节点到叶节点的唯一路径，该路径的叶节点的类别就是待分类样本的所属类别。决策树是一种简单且快速的非参数分类方法，一般情况下，还具有很好的准确率，然而当数据复杂或者存在噪声时，决策树容易出现过拟合问题，使得分类精度下降。


   随机森林是以决策树为基本分类器的一个集成学习模型，它包含多个由Bagging集成学习技术训练得到的决策树，当输入待分类的样本时，最终的分类结果由多个决策树的输出结果投票决定，随机森林克服了决策树过拟合问题，对噪声和异常值有较好的容忍性，对高维数据分类问题具有良好的可扩展性和并行性。


泛化能力的定义（来自百度百科）:

泛化能力（generalization ability）是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。

主要依靠了其中三个随机过程：

1.产生决策树的样本是随机生成,从原样本集X中有放回地随机抽取K个训练样本,每次大概37%的样本未被抽中；

2.构建决策树的特征值是随机选取，在对决策树每个节点进行分裂时，从全部属性中等概率随机抽取一个属性子集(通常取![](https://img-blog.csdn.net/20171206110625781?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzIwNDM0OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)个属性,M为特征总数),再从这个子集中选择一个最优属性来分裂节点;

3.树产生过程中裂变的时候是选择N个最佳方向中的随机一个裂变的,裂变条件的随机性(比如分为左节点和右节点的阈值选择,如下列公式中的阈值tao,可以是属性值范围内随机采样的值)。

![](https://img-blog.csdn.net/20171206112824432?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzIwNDM0OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


理论上的证明：

http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFQ&dbname=CJFD2013&filename=JCJI201301001&uid=WEEvREcwSlJHSldRa1FhdkJkcGp4dXQ5N1hhUmgydU5NS3UzNFVVZXZNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MjQ2ODR1eFlTN0RoMVQzcVRyV00xRnJDVVJMMmVaZVpwRnluaFZML0tMeTdCWjdHNEg5TE1ybzlGWllSOGVYMUw=




收敛定理:

    当随机森林产生的树的数目趋近无穷的时候，理论上根据大数定理可以证明训练误差与测试误差是收敛到一起的.

    当然实际过程中，由于不可能产生无穷多个决策树，模型参数的设置问题会影响在相同运行时间内拟合结果的过拟合程度的不同。但总而言之，调整参数后，随机森林可以有效的降低过拟合的程度。大数定律只说明不收敛的模型参数是存在的，并不能指出模型参数具体取什么值的时候不收敛。

泛化误差界:

   随机森林的泛化误差界与单个决策树的分类强度s成负相关，与决策树之间的相关性p成正相关，即分类强度s越大，相关性p越小，则泛化误差界越小，随机森林分类准确度越高。这也启发我们，对随机森林模型进行改进时，可以从两方面着手：一是提高单棵决策树的分类强度s，二是降低决策树之间的相关性p。





