# 六、回归——套索回归 (Lasso Regression)的基本应用 - Nicole的博客 - CSDN博客
2018年06月15日 17:44:17[Nicole_Liang](https://me.csdn.net/weixin_39541558)阅读数：4800
# 一、使用场合
与岭回归类似，套索 (Least Absolute Shrinkage and Selection Operator) 也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：
![10.png](https://ogmcsrgk5.qnssl.com/6327/7a3c9a9e0f024b79a709531ebef8d420.png)
套索回归与岭回归有一点不同，它在惩罚部分使用的是绝对值，而不是平方值。这导致惩罚（即用以约束估计的绝对值之和）值使一些参数估计结果等于零。使用的惩罚值越大，估计值会越趋近于零。这将导致我们要从给定的n个变量之外选择变量。
要点：
• 除常数项以外，这种回归的假设与最小二乘回归类似；
• 它将收缩系数缩减至零（等于零），这确实有助于特征选择；
• 这是一个正则化方法，使用的是 L1 正则化；
• 如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。
# 二、lasso族的功效
       在建立模型之初，为了尽量减小因缺少重要自变量而出现的模型偏差，通常会选择尽可能多的自变量。然而，建模过程需要寻找对因变量最具有强解释力的自变量集合，也就是通过自变量选择(指标选择、字段选择)来提高模型的解释性和预测精度。指标选择在统计建模过程中是极其重要的问题。Lasso算法则是一种能够实现指标集合精简的估计方法。
      Lasso(Least absolute shrinkage and selection operator, Tibshirani(1996))方法是一种压缩估计。它通过构造一个罚函数得到一个较为精炼的模型，使得它压缩一些系数，同时设定一些系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。
      Lasso 的基本思想是在回归系数的绝对值之和小于一个常数的约束条件下，使残差平方和最小化，从而能够产生某些严格等于0 的回归系数，得到可以解释的模型。R的Lars 算法的软件包提供了Lasso编程，我们根据模型改进的需要，可以给出Lasso算法，并利用AIC准则和BIC准则给统计模型的变量做一个截断，进而达到降维的目的。因此，我们通过研究Lasso可以将其更好的应用到变量选择中去。[]
      lasso estimate具有shrinkage和selection两种功能，shrinkage这个不用多讲，本科期间学过回归分析的同学应该都知道岭估计会有shrinkage的功效，lasso也同样。关于selection功能，Tibshirani提出，当t值小到一定程度的时候，lasso estimate会使得某些回归系数的估值是0，这确实是起到了变量选择的作用。当t不断增大时，选入回归模型的变量会逐渐增多，当t增大到某个值时，所有变量都入选了回归模型，这个时候得到的回归模型的系数是通常意义下的最小二乘估计。从这个角度上来看，lasso也可以看做是一种逐步回归的过程。[]
模型选择本质上是寻求模型稀疏表达的过程，而这种过程可以通过优化一个“损失”十“惩罚”的函数问题来完成。
# 三、与普通最小二乘法的区别
 使用最小二乘法拟合的普通线性回归是数据建模的基本方法。其建模要点在于误差项一般要求独立同分布（常假定为正态）零均值。t检验用来检验拟合的模型系数的显著性，F检验用来检验模型的显著性（方差分析）。如果正态性不成立，t检验和F检验就没有意义。
对较复杂的数据建模（比如文本分类，图像去噪或者基因组研究）的时候，普通线性回归会有一些问题：
（1）预测精度的问题 如果响应变量和预测变量之间有比较明显的线性关系，最小二乘回归会有很小的偏倚，特别是如果观测数量n远大于预测变量p时，最小二乘回归也会有较小的方差。但是如果n和p比较接近，则容易产生过拟合；如果n
（2）模型解释能力的问题 包括在一个多元线性回归模型里的很多变量可能是和响应变量无关的；也有可能产生多重共线性的现象：即多个预测变量之间明显相关。这些情况都会增加模型的复杂程度，削弱模型的解释能力。这时候需要进行变量选择（特征选择）。
针对OLS的问题，在变量选择方面有三种扩展的方法： （1）子集选择 这是传统的方法，包括逐步回归和最优子集法等，对可能的部分子集拟合线性模型，利用判别准则 （如AIC,BIC,Cp,调整R2 等）决定最优的模型。 （2）收缩方法（shrinkage method） 收缩方法又称为正则化（regularization）。主要是岭回归（ridge regression）和lasso回归。通过对最小二乘估计加入罚约束，使某些系数的估计为0。 (3)维数缩减 主成分回归（PCR）和偏最小二乘回归（PLS）的方法。把p个预测变量投影到m维空间（m
# 四、岭回归、lasso回归和elastic net三种正则化方法
## 1.岭回归
最小二乘估计是最小化残差平方和（RSS）：
岭回归在最小化RSS的计算里加入了一个收缩惩罚项（正则化的l2范数）
这个惩罚项中lambda大于等于0，是个调整参数。各个待估系数越小则惩罚项越小，因此惩罚项的加入有利于缩减待估参数接近于0。重点在于lambda的确定，可以使用交叉验证或者Cp准则。
岭回归优于最小二乘回归的原因在于方差-偏倚选择。随着lambda的增大，模型方差减小而偏倚（轻微的）增加。
岭回归的一个缺点：在建模时，同时引入p个预测变量，罚约束项可以收缩这些预测变量的待估系数接近0,但并非恰好是0（除非lambda为无穷大）。这个缺点对于模型精度影响不大，但给模型的解释造成了困难。这个缺点可以由lasso来克服。(所以岭回归虽然减少了模型的复杂度，并没有真正解决变量选择的问题)
## 2、lasso
lasso是在RSS最小化的计算中加入一个l1范数作为罚约束：
l1范数的好处是当lambda充分大时可以把某些待估系数精确地收缩到0。
关于岭回归和lasso当然也可以把它们看做一个以RSS为目标函数，以惩罚项为约束的优化问题。
## 3、调整参数lambda的确定
交叉验证法。对lambda的格点值，进行交叉验证，选取交叉验证误差最小的lambda值。最后，按照得到的lambda值，用全部数据重新拟合模型即可。
## 4、elastic net
elastic net融合了l1范数和l2范数两种正则化的方法，上面的岭回归和lasso回归都可以看做它的特例：
elastic net对于p远大于n,或者严重的多重共线性情况有明显的效果。 对于elastic net，当alpha接近1时，elastic net表现很接近lasso，但去掉了由极端相关引起的退化化或者奇怪的表现。一般来说，elastic net是岭回归和lasso的很好的折中，当alpha从0变化到1，目标函数的稀疏解（系数为0的情况）也从0单调增加到lasso的稀疏解。
LASSO的进一步扩展是和岭回归相结合，形成Elastic Net方法。[]
## 5、岭回归与lasso算法
这两种方法的共同点在于，将解释变量的系数加入到Cost Function中，并对其进行最小化，本质上是对过多的参数实施了惩罚。而两种方法的区别在于惩罚函数不同。但这种微小的区别却使LASSO有很多优良的特质（可以同时选择和缩减参数）。下面的公式就是在线性模型中两种方法所对应的目标函数：
公式中的lambda是重要的设置参数，它控制了惩罚的严厉程度，如果设置得过大，那么最后的模型参数均将趋于0，形成拟合不足。如果设置得过小，又会形成拟合过度。所以lambda的取值一般需要通过交叉检验来确定。
岭回归的一个缺点：在建模时，同时引入p个预测变量，罚约束项可以收缩这些预测变量的待估系数接近0,但并非恰好是0（除非lambda为无穷大）。这个缺点对于模型精度影响不大，但给模型的解释造成了困难。这个缺点可以由lasso来克服。(所以岭回归虽然减少了模型的复杂度，并没有真正解决变量选择的问题)
# 五、LARS算法对lasso的贡献[]
LAR把Lasso （L1-norm regularization）和Boosting真正的联系起来，如同打通了任督二脉。LAR结束了一个晦涩的时代：在LAR之前，有关Sparsity的模型几乎都是一个黑箱，它们的数学性质（更不要谈古典的几何性质了）几乎都是缺失。
近年来兴起的Compressed sensing（Candes & Tao, Donoho）也与LAR一脉相承，只是更加强调L1-norm regularization其他方面的数学性质，比如Exact Recovery。我觉得这是一个问题的多个方面，Lasso关注的是构建模型的准确性，Compressed sensing关注的是变量选择的准确性。
# 六、变量选择
当我们使用数据训练分类器的时候，很重要的一点就是要在过度拟合与拟合不足之间达成一个平衡。防止过度拟合的一种方法就是对模型的复杂度进行约束。模型中用到解释变量的个数是模型复杂度的一种体现。控制解释变量个数有很多方法，例如变量选择(feature selection)，即用filter或wrapper方法提取解释变量的最佳子集。或是进行变量构造(feature construction)，即将原始变量进行某种映射或转换，如主成分方法和因子分析。变量选择的方法是比较“硬”的方法，变量要么进入模型，要么不进入模型，只有0-1两种选择。但也有“软”的方法，也就是Regularization类方法，例如岭回归(Ridge Regression)和套索方法(LASSO:least absolute shrinkage and selection operator)。
