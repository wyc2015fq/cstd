# 三、集成学习方法——boosting和bagging - Nicole的博客 - CSDN博客
2018年09月18日 23:07:01[Nicole_Liang](https://me.csdn.net/weixin_39541558)阅读数：101
# 一、集成学习的基本概念
## 1、集成学习的原理
**集成学习（ensemble learning）**通过构建并结合多个学习器来完成学习任务，有时候也称为多分类器系统（mult-classifer system）、基于委员会的学习（committee - based learning）等。
集成学习的目的是结合一些基于某些算法训练得到的基学习器来改进其泛化能力和鲁棒性（相对于单个学习器来说）。
集成学习的理论基础来自于Kearns和Valiant提出的基于PAC（probably approximately correct）的可学习性理论 ，PAC 定义了学习算法的强弱：
弱学习算法：识别错误率小于1/2(即准确率仅比随机猜测略高的算法) 
强学习算法：识别准确率很高并能在多项式时间内完成的算法
根据这两个概念，后来产生了一个重要的结论： 
**强可学习与弱可学习是等价的，即：一个概念是强可学习的充要条件是这个概念是弱可学习的**。
据此，为了得到一个优秀的强学习模型，我们可以将多个简单的弱学习模型“提升”。
## 2、集成学习的分类
（1）集成学习按照学习器是否同质可分为：
**同质集成**：集成中只包含同类型的个体学习器。这时的个体学习器也称为“基学习器”，相应的算法称为“基学习算法”
**异质集成**：异质集成中的个体学习器由不同的学习算法生成，这时的个体学习器一般不称为基学习器，常称为”组件学习器”
（2）根据个体学习器的生成方式，目前的集成方法大致可以分为两大类，
**序列化方法**：个体学习器之间存在强依存关系，必须串行生成。代表有boosting。
                       Boosting流派主要有：AdaBoost、GBDT、XGBOOST
**并行化方法**：个体学习器之间不存在强依存关系，可同时生成。代表有Bagging和random Forest。
