# 三、机器学习算法的优点和缺点 - Nicole的博客 - CSDN博客
2018年07月30日 18:34:07[Nicole_Liang](https://me.csdn.net/weixin_39541558)阅读数：124
[数据人网](https://www.afenxi.com/author/shujuren) • 2018-06-01 23:37 • [人工智能](https://www.afenxi.com/ai) • 阅读 282
![机器学习算法的优点和缺点](https://www.afenxi.com/wp-content/uploads/2018/06/1527896277.jpg)
从Logistic回归开始，然后尝试Tree Ensembles和/或Neural Networks。
**奥卡姆的剃刀原理：**使用最简单的算法，可以满足您的需求，并且只有在严格需要的情况下才用更复杂的算法。
根据我自己的经验，只有神经网络和梯度增强决策树（GBDT）正在工业中广泛使用。 我目睹Logistic回归和[随机森林](https://www.afenxi.com/topic/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97)被弃用不止一次（这意味着它们是好的开始）。 从来没有人听说有人在公司中讨论SVM。
## 优点和缺点
这里讨论最流行的算法。 有关[机器学习](https://www.afenxi.com/topic/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)算法的完整列表，请查看[**cheatsheet**](https://www.hackingnote.com/en/machine-learning/algorithms/)。
### 朴素贝叶斯
- 超级简单，只是做了一堆计数。
- 如果NB条件独立假设实际成立，那么朴素贝叶斯分类器将比[逻辑回归](https://www.afenxi.com/topic/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92)等区分性模型更快地收敛，因此您需要更少的训练数据。 即使NB假设不成立，NB分类器在实践中仍经常表现出色。
- 如果你想做某种半监督式的学习，或者想要一些令人尴尬的简单表现很好的话，这是一个很好的选择。
- 没有分布要求，
- 适合少数类别变量
- 计算独立分布的乘积
- 受到多重共线性
### Logistic回归
逻辑回归仍然是使用最广泛的，[**了解更多**](https://www.hackingnote.com/en/machine-learning/logistic-regression/)
- 一个相当好的分类算法，只要你期望你的特征大致是线性的，并且问题是线性可分的，就可以进行训练。
- 可以做一些特征工程，将大多数非线性特征很容易地转化为线性特征。
- 它对噪声也很稳健，并且可以避免过度拟合，甚至可以使用l2或l1正则化来进行特征选择。
- 逻辑回归也可以用于[大数据](https://www.afenxi.com/bigdata)场景，因为它非常高效并且可以使用例如ADMM（请参阅logreg）进行并发。
- 输出可以被解释为一个概率：您可以将其用于排名而不是分类。
- 运行一个简单的l2正则化LR来提出一个基线
- 无分布要求
- 用少数类别分类变量表现良好
- 计算logistic分布
- 适合少数类别变量
- 容易解释
- 计算CI
- 遭受多重共线性
- 很多方法来调整你的模型
- 不需要担心相关的特征，就像朴素贝叶斯一样。
- 轻松更新模型以接收新数据（使用在线梯度下降法）
- 如果您需要一个概率框架（例如，轻松调整分类阈值，说出何时不确定，或获得置信区间）还是希望在将来能够接收更多的训练数据迅速融入您的模型。
### Lasso
- 没有分布要求
- 计算L1损失
- 具有变量选择特点
- 遭受多重共线性
### Ridge
- 没有分布要求
- 计算L2损失
- 不具有变量选择
- 不受多重共线性
**何时不用**
- 如果变量是正态分布的且分类变量都有5个以上类别：使用线性判别分析
- 如果相关性大部分是非线性的：使用SVM
- 如果稀疏性和多重共线性是一个问题：具有Ridge（权重）的自适应Lasso + Lasso
### 线性判别分析
LDA：线性判别分析，不是潜在的Dirichlet分布
- 需要正态分布
- 不适合少数类别变量
- 计算叠加的多元分布
- 计算CI
- 遭受多重共线性
### 支持向量机
SVM vs LR：
- 支持向量机（SVM）使用与LR不同的损失函数（Hinge）。
- 他们也有不同的解释（最大边缘间隔）。
- 然而，实际上，具有线性内核的SVM与Logistic回归没有太大区别（如果您好奇，可以看看Andrew Ng如何从他的Coursera机器学习课程中的Logistic回归中推导SVM）。
- 您希望使用SVM而不是Logistic回归的主要原因是您的问题可能不是线性可分的。在这种情况下，您将不得不使用具有非线性内核的SVM（例如RBF）。
- 事实是，逻辑回归也可以用于不同的内核，但在这一点上，出于实际原因，您可能更适合使用SVM。
- 使用SVM的另一个相关原因是如果您处于高维空间。例如，据报道支持向量机可以更好地用于文本分类。
- 高准确度，在考虑过拟合时有很好地理论保证。
- 使用合适的内核，即使数据在基本特征空间中不能线性分离，它们也可以很好地工作。
- 在非常高维空间是常态的文本分类问题中尤其受欢迎。
- 没有分布要求
- 计算铰链损失
- 灵活选择非线性相关的核
- 不受多重共线性
- 很难解释
**缺点：**
- 训练可能会很痛苦。不推荐有很多实例的任何问题。不推荐大多数“工业规模”应用的SVM。除了玩具/实验室问题之外的任何事情可能会更好地用不同的算法来处理。尽管如此，内存密集型和烦人的运行和调优，所以我认为随机森林正在开始抢夺冠军。
### 决策树
- 易于解释
- 非参数化的，所以你不必担心异常值或者数据是否可线性分离
- 他们的主要缺点是他们很容易过拟合，但这就是像随机森林（或提升树）这样的集成方法进来的地方。
- 另外，随机森林往往是分类问题的赢家（通常在SVM上略微领先一些，我相信），它们快速且可扩展，并且您不必担心像SVM那样要调整一堆参数，所以他们现在似乎很受欢迎。
- 没有分布要求
- 启发式
- 适合少数类别变量
- 不受多重共线性（通过选择其中之一）
- Bagging, boosting, 集成方法通常优于单一算法。
树集成：随机森林和梯度提升树。
**Tree Ensembles vs LR。**
- 他们并不期望线性特征，甚至线性相互作用的特征。 LR中没有提到的一点是，它很难处理分类（二元）特征。 Tree Ensembles，因为它们不过是一堆决策树的组合，可以很好地处理这个问题。另一个主要优点是，由于它们使用装袋或提升构成的，这些算法可以非常好地处理高维空间以及大量的训练实例。
- 两者都是快速和可扩展的，随机森林往往会在准确性方面击败逻辑回归，但逻辑回归可以在线更新并为您提供有用的概率。
### 随机森林
随机森林使用数据的随机样本独立训练每棵树。 这种随机性有助于使模型比单个决策树更稳健，并且不太过拟合训练数据。 RF中通常有两个参数 – 树数量和被选择的每个结点的特征数目（列抽样）。
- RF适用于并行或分布式计算。
- 几乎总是比决策树具有更低的分类错误和更好的f分数。
- 几乎总是表现出与SVM相同或更好的效果，但对于人类来说更容易理解。
- 非常适合具有缺失变量的不均匀数据集。
- 给你一个关于你的数据集中的哪些特征是最重要的免费的好主意。
- 通常训练速度比支持向量机要快（尽管这显然取决于你的实现）。
### 梯度提升决策树
GBDT一次构建一棵树，每棵新树有助于纠正先前训练过的树造成的错误。 每添加一棵树，该模型就会变得更具表现力。 通常有三个参数 – 树的数量，树的深度和学习速率，每棵树的建立一般都很浅。
- 容易过拟合
- GBDT通常表现比RF好，但它们很难达到正确。 更具体地说，GBDT具有更多的超参数要调整，并且更容易出现过拟合。 RF几乎可以“开箱即用”，这也是他们非常受欢迎的原因之一。
- GBDT训练通常需要更长的时间，因为树是按顺序构建的。
### 神经网络
**优点**
- 很好地拟合具有大量输入特征的非线性数据
- 广泛应用于工业
- 许多开源实现
**缺点**
- 神经网络仅适用于数值输入，具有常数值的向量和具有非缺失数据的数据集。
- 分类边界难以直观地理解，并且ANN在计算上昂贵。
- 黑盒子，使他们很难与之合作，就像试图通过审查人类潜意识来解释我们的意识行为背后的原因。
- 难以训练：训练结果可能是非确定性的，并且主要取决于初始参数的选择
- 当他们不像您期望的那样工作时，他们很难排除故障，当他们工作时，您将永远不会确信自己会很好地归纳未包含在您的训练集中的数据，因为从根本上说，您不了解你的网络如何解决问题
- 多层神经网络通常很难训练，并且需要调整大量参数
- 神经网络不是概率性的，不像其他统计学或贝叶斯统计学。一个神经网络可能会给你一个连续的数字作为它的输出（例如一个分数），但是把它转换成一个概率往往是困难的。具有更强大理论基础的方法通常会直接为您提供这些概率。
### 深度学习
- 不是通用的分类技术。
- 擅长图像分类，视频，音频，文字。
## 概要
**考虑的因素**
- 训练例子的数量，（你的训练集有多大？）
	- 如果训练集很小，高偏差/低方差分类器（例如朴素贝叶斯）比低偏差/高方差分类器（例如，kNN或逻辑回归）具有优势，因为后者会过度拟合。但是随着训练集的增长（它们具有较低的渐近误差），低偏差/高方差分类器开始赢得胜利，因为高偏差分类器的功能不足以提供准确的模型。您也可以将其视为生成模型与判别模型的区别。
- 特征空间的维度
- 我希望问题是线性可分的吗？
- 特征是否独立？
- 期望的特征将与目标变量呈线性关系吗？
- 过度拟合是否会成为问题？
- 在速度/性能/内存使用方面，系统的要求是什么……？
- 它需要变量满足正态分布吗？
- 它是否遭受多重共线性问题？
- 用分类变量做作为连续变量是否表现好？
- 它是否计算没有CV的CI？
- 它是否可以不要stepwise而进行变量选择？
- 它适用于稀疏数据吗？
从Logistic回归等简单的事情开始，设置一个基线，并且只在需要时才会使其更加复杂。此时，树集成，特别是随机森林，因为它们很容易调整，可能是正确的路。如果你觉得还有改进的空间，试试GBDT或者更有兴趣去尝试深度学习。
> 
原文链接：https://www.hackingnote.com/en/machine-learning/algorithms-pros-and-cons/
编译：数据人网
链接：http://shujuren.org/article/591.html
