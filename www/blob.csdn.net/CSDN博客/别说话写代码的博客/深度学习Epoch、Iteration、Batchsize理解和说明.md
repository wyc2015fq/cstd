# 深度学习Epoch、Iteration、Batchsize理解和说明 - 别说话写代码的博客 - CSDN博客





2018年12月18日 17:46:39[别说话写代码](https://me.csdn.net/qq_21997625)阅读数：145








参考：https://blog.csdn.net/xiaohuihui1994/article/details/80624593 

mnist 数据集有 60000 张图片作为训练数据，10000 张图片作为测试数据。现在选择 Batch Size = 100 对模型进行训练。

每个 Epoch 要训练的图片数量：60000(训练集上的所有图像)

训练集具有的 Batch 个数： 60000/100=600

每个 Epoch 需要完成的 Batch 个数： 600

每个 Epoch 具有的 Iteration 个数： 600（完成一个Batch，相当于参数迭代一次）

每个 Epoch 中发生模型权重更新的次数：600

训练 10 个Epoch后，模型权重更新的次数： 600*10=6000

不同Epoch的训练，其实用的是同一个训练集的数据。第1个Epoch和第10个Epoch虽然用的都是训练集的60000图片，但是对模型的权重更新值却是完全不同的。因为不同Epoch的模型处于代价函数空间上的不同位置，模型的训练代越靠后，越接近谷底，其代价越小。

总共完成30000次迭代，相当于完成了 30000/600=50 个Epoch




