# 深度学习模型压缩与加速方法 - 别说话写代码的博客 - CSDN博客





2019年03月05日 20:54:07[别说话写代码](https://me.csdn.net/qq_21997625)阅读数：91








大型神经网络具有大量的层级与节点，因此考虑如何减少他们所需要的内存与计算量就显得极为重要，特别对于在线学习和增量学习等实时应用。此外，近来只能可穿戴设备的流行也为研究院提供了在资源（内存，cpu，能耗和带宽等）有限的便携式设备上部署深度学习应用提供了机会。比如resnet50，他有50层卷积网络，超过95M的存储需求和计算每一张图片所需要的浮点数乘法时间。如果剪枝一些冗余的权重后，其大概能节约75%的参数和50%的计算时间。对于只有兆字节资源的手机和FPGA等设备，如何使用这些方法压缩模型就很重要。

**将这些方法分为4个类别**：参数修剪和共享，低秩分解，迁移/压缩卷积滤波器和知识精炼等
- **参数修剪**（parameter pruning）和共享的方法关注于探索模型参数中冗余的部分，并尝试去除冗余和不重要的参数
- **基于低秩分解**（low-rank factorization）技术的方法使用矩阵/张量分解以估计深层CNN中最具信息量的参数
- **基于迁移/压缩卷积滤波器**以减少存储和计算的复杂度。
- **知识精炼**（knowledge distillation知识蒸馏）则学习了一个精炼模型，即训练一个更加紧凑的神经网络以再现大型网络的输出结果

![preview](https://pic4.zhimg.com/v2-f6d6345a52d104917025137a5f0e8c8f_r.jpg)

（图片转自：[https://zhuanlan.zhihu.com/p/32223484](https://zhuanlan.zhihu.com/p/32223484)） 
- 前端压缩：不破坏网络结构的压缩技术，在原有模型上减少filter的数量或者网络的深度等；
- 后端压缩：目的在于尽可能减少模型大小，会对原始网络结构造成极大的改造。

如何让神经网络在手机上更方便的运行（让神经网络更好更快的方法）：（模型的准确率，运行速度和在手机中所占空间大小）
- **避免全连接层**：全连接是密集连接，需要存储和更新大量参数，对运行速度和磁盘空间非常不利。使用卷积神经网络能够在维持高准确率的同时，减少连接/权重的数量
- **缩减通道数量和内核大小**：去掉特征通道，能很容易节省空间，让模型运行更快。也可以减少滤波器大小，虽然卷积对局部特征的感受度下降，但涉及的参数数量会减少（这两种方法准确率不会太下降），其实可以使用空洞卷积或者形变卷积来代替
- **优化降采样**：对于固定数量的层，和固定数量的池化操作，神经网络的表现会不大相同。这是因为数据的表示以及计算负荷取决于池化操作所处的时间点。若很早进行池化操作，数据维度会降低。维度小就以为着神经网络的处理速度更快，但也意味着更少的信息和更低的准确率。若晚执行池化操作，就会保存大部分信息，模型就有很高的准确度。这就以为着计算对象有很多维度，需要耗费大量计算力。所以网络中一般均匀进行降采样，这样能在模型准确率和速度之间取得较好的平衡
- **剪切权重**：训练后的神经网络中，有些权重对神经元的激活影响很大，而其余权重则很少影响结果，但是计算时仍然会计算这些较弱的权重。剪切权重就是我们将那些影响量级最小的连接完全移除的过程，这样我们就可以略过他们的计算。虽然这样会降低模型准确度，但是能让模型更轻便，运行更快。要找到一个平衡点，移除尽可能多的连接，但又不会导致准确度大幅度下降
- **权重量化**：为将神经网络保存在磁盘，一般需要记录神经网络中每个权重的值。这就是要保存每个参数的浮点值，这会大量占用磁盘空间。比如c语言浮点数占4个字节即32bit。一个数亿参数的网络（VGG）轻松就能占据上百兆字节空间，这在移动设备上不可接受。若想让神经网络的内存占用量尽可能的小 ，一个方法就是通过量化权重来降低他们的精度。这个过程中，我们会修改数字的表示使其不再去用任意值，而是一些值的子集。这能让我们只需存储一次经过量化处理的值，然后将他们参考为神经网络的权重。  通过再次寻找“平衡点”来确定使用多少值。值越多准确率越高，也就是要有更大的数字表示。如使用256个量化后的值，每个权重斤用1个字节即8bit就能表示。和之前相比（32bit），模型大小缩小了4倍
- **编码模型的表示**：上面几种都是对权重处理，尚未优化神经网络。这种方法是应对权重分布不均的问题。模型的权重量化以后，我们并没有相同数量的权重来支撑每个量化后的值。这意味着在模型的表示中，某些索引的出现频率相对更高，可以充分利用这一点。**哈夫曼编码huffman可以有效解决这个问题**。它会给最常用的值分配最小索引，给最不常用的值分配最大索引，这样能够 缩小设备上的模型大小，**主要是不会造成准确度下降**。这种技巧能让我们进一步缩小神经网络占用的内存空间，通常能缩小30%。（每一层的量化和编码可以操作可以不同，从而提供更多灵活性）
- **最后修正准确度损失**：使用上面列举的方法后，我们几乎大修了手头的神经网络：移除了弱连接（剪切权重），甚至修改了权重（量化操作）。这样能让神经网络很轻便快速运行，但准确度也会发生变化。为了修正这个问题，我们需要在每一步迭代的重新训练神经网络，这样它就能够适应变化，不断重复这个过程指导权重不再大幅变化为止。

可参考：shufflenet，mobilenet，squeezenet

文章参考：

[https://zhuanlan.zhihu.com/p/45072374](https://zhuanlan.zhihu.com/p/45072374)

[https://blog.csdn.net/cheese_pop/article/details/79544981](https://blog.csdn.net/cheese_pop/article/details/79544981)



