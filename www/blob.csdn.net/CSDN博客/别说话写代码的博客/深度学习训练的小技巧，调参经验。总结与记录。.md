# 深度学习训练的小技巧，调参经验。总结与记录。 - 别说话写代码的博客 - CSDN博客





2018年05月14日 23:03:29[别说话写代码](https://me.csdn.net/qq_21997625)阅读数：146标签：[调参技巧](https://so.csdn.net/so/search/s.do?q=调参技巧&t=blog)
个人分类：[Deep learning](https://blog.csdn.net/qq_21997625/article/category/7283460)









Ilya Sutskever（Hinton的学生）讲述了有关深度学习的见解及实用建议：




获取数据：确保要有高质量的输入/输出数据集，这个数据集要足够大、具有代表性以及拥有相对清楚的标签。缺乏数据集是很难成功的。

预处理：将数据进行集中是非常重要的，也就是要使数据均值为0，从而使每个维度的每次变动为1。有时，当输入的维度随量级排序变化时，最好使用那个维度的log(1+x)。基本上，重要的是要找到一个0值的可信编码以及自然分界的维度。这样做可使学习工作得更好。情况就是这样的，因为权值是通过公式来更新的：wij中的变化 \propto xidL/dyj（w表示从层x到层y的权值，L是损失函数）。如果x的均值很大（例如100），那么权值的更新将会非常大，并且是相互关联的，这使得学习变得低劣而缓慢。保持0均值和较小的方差是成功的关键因素。

批处理：在如今的计算机上每次只执行一个训练样本是很低效的。反之如果进行的是128个例子的批处理，效率将大幅提高，因为其输出量是非常可观的。事实上使用数量级为1的批处理效果不错，这不仅可获得性能的提升同时可降低过度拟合；不过这有可能会被大型批处理超越。但不要使用过大的批处理，因为有可能导致低效和过多过度拟合。所以我的建议是：根据硬件配置选取适合的批处理规模，量力而为会更加高效。

梯度归一化：根据批处理的大小来拆分梯度。这是一个好主意，因为如果对批处理进行倍增（或倍减），无需改变学习率（无论如何，不要太多）。

学习率计划：从一个正常大小的学习率（LR）开始，朝着终点不断缩小。

> 1LR的典型取值是0.1，令人惊讶的是，对于大量的神经网络问题来说，0.1是学习率的一个很好的值。通常学习率倾向于更小而非更大。
使用一个验证集——一个不进行训练的训练集子集，来决定何时降低学习率以及何时停止训练（例如当验证集的错误开始增多的时候）。
学习率计划的实践建议：若发现验证集遭遇瓶颈，不妨将LR除以2（或5），然后继续。最终，LR将会变得非常小，这也到了停止训练的时候了。这样做可以确保在验证性能受到损害的时候，你不会拟合（或过度拟合）训练数据。降低LR是很重要的，通过验证集来控制LR是个正确的做法。

但最重要的是要关注学习率。一些研究人员（比如Alex Krizhevsky）使用的方法是，监视更新范数和权值范数之间的比率。比率取值大约为10¯³。如果取值过小，那么学习会变得非常慢；如果取值过大，那么学习将会非常不稳定甚至失败。

权值初始化。关注权值在学习开始时的随机初始化。

> 如果想偷懒，不妨试试0.02*randn(num_params)。这个范围的值在许多不同的问题上工作得很好。当然，更小（或更大）的值也值得一试。
如果它工作得不好（例如是一个非常规的和/或非常深的神经网络架构），那么需要使用init_scale/sqrt(layer_width)*randn来初始化每个权值矩阵。在这种情况下，init_scale应该设置为0.1或者1，或者类似的值。
对于深度且循环的网络，随机初始化是极其重要的。如果没有处理好，那么它看起来就像没有学习到任何东西。我们知道，一旦条件都设置好了，神经网络就会学习。
一个有趣的故事：多年来，研究人员相信SGD不能训练来自随机初始化的深度神经网络。每次尝试都以失败告终。令人尴尬的是，他们没有成功是因为使用“小的随机权值”来进行初始化，虽然小数值的做法在浅度网络上工作得非常好，但在深度网络上的表现一点也不好。当网络很深时，许多权值矩阵之间会进行乘积，所以不好的结果会被放大。
但如果是浅度网络，SGD可以帮助我们解决该问题。

所以关注初始化是很有必要的。尝试多种不同的初始化，努力就会得到回报。如果网络完全不工作（即没法实施），继续改进随机初始化是正确的选择。

如果正在训练RNN或者LSTM，要对梯度（记得梯度已除以批量大小）范数使用一个硬约束。像15或者5这样的约束在我个人的实验中工作得很好。请将梯度除以批处理大小，再检查一下它的范数是否超过15（或5）。如果超过了，将它缩小到15（或5）。这个小窍门在RNN和LSTM的训练中发挥着巨大作用，不这样做的话，爆炸性的梯度将会导致学习失败，最后不得不使用像1e-6这样微小而无用的学习率。

数值梯度检查：如果没有使用过Theano或者Torch，梯度实现只能亲力亲为了。在实现梯度的时候很容易出错，所以使用数值梯度检查是至关重要的。这样做会让你对自己的代码充满信心。调整超级参数（比如学习率和初始化）是非常有价值的，因此好刀要用在刀刃上。

如果正在使用LSTM同时想在具有大范围依赖的问题上训练它们，那么应该将LSTM遗忘关口的偏差初始化为较大的值。默认状态下，遗忘关口是S型的全部输入，当权值很小时，遗忘关口会被设置为0.5，这只能对部分问题有效。这是对LSTM初始化的一个警示。

数据增加（Data augmentation）：使用算法来增加训练实例数量是个有创意的做法。如果是图像，那么应该转换和旋转它们；如果是音频，应该将清晰的部分和所有类型的杂音进行混合处理。数据添加是一门艺术（除非是在处理图像），这需要一定的常识。

dropout：dropout提供了一个简单的方法来提升性能。记得要调整退出率，而在测试时不要忘记关闭dropout，然后对权值求乘积（也就是1-dropout率）。当然，要确保将网络训练得更久一点。不同于普通训练，在进入深入训练之后，验证错误通常会有所增加。dropout网络会随着时间推移而工作得越来越好，所以耐心是关键。

综合（[Ensembling](http://en.wikipedia.org/wiki/Ensemble_learning)）。训练10个神经网络，然后对其预测数据进行平均。该做法虽然简单，但能获得更直接、更可观的性能提升。有人可能会困惑，为什么平均会这么有效？不妨用一个例子来说明：假如两个分类器的错误率为70%，如果其中一个的正确率保持较高，那么平均后的预测会更接近正确结果。这对于可信网络的效果会更加明显，当网络可信时结果是对的，不可信时结果是错的。
（下面几点是上面的简化版）


1：准备数据：务必保证有大量、高质量并且带有干净标签的数据，没有如此的数据，学习是不可能的

2：预处理：这个不多说，就是0均值和1方差化

3：minibatch：建议值128,1最好，但是效率不高，但是千万不要用过大的数值，否则很容易过拟合

4：梯度归一化：其实就是计算出来梯度之后，要除以minibatch的数量。这个不多解释

5：下面主要集中说下学习率

5.1：总的来说是用一个一般的学习率开始，然后逐渐的减小它

5.2：一个建议值是0.1，适用于很多NN的问题，一般倾向于小一点。

5.3：一个对于调度学习率的建议：如果在验证集上性能不再增加就让学习率除以2或者5，然后继续，学习率会一直变得很小，到最后就可以停止训练了。

5.4：很多人用的一个设计学习率的原则就是监测一个比率（每次更新梯度的norm除以当前weight的norm），如果这个比率在10-3附近，如果小于这个值，学习会很慢，如果大于这个值，那么学习很不稳定，由此会带来失败。

6：使用验证集，可以知道什么时候开始降低学习率，和什么时候停止训练。

7：关于对weight初始化的选择的一些建议：

7.1：如果你很懒，直接用0.02*randn(num_params)来初始化，当然别的值你也可以去尝试

7.2：如果上面那个不太好使，那么久依次初始化每一个weight矩阵用init_scale / sqrt(layer_width) * randn,init_scale可以被设置为0.1或者1

7.3：初始化参数对结果的影响至关重要，要引起重视。

7.4：在深度网络中，随机初始化权重，使用SGD的话一般处理的都不好，这是因为初始化的权重太小了。这种情况下对于浅层网络有效，但是当足够深的时候就不行了，因为weight更新的时候，是靠很多weight相乘的，越乘越小，有点类似梯度消失的意思（这句话是我加的）

8：如果训练RNN或者LSTM，务必保证gradient的norm被约束在15或者5（前提还是要先归一化gradient），这一点在RNN和LSTM中很重要。

9：检查下梯度，如果是你自己计算的梯度。

10：如果使用LSTM来解决长时依赖的问题，记得初始化bias的时候要大一点

12：尽可能想办法多的扩增训练数据，如果使用的是图像数据，不妨对图像做一点扭转啊之类的，来扩充数据训练集合。

13：使用dropout

14：评价最终结果的时候，多做几次，然后平均一下他们的结果。



