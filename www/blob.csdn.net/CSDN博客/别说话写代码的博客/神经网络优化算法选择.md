# 神经网络优化算法选择 - 别说话写代码的博客 - CSDN博客





2018年05月09日 10:56:53[别说话写代码](https://me.csdn.net/qq_21997625)阅读数：195








# 本文转自：https://blog.csdn.net/ybdesire/article/details/51792925

# 优化算法

解决优化问题，有很多算法（最常见的就是梯度下降），这些算法也可以用于优化神经网络。每个深度学习库中，都包含了大量的优化算法，用于优化学习速率，让网络用最快的训练次数达到最优，还能防止过拟合。 
keras中就提供了这样一些优化器[1]：
- SGD：随机梯度下降
- SGD+Momentum: 基于动量的SGD（在SGD基础上做过优化）
- SGD+Nesterov+Momentum：基于动量，两步更新的SGD（在SGD+Momentum基础上做过优化）
- Adagrad：自适应地为各个参数分配不同学习速率
- Adadelta： 针对Adagrad问题，优化过的算法（在Adagrad基础上做过优化）
- RMSprop：对于循环神经网络（RNNs）是最好的优化器（在Adadelta基础上做过优化）
- Adam：对每个权值都计算自适应的学习速率（在RMSprop基础上做过优化）
- Adamax：针对Adam做过优化的算法（在Adam基础上做过优化）

# 如何选择

有那么多优化算法，那么我们该怎么选择呢。有大神为我们给出了一些建议[2][3]
- 如果你的数据输入量很小，那就选一种自适应学习速率的方法。这样你就不用对学习速率进行调优，因为你的数据本来就小，NN学习耗时也小。这种情况你更应该关心网络分类的准确率。
- RMSprop, Adadelta, 和 Adam 非常相似，在相同的情况下表现都很好。
- 偏置校验让Adam的效果稍微比RMSprop好一点
- 进行过很好的参数调优的SGD+Momentum算法效果好于Adagrad/Adadelta

结论：到目前（2016.04）为止，如果你不知道为你的神经网络选择哪种优化算法，就直接选Adam吧！（Insofar, Adam might be the best overall choice.[2]）

# 参考
- [1] keras优化算法，[http://keras.io/optimizers/](http://keras.io/optimizers/)
- [2] 梯度下降优化总结，[http://sebastianruder.com/optimizing-gradient-descent/](http://sebastianruder.com/optimizing-gradient-descent/)
- [3] MNIST数据集上的优化结论，[http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html](http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html)
- [4] [http://blog.csdn.net/luo123n/article/details/48239963](http://blog.csdn.net/luo123n/article/details/48239963)



