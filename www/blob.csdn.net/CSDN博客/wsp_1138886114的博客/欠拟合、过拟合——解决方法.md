# 欠拟合、过拟合——解决方法 - wsp_1138886114的博客 - CSDN博客





2019年01月19日 22:11:11[SongpingWang](https://me.csdn.net/wsp_1138886114)阅读数：2107











### 文章目录
- [1  过拟合原因](#1___3)
- [2 判断是否过拟合](#2__24)
- [3 欠拟合--解决方法](#3__46)
- [4 过拟合--解决方法](#4__58)
- [5 神经网络过拟合解决方案](#5__77)



在机器学习或者深度神经网络中经常会出现：欠拟合和过拟合。这些问题的出现原因以及解决之道如下文。


##### 1  过拟合原因

（1）建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误， 抽样时没有足够正确考虑业务场景或业务特点，不能有效足够代表业务逻辑或业务场景。

（2）样本里的噪音数据干扰过大，模型学习了噪音特征，反而忽略了真实的输入输出间的关系。

（3）建模时的“逻辑假设”到了模型应用时已经不能成立了。 任何预测模型都是在假设的基础上才可以搭建和应用的。常用的假设包括：
- 假设历史数据可以推测未来，
- 假设业务环节没有发生显著变化，
- 假设建模数据与后来的应用数据是相似的，等等。
- 如果上述假设违反了业务场景的话，根据这些假设搭建的模型当然是无法有效应用的。

（4）参数太多、模型复杂度高。

（5）决策树模型。
- 如果我们对于决策树的生长没有合理的限制和修剪的话， 决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event）， 可以想象，这种决策树当然可以完美匹配（拟合）训练数据， 但是一旦应用到新的业务真实数据时，效果是一塌糊涂。

（6）神经网络模型。
- 由于对样本数据,可能存在隐单元的表示不唯一，即产生的分类的决策面不唯一，随着学习的进行, BP算法使权值可能收敛过于复杂的决策面，并至极致。
- 权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征.

##### 2 判断是否过拟合
- 首先看一下三种误差的计算方法：

training error (训练误差)
$$J_{train}(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$

cross validation error (交叉验证误差)
$$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$$

test error (测试误差)
$$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2$$- 判断究模型否过拟合方法：
- 1）学习曲线（learning curves）

学习曲线就是比较 $j_{train}$  和 $j_{cv}$。

如下图所示，为一般的学习曲线，蓝线:训练误差 $j_{train}$  , 粉色的线:验证集上的误差 $j_{cv}$，横轴表示训练集合的大小。
- 2）交叉验证（cross-validation）

模型的Error = Bias + Variance

Error反映的是整个模型的准确度

Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度

Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。
![这里写图片描述](https://img-blog.csdn.net/20180717192309372?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### 3 欠拟合–解决方法

首先欠拟合就是模型没有很好地捕捉到数据特征，不能够很好地拟合数据。
![这里写图片描述](https://img-blog.csdn.net/20180717174701563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**解决方法：**
- 
添加其他特征项，模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。

例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段， 无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。 除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。

- 
添加多项式特征，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。

- 
减少正则化参数，正则化的目的是用来防止过拟合的，当模型出现了欠拟合，则需要减少正则化参数。


##### 4 过拟合–解决方法

通俗一点地来说过拟合就是模型把数据学习的太彻底(强行拟合)，以至于把噪声数据的特征也学习到了， 这样不能够很好的分离（识别）测试数据，模型泛化能力太差。例如下面的例子：
![这里写图片描述](https://img-blog.csdn.net/20180717175751929?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
解决方法：
- 
重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的， 如果出现了过拟合就需要我们重新清洗数据。

- 
增大数据的训练量，之前用于训练的数据量太小导致的，训练数据占总数据的比例过小。

- 
采用正则化方法。正则化方法包括 L0正则、L1正则和L2正则， 而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则，下面看具体的原因。
- 
**L0 范数**是指向量中非0的元素的个数。

- 
**L1 范数**是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。

两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题）， 两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题）， 二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。

- 
**L2 范数**是指向量各元素的平方和然后求平方根。

可以使得W的每个元素都很小，都接近于0， 可以使得W的每个元素都很小，都接近于0， 但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，

但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低， 对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合， 以提高模型的泛化能力。还有就是看到有人说L2范数有助于处理 condition number不好的 情况下矩阵求逆很困难的问题（具体这儿我也不是太理解）。
##### 5 神经网络过拟合解决方案

```
（1）权值衰减.它在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,  
加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay, 从而使学习过程向着复杂决策面的反方向偏。 

（2）适当的stopping criterion （如图）
    在二次误差函数的情况下，关于早停止和权值衰减类似结果的原因说明。 
    椭圆给出了常数误差函数的轮廓线，Wml表示误差函数的最小值。 
    如果权向量的起始点为原点，按照局部负梯度的方向移动，那么它会沿着曲线给出的路径移动。 
    通过对训练过程早停止，我们找到了一个权值向量w。 
    定性地说，它类似于使用检点的权值衰减正则化项，然后最小化正则化误差函数的方法得到的权值。  

（3）验证数据
	一个最成功的方法是在训练数据外再为算法提供一套验证数据,
	应该使用在验证集合上产生最小误差的迭代次数,不是总能明显地确定验证集合何时达到最小误差. 
	（通常30%的训练模式；每个时期检查验证集错误；如果验证错误上升，停止训练）  
	
（4）交叉验证
	交叉验证方法在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重.   
	
（5）采用dropout方法。这个方法在神经网络里面很常用。 
	dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作，如下图。
```

![这里写图片描述](https://img-blog.csdn.net/20180717194545674?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

参考资料：
[https://blog.csdn.net/willduan1/article/details/53070777](https://blog.csdn.net/willduan1/article/details/53070777)
[https://blog.csdn.net/tansuo17/article/details/79129504](https://blog.csdn.net/tansuo17/article/details/79129504)










