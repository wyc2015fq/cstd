# faster RCNN详解 - wsp_1138886114的博客 - CSDN博客





2018年11月13日 21:46:04[SongpingWang](https://me.csdn.net/wsp_1138886114)阅读数：97











### 文章目录
- [faster RCNN 网络整体框架](#faster_RCNN__1)
- [一、 基础网络](#__14)
- [二、 区域建议网络(Region Proposal Network，RPN)](#_Region_Proposal_NetworkRPN_41)
- [2.1 RPN网络综述：](#21_RPN_52)
- [2.2 训练、目标和损失函数](#22__78)
- [FasterR-CNN 损失函数](#FasterRCNN__88)
- [非极大抑制(NMS)](#NMS_111)
- [独立应用程序](#_124)
- [三、兴趣区域池化(Rolpooling)](#Rolpooling_132)
- [四、基于区域的卷积神经网络 R-CNN](#_RCNN_143)
- [训练和目标](#_159)
- [后处理](#_168)
- [训练](#_175)
- [评估](#_190)
- [结论](#_195)




## faster RCNN 网络整体框架

Fast R-CNN 从 R-CNN 演变优化而来，其中的感兴趣区域池化的技术，使得网络可以共享计算结果，从而让模型提速。这一系列算法最终被优化为 Faster R-CNN，这是第一个完全可微分的模型。

faster R-CNN由几个部分组成，我将通过框架图来向大家展示图像的几个处理步骤。
![RPN](https://img-blog.csdnimg.cn/20190322112851148.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0,size_16,color_FFFFFF,t_70)
(1) 输入测试图像；

(2) 将整张图片输入CNN，进行特征提取；

(3) 用**RPN生成建议窗口(proposals)**，每张图片生成300个建议窗口；

(4) 把建议窗口映射到CNN的**最后一层卷积feature map**上；

(5) 通过RoI pooling层使每个RoI生成固定尺寸的feature map；

(6) 利用**Softmax Loss(探测分类概率)** 和 **Smooth L1 Loss(探测边框回归)** 对分类概率和边框回归(Bounding box regression)联合训练.
### 一、 基础网络

**VGG**
- 
分类任务中输入图片尺寸是固定的，因为网络最后一部分的全连接层需要固定长度的输入。在接入全连接层前，通常要将最后一层卷积的输出展开成一维张量。

- 
Faster R-CNN使用的卷积网络中间层的输出，所以输入图片的尺寸不再有限制。Faster R-CNN 论文中作者使用的是 conv5/conv5_1 这一层 (caffe 代码)。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190322113213714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0,size_16,color_FFFFFF,t_70)
**VGG vs ResNet**
- ResNet 已经取代大多数 VGG 网络作为提取特征的基础框架。

ResNet 对比 VGG 的优势在于它是一个更深层、大型的网络，因此有更大的容量去学习所需要的信息。这些结论在图片分类任务中可行，在目标探测的问题中也应该同样有效。

ResNet 在使用残差连接和批归一化的方法后更加易于训练
**锚点**

锚点是用固定的边框置于不同尺寸和比例的图片上，并且在之后目标位置的预测中用作参考边框。

我们的目标是寻找图片中的边框。这些边框是不同尺寸、不同比例的矩形。设想我们在解决问题前已知图片中有两个目标。那么首先想到的应该是训练一个网络，这个网络可以返回 8 个值：包含（xmin, ymin, xmax, ymax）的两个元组，每个元组都用于定义一个目标的边框坐标。
- 例如，由于图片可能是不同尺寸和比例的，因此训练一个可以直接准确预测原始坐标的模型是很复杂的。为解决无效预测：当预测（xmin,xmax）和（ymin,ymax）时，强制设定 xmin < xmax，ymin < ymax。
- 另一种更加简单的方法是去预测参考边框的偏移量。使用参考边框（xcenter, ycenter, width, height），学习预测偏移量（Δxcenter,Δycenter,Δwidth,Δheight），因此我们只得到一些小数值的预测结果并挪动参考变量就可以达到更好的拟合结果。

我们在处理的卷积特征图的尺寸分别是 convwidth×convheight×convdepth，因此在卷积图的 convwidth×convheight 上每一个点都生成一组锚点。很重要的一点是即使我们是在特征图上生成的锚点，这些锚点最终是要映射回原始图片的尺寸。
**映射回原始图片**：因为我们只用到了卷积和池化层，所以特征图的最终维度与原始图片是呈比例的。数学上，如果图片的尺寸是 w×h，那么特征图最终会缩小到尺寸为 w/r 和 h/r，其中 r 是次级采样率。如果我们在特征图上每个空间位置上都定义一个锚点，那么最终图片的锚点会相隔 r 个像素，在 VGG 中，r=16。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190322113248918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0,size_16,color_FFFFFF,t_70)
为了选择一组合适锚点，我们通常定义一组固定尺寸 (例如，64px、128px、256px，此处为边框大小) 和比例 (例如，0.5、1、1.5，此处为边框长宽比) 的边框，使用这些变量的所有可能组合得到候选边框 (这个例子中有 1 个锚点和 9 个边框)。

### 二、 区域建议网络(Region Proposal Network，RPN)

RPN 接受所有的参考框（锚点）并为目标输出一套好的建议。它通过为每个锚点提供两个不同的输出来完成。
- 第一个输出是锚点作为目标的概率「目标性得分」。注意，RPN 不关心目标的类别，只在意它实际上是不是一个目标（而不是背景）。我们将用这个目标性得分来过滤掉不好的预测，为第二阶段做准备。
- 第二个输出是边框回归，用于调整锚点以更好的拟合其预测的目标。

RPN 是用完全卷积的方式高效实现的，用基础网络返回的卷积特征图作为输入。使用一个有 512 个通道和 3x3 卷积核大小的卷积层，两个使用 1x1 卷积核的并行卷积层，其通道数量取决于每个点的锚点数量。
- 对于分类层，我们对每个锚点输出两个预测值：前景（实际的目标）的分数，背景（非目标）的分数。
- 对于回归或边框调整层，我们输出四个预测值：Δxcenter、Δycenter、Δwidth、Δheight，我们将会把这些值用到锚点中来得到最终的建议。使用最终的建议坐标和它们的目标性得分，然后可以得到一套很好的对于目标的建议。

##### 2.1 RPN网络综述：

整个**训练过程batchsize=1**，即每次输入一张图片，所以feature map的shape为（1，512，hh,  ww）。那么RPN的输入便是（1，512，hh,  ww）。然后经过512个3X3且含pad的卷积后仍为（1，512，hh，ww）。此卷积后shape并没有发生变化，意义是转换语义空间？然后分支出现了。有两路分支，左路是18个1X1卷积，右路是36个1X1卷积。**1X1卷积的意义是改变特征维度**。那左路卷积后shape为（1，18，hh，ww），右路卷积后shape为（1，36，hh，ww）。左路通道数变为18，是因为每个点对应的9个anchor实现2分类概率**预测**，所以是9X2  = 18！右路通道数变为36，是因为每个点对应的9个anchor实现4个坐标值的**预测**，所以是9X4 = 36！

**RPN网络中AnchorTargetCreator分析：**

将20000多个候选的anchor选出**256个**anchor进行**二分类和所有的**anchor进行**回归位置** 。为上面的**预测**值提供相应的**真实值**。选择方式如下：
- 对于每一个ground truth bounding box (`gt_bbox`)，选择和它重叠度（IoU）最高的一个anchor作为**正样本**。
- 对于剩下的anchor，从中选择和任意一个`gt_bbox`重叠度超过0.7的anchor，作为**正样本**，正样本的数目不超过128个。
- 随机选择和`gt_bbox`重叠度小于0.3的anchor作为**负样本**。负样本和正样本的总数为256。

对于每个anchor, gt_label 要么为1（前景），要么为0（背景），所以这样实现二分类。在计算回归损失的时候，只计算正样本（前景）的损失，不计算负样本的位置损失。

**RPN网络中ProposalCreator分析：**

RPN利用 **AnchorTargetCreator**自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程(**ProposalCreator**)如下：
- 对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。
- 选取概率较大的12000个anchor
- 利用回归的位置参数，修正这12000个anchor的位置，得到RoIs
- 利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs

注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.

这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。

RPN的输出：RoIs（形如2000×4或者300×4的tensor）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190322170813907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0,size_16,color_FFFFFF,t_70)
##### 2.2 训练、目标和损失函数

RPN 执行两种不同类型的预测：二进制分类和边框回归调整。为了训练，我们把所有的锚点分成两类。一类是「前景」，它与真实目标重叠并且其 IoU（Intersection of Union）值大于 0.5；另一类是「背景」，它不与任何真实目标重叠或与真实目标的 IoU 值 小于 0.1。这些锚点随机采样，构成大小为 256 的 mini batch——维持前景锚点和背景锚点之间的平衡比例。

RPN 用所有以 mini batch 筛选出来的锚点和二进制交叉熵（binary cross entropy）来计算分类损失。然后它只用那些标记为前景的 mini batch 锚点来计算回归损失。为了计算回归的目标，我们使用前景锚点和最接近的真实目标，并计算将锚点转化为目标所需的正确 Δ。

论文中建议使用 Smooth L1 loss 来计算回归误差，而不是用简单的 L1 或 L2 loss。Smooth L1 基本上就是 L1，但是当 L1 的误差足够小，由确定的 σ 定义时，可以认为误差几乎是正确的且损失以更快的速率减小。

使用 dynamic batches 是具有挑战性的，这里的原因很多。即使我们试图维持前景锚点和背景锚点之间的平衡比例，但这并不总是可能的。根据图像上的真实目标以及锚点的大小和比例，可能会得到零前景锚点。在这种情况下，我们转而使用对于真实框具有最大 IoU 值的锚点。这远非理想情况，但是为了总是有前景样本和目标可以学习，这还是挺实用的。

##### FasterR-CNN 损失函数

FasterR-CNN中对一个图像的函数定义为：
$$L( \{p_i\} ,\{u_i\}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i ,p_i^*) + \lambda \frac{1}{N_{reg}} \sum_i  p_i^* L_{reg}(t_i ,t_i^*)$$

其中：- $p_i$ 为anchor 预测为目标的概率；
- $GT$ 标签：$\rm p_i^* = \begin{cases} 0 &\text{negative label } \\ 1 &\text{positive  label } \end{cases}$
- $t_i = \{t_x , t_y , t_w ,t_h \}$ 该向量表示预测的 bounding box（包围盒）的4个参数化坐标
- $p_i^*$是 positive anchor 对应的ground truth  包围盒的坐标向量

$\rm L_{reg}$ 公式如下，其中 $x = t_i - t_i^* ,$
$$\rm smooth_{L_1}(x) = \begin{cases}   0.5x^2 &\text{if } |x|<1 \\   |x|-0.5 &\text{if } \rm otherwise\end{cases}$$

关于网络详情：[https://blog.csdn.net/wsp_1138886114/article/details/81906661](https://blog.csdn.net/wsp_1138886114/article/details/81906661)

##### 非极大抑制(NMS)

非极大抑制（Non-maximum suppression）：由于锚点经常重叠，因此建议最终也会在同一个目标上重叠。为了解决重复建议的问题，我们使用一个简单的算法，称为非极大抑制（NMS）。NMS 获取按照分数排序的建议列表并对已排序的列表进行迭代，丢弃那些 IoU 值大于某个预定义阈值的建议，并提出一个具有更高分数的建议。

IoU 的阈值设定:
- 太低，你可能会丢失对目标的建议；
- 太高，你可能会得到对同一个目标的很多建议。常用值是 0.6。

建议选择：应用 NMS 后，我们保留评分最高的 N 个建议。论文中使用 N=2000，但是将这个数字降低到 50 仍然可以得到相当好的结果。

##### 独立应用程序

RPN 可以独立使用，而不需要第二阶段的模型。在只有一类对象的问题中，目标性概率可以用作最终的类别概率。这是因为在这种情况下，「前景」=「目标类别」以及「背景」=「不是目标类别」。

一些从独立使用 RPN 中受益的机器学习问题的例子包括流行的（但仍然是具有挑战性的）人脸检测和文本检测。仅使用 RPN 的优点之一是训练和预测的速度都有所提高。由于 RPN 是一个非常简单的仅使用卷积层的网络，所以预测时间比使用分类基础网络更快。

### 三、兴趣区域池化(Rolpooling)

目标：在 RPN 步骤之后，我们有很多没有分配类别的目标建议。接下来就是如何将这些边框分类到我们想要的类别中。

最简单的方法是采用每个建议，裁剪出来，然后让它通过预训练的基础网络。然后，我们可以用提取的特征作为基础图像分类器的输入。这种方法的主要问题是运行所有 2000 个建议的计算效率和速度都是非常低的。

Faster R-CNN 试图通过复用现有的卷积特征图来解决或至少缓解这个问题。这是通过用兴趣区域池化为每个建议提取固定大小的特征图实现的。R-CNN 需要固定大小的特征图，以便将它们分类到固定数量的类别中。

一种更简单的方法（被包括 Luminoth 版本的 Faster R-CNN 在内的目标检测实现方法所广泛使用），是用每个建议来裁剪卷积特征图，然后用插值（通常是双线性的）将每个裁剪调整为固定大小（14×14×convdepth）。裁剪之后，用 2x2 核大小的最大池化来获得每个建议最终的 7×7×convdepth 特征图。

选择这些确切形状的原因与下一模块（R-CNN）如何使用它有关，这些设定是根据第二阶段的用途得到的。

### 四、基于区域的卷积神经网络 R-CNN

基于区域的卷积神经网络（R-CNN）是 Faster R-CNN 工作流的最后一步。从图像上获得卷积特征图之后，用它通过 RPN 来获得目标建议并最终为每个建议提取特征（通过 RoI Pooling），最后我们需要使用这些特征进行分类。R-CNN 试图模仿分类 CNNs 的最后阶段，在这个阶段用一个全连接层为每个可能的目标类输出一个分数。

R-CNN 有两个不同的目标：
- 将建议分到一个类中，加上一个背景类（用于删除不好的建议）。
- 根据预测的类别更好地调整建议的边框。

在最初的 Faster R-CNN 论文中，R-CNN 对每个建议采用特征图，将它平坦化并使用两个大小为 4096 的有 ReLU 激活函数的全连接层。

然后，它对每个不同的目标使用两种不同的全连接层：

一个有 N+1 个单元的全连接层，其中 N 是类的总数，另外一个是背景类。

一个有 4N 个单元的全连接层。我们希望有一个回归预测，因此对 N 个类别中的每一个可能的类别，我们都需要 Δcenterx、Δcentery、Δwidth、Δheight。
##### 训练和目标

R-CNN 的目标与 RPN 的目标的计算方法几乎相同，但是考虑的是不同的可能类别。我们采用建议和真实边框，并计算它们之间的 IoU。

那些有任何真实边框的建议，只要其 IoU 大于 0.5，都被分配给那个真实数据。那些 IoU 在 0.1 和 0.5 之间的被标记为背景。与我们在为 RPN 分配目标时相反的是，我们忽略了没有任何交集的建议。这是因为在这个阶段，我们假设已经有好的建议并且我们对解决更困难的情况更有兴趣。当然，这些所有的值都是可以为了更好的拟合你想找的目标类型而做调整的超参数。

边框回归的目标是计算建议和与其对应的真实框之间的偏移量，仅针对那些基于 IoU 阈值分配了类别的建议。我们随机抽样了一个尺寸为 64 的 balanced mini batch，其中我们有高达 25% 的前景建议（有类别）和 75% 的背景。

按照我们对 RPN 损失所做的相同处理方式，现在的分类损失是一个多类别的交叉熵损失，使用所有选定的建议和用于与真实框匹配的 25% 建议的 Smooth L1 loss。由于 R-CNN 边框回归的全连接网络的输出对于每个类都有一个预测，所以当我们得到这种损失时必须小心。在计算损失时，我们只需要考虑正确的类。

#### 后处理

与 RPN 相似，我们最终得到了很多已经分配了类别的目标，在返回它们之前需要进一步处理。

为了实施边框调整，我们必须考虑哪个类别具有对该建议的最高概率。我们也需要忽略具有最高概率的背景类的建议。在得到最终目标和忽略被预测为背景的目标之后，我们应用基于类的 NMS。这通过按类进行分组完成，通过概率对其排序，然后将 NMS 应用于每个独立的组。对于我们最后的目标列表，我们也可以设置一个概率阈值并且对每个类限制目标的数量。

#### 训练

在最初的论文中，Faster R-CNN 是用多步法训练的，独立地训练各部分并且在应用最终的全面训练方法之前合并训练的权重。之后，人们发现进行端到端的联合训练会带来更好的结果。

把完整的模型放在一起后，我们得到 4 个不同的损失，两个用于 RPN，另外两个用于 R-CNN。我们在 RPN 和 R-CNN 中有可训练的层，我们也有可以训练（微调）或不能训练的基础网络。

是否训练基础网络的决定取决于我们想要学习的目标特性和可用的计算能力。如果我们想检测与在原始数据集（用于训练基础网络）上的数据相似的目标，那么除了尝试压缩我们能获得的所有可能的性能外，其他做法都是没有必要的。另一方面，为了拟合完整的梯度，训练基础网络在时间和必要的硬件上都是昂贵的。

用加权和将四种不同的损失组合起来。这是因为相对于回归损失，我们可能希望给分类损失更大的权重，或者相比于 RPN 可能给 R-CNN 损失更大的权重。

除了常规的损失之外，我们也有正则化损失，为了简洁起见，我们可以跳过这部分，但是它们在 RPN 和 R-CNN 中都可以定义。我们用 L2 正则化一些层。根据正在使用哪个基础网络，以及如果它经过训练，也有可能进行正则化。

我们用随机梯度下降的动量算法训练，将动量值设置为 0.9。你可以轻松的用其他任何优化方法训练 Faster R-CNN，而不会遇到任何大问题。

学习率从 0.001 开始，然后在 50K 步后下降到 0.0001。这是通常最重要的超参数之一。

#### 评估

在一些特定的 IoU 阈值下，使用标准平均精度均值（mAP）来完成评估（例如，mAP@0.5）。mAP 是源于信息检索的度量标准，并且常用于计算排序问题中的误差和评估目标检测问题。

我们不会深入讨论细节，因为这些类型的度量标准值得用一篇完整博客来总结，但重要的是，当你错过了你应该检测到的框，以及当你发现一些不存在的东西或多次检测到相同的东西时，mAP 会对此进行惩罚。

#### 结论

到目前为止，你应该清楚 Faster R-CNN 的工作方式、设计目的以及如何针对特定的情况进行调整。Faster R-CNN 是被证明可以用相同的原理解决复杂的计算机视觉问题的模型之一，在这个新的深度学习革命刚开始的时候，它就展现出如此惊人的结果。目前正在建立的新模型不仅用于目标检测，还用于基于这种原始模型的语义分割、3D 目标检测等等。有的借用 RPN，有的借用 R-CNN，还有的建立在两者之上。因此，充分了解底层架构非常重要，从而可以解决更加广泛的和复杂的问题。












