# 回归模型-评估指标 - wsp_1138886114的博客 - CSDN博客





2018年06月02日 21:22:04[SongpingWang](https://me.csdn.net/wsp_1138886114)阅读数：1634
所属专栏：[机器学习算法](https://blog.csdn.net/column/details/24447.html)











- - - [一、多元线性回归](#一多元线性回归)
- [二、正则化防止过拟合](#二正则化防止过拟合)
- [三、非线性回归：多项式回归](#三非线性回归多项式回归)- - [3.1 回归模型评估指标](#31-回归模型评估指标)


- [四、决策树（分类回归树）分类标准](#四决策树分类回归树分类标准)
- [五、相关和回归](#五相关和回归)- - [5.1 相关和回归的关系](#51-相关和回归的关系)
- [5.2 线性相关性度量：皮尔逊相关系数](#52-线性相关性度量皮尔逊相关系数)


- [六、一元线性回归](#六一元线性回归)- - [6.1 一元线性回归模型](#61-一元线性回归模型)


- [七、课程总结](#七课程总结)






![这里写图片描述](https://img-blog.csdn.net/20180527220142245?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 一、多元线性回归

```
多元线性回归示例：
```



$y = b+a_1*x_1+a_2*x_2+···+a_n*x_n$
![这里写图片描述](https://img-blog.csdn.net/20180602154727981?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
```
房价预测案例：
多重共线性（Multicollinearty）:
    是指线性回归模型中的 解释变量（X）之间
    由于存在高度相关关系而使模型估计失真或难以估计准确
多重共线性的影响:
    上述模型参数（$a_1,a_2...$）估值不准，有时候会导致出现相关性反转。

如何发现多重共线性
    对X变量探索两两之间的相关性（相关矩阵）

逐步回归概念是一种多元回归模型进行变量筛选的方法，筛选最少的变量来获取最大化预测能力
三种方法：
    向前选择法
    向后剔除法
    逐步回归法
```

### 二、正则化防止过拟合
- L2正则化–岭回归 Ridge Regression


$min\sum_{i=1}^{n}\left ( Y_i-\hat{Y_i} \right )=min\sum_{i=1}^{n}\hat{\varepsilon}_i^2 $

在最小化残差平方和的基础上，增加L2范数的惩罚项： 


$\sum_{i=1}^{n}\left ( y_i-\beta_0-\sum_{j=1}^{p}\beta _jx_{ij} \right )^2+\lambda\sum_{j=1}^{p}\beta_j^2=RSS +\lambda\sum_{j=1}^{p}\beta_j^2$- L1正则化–lasso回归


$min\sum_{i=1}^{n}\left ( Y_i-\hat{Y_i} \right )=min\sum_{i=1}^{n}\hat{\varepsilon}_i^2 $

在最小化残差平方和的基础上，增加L1范数的惩罚项： 


$\sum_{i=1}^{n}\left( y_i-\beta_0-\sum_{j=1}^{p}\beta _jx_{ij} \right )^2+\lambda\sum_{j=1}^{p}|\beta_j|=RSS +\lambda\sum_{j=1}^{p}|\beta_j|$
### 三、非线性回归：多项式回归
- 方法：
非线性回归的转换——取对数
```python
多项式回归代码实现：
sklearn.preprocession.PolynomialFeatures(
                degree = 2,              #阶数
                interaction_only = False,
                include_bias = True
               ) 

sklearn.linear_model.LinearRegression(
                fit_intercept = True,
                noemalize = False,
                copy_X = True
                )
```

##### 3.1 回归模型评估指标
- 解释方差（Explianed variance score）：


$Explianed\_variance(y,\hat{y}) = 1-\frac{Var\left \{ y-\hat{y} \right \}}{Var\left \{ y \right \}}$- 绝对平均误差（Mean absolute error）：


$MAE(y,\hat{y}) = \frac{1}{n_{samplies}}\sum_{i=0}^{n_{samplies}-1}|y_i-\hat{y}|$- 均方误差（Mean squared error）：


$MSE(y,\hat{y}) = \frac{1}{n_{samplies}}\sum_{i=0}^{n_{samplies}-1}(y_i-\hat{y})^2$- 决定系数（$R^2 $ score）


$R^2(y,\hat{y}) =1-\frac{\sum_{i=0}^{{n_{samplies}}^{-1}}(y_i-\hat{y_i})^2}{\sum_{i=0}^{{n_{samplies}}^{-1}}(y_i-\bar{y})^2} $
```python
代码：
sklearn.metrics
from sklearn.metrics import explained_variance_score
explained_variance_score(y_true,y_pred)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_true,y_pred)

from sklearn.metrics import mean_squared_error
mean_squared_error(y_true,y_pred)

from sklearn.metrics import r2_score
r2_score(y_true,y_pred)
```

### 四、决策树（分类回归树）分类标准

![这里写图片描述](https://img-blog.csdn.net/20180602161538774?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> 

    Gain(A) = Variance(父) - Variance(子)        #Gain(A)信息增益
### 五、相关和回归

##### 5.1 相关和回归的关系

```
都是研究变量相互关系的分析方法
    相关分析是回归分析基础和前提，回归分析是变量之间相关程度的具体形式
    相关分析：正相关,负相关
    相关形式: 线性, 非线性
```

![这里写图片描述](https://img-blog.csdn.net/20180602162235868?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![这里写图片描述](https://img-blog.csdn.net/20180602162355276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

>
##### 5.2 线性相关性度量：皮尔逊相关系数



$r = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}$
![这里写图片描述](https://img-blog.csdn.net/20180602163536955?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

相关VS回归: 
![这里写图片描述](https://img-blog.csdn.net/20180602164437405?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
### 六、一元线性回归

##### 6.1 一元线性回归模型

![这里写图片描述](https://img-blog.csdn.net/20180602164926189?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- 寻找最佳拟合直线：最小二乘法
该方法是寻找最佳拟合直线的参数（斜率和截距） 


$min\sum_{i=1}^{n}\left ( Y_i-\hat{Y_i} \right )^2 = min\sum_{i=1}^{n}\hat{\varepsilon _i}^2$
参数估计**回归表达式**：$\hat{Y_i} = \hat{β_0}+\hat{β_1}x_i$

$斜率:~~~~~~\hat{\beta_1}=\frac{SS_{xy}}{SS_{xx}} = \frac{\sum(x_i-\bar{x})y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$


$截 距:~~~~~~~~ \hat{β_0} = \bar{y}-\hat{β_1}\bar{x}~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~$
### 七、课程总结

```
分类与回归 区别与联系
相似之处：
    都是有监督学习
    最重要的两种预测模型
    决策树既可以分类 也可以做回归
    二元分类模型的经典算法逻辑回归算法，本质上也是一种回归算法

区别：
    回归目标变量是连续型变量
    分类目标变量是类别型变量

常见的饿回归算法和模型
    1 基于最小二乘法的一元/多元线性回归
    2 多项式回归（非线性）
    3 Ridge 回归（L2正则化回归），岭回归
    4 Lasso 回归（L1正则化回归），套索回归
    5 决策树（CART，分类回归树）
    6 逻辑回归
```







