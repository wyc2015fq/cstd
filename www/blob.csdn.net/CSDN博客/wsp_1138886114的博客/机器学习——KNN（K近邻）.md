# 机器学习——KNN（K近邻） - wsp_1138886114的博客 - CSDN博客





2018年05月28日 20:47:24[SongpingWang](https://me.csdn.net/wsp_1138886114)阅读数：176
所属专栏：[机器学习算法](https://blog.csdn.net/column/details/24447.html)











- - - [K近邻（KNN）K Nearest Neighbors](#k近邻knnk-nearest-neighbors)
- [一. 什么是K近邻](#一-什么是k近邻)

- - [二 .  K近邻的距离度量公式](#二-k近邻的距离度量公式)
- [三.    K值选择](#三-k值选择)- - [3.1 近似误差（训练集误差）与 估计误差（测试集误差）](#31-近似误差训练集误差与-估计误差测试集误差)
- [3.2 K值确定标准：](#32-k值确定标准)


- [四 kd-tree （k-dimensional树）](#四-kd-tree-k-dimensional树)- - [4.1 原理](#41-原理)
- [4.2 构造方法](#42-构造方法)


- [五.   K近邻算法代码](#五-k近邻算法代码)
- [六.   K近邻模型优化](#六-k近邻模型优化)- - [6.1 K值](#61-k值)
- [6.2 选择距离格式：](#62-选择距离格式)
- [6.3 K 近邻投票加权方法](#63-k-近邻投票加权方法)








### K近邻（KNN）K Nearest Neighbors
|$~~~~~$|有监督学习|无监督学习|
|----|----|----|
|样本|必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。|没有训练集，只有一组数据，在该组数据集内寻找规律。|
|目标|方法是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。|方法只有要分析的数据集的本身，预先没有什么标签。   如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。|

```
（1）无监督学习是在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。
    这一点是比有监督学习方法的用途要广。譬如分析一堆数据的主分量，或分析数据集有什么特点都可以归于
    非监督学习方法的范畴。

（2）用非监督学习方法分析数据集的主分量与用K-L变换计算数据集的主分量又有区别。后者从方法上讲不是学习方法。
    因此用K-L变换找主分量不属于无监督学习方法，即方法上不是。而通过学习逐渐找到规律性这体现了学习方法这一点。
    在人工神经元网络中寻找主分量的方法属于无监督学习方法。
```
|$~~~~~~~$|监督学习|强化学习|
|----|----|----|
|反馈映射|都会学习出输入到输出的一个映射，监 督式学习出的是之间的关系，可以告诉 算法什么样的输入对应着什么样的输出。|强化学习出的是给机器的反馈 reward function，即用来判断这个行为是好是坏。|
|反馈时间|做了比较坏的选择会立刻反馈给算法。|结果反馈有延时，有时候可能需 要走了很多步以后才知道以前的 某一步的选择是好还是坏。|
|输入特征|输入是独立同分布的。|面对的输入总是在变化，每当算 法做出一个行为，它影响下一次 决策的输入。|
|行为模式|不考虑行为间的平衡，只是 exploitative。|一个 agent 可以在探索和开发（exploration and exploitation）之间做权衡，并且选择一个最大的 回报。 exploration 会尝试很多不同的事情，看它们是否 比以前尝试过的更好。   exploitation 会尝试过去经验中最有效的行为。|

### 一. 什么是K近邻

```
思想：只要知道你朋友（邻居）是什么人，就能知道你是什么人
K近邻是一种 懒惰的学习方法：（基于实例学习）

Lazy learners： instance-based learning

新数据来时，才开始学习给出分类
KNN 没有训练模型，训练和预测结合一起
```

## ![这里写图片描述](https://img-blog.csdn.net/20180704162111231?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 二 .  K近邻的距离度量公式

```
距离（distance）--衡量样本之间的相相识度
欧式距离（平面几何之间的距离）
曼哈顿距离（两点之间X的距离+Y的距离：类似楼梯铺红毯，红毯的长度）
闵氏距离（闵可夫斯基距离）
```

**通用距离公式 ( 闵氏距离 )**


$D(X,Y)=\sqrt[p]{\sum _{i=1}^n|X_i-Y_i|^p}$

当 $p = 1$ 为曼哈顿距离：$D(X,Y)={\sum _{i=1}^n|X_i-Y_i|}$

当 $p = 2$ 为欧式距离：$D(X,Y)=\sqrt[]{\sum _{i=1}^n(X_i-Y_i)^2}$
### 三.    K值选择

> 
##### 3.1 近似误差（训练集误差）与 估计误差（测试集误差）

```
近似误差：对现有训练集的训练误差，关注训练集，如果近似误差过小可能会出现过拟合的现象，
         对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。
         模型本身不是最接近最佳模型

估计误差：可以理解为对测试集的测试误差，关注测试集，估计误差小说明对未知数据的预测能力好，
         模型本身最接近最佳模型。
```

##### 3.2 K值确定标准：

```
K值过小：k值小，特征空间被划分为更多子空间（模型的项越多），整体模型变复杂，容易过拟合，
        k值越小，选择的范围就比较小，训练的时候命中率较高，近似误差小，
        而用test的时候就容易出错，估计误差大，容易过拟合。

K值=N： 无论输入实例是什么，都将简单的预测他属于训练实例中最多的类
```

![这里写图片描述](https://img-blog.csdn.net/20180704164324709?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

      K近邻算法基本流程
```
开始
    指定K值
    计算输入样本与训练样本之间的距离
    按距离排序
    筛选K个最近邻居
    投票判断分类
    结束
```

### 四 kd-tree （k-dimensional树）

> 
##### 4.1 原理

```
一种分割k维数据空间的数据结构。应用于多维空间关键数据搜索（如：范围和最近邻搜索）。

kd树是是一种二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面
    将K维空间切分，构成一系列的K维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。
    利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。
```


![这里写图片描述](https://img-blog.csdn.net/20180704170855701?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


##### 4.2 构造方法

```
（1）构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；

（2）通过递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和
     在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，
     将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域

（3）上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。

（4）通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，
    这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，
    且它的左子树和右子树都是平衡二叉树）。 

    注意：平衡的kd树搜索时的效率未必是最低的。特殊情况有时在稀疏点切
```



![这里写图片描述](https://img-blog.csdn.net/20180704172206937?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


### 五.   K近邻算法代码

```
sklearn.neighbors  KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(
                            n_neighbors = 5,
                            weights = 'uniform',
                            algorithm = 'auto',
                            leaf_size = 30,
                            p = 2,
                            metric = 'minkowski',
                            metric_param = None,
                            n_jobs = 1
                            )
一些重要的参数：
n_neighbors --K值
weights     --邻居权重，uniform邻居一样大/distance距离小权重大
metric_param--距离计算公式   minkowski闵氏距离
p           ---闵氏距离参数  p = 1(曼哈顿距离) p = 2(欧式距离)
```

### 六.   K近邻模型优化

> 
##### 6.1 K值

```
K过小（理论最小=1）也就是邻居数为1，会受到噪声数据影响，降低分类精度
    K过大（理论=训练样本数）会受到不相关数据影响，降低分类精度
使用交叉验证寻找最好的K值
经验值 k = sqr(n)/2, n时训练样本数
```

##### 6.2 选择距离格式：

##### 6.3 K 近邻投票加权方法

```
平均加权    uniform 与邻居一样大
距离加权    distance 距离小权重大
用强大的 Gridsearch（网格搜索）寻找最优参数
parameters = {
                'n_neighbors':[5,10,15,20,30],
                'weights':['uniform','distance'],
                'p':[1,2]
             }
knn = KNeighborsClassifier()
grid_search = GridSearchCV(lnn,parameters,scoring = 'accuracy',cv = 5)
grid_search.fit(x,y)
```







