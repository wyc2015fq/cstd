# 最小二乘法—梯度下降专题 - wsp_1138886114的博客 - CSDN博客





2018年08月01日 16:22:59[SongpingWang](https://me.csdn.net/wsp_1138886114)阅读数：129标签：[梯度下降																[最全的梯度下降](https://so.csdn.net/so/search/s.do?q=最全的梯度下降&t=blog)](https://so.csdn.net/so/search/s.do?q=梯度下降&t=blog)
个人分类：[深度学习																[机器学习](https://blog.csdn.net/wsp_1138886114/article/category/7692618)](https://blog.csdn.net/wsp_1138886114/article/category/7729523)









- - - [一、梯度下降优化简介](#一梯度下降优化简介)
- [二、经典梯度下降优化](#二经典梯度下降优化)- - [2.1—A 批量梯度下降法BGD](#21a-批量梯度下降法bgd)
- [2.1—B  随机梯度下降法SGD](#21b-随机梯度下降法sgd)
- [2.1—C 小批量梯度下降法 Mini-batch Gradient Descent](#21c-小批量梯度下降法-mini-batch-gradient-descent)


- [三 动量与自适应梯度下降](#三-动量与自适应梯度下降)- - [3.2—A： Momentum](#32a-momentum)
- [3.2—B： Nesterov Momentum](#32b-nesterov-momentum)
- [3.3—A：  Adagrad](#33a-adagrad)
- [3.3—B： Adadelta](#33b-adadelta)
- [3.4 RMSprop](#34-rmsprop)
- [3.5 Adam](#35-adam)
- [3.6 Adamax](#36-adamax)








### 一、梯度下降优化简介

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，**当目标函数是凸函数时，梯度下降法的解是全局解**。  

一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。  

最速下降法越接近目标值，步长越小，前进越慢。 
![这里写图片描述](https://img-blog.csdn.net/2018080112410553?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

梯度下降法的缺点： 

　　（1）靠近极小值时收敛速度减慢，如下图所示； 

　　（2）直线搜索时可能会产生一些问题； 

　　（3）可能会“之字形”地下降。 


在机器学习中：  

基于基本的梯度下降法发展了两种梯度下降方法，分别为**随机梯度下降法**和**批量梯度下降法**。  
一般线性回归函数的假设函数为：$h_θ=∑_{j=0}^{n}θ_jx_j$

对应 损失函数：$J_{train}(θ)=\frac{1}{(2m)}∑_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})^2$

　　下图为一个二维参数$（θ_0和θ_1）$组对应能量函数的可视化图： 
![这里写图片描述](https://img-blog.csdn.net/20180801134802188?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
### 二、经典梯度下降优化

##### 2.1—A 批量梯度下降法BGD

Batch Gradient Descent：是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新： 

　　(1) 对上述的能量函数求偏导： 
![这里写图片描述](https://img-blog.csdn.net/20180801135207668?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


　　(2) 由于是最小化风险函数，所以按照每个参数θ的梯度负方向来更新每个θ： 

![这里写图片描述](https://img-blog.csdn.net/20180801135243118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


从上面公式可知：得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目m很大，那么可想而知这种方法的迭代速度！ 

```
优点：全局最优解；易于并行实现；
缺点：当样本数目很多时，训练过程会很慢。
```

##### 2.1—B  随机梯度下降法SGD

由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。 
![这里写图片描述](https://img-blog.csdn.net/2018080114051979?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本。但是，**SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向**。 如上图 
```
优点：训练速度快；可以在线更新
  缺点：准确度下降，有几率收敛到全局最优；不易于并行实现。
```

##### 2.1—C 小批量梯度下降法 Mini-batch Gradient Descent

两种方法的性能之间的一个折衷，即，算法的训练过程比较快，而且也保证最终参数训练的准确率 

```
批量梯度下降法；随机梯度下降法；小批量梯度下降法三个主要挑战：
    1. 选择适当的学习率比较困难，太小收敛慢，太大造成波动，妨碍收敛。  
    2. 目前采用可变学习率。例如退火算法。 
       设定迭代次数和损失函数阈值，每执行完m次训练便减少学习率；或者损失函数低于设定阈值时减少学习率。
```

### 三 动量与自适应梯度下降

##### 3.2—A： Momentum

momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力： 
![这里写图片描述](https://img-blog.csdn.net/20180801152336185?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)- 特点：
下降初期时，使用上一次参数更新，下降方向一致，乘上较大的$\mu$能够进行很好的加速下降中后期时，在局部最小值来回震荡的时候，$gradient\to0，\mu$使得更新幅度增大，跳出陷阱在梯度改变方向的时候，$\mu$能够减少更新总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛.
##### 3.2—B： Nesterov Momentum

这是对传统momentum方法的一项改进 

nesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。 将上一节中的公式展开可得： 


$\Delta{\theta_t}=-\eta*\mu*m_{t-1}-\eta*g_t$

可以看出，$m_{t-1}$并没有直接改变当前梯度$g_t$，所以Nesterov的改进就是让之前的动量直接影响当前的动量。即： 


$g_t=\nabla_{\theta_{t-1}}{f(\theta_{t-1}-\eta*\mu*m_{t-1})}$



$m_t=\mu*m_{t-1}+g_t~~~~~~~~~~~~~~~~~~~~~~~~~$



$\Delta{\theta_t}=-\eta*m_t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$

所以，加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。如下图： 
![这里写图片描述](https://img-blog.csdn.net/20180801160103159?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量)  

##### 3.3—A：  Adagrad

Adagrad其实是对**学习率进行了一个约束**。即： 


$n_t=n_{t-1}+g_t^2$



$\Delta{\theta_t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t$

此处，对$g_t$从1到t进行一个递推形成一个约束项regularizer，$-\frac{1}{\sqrt{\sum_{r=1}^t(g_r)^2+\epsilon}}，\epsilon$ 用来保证分母非0
- 特点：
前期 $g_t$较小的时候， regularizer较大，能够放大梯度 

后期 $g_t$较大的时候，regularizer较小，能够约束梯度 

适合处理稀疏梯度- 缺点：
由公式可以看出，仍依赖于人工设置一个全局学习率 
$\eta$ 设置过大的话，会使regularizer过于敏感，对梯度的调节太大 

中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束
##### 3.3—B： Adadelta

Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。即： 


$n_t=\nu*n_{t-1}+(1-\nu)*g_t^2$



$\Delta{\theta_t} = -\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t$

在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后： 


$E|g^2|_t=\rho*E|g^2|_{t-1}+(1-\rho)*g_t^2$



$\Delta{x_t}=-\frac{\sqrt{\sum_{r=1}^{t-1}\Delta{x_r}}}{\sqrt{E|g^2|_t+\epsilon}}$

其中，E代表求期望。 

此时，可以看出Adadelta已经不用依赖于全局学习率了。
- 特点：
训练初中期，加速效果不错，很快 

训练后期，反复在局部最小值附近抖动
##### 3.4 RMSprop

RMSprop可以算作Adadelta的一个特例：

当$\rho=0.5时，E|g^2|_t=\rho*E|g^2|_{t-1}+(1-\rho)*g_t^2$就变为了求梯度平方和的平均数。

如果再求根的话，就变成了RMS(均方根)： 


$RMS|g|_t=\sqrt{E|g^2|_t+\epsilon}$

此时，这个RMS就可以作为学习率$\eta$的一个约束： 


$\Delta{x_t}=-\frac{\eta}{RMS|g|_t}*g_t$
- 特点：
其实RMSprop依然依赖于全局学习率 

RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间 

适合处理非平稳目标- 对于RNN效果很好
##### 3.5 Adam

Adam(Adaptive Moment Estimation)本质上是**带有动量项的RMSprop**，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。  

Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下： 


$m_t=\mu*m_{t-1}+(1-\mu)*g_t$


$n_t=\nu*n_{t-1}+(1-\nu)*g_t^2$



$\hat{m_t}=\frac{m_t}{1-\mu^t}$



$\hat{n_t}=\frac{n_t}{1-\nu^t}$



$\Delta{\theta_t}=-\frac{\hat{m_t}}{\sqrt{\hat{n_t}}+\epsilon}*\eta$

其中，$m_t，n_t$分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望$E|g_t|，E|g_t^2|$的估计；  
$\hat{m_t}，\hat{n_t}$是对m_t，n_t的校正，这样可以近似为对期望的无偏估计。可以看出，直接对梯度的矩估计对内存没有额外的要求，而且可以根据梯度进行动态调整，而$-\frac{\hat{m_t}}{\sqrt{\hat{n_t}}+\epsilon}$对学习率形成一个动态约束，而且有明确的范围。
- 特点：
结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 

对内存需求较小 

为不同的参数计算不同的自适应学习率 

也适用于大多非凸优化- 适用于大数据集和高维空间
##### 3.6 Adamax

Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。公式上的变化如下： 


$n_t=max(\nu*n_{t-1},|g_t|)$



$\Delta{x}=-\frac{\hat{m_t}}{n_t+\epsilon}*\eta$

可以看出，Adamax学习率的边界范围更简单Nadam  

Nadam类似于带有Nesterov动量项的Adam。公式如下： 


$\hat{g_t}=\frac{g_t}{1-\Pi_{i=1}^t\mu_i}$


$m_t=\mu_t*m_{t-1}+(1-\mu_t)*g_t$



$\hat{m_t}=\frac{m_t}{1-\Pi_{i=1}^{t+1}\mu_i}$



$n_t=\nu*n_{t-1}+(1-\nu)*g_t^2$



$\hat{n_t}=\frac{n_t}{1-\nu^t}\bar{m_t}=(1-\mu_t)*\hat{g_t}+\mu_{t+1}*\hat{m_t}$



$\Delta{\theta_t}=-\eta*\frac{\bar{m_t}}{\sqrt{\hat{n_t}}+\epsilon}$

可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。

```
对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。
在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果
```

鸣谢 
[https://blog.csdn.net/fishmai/article/details/52510826](https://blog.csdn.net/fishmai/article/details/52510826)
[https://blog.csdn.net/blue_jjw/article/details/50650248](https://blog.csdn.net/blue_jjw/article/details/50650248)













