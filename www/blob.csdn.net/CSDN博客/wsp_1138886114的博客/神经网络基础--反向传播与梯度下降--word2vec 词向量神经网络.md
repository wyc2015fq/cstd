# 神经网络基础--反向传播与梯度下降--word2vec 词向量神经网络 - wsp_1138886114的博客 - CSDN博客





2018年08月02日 09:39:19[SongpingWang](https://me.csdn.net/wsp_1138886114)阅读数：477
所属专栏：[深度学习—神经网络](https://blog.csdn.net/column/details/27368.html)









![这里写图片描述](https://img-blog.csdn.net/20180801112429893?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dzcF8xMTM4ODg2MTE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 1  项目背景：个性化推荐意义

```
注册商标问题，papi酱 网名被注册
初始词向量
项目环境：Python和Tensorflow
    python:     pip install tensorflow
    Anaconda:   conda install tensorflow
```

### 2 神经网络基础知识精讲

```
基础知识
    是一个黑箱算法，可解释性不强
    hidden layer 隐藏层，不可见，数值不是训练前可以知道的
    input layer 输入层：都是实数
    输入-->函数：线性函数（可以嵌套）但是还是线性函数，
                 所以引用非线性函数（activation function）增强神经网络表达能力
    激活函数不可导的梯度函数，在求极值的时候，无法使用梯度下降法（求导数来求极值）
    神经网络表达能力和可解释性:
        足够的深度和宽度可以拟合任何一个复杂的函数（神经网络 manifold 理论），反之则拟合不足
        若宽度不足，深度很深也不行 

反向传播与梯度下降法
    loss 损失函数
    机器学习/数据挖掘模型 的目的，拟合一个函数，通过构造损失函数（loss），希望损失尽可能小，找一个函数使得loss尽可能小
    用导数求极小值，求导就叫做反向传播
    梯度问题
        导数链式法则
            若导数（变化率）小，经过很多层网络求导，他们的积接近于零，-->梯度消失
            若导数（变化率）大，经过很多层网络求导，他们的积接近无限大，-->梯度爆炸
        激活函数导数
        梯度下降方法：
            在神经网络中，参数初始值的选取，是优化的重要参数
            随机选取n个初始值，进行n次迭代
        梯度下降（Gradient Descent），步长抖动（步长不能太大，太小收敛太慢）

        解决梯度的方法：
            1--Lstm(long short-term memory)长短期记忆，用来解决RNN模型梯度问题
            （recursive/recurrent neural network 递归/周期神经网络，早期的记忆也会成为输入）
            2--选择特殊的激活函数ReLU(修正线性单元)
        Batch size 训练样本大小
            迭代算法对每一批样本，只走一步
            Batch size太小抖动大，太大则训练不明显，收敛满，要适当的小
```

### 3  word2vec 词向量原理

```
对词进行编码及评价效果--连续性
    用夹角的余弦值表示词意思之间的距离，余弦值越大，两个词之间的意思越接近
传统方法：利用term-doc matrix（词-文档矩阵）对词进行编码
          只能表示词出现的频次，词与词之间的距离无法体现
```
- onehot 编码优缺点，以及限制
纬度高，新词没有对应的编码 

        因为onehot编码很多位是0，最终神经网络计算量不大 

        可以只考虑高频词，从而避免过分膨胀- word2vec优点

1 分布假说（只考虑近距离的词，语境/上下文） 

    2 在小范围内，可以不考虑字的顺序 

    3 试验表明，考虑太多细节（比如语法）效果不一定更好
##### word2vec 神经网络模型
- cbow方法的网络模型
cbow(continuous bag of words l连续词袋)  

与BoW一样，抛弃了词序信息，然后窗口在语料上滑动，成就了连续词袋 

        输入上下文—> 输出中心词- skip-gram 方法的网络模型
1 三层神经网络2 中间层（第二层）没有激活函数（sigmoid）。避免语义过度抽象3 输出层（第三层）又激活函数softmax 

    采样过程，忽略一部分的词 

    输入中心词—> 输出上下文- 从 onehot 编码–>自编码
word2vec 代码剖析 skip-gram 

word2vec 之cbow 

word2vec 运行结果分析 

    模型训练标准 

    模型测试标准
### 4 协同过滤：item embedding
- 采用embedding技术实现推荐系统时效性–新闻阅读和个性化推荐
1 关键词 textrank 根据两个词互相投票来确定一个词的重要性 pagerank’财经’，’发展’，’银行’/3  来计算平均向量2 投票用词向量夹角余弦来计算3 textrank 

            优点：不管一片文章中两个词的相隔距离，根据相应的词向量，计算相似度4 归类，计算平均向量 与各个类别向量的距离  可以对文章进行归类
### 5 应用方向

```
初始推荐系统
推荐系统知识
    新浪网：猜你喜欢
    网易： 今日点击
新闻T字形标签提取
新闻推荐

总结和拓展
    总结
    自编码网络
```




