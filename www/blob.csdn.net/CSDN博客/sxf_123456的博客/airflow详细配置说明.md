# airflow详细配置说明 - sxf_123456的博客 - CSDN博客
2018年01月23日 17:19:58[sxf_0123](https://me.csdn.net/sxf_123456)阅读数：1176
**[core]**# The home folder for airflow, default is ~/airflowairflow_home=/home/frappe/airflow# The folder where your airflow pipelines live, most likely a# subfolder in a code repositorydags_folder=/home/frappe/airflow/dags# The folder where airflow should store its log files. This locationbase_log_folder=/home/frappe/airflow/logs# Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users# must supply a remote location URL (starting with either 's3://...' or# 'gs://...') and an Airflow connection id that provides access to the storage# location.remote_base_log_folder=remote_log_conn_id=# Use server-side encryption for logs stored in S3encrypt_s3_logs=False# deprecated option for remote log storage, use remote_base_log_folder instead!# s3_log_folder =# The executor class that airflow should use. Choices include# SequentialExecutor, LocalExecutor, CeleryExecutorexecutor=LocalExecutor
#这是airflow最关键的一个配置，它指示了airflow以何种方式来执行任务。它有三个选项：
#SequentialExecutor：表示单进程顺序执行，通常只用于测试
#LocalExecutor：表示多进程本地执行，它用python的多进程库从而达到多进程跑任务的效果。
#CeleryExecutor：表示使用celery作为执行器，只要配置了celery，就可以分布式地多机跑任务，一般用于生产环境。
# The SqlAlchemy connection string to the metadata database.# SqlAlchemy supports many different database engine, more information# their websitesql_alchemy_conn=mysql://airflow:xxx@airflow:3306/airflow# dialect+driver://username:password@host:port/database # The SqlAlchemy pool size is the maximum number of database connections# in the pool.sql_alchemy_pool_size=5# The SqlAlchemy pool recycle is the number of seconds a connection# can be idle in the pool before it is invalidated. This config does# not apply to sqlite.sql_alchemy_pool_recycle=3600# The amount of parallelism as a setting to the executor. This defines# the max number of task instances that should run simultaneously# on this airflow installationparallelism=4 # The number of task instances allowed to run concurrently by the schedulerdag_concurrency=8 # Are DAGs paused by default at creationdags_are_paused_at_creation=True# When not using pools, tasks are run in the "default pool",# whose size is guided by this config elementnon_pooled_task_slot_count=128# The maximum number of active DAG runs per DAGmax_active_runs_per_dag=16# Whether to load the examples that ship with Airflow. It's good to# get started, but you probably want to set this to False in a production# environmentload_examples=False# Where your Airflow plugins are storedplugins_folder=/home/frappe/airflow/plugins# Secret key to save connection passwords in the dbfernet_key=bNyVEIcfk-itUWnbErA9LaVbxH7eOqsOnuA=# Whether to disable pickling dagsdonot_pickle=False# How long before timing out a python file import while filling the DagBagdagbag_import_timeout=30
[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8089
[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default
**[operators]**# The default owner assigned to each new operator, unless# provided explicitly or passed via `default_args`default_owner=Airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0
**[webserver]**# The base url of your website as airflow cannot guess what domain or# cname you are using. This is used in automated emails that# airflow sends to point links to the right web serverbase_url=http://localhost:8080# The ip specified when starting the web serverweb_server_host=0.0.0.0# The port on which to run the web serverweb_server_port=8080# The time the gunicorn webserver waits before timing out on a workerweb_server_worker_timeout=120# Secret key used to run your flask appsecret_key=temporary_key# Number of workers to run the Gunicorn web serverworkers=4# The worker class gunicorn should use. Choices include# sync (default), eventlet, geventworker_class=sync# Expose the configuration file in the web serverexpose_config=true# Set to true to turn on authentication:# http://pythonhosted.org/airflow/installation.html#web-authenticationauthenticate=Trueauth_backend=airflow.contrib.auth.backends.password_auth # Filter the list of dags by owner name (requires authentication to be enabled)filter_by_owner=False**[email]**email_backend=airflow.utils.email.send_email_smtp**[smtp]**# If you want airflow to send emails on retries, failure, and you want to use# the airflow.utils.email.send_email_smtp function, you have to configure an smtp# server heresmtp_host=localhostsmtp_starttls=Truesmtp_ssl=Falsesmtp_user=airflowsmtp_port=25smtp_password=airflowsmtp_mail_from=airflow@airflow.com**[celery]**# This section only applies if you are using the CeleryExecutor in# [core] section above# The app name that will be used by celerycelery_app_name=airflow.executors.celery_executor# The concurrency that will be used when starting workers with the# "airflow worker" command. This defines the number of task instances that# a worker will take, so size up your workers based on the resources on# your worker box and the nature of your tasksceleryd_concurrency=16# When you start an airflow worker, airflow starts a tiny web server# subprocess to serve the workers local log files to the airflow main# web server, who then builds pages and sends them to users. This defines# the port on which the logs are served. It needs to be unused, and open# visible from the main web server to connect into the workers.worker_log_server_port=8793# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally# a sqlalchemy database. Refer to the Celery documentation for more# information.broker_url=sqla+mysql://airflow:airflow@localhost:3306/airflow# dialect+driver://username:password@host:port/database # Another key Celery settingcelery_result_backend=db+mysql://airflow:airflow@localhost:3306/airflow# dialect+driver://username:password@host:port/database # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start# it `airflow flower`. This defines the port that Celery Flower runs onflower_port=5555# Default queue that tasks get assigned to and that worker listen on.default_queue=default**[scheduler]**# Task instances listen for external kill signal (when you clear tasks# from the CLI or the UI), this defines the frequency at which they should# listen (in seconds).job_heartbeat_sec=10# The scheduler constantly tries to trigger new tasks (look at the# scheduler section in the docs for more information). This defines# how often the scheduler should run (in seconds).scheduler_heartbeat_sec=30# Statsd (https://github.com/etsy/statsd) integration settings# statsd_on =  False# statsd_host =  localhost# statsd_port =  8125# statsd_prefix = airflow# The scheduler can run multiple threads in parallel to schedule dags.# This defines how many threads will run. However airflow will never# use more threads than the amount of cpu cores available.max_threads=1**[mesos]**# Mesos master address which MesosExecutor will connect to.master=localhost:5050# The framework name which Airflow scheduler will register itself as on mesosframework_name=Airflow# Number of cpu cores required for running one task instance using# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'# command on a mesos slavetask_cpu=1# Memory in MB required for running one task instance using# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'# command on a mesos slavetask_memory=256# Enable framework checkpointing for mesos# See http://mesos.apache.org/documentation/latest/slave-recovery/checkpoint=False# Failover timeout in milliseconds.# When checkpointing is enabled and this option is set, Mesos waits# until the configured timeout for# the MesosExecutor framework to re-register after a failover. Mesos# shuts down running tasks if the# MesosExecutor framework fails to re-register within this timeframe.# failover_timeout = 604800# Enable framework authentication for mesos# See http://mesos.apache.org/documentation/latest/configuration/authenticate=False# Mesos credentials, if authentication is enabled# default_principal = admin# default_secret = admin
