# 自适应学习速率调整和常用的梯度下降算法 - 墨流觞的博客 - CSDN博客





2018年09月09日 23:51:54[墨氲](https://me.csdn.net/dss_dssssd)阅读数：1710








# chapter 4 Beyond Gradient Desent

## 英语词汇

intracttable  棘手的，难以解决的

hundle 障碍

## 内容

主要讲述几种优化算法。 

接下来探索局部最小值以及由此在训练网络中可能引起的问题。接着探索由于网络模型过深而引起的误差非凸面化，在这种情况下，小批量梯度下降算法失效。接着将探索非凸优化方法是如何克服这种问题的。

### 1. 误差曲面的非凸性

有很多的局部最小值，而神经网络需要找到全局最小值 
![这里写图片描述](https://img-blog.csdn.net/20180909234515322?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Rzc19kc3Nzc2Q=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这个是题外话，了解一下阿偶： 

* 模型唯一性 
![](a.jpg)![这里写图片描述](https://img-blog.csdn.net/20180909234454329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Rzc19kc3Nzc2Q=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

对于一个l层，每层有n个神经元的网络，有$n^{!^l}$中安排参数的方法，最终网络的输出相同。
#### 解决局部最小值的方法：
- 
mini-batch gradient descent

- 
蒙特卡洛方法

使用随机数（或更常见的伪随机数）来解决很多计算问题的方法,很经典的一个问题：利用投针实验求解圆周率$\pi$

- 
二阶方法(second-order methods)


### 2.  学习比率的问题

学习速率调整(learning rate adaptation)

选择正确的学习速率很重要，学习速率过小，收敛速度过慢 

学习速率太大， 直接忽略某些局部最小值

学习速率自适应调整（learning rate adaption）：在训练过程中适当的修改学习速率来达到更好的收敛性

#### 三个最流行的学习速率自调整算法
- 
AdaGrad  

利用每次迭代历史的梯度平方根的和来修改学习速率

$\bigodot$是矩阵的点乘，$\epsilon$是初始学习速率 


$r_0 = 0$


$r_i = r_{i-1} + g_i \bigodot g_i$


$\theta_i = \theta_{i-1} - \frac{\epsilon}{\delta \bigoplus \sqrt{r_i}} \bigodot g$
$\delta \approx 10^{-7}$避免除以0 
**python tensorflow调用代码：**

```python
tf.train.AdagradOptimizer(
  learning_rate,
  initial_accumulator_valuer=0.1,
  use_locking=False,
  name='Adagrad'
)
```

在tensorflow中，$\delta$和initial_accumulator_valuer在一起.
- RMSProp  


$r_i = \rho r_{i-1} + (1 - \rho)g_i \bigodot g_i $

衰退因子$(decay factor)$$\rho$决定对历史梯度的保留程度
**python tensorflow调用代码：**

```python
tf.train.RMSPropOptimizer(
learning_rate,
decay=0.9,
momentum=0.0,
epsilon=1e-10,
use_locking=False,
name='RMSProp'
)
```

通过调整momentum来确定是否使用蒙特卡洛方法

RMSProp是一个非常高效的算法，对于很多的资深从业者而言是默认的选择
- 
Adam 将蒙特卡洛和RMSProp结合起来

```python
tf.train.AdamOptimizer(
learning_rate=0.001,
beta1=0.9,
beta2=0.999,
epsilon=1e-8,
use_locking=False,
name='Adam'
)
```

很先进高效 

默认参数下已经表现的很好了，只需要修改参数learning_rate就好了。

## 总结

最流行的算法：

- 误差曲面非凸 

    1. mini-batch gradient descent 

    2. mini-batch gradient with momentum 
- 学习速率 
- RMSProp
- RMSProp with  momentum
- Adam


主要是好的网络结构可以比训练算法有更好的效果，所有好的网络结构更值得花时间构建









