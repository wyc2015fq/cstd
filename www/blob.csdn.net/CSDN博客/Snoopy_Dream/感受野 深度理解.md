# 感受野 深度理解 - Snoopy_Dream - CSDN博客





2018年11月30日 18:20:35[Snoopy_Dream](https://me.csdn.net/e01528)阅读数：888








知乎是个好东西，深入理解一些理念，靠博客是不行的。

感受野计算和理解的内容参考自：[https://zhuanlan.zhihu.com/p/44106492](https://zhuanlan.zhihu.com/p/44106492) / [https://zhuanlan.zhihu.com/p/40267131](https://zhuanlan.zhihu.com/p/40267131)

后两个卷积的内容参考自： [https://www.zhihu.com/question/54149221](https://www.zhihu.com/question/54149221)

**目录**

[一、卷积后特征图维度的公式](#%E4%B8%80%E3%80%81%E5%8D%B7%E7%A7%AF%E5%90%8E%E7%89%B9%E5%BE%81%E5%9B%BE%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%85%AC%E5%BC%8F)

[二：感受野介绍：](#%E4%BA%8C%EF%BC%9A%E6%84%9F%E5%8F%97%E9%87%8E%E4%BB%8B%E7%BB%8D%EF%BC%9A)

[三、感受野的直观感受 和 作用](#%E4%B8%89%E3%80%81%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E7%9B%B4%E8%A7%82%E6%84%9F%E5%8F%97%C2%A0%E5%92%8C%20%E4%BD%9C%E7%94%A8)

[四、感受野大小计算方式](#%E5%9B%9B%E3%80%81%E6%84%9F%E5%8F%97%E9%87%8E%E5%A4%A7%E5%B0%8F%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F)

[五、从 感受野 分析 典型网络（vgg、resnet、rpn结构）](#%E4%BA%94%E3%80%81%E4%BB%8E%20%E6%84%9F%E5%8F%97%E9%87%8E%20%E5%88%86%E6%9E%90%20%E5%85%B8%E5%9E%8B%E7%BD%91%E7%BB%9C%EF%BC%88vgg%E3%80%81resnet%E3%80%81rpn%E7%BB%93%E6%9E%84%EF%BC%89)

[六、 有效感受野](#%E5%85%AD%E3%80%81%20%E6%9C%89%E6%95%88%E6%84%9F%E5%8F%97%E9%87%8E)

[七、 论文中用法](#%E4%B8%83%E3%80%81%20%E8%AE%BA%E6%96%87%E4%B8%AD%E7%94%A8%E6%B3%95)

### 一、卷积后特征图维度的公式

首先，补充下计算卷积后特征图维度的公式：

> 
N = (*W − F* + 2*P* )/*S*+1 （原图大小-kenal+2pad）/步长 +1

- 输出图片大小为 **N×N**
- 输入图片大小 *W×W*
- Filter大小 *F×F*
- padding： *P*
- 步长 *S*

### **二：感受野**介绍：

**stride ：** 网络中的每一个层有一个strides，该strides是之前所有层stride的乘积，即：

![stride\left( i\right) =stride(1)\ast stride(2)\ast ...\ast stride\left( i-1 \right)](http://www.zhihu.com/equation?tex=stride%5Cleft%28+i%5Cright%29+%3Dstride%281%29%5Cast+stride%282%29%5Cast+...%5Cast+stride%5Cleft%28+i-1+%5Cright%29+)

**感受野：**cnn中的特征图上一点，相对于原图的大小。

### 三、感受野的直观感受 和 作用

下图（该图为了方便，将二维简化为一维），这个三层的神经卷积神经网络，每一层卷积核的 ![kernel\_size=3](http://www.zhihu.com/equation?tex=kernel%5C_size%3D3), ![stride=1](http://www.zhihu.com/equation?tex=stride%3D1) ，那么最上层特征所对应的感受野就为如图所示的7x7。*（看箭头的时候从上往下反着看）

![](https://pic2.zhimg.com/80/v2-0d5f2bd822621d3689a105cc17e75111_hd.jpg)

**作用：**这个重要的思想是在**VGG的主要contribution（** 3 个 3 x 3 的卷积层的叠加**可以替代7*7的卷积**，而这样的设计不仅可以大幅度的减少参数，其本身带有多次正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。**）**
- 小卷积可以代替大卷积层
- 密集预测task要求输出像素的感受野足够的大，确保做出决策时没有忽略重要信息，一般也是越深越好
- 一般task要求感受野越大越好，如图像分类中最后卷积层的感受野要大于输入图像，网络深度越深感受野越大性能越好
- 目标检测task中设置anchor要严格对应感受野，anchor太大或偏离感受野都会严重影响检测性能

用这种等效的思想从感受野上看：两个堆叠的conv3x3感受野可以等于一个conv5x5，推广之，**一个多层卷积构成的FCN感受野等于一个conv r*r，即一个卷积核很大的单层卷积，其kernelsize=r，padding=P，stride=S**。cnn从gap划开，*看成是FCN *(全卷积网络)*+**MLP (多层感知机)**，前面提取特征后面加个分类器，可以理解成sobel+svm呗~CNN是不是就没那么神秘了~*）

再来一个二维的图：

![](https://pic4.zhimg.com/80/v2-ef5cac4e1ef50a396f2fccad2d90a623_hd.jpg)

这里面有两个 3 x 3的的卷积，可以替代一个5*5的卷积。

### **四、感受野大小计算方式**

![RF_{l+1}=RF_{l}+(kernel\_size_{l+1}-1)\times feature\_stride_{l}](http://www.zhihu.com/equation?tex=RF_%7Bl%2B1%7D%3DRF_%7Bl%7D%2B%28kernel%5C_size_%7Bl%2B1%7D-1%29%5Ctimes+feature%5C_stride_%7Bl%7D)

其中 ![RF](http://www.zhihu.com/equation?tex=RF) 表示特征感受野大小， ![l](http://www.zhihu.com/equation?tex=l) 表示层数， ![feature\_stride_l=\prod_{i=1}^{l}stride_i](http://www.zhihu.com/equation?tex=feature%5C_stride_l%3D%5Cprod_%7Bi%3D1%7D%5E%7Bl%7Dstride_i),

输入层的： ![l=0](http://www.zhihu.com/equation?tex=l%3D0)， ![RF_{0}=1](http://www.zhihu.com/equation?tex=RF_%7B0%7D%3D1) , ![feature\_stride_0=1](http://www.zhihu.com/equation?tex=feature%5C_stride_0%3D1) 。
- 第一层特征，感受野为3

![RF_{1}=RF_{0}+(kernel\_size_{1}-1)\times feature\_stride_{0}=1+(3-1)\times 1=3](http://www.zhihu.com/equation?tex=RF_%7B1%7D%3DRF_%7B0%7D%2B%28kernel%5C_size_%7B1%7D-1%29%5Ctimes+feature%5C_stride_%7B0%7D%3D1%2B%283-1%29%5Ctimes+1%3D3)

![](https://pic3.zhimg.com/80/v2-59143068c39a7e0e245d8560e9d38ab6_hd.jpg)

第1层感受野[1]
- 第二层特征，感受野为5

![RF_{2}=RF_{1}+(kernel\_size_{2}-1)\times feature\_stride_{1}=3+(3-1)\times 1=5](http://www.zhihu.com/equation?tex=RF_%7B2%7D%3DRF_%7B1%7D%2B%28kernel%5C_size_%7B2%7D-1%29%5Ctimes+feature%5C_stride_%7B1%7D%3D3%2B%283-1%29%5Ctimes+1%3D5)

![](https://pic1.zhimg.com/80/v2-4763bc767d0296e74a6827d457eb1360_hd.jpg)

第2层感受野[1]
- 第三层特征，感受野为7

![RF_{3}=RF_{2}+(kernel\_size_{3}-1)\times feature\_stride_{2}=5+(3-1)\times 1=7](http://www.zhihu.com/equation?tex=RF_%7B3%7D%3DRF_%7B2%7D%2B%28kernel%5C_size_%7B3%7D-1%29%5Ctimes+feature%5C_stride_%7B2%7D%3D5%2B%283-1%29%5Ctimes+1%3D7)

![](https://pic2.zhimg.com/80/v2-0d5f2bd822621d3689a105cc17e75111_hd.jpg)

第3层感受野[1]

如果有dilated conv的话，计算公式为

![RF_{l+1}=RF_{l}+(kernel\_size_{l+1}-1)\times feature\_stride_{l}\times dilation_{l+1}](http://www.zhihu.com/equation?tex=RF_%7Bl%2B1%7D%3DRF_%7Bl%7D%2B%28kernel%5C_size_%7Bl%2B1%7D-1%29%5Ctimes+feature%5C_stride_%7Bl%7D%5Ctimes+dilation_%7Bl%2B1%7D)

### 五、从 感受野 分析 典型网络（vgg、resnet、rpn结构）

计算Faster R-CNN（vgg16）中conv5-3+RPN的感受野，RPN的结构是一个conv3x3+两个并列conv1x1：

![](https://img-blog.csdnimg.cn/20181130174251904.png)

> 声明：**输入图片**224*224，**r**表示感受野 ，**S**表示stride， **P**表示padding， **P的计算**可以通过反推** N = (*W − F* + 2*P* )/*S*+1 **

> r = 1 +2 +2 )x2 +2+2 )x2 +2+2+2 )x2 +2+2+2 )x2 +2 = 156

S = 2x2x2x2 = 16

P = ((14-1)x16-224+228)/2 = 106

分布方式为在paddding=106的输入224x224图像上，大小为156x156的正方形感受野区域以stride=16平铺。

接下来是Faster R-CNN+++和R-FCN等采用的重要backbone的**ResNet**，常见ResNet-50和ResNet-101，结构特点是block由conv1x1+conv3x3+conv1x1构成，下采样block中conv3x3 s2影响感受野。先计算ResNet-50在conv4-6 + RPN的感受野 (为了写起来简单堆叠卷积层合并在一起)：

> r = 1 +2 +2x5 )x2+1 +2x3 )x2+1 +2x3 )x2+1 )x2+5 = 299

S = 2x2x2x2 = 16

P = ((14-1)x16-224+299)/2 = 141.5

P不是整数，表示conv7x7 s2卷积有多余部分。分布方式为在paddding=142的输入224x224图像上，大小为299x299的正方形感受野区域以stride=16平铺。

ResNet-101在conv4-23 + RPN的感受野：

> r = 1 +2 +2x22 )x2+1 +2x3 )x2+1 +2x3 )x2+1 )x2+5 = 843

S = 2x2x2x2 = 16

P = ((14-1)x16-224+843)/2 = 413.5

分布方式为在paddding=414的输入224x224图像上，大小为843x843的正方形感受野区域以stride=16平铺。

以上结果都可以反推验证，并且与后一种方法结果一致。从以上计算可以发现一些的结论：
- 步进1的卷积层**线性增加**感受野，深度网络可以通过堆叠多层卷积增加感受野
- 步进2的下采样层**乘性增加**感受野，但受限于输入分辨率不能随意增加
- 步进1的卷积层加在网络后面位置，会比加在前面位置增加更多感受野，如stage4加卷积层比stage3的感受野增加更多
- **深度**CNN的**感受野往往是大于输入分辨率**的，如上面ResNet-101的843比输入分辨率大3.7倍
- 深度CNN为**保持分辨率每个conv都要加padding**，所以等效到输入图像的padding非常大

### 六、 有效感受野

NIPS 2016论文Understanding the Effective Receptive Field in Deep Convolutional Neural Networks提出了**有效感受野（Effective Receptive Field, ERF）**理论，论文发现并不是感受野内所有像素对输出向量的贡献相同，在很多情况下感受野区域内像素的影响分布是高斯，有效感受野仅占理论感受野的一部分，且高斯分布从中心到边缘快速衰减，下图第二个是训练后CNN的典型有效感受野。

![](https://img-blog.csdnimg.cn/2018113018182612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2UwMTUyOA==,size_16,color_FFFFFF,t_70)

![](https://pic3.zhimg.com/80/v2-b71bef501d068876c76212ae1b396552_hd.jpg)![](https://pic3.zhimg.com/80/v2-b71bef501d068876c76212ae1b396552_hd.jpg)![](https://pic3.zhimg.com/80/v2-b71bef501d068876c76212ae1b396552_hd.jpg)![](https://img-blog.csdnimg.cn/20181130181801315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2UwMTUyOA==,size_16,color_FFFFFF,t_70)

下面我从直观上解释一下有效感受野背后的原因。以一个两层 ![kernel\_size=3](http://www.zhihu.com/equation?tex=kernel%5C_size%3D3)， ![stride=1](http://www.zhihu.com/equation?tex=stride%3D1) 的网络为例，该网络的理论感受野为5，计算流程可以参加下图。其中 ![x](http://www.zhihu.com/equation?tex=x) 为输入， ![w](http://www.zhihu.com/equation?tex=w) 为卷积权重， ![o](http://www.zhihu.com/equation?tex=o) 为经过卷积后的输出特征。

很容易可以发现， ![x_{1,1}](http://www.zhihu.com/equation?tex=x_%7B1%2C1%7D) 只影响第一层feature map中的 ![o_{1,1}^1](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1) ；而 ![x_{3,3}](http://www.zhihu.com/equation?tex=x_%7B3%2C3%7D) 会影响第一层feature map中的所有特征，即 ![o_{1,1}^1,o_{1,2}^1,o_{1,3}^1,o_{2,1}^1,o_{2,2}^1,o_{2,3}^1,o_{3,1}^1,o_{3,2}^1,o_{3,3}^1](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1%2Co_%7B1%2C2%7D%5E1%2Co_%7B1%2C3%7D%5E1%2Co_%7B2%2C1%7D%5E1%2Co_%7B2%2C2%7D%5E1%2Co_%7B2%2C3%7D%5E1%2Co_%7B3%2C1%7D%5E1%2Co_%7B3%2C2%7D%5E1%2Co_%7B3%2C3%7D%5E1) 。

第一层的输出全部会影响第二层的 ![o_{1,1}^2](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2) 。

于是 ![x_{1,1}](http://www.zhihu.com/equation?tex=x_%7B1%2C1%7D) 只能通过 ![o_{1,1}^1](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1) 来影响 ![o_{1,1}^2](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2) ；而 ![x_{3,3}](http://www.zhihu.com/equation?tex=x_%7B3%2C3%7D) 能通过 ![o_{1,1}^1,o_{1,2}^1,o_{1,3}^1,o_{2,1}^1,o_{2,2}^1,o_{2,3}^1,o_{3,1}^1,o_{3,2}^1,o_{3,3}^1](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1%2Co_%7B1%2C2%7D%5E1%2Co_%7B1%2C3%7D%5E1%2Co_%7B2%2C1%7D%5E1%2Co_%7B2%2C2%7D%5E1%2Co_%7B2%2C3%7D%5E1%2Co_%7B3%2C1%7D%5E1%2Co_%7B3%2C2%7D%5E1%2Co_%7B3%2C3%7D%5E1) 来影响 ![o_{1,1}^2](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2) 。显而易见，虽然 ![x_{1,1}](http://www.zhihu.com/equation?tex=x_%7B1%2C1%7D) 和 ![x_{3,3}](http://www.zhihu.com/equation?tex=x_%7B3%2C3%7D) 都位于第二层特征感受野内，但是二者对最后的特征 ![o_{1,1}^2](http://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2) 的影响却大不相同，输入中越靠感受野中间的元素对特征的贡献越大。

![](https://pic2.zhimg.com/80/v2-0cc04fa062844f53ea707c67094c9645_hd.jpg)

![](https://pic3.zhimg.com/80/v2-b71bef501d068876c76212ae1b396552_hd.jpg)

### 七、 论文中用法

ECCV2016的**SSD**论文指出**更好的anchar的设置应该依据感受野：**

![](https://pic1.zhimg.com/80/v2-cb061622a0d69efab426a8c523e3c300_hd.jpg)

ICCV2017的SFD依据有效感受野设置anchor并使其密集化，这一做法在RefineNet中延续：

![](https://pic1.zhimg.com/80/v2-4754fdac321b3d83216abecb01930718_hd.jpg)![](https://img-blog.csdnimg.cn/20181130181958939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2UwMTUyOA==,size_16,color_FFFFFF,t_70)

DeepLab提出Atrous conv (带孔卷积)高效控制感受野，而不增加参数数量和计算量：


- 分类

Xudong Cao写过一篇叫《A practical theory for designing very deep convolutional neural networks》的technical report，里面讲设计基于深度卷积神经网络的图像分类器时，为了保证得到不错的效果，需要满足两个条件：

> Firstly, for each convolutional layer, its capacity of learning more complex patterns should be guaranteed; Secondly,** the receptive field of the top most layer should be no larger than the image region**.

其中第二个条件就是对卷积神经网络最高层网络特征感受野大小的限制。
- 目标检测

现在流行的目标检测网络大部分都是基于anchor的，比如SSD系列，v2以后的yolo，还有faster rcnn系列。

基于anchor的目标检测网络会预设一组大小不同的anchor，比如32x32、64x64、128x128、256x256，这么多anchor，我们应该放置在哪几层比较合适呢？这个时候感受野的大小是一个重要的考虑因素。

放置anchor层的特征感受野应该跟anchor大小相匹配，感受野比anchor大太多不好，小太多也不好。如果感受野比anchor小很多，就好比只给你一只脚，让你说出这是什么鸟一样。如果感受野比anchor大很多，则好比给你一张世界地图，让你指出故宫在哪儿一样。

《S3FD: Single Shot Scale-invariant Face Detector》这篇人脸检测器论文就是依据感受野来设计anchor的大小的一个例子，文中的原话是

> we design anchor scales based on **the effective receptive field**

《FaceBoxes: A CPU Real-time Face Detector with High Accuracy》这篇论文在设计多尺度anchor的时候，依据同样是感受野，文章的一个贡献为

> We introduce the Multiple Scale Convolutional Layers

(MSCL) to handle various scales of face via **enriching**
**receptive fields and discretizing anchors over layers**

![](https://pic1.zhimg.com/80/v2-a9f4726e9bc6ac7fcf17ca8c6704b35c_hd.jpg)

![](https://pic1.zhimg.com/80/v2-8e009501a78f798ff668b81569c5fec4_hd.jpg)







