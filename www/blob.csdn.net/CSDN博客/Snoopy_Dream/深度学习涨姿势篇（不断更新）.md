# 深度学习涨姿势篇（不断更新） - Snoopy_Dream - CSDN博客





2019年04月08日 17:40:53[Snoopy_Dream](https://me.csdn.net/e01528)阅读数：89








**目录**

[1. 非线性激活层到底是个什么样的东西？](#)

[2. 使用激活层的原则：](#%E4%BD%BF%E7%94%A8%E6%BF%80%E6%B4%BB%E5%B1%82%E7%9A%84%E5%8E%9F%E5%88%99%EF%BC%9A)

[3. ResNet之所以work的本质原因](#ResNet%E4%B9%8B%E6%89%80%E4%BB%A5work%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8E%9F%E5%9B%A0)

### **1. 非线性激活层到底是个什么样的东西？**

其实，对于一个数据，利用非线性激活层对其进行激活，其实是从该数据的信息中提取出其潜在的稀疏性（降维，从原先M->N），但是这种提取的结果是否正确，就要分情况讨论了。维度低的数据其实就是这么一种情况：其信息的冗余度高的可能性本来就低，如果强行对其进行非线性激活（维度压缩），则很有可能丢失掉有用信息，甚至丢失掉全部信息（输出为全0）。



### 2. 使用激活层的原则：
- 
对**含有冗余信息的数据使用非线性激活**（如ReLU），对**不含冗余信息的数据使用线性激活**（如一些线性变换）。

- 
两种类型的激活交替灵活使用，以同时兼顾非线性和信息的完整性。

- 
由于冗余信息和非冗余信息所携带的有用信息是一样多的，因此在设计网络时，对内存消耗大的结构最好是用在非冗余信息上。




### 3. ResNet之所以work的本质原因

ResNet本质上就干了一件事：**降低数据中信息的冗余度。**

具体说来，就是对**非冗余信息采用了线性激活**（通过skip connection获得无冗余的identity部分），然后**对冗余信息采用了非线性激活**（通过ReLU对identity之外的其余部分进行信息提取/过滤，提取出的有用信息即是残差）。

其中，**提取identity这一步，就是ResNet思想的核心。**

从**特征复用**的观点来看，提取identity部分，可以让网络不用再去学习一个identity mapping，而是直接学习residual。这就轻松愉快多了：站在巨人的肩膀上，做一点微小的工作什么的...

既然说了ResNet解决的痛点，也顺便多说几句它带来的好处：
- 
**由于identity之外的其余部分的信息冗余度较高，因此在对其使用ReLU进行非线性激活时，丢失的有用信息也会较少，ReLU层输出为0的可能性也会较低。这就降低了在反向传播时ReLU的梯度消失的概率，从而便于网络的加深，以大大地发挥深度网络的潜能。**

- 
**特征复用能加快模型的学习速度，因为参数的优化收敛得快（从identity的基础上直接学习残差，总比从头学习全部来得快）。**


最后是两个小tips：
- 
如果一个信息可以完整地流过一个非线性激活层，则这个非线性激活层对于这个信息而言，相当于仅仅作了一个线性激活。

- 
**解决由非线性激活导致的反向传播梯度消失的窍门，就是要提高进行非线性激活的信息的冗余度。**


摘自：[对ResNet本质的一些思考](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247488338&idx=2&sn=cd9e46a794644b3bfe972648bfcbfa11&chksm=f9a261ddced5e8cb28c423f58f34076264e52bf8bf51f10ffc28d9e0521798bd30b36d76bb40&mpshare=1&scene=23&srcid=#rd)





