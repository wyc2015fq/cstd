# 最大最小爬山算法的一些总结 - ibelieve8013的博客 - CSDN博客





2018年04月02日 19:10:33[ibelieve8013](https://me.csdn.net/ibelieve8013)阅读数：1737








据说MMHC是现行的比较成功的一种混合贝叶斯结构学习算法，其主要思想是：先建立一个贝叶斯网络的骨架，再通过贪心算法确定最终结构。那么要搞懂的是以下：

1.框架是如何建立的

2.贪心算法是怎么运行的

3.最大最小体现在哪里？

4.为什么会优于一般的算法？

通过MMHC算法的论文《The max-min hill-climbing Bayesiannetwork

structure learning algorithm》，对以上几个问题，总结一下：

1.贝叶斯框架是通过MMPC算法确定的，这是一种局部挖掘算法。第二步是通过贝叶斯评分，利用爬山搜索确定方位。



MMPC：

![](https://img-blog.csdn.net/20180402190906882?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2liZWxpZXZlODAxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


对以上算法的解释：以上是建立的贝叶斯网络框架，主要过程分两步走。

第一步：前向，填充目标变量T的父子节点表CPC，用最大最小启发式方法，最大最小的意思是，在建立CPC表格时，使得在给定CPC表时，待选节点与目标变量最小联系最大化，于是得到初步CPC表。

第二步，后向，在CPC中，给定其中的变量S，若目标变量T与CPC表中元素X是独立的，则去除X（证明非父子关系）。



建立了CPC表后，然后该算法是利用了爬山搜索算法，在缩小了范围的搜索空间里进行搜索。但是缺点是仍然有陷入局部最优的可能。对于此，论文《基于MMHC算法的贝叶斯网络结构学习算法研究》进行了优化，使用了模拟退火，随机重启以及禁忌搜索进行改进，发现效果有所提升。

MMHC的优点：由于是混合式学习方法，很显然，能够集两种典型算法的优点于一身（基于约束的方法，基于搜索评分的方法）。基于约束的方法的缺点是当面临大量的节点是，付出的运算代价很大。而搜索评分的缺点是搜索的空间也是巨大的。当两种方法融合在一起后，建立的基本的联系，然后缩小了搜索的范围，自然比以前的方法更好。问题来了：混合式搜索比较好，有没有其他的混合式方法？或者就在此算法上，是否能够进行进一步的改进，（除了《基于MMHC算法的贝叶斯网络结构学习算法研究》进行的改进）。

一般认为MMHC算法是一种比较成功的结构学习算法，那么在此基础上进行增量的方法呢？进行增量学习的算法，我记得有一篇文章就是提出的iMMHC算法，这个增量算法在下一篇文章中进行总结。

一些思考：其实一直以来都是在找一个合适的增量学习算法，如何减少时间运算，节约存储空间。但是增量学习肯定是基于一般的结构学习的，然后考虑的是新数据增加时如何进行学习。那么考虑哪些算法是可以进行拓展的，或者哪种拓展是最好的。即使有iMMHC,在它的基础上能否进行改进？？？或者，能否利用新的一些前沿的方法，进行改进和迁移，应用在贝叶斯网络学习上？？



