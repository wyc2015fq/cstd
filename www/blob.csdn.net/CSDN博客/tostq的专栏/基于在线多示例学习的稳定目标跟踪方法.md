# 基于在线多示例学习的稳定目标跟踪方法 - tostq的专栏 - CSDN博客





2016年02月22日 21:20:56[tostq](https://me.csdn.net/tostq)阅读数：2832








目标跟踪问题一直以来都是图像处理的领域的热门话题，近年来，基于在线多示例学习的目标跟踪方法倍受关注（主要参考论文*Robust Object Tracking with Online Multiple Instance Learning，PAMI，2011*），这是一种基于检测的目标跟踪方法（track by dection）。常见的目标跟踪系统主要包含了三个方面的内容：（1）目标特征建模（appearance
 model）（2）运动建模（motion model）（3）搜索目标策略（search strategy）。这种方法主要是考虑上面三种方向的第一方面问题。其主要在传统的目标跟踪方法做了两个方面的改进：

（1）对目标进行了多示例的建模[可参考 *Multiple Instance Boosting for Object Detection*]

（2）在分类时，采用了AdaBoost的方法[可参考*Real-Time Tracking via Online Boosting*]

## **一、传统的目标跟踪方法**

（1）目标建模（appearance model），最简单的形式是一个图像块

（2）在目标周围应用模型，确定其概率分布

（3）根据其概率分布应用分类器（classifier）

本文主要是在目标建模时应用了多示例的方法，而分类器上使用了AdaBoost

### **二、多示例学习（Multiple Instance Learning）**

    多示例就是将一个目标用多个示例（部分）来表示，在本文中指得就是一个图像块表示的特征同时包含了该图像块邻域的多个图像块，这么做的原因是由于在目标图像块的检测中，除了目标图像块最中心的那点被视为最正确外，其实该点附近的其他点也可以被检测为正确的目标，特别是在目标发生形变时，单纯检测概率分布图（probability
 map）中最高的那点（有时还可能不至一点），并不能得到较好的检测结果，我们还需要考虑该点附近其他点，这就是多示例建模，即不单纯考虑一个图像块，还是将该图像块附近的其他图像块视为一个整体包来考虑（Bag），下图说明了这个过程：

![](https://img-blog.csdn.net/20160222210804087?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


单示例考虑单个图像块x，通过分类器，获得分类结果y（y=0,1）

而多示例考虑一个图像块包![](https://img-blog.csdn.net/20160222211118307?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)，同样通过分类器，获得分类结果y（y=0,1），只要该图像块包内有一个图像块x（也称为某示例）的分类结果为1，则包的结果也为1，即：

![](https://img-blog.csdn.net/20160222211147042?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


我们需要通过训练样本![](https://img-blog.csdn.net/20160222211228620?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)，获得分类器，最优结果即最大化正确分类率，即最大化下式：

![](https://img-blog.csdn.net/20160222211215169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


![](https://img-blog.csdn.net/20160222211241198?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


通过上式，我们可以分析出，如果

1. 每个图像块通过分类器可以计算其是否目标，如果有一个图像块（示例）被检测为目标，则将整个包（bag）视为目标（y=1），否则（y=0）

2. 每个图像块通过分类器可以计算其正确检测概率，而综合各图像块（示例）概率求出整体包的正确检测概率。

3. 最大化正确检测概率（这里的方法很，可以用最大似然估计）

这里需要估计的参数主要就是分类器H：

![](https://img-blog.csdn.net/20160222211321589?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)![](https://img-blog.csdn.net/20160222211333187?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

本文采用的分类方法是AdaBoost方法

### **三、在线AdaBoost方法**

AdaBoost主要原理是通过综合多个弱分类器h得到一个强分类器H

![](https://img-blog.csdn.net/20160222211417808?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


本文将Haar形状的特征子f视为一个弱分类器如下（f(x)表示图像块对该特征子的响应值），表示该特征子f能否有效区分目标与非目标

![](https://img-blog.csdn.net/20160222211434532?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


特征子对于目标的概率建模为![](https://img-blog.csdn.net/20160222211502970?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)，其在线更新公式为：

![](https://img-blog.csdn.net/20160222211451918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


整个在线学习过程如下：

![](https://img-blog.csdn.net/20160222211525246?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


### **自己英文原文的一些笔记：**

*The problem of object tracking depend on several factors, such as the amount of prior knowledge about the target object and the number and type of parameters being tracked (e.g., location, scale, detailed contour).*

*A typical tracking system consists of three components: 1) an appearance model, 2) a motion model and 3) a search strategy for finding the most likely location in the current frame [ cited by Object Tracking: A Survey]*

 This paper addressed the problem of tracking an object only using its location in the first frame of the video.  The method in this paper used one positive bag consisting of several
 image patches to update a MIL classifier.

This class of tracking method is called "tracking by detection", the method needs a classifier to separate the object from background. The design of appearance models is whether to
 model only the object or both the object and background. A major challenge is how to choose positive and negative examples.

Tracking with online MIL

(1) the OverView of tracking system

![](https://img-blog.csdn.net/20160222211707285?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


Once the tracker location is updated, we proceed to update the appearance model, this paper corp out a set of patches ![](https://img-blog.csdn.net/20160222211727278?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) and
 label this bag postive, and the negative bag is ![](https://img-blog.csdn.net/20160222211749810?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

(2) Multiple Instance learning


a bag is considered positive if it contains at least one positive instance,and every instance in a negative bag is negative. The equation above has the desired property that if one of the instances in a bag has a high probability, the bag probability will be high as well. 


![](https://img-blog.csdn.net/20160222211839279?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

(3) Online Multiple Instance Boosting (AdaBoost)

![](https://img-blog.csdn.net/20160222211857638?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)






