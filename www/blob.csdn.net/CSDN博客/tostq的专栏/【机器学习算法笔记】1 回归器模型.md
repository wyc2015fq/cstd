# 【机器学习算法笔记】1. 回归器模型 - tostq的专栏 - CSDN博客





2017年03月28日 19:15:52[tostq](https://me.csdn.net/tostq)阅读数：2825
所属专栏：[机器学习算法笔记](https://blog.csdn.net/column/details/15045.html)









# 【机器学习算法笔记】1. 回归器模型

回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。常见的回归算法包括：最小二乘法（线性回归），逻辑回归，逐步式回归，多元自适应回归样条等。

## 1.1 线性回归器

所谓线性回归：对于一组输入值X=[x1,x2,…,xn]，存在输出y，为了代表输入与输出的函数关系，假定输出估计 
![这里写图片描述](https://img-blog.csdn.net/20170328190153162?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

线性回归器所要求的问题： 
![这里写图片描述](https://img-blog.csdn.net/20170328190206974?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

一元线性回归的基本假设有 

1、随机误差项是一个服从相同正态分布的彼此不相关随机变量，其期望值为0；  

2、解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立； 

3、解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵； 

回归参数的最小二乘估计是一致最小方差无偏估计。
### 1.1.1 最大后验估计（MAP）

![这里写图片描述](https://img-blog.csdn.net/20170328190315366?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20170328190330507?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

上述建模所要达到的目的：给定参数向量w，使得观测概率最大！ 

所以我们需要建立观测密度的似然函数： 
![这里写图片描述](https://img-blog.csdn.net/20170328190356569?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

然后求对W的最大似然估计（ML）： 
![这里写图片描述](https://img-blog.csdn.net/20170328190412962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

而最大后验估计器（MAP）考虑到了W的后验概率，采用了参数向量所有的可能信息，并不忽略w的先验信息 
![这里写图片描述](https://img-blog.csdn.net/20170328190438290?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](https://img-blog.csdn.net/20170328190448632?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**高斯情况下的参数估计**
![这里写图片描述](https://img-blog.csdn.net/20170328190502994?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

通过利用正则化（即引入先验知识），来改进最大似然估计器的稳定性，其最大后验估计器是有偏的。
### 1.1.2 最小二乘估计

![这里写图片描述](https://img-blog.csdn.net/20170328190529026?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

可以看出最小二乘估计同高斯估计下的ML是相似的，回归参数的最小二乘估计是一致最小方差无偏估计，同理，为了得到唯一性和稳定性的解，添加了正则项： 
![这里写图片描述](https://img-blog.csdn.net/20170328190542570?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

正则最小二乘估计同MAP是相似的。
## 1.2 感知器

感知器是用于线性可分模式分类的最简单的神经网络模型。 

Rosenblatt感知器如下图所示 
![这里写图片描述](https://img-blog.csdn.net/20170328190553929?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

权值向量的自适应更新： 
![这里写图片描述](https://img-blog.csdn.net/20170328190652747?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 1.3 逻辑回归模型(Logistic Regression, LR)

### 1.3.1逻辑回归含义

Logistic回归与多重线性回归实际上有很多相同之处，最大的区别就在于它们的因变量不同，其他的基本都差不多。正是因为如此，这两种回归可以归于同一个家族，即广义线性模型。这一家族中的模型形式基本上都差不多，不同的就是因变量不同：
- 如果是连续的，就是多重线性回归；
- 如果是二项分布，就是Logistic回归；
- 如果是Poisson分布，就是Poisson回归；
- 如果是负二项分布，就是负二项回归。

Logistic回归主要针对于解决因变量是二分类的问题。Logistic回归的主要用途：
- 寻找危险因素：寻找某一疾病的危险因素等；
- 预测：根据模型，预测在不同的自变量情况下，发生某病或某种情况的概率有多大；
- 判别：实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。这里的因变量就是是否胃癌，即“是”或“否”，自变量就可以包括很多了，例如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量既可以是连续的，也可以是分类的。

### 1.3.2 逻辑回归模型

逻辑回归的模型是一个非线性模型，但是它本质上又是一个线性回归模型，又或者说是感知器模型。其相当于在普通线性模型输出后，再增加了一个sigmoid函数，又称逻辑回归函数。又或者相当于在感知器模型的sign函数用sigmoid函数来代替。 

Logistic regression是针对于二分类问题的，所学习的系统的方程为，这里的h指的是分类正确概率： 
![这里写图片描述](https://img-blog.csdn.net/20170328190925203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其对应的损失函数为： 
![这里写图片描述](https://img-blog.csdn.net/20170328190935243?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

添加正则项后的目标函数： 
![这里写图片描述](https://img-blog.csdn.net/20170328190946071?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
### 1.3.2 逻辑回归解释
- 从最小熵上考虑 

函数h(θ)的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为： 
![这里写图片描述](https://img-blog.csdn.net/20170328191008642?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)- 从最大似然角度上考虑 
![这里写图片描述](https://img-blog.csdn.net/20170328191032728?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)- 求解方法 

仍然是梯度下降法： 
![这里写图片描述](https://img-blog.csdn.net/20170328191053072?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 1.4 Softmax回归模型

上节可以看出逻辑回归是针对于二分类问题的，如果现在的假设是多分类问题，比如说总共有k个类别。这个时候就需要softmax回归，其是逻辑回归在多分类上的推广。 

在softmax regression中这时候的系统的方程为： 
![这里写图片描述](https://img-blog.csdn.net/20170328191108987?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中的参数θ不再是列向量，而是一个矩阵，矩阵的每一行可以看做是一个类别所对应分类器的参数，总共有k行。所以矩阵θ可以写成下面的形式： 
![这里写图片描述](https://img-blog.csdn.net/20170328191121002?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

此时，系统损失函数的方程为： 
![这里写图片描述](https://img-blog.csdn.net/20170328191131300?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中的1{.}是一个指示性函数，即当大括号中的值为真时，该函数的结果就为1，否则其结果就为0。softmax regression中损失函数的偏导函数如下所示： 
![这里写图片描述](https://img-blog.csdn.net/20170328191146869?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

公式表示的是损失函数对第j个类别的参数的偏导。所以上面的公式还只是一个类别的偏导公式，我们需要求出所有类别的偏导公式。 

实际上，softmax回归中对参数的最优化求解通常不只一个，每当求得一个优化参数时，如果将这个参数的每一项都减掉同一个数，其得到的损失函数值也是一样的。这说明这个参数不是唯一解。用数学公式证明过程如下所示： 
![这里写图片描述](https://img-blog.csdn.net/20170328191218401?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

那这个到底是什么原因呢？从宏观上可以这么理解，因为此时的损失函数不是严格非凸的，也就是说在局部最小值点附近是一个“平坦”的，所以在这个参数附近的值都是一样的了。那么怎样避免这个问题呢？其实加入正则项就可以解决（比如说，用牛顿法求解时，hession矩阵如果没有加入规则项，就有可能不是可逆的从而导致了刚才的情况，如果加入了规则项后该hession矩阵就不会不可逆了），加入规则项后的损失函数表达式如下： 
![这里写图片描述](https://img-blog.csdn.net/20170328191232119?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

这个时候的偏导函数表达式如下所示： 
![这里写图片描述](https://img-blog.csdn.net/20170328191242817?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG9zdHE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

接下来剩下的问题就是用数学优化的方法来求解了。1.5 HK算法

HK算法思想很朴实,就是在最小均方误差准则下求得权矢量。 

他相对于感知器算法的优点在于，他适用于线性可分和非线性可分得情况，对于线性可分的情况，给出最优权矢量，对于非线性可分得情况，能够判别出来，以退出迭代过程。1.5.1 关于收敛条件的判决：

对于误差矢量：e=x*w-b 

若e>0 则继续迭代 

若e=0 则停止迭代，得到权矢量 

若e<0 则停止迭代，样本是非线性可分得， 

若e有的分量大于0，有的分量小于0 ，则在各分量都变成零，或者停止由负值转变成正值时，停机。1.6 MLP多层感知器

多层感知器的特征： 

1、网络中每个神经元包含了一个可微的非线性激活函数 

2、网络中包含了一个或多个隐藏在输入和输出神经节点之间的层 

3、网络表现出高度的连接性，其强度是由网络的突触权重所决定 

要点： 

反向传播法 

批量学习：多个样例构成一个回合，代价函数是由平均误差能量来定义的，能够精确估计梯度向量。 

在线学习：代价函数是由全体瞬时误差能量，容易执行，不容易陷入局部极值点，存储空间少，速度要慢。 

机器学习中常见误差： 

1、逼近误差：这是给定训练样本的固定大小N时，由训练神经网络或者机器学习所招致的误差 

2、估计误差：用以前没有出现过的数据测试其性能所招致的误差 

3、最优化误差：在给定计算时间下，训练机器的计算精确度所引起的
> 


这篇博文是个人的学习笔记，内容许多来源于网络（包括CSDN、博客园及百度百科等），博主主要做了些微不足道的整理工作。由于在做笔记的时候没有注明来源，所以如果有作者看到上述文字中有自己的原创内容，请私信本人修改或注明来源，非常感谢>_<











