# 正则化与数据先验分布的关系 - YZXnuaa的博客 - CSDN博客
2018年05月07日 11:19:27[YZXnuaa](https://me.csdn.net/YZXnuaa)阅读数：107
作者：Charles Xiao
链接：https://www.zhihu.com/question/23536142/answer/90135994
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
> 先抛给大家一个结论：从贝叶斯的角度来看，正则化等价于对模型参数引入 **先验分布** 。
**一. Linear Regression**
我们先看下最原始的Linear Regression:
![](https://pic4.zhimg.com/50/c24e0befa5ee8a84de2afccef6056a29_hd.jpg)![](https://pic4.zhimg.com/80/c24e0befa5ee8a84de2afccef6056a29_hd.jpg)![\begin{align*} & p(\epsilon^{(i)})  = \frac{1}{\sqrt{2\pi}\delta}exp\left(  -\frac{(\epsilon^{(i)})^2}{2\delta^2} \right)\\ \Rightarrow & p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)\end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D%0A+%26+p%28%5Cepsilon%5E%7B%28i%29%7D%29++%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28++-%5Cfrac%7B%28%5Cepsilon%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Cdelta%5E2%7D+%5Cright%29%5C%5C%0A+%5CRightarrow+%26+p%28y%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28+-%5Cfrac%7B%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Cdelta%5E2%7D++%5Cright%29%0A%5Cend%7Balign%2A%7D)
由最大似然估计(MLE):
![\begin{align*}L(w) & = p(\vec{y}|X;w)\\& = \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\theta)\\& = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)\end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D%0AL%28w%29+%26+%3D+p%28%5Cvec%7By%7D%7CX%3Bw%29%5C%5C%0A%26+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D+p%28y%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29%5C%5C%0A%26+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28+-%5Cfrac%7B%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Cdelta%5E2%7D++%5Cright%29%0A%5Cend%7Balign%2A%7D)
取对数：
![\begin{align*}l(w) & = \log L(w)\\& =\log \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})}{2\delta^2}  \right)\\& = \sum_{i=1}^{m} \log \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)\\& = m \log \frac{1}{\sqrt{2\pi}\delta} - \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2\end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D%0Al%28w%29+%26+%3D+%5Clog+L%28w%29%5C%5C%0A%26+%3D%5Clog+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28+-%5Cfrac%7B%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%7D%7B2%5Cdelta%5E2%7D++%5Cright%29%5C%5C%0A%26+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%5Clog+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28+-%5Cfrac%7B%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Cdelta%5E2%7D++%5Cright%29%5C%5C%0A%26+%3D+m+%5Clog+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7D+-+%5Cfrac%7B1%7D%7B%5Cdelta%5E2%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2%0A%5Cend%7Balign%2A%7D)
即：
![w_{MLE} = \arg \underset{w}{\min} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2](https://www.zhihu.com/equation?tex=w_%7BMLE%7D+%3D+%5Carg+%5Cunderset%7Bw%7D%7B%5Cmin%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2)
这就导出了我们原始的 **least-squares **损失函数，但这是在我们对参数 w 没有加入任何**先验分布**的情况下。在数据维度很高的情况下，我们的模型参数很多，模型复杂度高，容易发生过拟合。
比如我们常说的 “**small n, large p problem**”。（我们一般用 n 表示数据点的个数，用 p 表示变量的个数 ，即数据维度。当 ![p\gg n](https://www.zhihu.com/equation?tex=p%5Cgg+n) 的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话， p 越大，通常会导致模型越复杂，但是反过来 n 又很小，于是就会出现很严重的 overfitting 问题。
![](https://pic4.zhimg.com/50/52de406727100593dafe9d3f696a71e7_hd.jpg)![](https://pic4.zhimg.com/80/52de406727100593dafe9d3f696a71e7_hd.jpg)这个时候，我们可以对参数 w 引入**先验分布**，降低模型复杂度。
**二. Ridge Regression**
> 我们对参数 w 引入协方差为 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 的零均值高斯先验。
![\begin{align*}L(w) & = p(\vec{y}|X;w)p(w)\\& = \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\theta)p(w)\\& = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)\prod_{j=1}^{n}\frac{1}{\sqrt{2\pi\alpha}}exp\left( -\frac{(w^{(j)})^2}{2\alpha}  \right)\\& = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)\frac{1}{\sqrt{2\pi\alpha}}exp\left( -\frac{w^Tw}{2\alpha}  \right)\end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D%0AL%28w%29+%26+%3D+p%28%5Cvec%7By%7D%7CX%3Bw%29p%28w%29%5C%5C%0A%26+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D+p%28y%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29p%28w%29%5C%5C%0A%26+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28+-%5Cfrac%7B%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Cdelta%5E2%7D++%5Cright%29%5Cprod_%7Bj%3D1%7D%5E%7Bn%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Calpha%7D%7Dexp%5Cleft%28+-%5Cfrac%7B%28w%5E%7B%28j%29%7D%29%5E2%7D%7B2%5Calpha%7D++%5Cright%29%5C%5C%0A%26+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7Dexp%5Cleft%28+-%5Cfrac%7B%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Cdelta%5E2%7D++%5Cright%29%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Calpha%7D%7Dexp%5Cleft%28+-%5Cfrac%7Bw%5ETw%7D%7B2%5Calpha%7D++%5Cright%29%0A%5Cend%7Balign%2A%7D)
取对数：
![\begin{align*}l(w) & = \log L(w)\\& = m \log \frac{1}{\sqrt{2\pi}\delta}+ n \log \frac{1}{\sqrt{2\pi\alpha}} - \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2 - \frac{1}{\alpha}\cdot \frac{1}{2} w^Tw\\ \Rightarrow & w_{MAP_{Guassian}} = \arg \underset{w}{\min} \left( \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2 + \frac{1}{\alpha}\cdot \frac{1}{2} w^Tw \right) \end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D%0Al%28w%29+%26+%3D+%5Clog+L%28w%29%5C%5C%0A%26+%3D+m+%5Clog+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Cdelta%7D%2B+n+%5Clog+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Calpha%7D%7D+-+%5Cfrac%7B1%7D%7B%5Cdelta%5E2%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2+-+%5Cfrac%7B1%7D%7B%5Calpha%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+w%5ETw%5C%5C%0A+%5CRightarrow+%26+w_%7BMAP_%7BGuassian%7D%7D+%3D+%5Carg+%5Cunderset%7Bw%7D%7B%5Cmin%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B%5Cdelta%5E2%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2+%2B+%5Cfrac%7B1%7D%7B%5Calpha%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+w%5ETw+%5Cright%29+%0A%5Cend%7Balign%2A%7D)
等价于：
![J_R(w) = \frac{1}{n}\lVert y- w^TX \rVert_2 + \lambda \lVert w \rVert_2](https://www.zhihu.com/equation?tex=J_R%28w%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%5ClVert+y-+w%5ETX+%5CrVert_2+%2B+%5Clambda+%5ClVert+w+%5CrVert_2)
这不就是 **Ridge Regression **吗？
![](https://pic1.zhimg.com/50/c2ca1a4ca98ec92ef60d4f9234d9d6fd_hd.jpg)![](https://pic1.zhimg.com/80/c2ca1a4ca98ec92ef60d4f9234d9d6fd_hd.jpg)
看我们得到的参数，在零附近是不是很密集，老实说 ridge regression 并不具有产生**稀疏解**的能力，也就是说参数并不会真出现很多零。假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予**高的权重**；而L1正则倾向于选择影响较大的参数，而**舍弃**掉影响较小的那个。实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的**计算量**。
> Typically ridge or ℓ2 penalties are **much better** for minimizing prediction error rather than ℓ1 penalties. The reason for this is that when two predictors are highly correlated, ℓ1 regularizer will simply pick one of the two predictors. In contrast, the ℓ2 regularizer will keep both of them and jointly shrink the corresponding coefficients a little bit. Thus, while the ℓ1 penalty can certainly reduce overfitting, you may also experience a loss in predictive power.
那现在我们知道了，对参数引入** 高斯先验** 等价于L2正则化。
**三. LASSO**
上面我们对 w 引入了高斯分布，那么**拉普拉斯分布**(Laplace distribution)呢？
注：LASSO - least absolute shrinkage and selection operator.
![](https://pic2.zhimg.com/50/adc33c20d5770517e2a7fd4f2af74f73_hd.jpg)![](https://pic2.zhimg.com/80/adc33c20d5770517e2a7fd4f2af74f73_hd.jpg)
我们看下拉普拉斯分布长啥样：
![f(x\mid\mu,b) = \frac{1}{2b} \exp \left( -\frac{|x-\mu|}{b} \right)](https://www.zhihu.com/equation?tex=f%28x%5Cmid%5Cmu%2Cb%29+%3D+%5Cfrac%7B1%7D%7B2b%7D+%5Cexp+%5Cleft%28+-%5Cfrac%7B%7Cx-%5Cmu%7C%7D%7Bb%7D+%5Cright%29)
![](https://pic2.zhimg.com/50/01ba9d9e6a605b4c4ea1f0cebe629dae_hd.jpg)![](https://pic2.zhimg.com/80/01ba9d9e6a605b4c4ea1f0cebe629dae_hd.jpg)关于拉普拉斯和正态分布的渊源，大家可以参见 正态分布的前世今生。
重复之前的推导过程我们很容易得到：
![w_{MAP_{Laplace}} = \arg \underset{w}{\min} \left( \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2 + \frac{1}{b^2}\cdot \frac{1}{2} \lVert w \rVert_1 \right)](https://www.zhihu.com/equation?tex=w_%7BMAP_%7BLaplace%7D%7D+%3D+%5Carg+%5Cunderset%7Bw%7D%7B%5Cmin%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B%5Cdelta%5E2%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D+-+w%5ETx%5E%7B%28i%29%7D%29%5E2+%2B+%5Cfrac%7B1%7D%7Bb%5E2%7D%5Ccdot+%5Cfrac%7B1%7D%7B2%7D+%5ClVert+w+%5CrVert_1+%5Cright%29+)
该问题通常被称为 LASSO (least absolute shrinkage and selection operator) 。LASSO 仍然是一个 convex optimization 问题，不具有解析解。它的优良性质是能产生**稀疏性**，导致 w 中许多项变成零。
> 再次总结下，对参数引入** 拉普拉斯先验** 等价于 L1正则化。
**四. Elastic Net**
可能有同学会想，既然 L1和 L2正则各自都有自己的优势，那我们能不能将他们 combine 起来？
可以，事实上，大牛早就这么玩过了。
![](https://pic3.zhimg.com/50/06eb8dbcb11029c1c0fa662c8b8dd8cf_hd.jpg)![](https://pic3.zhimg.com/80/06eb8dbcb11029c1c0fa662c8b8dd8cf_hd.jpg)
因为lasso在解决之前提到的“small n, large p problem”存在一定缺陷。
![](https://pic3.zhimg.com/50/27d9e05cc744ee775a4f308d793edc28_hd.jpg)![](https://pic3.zhimg.com/80/27d9e05cc744ee775a4f308d793edc28_hd.jpg)
这个我们就直接给结果了，不推导了哈。（好麻烦的样子。。。逃）
![\hat{\beta} = \arg \underset{\beta}{\min} \lVert y - X\beta \rVert_2 + \lambda_2 \lVert \beta \rVert_2 + \lambda_1 \lVert \beta \rVert_1](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbeta%7D+%3D+%5Carg+%5Cunderset%7B%5Cbeta%7D%7B%5Cmin%7D+%5ClVert+y+-+X%5Cbeta+%5CrVert_2+%2B+%5Clambda_2+%5ClVert+%5Cbeta+%5CrVert_2+%2B+%5Clambda_1+%5ClVert+%5Cbeta+%5CrVert_1)
![](https://pic2.zhimg.com/50/9cec1b91eac7877800e854bd9adfe6eb_hd.jpg)![](https://pic2.zhimg.com/80/9cec1b91eac7877800e854bd9adfe6eb_hd.jpg)
![](https://pic4.zhimg.com/50/bc6674ab71a7dfe5a403d5c58071d856_hd.jpg)![](https://pic4.zhimg.com/80/bc6674ab71a7dfe5a403d5c58071d856_hd.jpg)
**五. 总结**
> 正则化参数等价于对参数引入 **先验分布**，使得 **模型复杂度** 变小（缩小解空间），对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项 对应后验估计中的 **先验信息**，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。
欢迎访问我博客：[Regularized Regression: A Bayesian point of view](https://link.zhihu.com/?target=http%3A//charleshm.github.io/2016/03/Regularized-Regression/)
吐槽：（博客太冷清）知乎不支持Markdown，把博文弄过来真费劲。。。。
ps: 本文写作过程中参考了知乎和网上的很多文章,同时也加入了自己的一些理解，热烈欢迎广大机器学习爱好者一起讨论问题，互通有无！
  [1]: [Bias 和 Variance](https://link.zhihu.com/?target=http%3A//charlesx.top/2016/03/Bias-Variance/)
  [2]: [《A Few useful things to Know About machine Learning》读后感](https://link.zhihu.com/?target=http%3A//blog.csdn.net/danameng/article/details/21563093)
  [3]: What is the difference between L1 and L2 regularization?
  [4]:  Bayesian Linear Regression
  [5]: Bayesian statistics and regularization
  [6]: [最大似然估计和最小二乘法怎么理解？ - 计算机](https://www.zhihu.com/question/20447622)
  [7]: [Sparsity and Some Basics of L1 Regularization](https://link.zhihu.com/?target=http%3A//freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/)
