# 浅谈流形学习 - YZXnuaa的博客 - CSDN博客
2018年03月22日 13:47:56[YZXnuaa](https://me.csdn.net/YZXnuaa)阅读数：636
作者：暮暮迷了路
链接：https://www.zhihu.com/question/24015486/answer/194284643
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
最高票解释的很学术~我就说个定性而非定量的解释。
流形学习的观点是认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。
举个例子，比如说我们在平面上有个圆，如何表示这个圆呢？如果我们把圆放在一个平面直角坐标系中，那一个圆实际上就是由一堆二维点构成的。
比如一个单位圆： ![(1, 0)](https://www.zhihu.com/equation?tex=%281%2C+0%29) 是一个在圆上的点， ![(0, 1)](https://www.zhihu.com/equation?tex=%280%2C+1%29) 也是一个在圆上的点，但 ![(0,0)](https://www.zhihu.com/equation?tex=%280%2C0%29) 和 ![(2,3)](https://www.zhihu.com/equation?tex=%282%2C3%29) 等等很多点是不在这个圆上的。
显然如果用二维坐标来表示，我们没有办法让这个二维坐标系的所有点都是这个圆上的点。也就是说，用二维坐标来表示这个圆其实是有冗余的。
我们希望，如果能建立某一种描述方法，让这个描述方法所确定的所有点的集合都能在圆上，甚至能连续不间断地表示圆上的点，那就好了！
有没有这种方法呢？对于圆来说，当然有！那就是用极坐标。在极坐标的表示方法下，圆心在原点的圆，只需要一个参数就能确定：半径。
当你连续改变半径的大小，就能产生连续不断的“能被转换成二维坐标表示”的圆。所以说，实际上二维空间中的圆就是一个一维流形。
与之相似的，三维空间中一个球面，用x, y, z三个坐标轴确定时会产生冗余（很多在三维空间中的数据点并不在球面上）。但其实只需要用两个坐标就可以确定了，比如经度和维度。
只要给定任何合法的精度和维度，我们就都能保证这个点肯定在球面上！
那么，流形学习有什么用呢？我现在能想到的主要有两个方面。
先说第一个方面。高维空间有冗余，低维空间没冗余。也就是说，流形可以作为一种数据降维的方式。传统很多降维算法都是用欧氏距离作为评价两个点之间的距离函数的。但是仔细想想这种欧氏距离直觉上并不靠谱。“我们只是看到了三维数据，就要用三维坐标系内的尺度去对事物进行评价？”总觉得有些怪怪的。
举个例子，从北京到上海有多远？你可以找一个地球仪，然后用一把能弯曲的软软的尺子，经过地球仪表面然后测量一下这两个点的距离。
但是如果我用一个直直的线，将地球仪从北京到上海洞穿，测量出一个更短的距离，你一定会觉得我疯了。显然对于“从北京到上海的距离”这件事，我们关注的是把三维地球展开成二维平面，然后测量的地表上的距离，而不是三维空间中球面上两个点的空间直线距离（相信没有人从北京到上海会挖一条直通上海的地道的！
将这个问题推广一些，假如说决策部门打算把一些离得比较近的城市聚成一堆，然后组建个大城市。这时候“远近”这个概念显然是指地表上的距离，因为说空间直线距离并没有什么意义。
而对于降维算法来说，如果使用传统的欧氏距离来作为距离尺度，显然会抛弃“数据的内部特征”。如果测量球面两点距离采用空间欧氏距离，那就会忽略掉“这是个球面”这个信息。
其实用一幅图就都明白了，那就是传说中的瑞士卷（图转自 [浅谈流形学习 " Free Mind](https://link.zhihu.com/?target=http%3A//blog.pluskid.org/%3Fp%3D533) ，侵删）：
![](https://img-blog.csdn.net/20180321200720577)
![](https://pic1.zhimg.com/50/v2-66367b326c102c15b07a8d40f4b861db_hd.jpg)
如果我们观察到的数据是三维的，但其本质是一个二维流形。图上所标注的两个圈圈，在流形（把卷展开）上本距离非常远，但是用三维空间的欧氏距离来计算则它们的距离要近得多。
所以说，流形学习的一个主要应用就是“非线性降维” (参见Wikipedia: [Nonlinear dimensionality reduction](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction))。而非线性降维因为考虑到了流形的问题，所以降维的过程中不但考虑到了距离，更考虑到了生成数据的拓扑结构。
第二个方面，流形能够刻画数据的本质。这方面也是深度学习一直在搞的事情。深度学习主要的特点就是“特征学习”，所谓特征，就是能“表示事物本质的内容”，一般来说特征的维度应该小于数据本身。有一些实证证实，大脑处理数据其实是通过记忆、重现的方式。数据那么多，大脑怎么能一一记住？那就可以只记住“特征”！例如我们直到“人”都是两只眼睛一个鼻子一张嘴。而具体到每一个人则是再这个基本特征之上添加了一些其他的特异性的内容仅此而已。
深度学习一直以来就是在模仿大脑的结构，或者说在模仿大脑对数据的处理能力：从底层感受器输入原始数据，逐步求精得到特征。
所谓的特征，在一定程度上就可以用流形的概念来解释。我们希望我们的模型能够学习到“数据在流行空间中的表示”。如果能做到这一点，则说明我们的模型离“模拟大脑”又前进了一步。
拓展一下，有一个有趣的事情：
我们如何来说明“模型学习到了流形”？前面提到了高维数据其实是由低维流形生成的。如果我们能模拟这个生成过程，再通过对低维流形的微调，应该能得到对应的“有意义且有道理”的高维数据。
