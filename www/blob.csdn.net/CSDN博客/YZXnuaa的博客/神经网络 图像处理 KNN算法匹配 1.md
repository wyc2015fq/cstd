# 神经网络 图像处理 KNN算法匹配 1 - YZXnuaa的博客 - CSDN博客
2018年01月30日 10:32:39[YZXnuaa](https://me.csdn.net/YZXnuaa)阅读数：1189
1、kNN算法又称为k近邻分类(k-nearest neighbor classification)算法。
最简单平凡的分类器也许是那种死记硬背式的分类器，记住所有的训练数据，对于新的数据则直接和训练数据匹配，如果存在相同属性的训练数据，则直接用它的分类来作为新数据的分类。这种方式有一个明显的缺点，那就是很可能无法找到完全匹配的训练记录。
kNN算法则是从训练集中找到和新数据最接近的k条记录，然后根据他们的主要分类来决定新数据的类别。该算法涉及3个主要因素：训练集、距离或相似的衡量、k的大小。
2. 最近邻法：下面通过一个简单的例子说明一下KNN的出现：最近邻法是KNN算法的前身，最近邻法为了判别未知样本的类别，把全部训练样本作为代表点，计算未知样本与所有训练样本的距离，并以最近邻者的类别作为决策未知样本的唯一依据。
![](https://img-blog.csdn.net/20170707140048888?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVUVTVENfQzJfNDAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
由上面的例子说明，绿色的用最近邻算法该分为哪一类，蓝色的，如果是用最近邻算法，但是很有可能蓝色的这个点是错误的，也就是被噪声干扰才这样，实际是红色这一类的，所以可能发现最近算法存在明显的缺陷，那就是对噪声过于敏感，为了解决这个问题，我们可以计算与它距离比较近的多个样本，提高算法鲁棒性，避免个别数据决定命运的情况，所以引进K近邻算法(KNN)。
3. KNN的基本思路：它是最近邻算法的一个延伸，选择未知样本在一定范围内确定个数的K个样本，该K个样本大多数属于哪一类型，就判定未知样本属于哪一类型。这K个样本的选择就是选择最相似（距离的话就是最近的距离）的K个样本。
算法实现步骤：
step1：计算未知样本和每个训练样本的相似度
step2：得到前k个最相似的样本
step3：确定前K个点所在类别的出现频率；
step4：返回前K个点中出现频率最高的类别作为测试数据的预测分类。
4.距离或者相似度的衡量
距离度量：距离度量用于衡量个体在空间上存在的差异，距离越远说明个体间的差异越大。
 欧几里得距离（Euclidean Distance）
欧氏距离是最常见的距离度量，衡量的是多维空间中各个点之间的绝对距离。公式如下：
![](http://dl2.iteye.com/upload/attachment/0098/4314/bb71ff05-fe7f-3045-bfc7-1bfad452af9f.png)
明可夫斯基距离（Minkowski Distance）
明氏距离是欧氏距离的推广，是对多个距离度量公式的概括性的表述。公式如下：
![](http://dl2.iteye.com/upload/attachment/0098/4316/9567216c-ffd4-3d7f-a871-f8685a304cdd.png)
 这里的p值是一个变量，当p=2的时候就得到了上面的欧氏距离。
曼哈顿距离（Manhattan Distance）
曼哈顿距离来源于城市区块距离，是将多个维度上的距离进行求和后的结果，即当上面的明氏距离中p=1时得到的距离度量公式，如下：
![](http://dl2.iteye.com/upload/attachment/0098/4318/87bb1b15-ee66-34ec-890e-f09a3f7aa1ab.png)
切比雪夫距离（Chebyshev Distance）
切比雪夫距离起源于国际象棋中国王的走法，我们知道国际象棋国王每次只能往周围的8格中走一步，那么如果要从棋盘中A格（x1， y1）走到B格（x2， y2）最少需要走几步？扩展到多维空间，其实切比雪夫距离就是当p趋向于无穷大时的明氏距离：
![](http://dl2.iteye.com/upload/attachment/0098/4320/069ae2ec-f9c9-3307-80e9-0d3fb9d0d5dd.png)
 其实上面的曼哈顿距离、欧氏距离和切比雪夫距离都是明可夫斯基距离在特殊条件下的应用。
马哈拉诺比斯距离（Mahalanobis Distance）
既然欧几里得距离无法忽略指标度量的差异，所以在使用欧氏距离之前需要对底层指标进行数据的标准化，而基于各指标维度进行标准化后再使用欧氏距离就衍生出来另外一个距离度量——马哈拉诺比斯距离（Mahalanobis Distance），简称马氏距离。
相似度度量
相似度度量（Similarity），即计算个体间的相似程度，与距离度量相反，相似度度量的值越小，说明个体间相似度越小，差异越大。
向量空间余弦相似度（Cosine Similarity）
余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。公式如下：
![](http://dl2.iteye.com/upload/attachment/0098/4322/112dba47-bb66-3569-870d-5e781ce4eb6f.png)
皮尔森相关系数（Pearson Correlation Coefficient）
即相关分析中的相关系数r，分别对X和Y基于自身总体标准化后计算空间向量的余弦夹角。公式如下：
![](http://dl2.iteye.com/upload/attachment/0098/4324/ce9b83b2-5ebe-3349-802a-79c030527c11.png)
Jaccard相似系数（Jaccard Coefficient）
Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数，公式如下：
![](http://dl2.iteye.com/upload/attachment/0098/4326/7ac17861-751a-3725-85f5-960a9a1c04d0.png)
欧氏距离与余弦相似度
欧氏距离是最常见的距离度量，而余弦相似度则是最常见的相似度度量，很多的距离度量和相似度度量都是基于这两者的变形和衍生，所以下面重点比较下两者在衡量个体差异时实现方式和应用环境上的区别。
借助三维坐标系来看下欧氏距离和余弦相似度的区别：
![](http://dl2.iteye.com/upload/attachment/0098/4328/b098ef1d-7037-3729-9531-fd55c4c9a30b.png)
从图上可以看出距离度量衡量的是空间各点间的绝对距离，跟各个点所在的位置坐标（即个体特征维度的数值）直接相关；而余弦相似度衡量的是空间向量的夹角，更加的是体现在方向上的差异，而不是位置。如果保持A点的位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦相似度cosθ是保持不变的，因为夹角不变，而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦相似度的不同之处。
根据欧氏距离和余弦相似度各自的计算方式和衡量特征，分别适用于不同的数据分析模型：欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异；而余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感）。
高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。
变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。
5. 类别的判定：传统的KNN算法采用的类别判定是投票的方式，哪一类的票数多，未知样本就归为哪一类，但是在实际应用中，如果类别直接的训练数据样本不平衡，也就是每一类的训练样本的个数相差比较大，这就会导致错误的分类，为了解决样本类别不平衡问题，最后分类采用加权的方式，前K个样本的权重就是距离或者相似度的倒数，未知样本就分为权重最大的那一类。投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。实际中采用加权多一些。
6. 优缺点
6.1 优点
简单，易于理解，易于实现，无需估计参数，无需训练
特别适合于多分类问题(multi-modal,对象具有多个类别标签)
6.2 缺点
懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢
可解释性较差，无法给出决策树那样的规则。
7. K值设定为多大？
k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响）
k值通常是采用交叉检验来确定（以k=1为基准）
经验规则：k一般低于训练样本数的平方根
8. 训练样本是否要一视同仁？
在训练集中，有些样本可能是更值得依赖的。
可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。
9. 为了解决KNN算法的核心缺点：计算复杂度高的问题，下面介绍两种方法
9.1 
分组进行搜索，也就是你先把训练样本分为N类，得到这N类的中心，计算未知样本到这N类的中心的距离，选出M（自己觉得选几个）个，再利用这M类数据采用KNN算法得到结果。
9.2 压缩KNN：这个想法就是删除对对分类结果乜有多大影响的样本。步骤如下：
![](https://img-blog.csdn.net/20170707142623772?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVUVTVENfQzJfNDAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
10. KNN算法如何做拟合：
先利用KNN算法得到最近的K个训练样本，可以采用加权平均或者直接利用平均得到结果。例如：以对某一电影评分为例，如果与未知样本的最相似的前两个人的评分为0.5,0.6，那么这个未知样本的评分就是0.55（直接平均）。加权平均的结果就是这么计算0.5*p1+0.6*p2(p1,p2代表两个样本的权重)。
参考：
1.https://wenku.baidu.com/view/94aea4e8d15abe23482f4d5b.html
2.http://blog.csdn.net/jmydream/article/details/8644004
3.http://liyonghui160com.iteye.com/blog/2084557
