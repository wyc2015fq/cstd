# 深度学习的技术困难 - wishchinYang的专栏 - CSDN博客
2015年05月01日 20:43:43[wishchin](https://me.csdn.net/wishchin)阅读数：899
      DL甚至并不是AI的任一方法的一种....
      深度学习甚至不是AI方法的一种，但近期确实取得了非常好的效果，深度学习在理论上的完备性暂且不提，其若想获取更大的成功，更需要解决科学实验本身——深度学习的技术问题。
      原文链接：[深度学习算法的几个难点](http://www.36dsj.com/archives/19932)
![深度学习](http://www.36dsj.com/wp-content/uploads/2014/12/3512-600x419.jpg)
**1、局部最优问题。**
        深度学习算法的目标函数，几乎全都是非凸的。而目前寻找最优解的方法，都是基于梯度下降的。稍微有点背景知识的人都知道，梯度下降方法是解决不了非凸问题的。因此，如果找到最优解，将是深度学习领域，非常值得研究的课题。
       andrew在google的工作，也就是那只猫，其实训练过程是让人很费解的。为了缩短训练时间，项目组采用了分布式训练的方式。采用了1000台计算机，在不同的计算机上存储不同的训练数据，不同的训练服务器通过参数服务器进行参数的交换。训练过程开始后，所有的训练计算机从参数服务器更新当前参数，然后利用当前参数以及本机器上的训练数据，计算得到当前的梯度，通过贪婪式方法，训练到不能再训练为止，然后将参数的更新量提交给服务器，再获取新的参数进行更新。
       在这个过程中，出现了不同机器在同步时间上的一个大问题。具体阐述如下：梯度下降这种方法，在计算梯度的时候，一定要知道当前参数的具体值，梯度是针对某一个具体的参数值才有意义的。但是，由于在这个系统中，计算机非常多，当计算机A从服务器上获得参数值后，完成梯度的计算得到步进量的时候，可能在它提交结果之前，计算机B已经修改了参数服务器上的参数了。也就是说，A所得到的步进量，并不是针对当前的参数值的。
       论文中，作者注意到了这个问题，但是故意不去理会，结果训练结果居然不错。作者的解释是：这是一种歪打正着的现象。
       为什么能够歪打正着呢？有可能是这样的：非凸问题，本来就不是梯度下降法能够解决的。如果不存在同步难题，那么随着训练的深入，结果肯定会收敛到某一个局部最优解上面去。而现在这种同步问题，恰好能够有助于跳出局部最优解。因此最终的训练结果还算不错。
       作者并没有证明，这种方式，对于寻找全局最优一定是有帮助的。对于最终的结果是否一定是经验最优的，也没有证明。因此我感觉，深度学习里面，这种超高维参数的最优结果的寻优，是一个很值得深入研究的问题。它对于最终的效果也确实影响很大。
**2、内存消耗巨大，计算复杂。**
**内存消耗巨大和计算复杂体现在两个方面。（1）训练过程。（2）检测过程。**
       这两个过程的计算复杂，根本原因都是庞大的参数规模造成的。比如google的这个项目，每一个位置都用到了8个模版，每一个像素，这8个模版都是不同的，因此导致最后的模版总数很大，所以训练和检测都很慢。当然，这种模版的设计法，让人不好理解，为什么不同的像素位置，模版完全不同。我还是支持以前的卷积神经网络里面的思想，不同位置的模版都是一样的，但没一个位置，模版数量就远不止8个了。这样的好处是，内存空间中，总的模板数下降了；但缺点是，计算更复杂了。
       因此，如果能够找到一个好的方法，能够有效的较低计算复杂度，将是很有意义的。（比如某个邻域内如果方差极小，其实根本就没必要计算了，直接赋0.）
**3、人脑机理还有很多没用上。**
       深度学习模拟的是人脑的其中一个很小的方面，就是：深度结构，以及稀疏性。
       但事实上，人脑是相当复杂滴。关于视觉注意机制、多分辨率特性、联想、心理暗示等功能，目前根本就没有太多的模拟。所以神经解剖学对于人工智能的影响应该是蛮大的。将来要想掀起机器智能的另一个研究高潮，估计还得继续借鉴神经解剖学。
**4、人为设计模版的可行性。**
       一直在想，为什么第一层用于检测角点和边缘这种简单特征的模版，一定需要通过无监督训练得到，如果人为实现模拟的话，能否也得到较为理想的结果呢？
       从神经解剖学的成果上来看，人脑的v1区和v2区，神经细胞确实是按照规律排列的。而且都是可以人为设计的。而且，一个让人怀疑的地方就是，v1区和v2区的神经细胞，是先天发育好的，还是后天训练出来的？如果是先天的，那就是说，这种模版是可以人为设计的。
**5、代价函数的设计方法。**
       代价函数的设计，在初学者看来，是很奇怪的。代价函数的设计，直接影响到最终的模版训练结果，可以说是深度学习中最核心的模块。
       从目前已经发表的论文来看，一是考虑重构误差，二是加入某种惩罚项。惩罚项的设计有多种模式，有考虑一阶范式的，有考虑二阶范式的，各种设计可谓千奇百怪。有博文上讲到，惩罚项的作用是为了防止过拟合，但也有博文的观点是，惩罚项是为了保证稀疏性。（感觉过拟合与稀疏性是否存在某种内在联系。）
       当然，**代价函数的设计方法，目前还在不断探索**，感觉这是一个可以发论文的点。
**6、整个神经网络系统的设计**。
        神经网络的设计方法，包含了研究人员对人脑的理解方式。CNN、RBM，以及andrew项目组设计的变态网络，都各有各的特色。要把整个网络框架设计好，还是比较需要经验的，也是相当费脑力的。当然，这是整个领域最有研究价值的模块。
       作者：denghp83
       End.   网络的设计太难了，有谁揭开了神经系统的真实运行原理呢..................
