# AI：IPPR的数学表示-CNN可视化语义分析 - wishchinYang的专栏 - CSDN博客
2017年07月24日 11:46:23[wishchin](https://me.csdn.net/wishchin)阅读数：300
**前言**：
          ANN是个语义黑箱的意思是没有通用明确的函数表示，参数化的模型并不能给出函数的形式，更进而不能表示函数的实际意义。
          而CNN在图像处理方面具有天然的理论优势，而Conv层和Polling层，整流层等都有明确的意义。可以跳过函数形式直接进行语义级别的解析。
         可视化是直观理解的一个重要方式，CNN可视化可以辅助对特定数据集绕过语法，直接进行特定网络语义级别的解析。在CNN可视化之后，你可以看到整个特征提取的表象和结果。
![](https://img-blog.csdn.net/20170920114904692?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lzaGNoaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
          这就是一个有趣的地方，我们难以规约卷积核有怎样的函数形式，有怎么样的语法，但函数结果却真正地描述出了我们所需要的东西。底层卷积层得到相应的底层特征，高层卷积层得到高层特征，这仅仅需要一个从语义到语法的规约，而不是通常情况下的演绎——模型泛化。
**Deep Learning论文笔记之（七）深度网络高层特征可视化**
[zouxy09@qq.com](http://blog.csdn.net/zouxy09/article/details/mailto:zouxy09@qq.com)
[http://blog.csdn.net/zouxy09](http://blog.csdn.net/zouxy09)
自己平时看了一些论文，但老感觉看完过后就会慢慢的淡忘，某一天重新拾起来的时候又好像没有看过一样。所以想习惯地把一些感觉有用的论文中的知识点总结整理一下，一方面在整理过程中，自己的理解也会更深，另一方面也方便未来自己的勘察。更好的还可以放到博客上面与大家交流。因为基础有限，所以对论文的一些理解可能不太正确，还望大家不吝指正交流，谢谢。
本文的论文来自：
Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent.[Visualizing Higher Layer Features
 of a Deep Network](http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf). Spotlight presentation and poster at the ICML 2009 Workshop on Learning Feature Hierarchies, Montréal, Canada
下面是自己对其中的一些知识点的理解：
**《Visualizing Higher-Layer Features of a Deep Network》**
Deep Learning很吸引人，也很玄乎的一个点就是大家都说它可以提取到分级的逐层抽象的特征。但对我们来说，总是耳听为虚，眼见为实。所以，每当我们训练完一个深度模型后，我们还特别想把这个深度模型学到的东西给可视化出来，好弄明白它到底学到了什么东西，是不是有意义的，是不是像传说中的那样神奇。那怎样有意义的可视化呢？对了，我们用deep net来做什么的了？来提取特征的。那它提取什么特征呢？如果它像我们所说的提取的是底层到高层的特征，例如边缘，到形状，到目标等等，那就证明我们的目的是达到的。
另外，在对深度模型定量的分析上，我们还需要一个定性的分析方法去比较不同的深度架构学习到的特征。本文的目的就是寻找深度模型所提取到的高级特征的比较好的定性解释。我们通过在几个视觉数据库中训练堆叠降噪自动编码器和DBN深信网络，并比较几种不同的高级特征可视化的方法。虽然这些特征的显示是在单元级别上面的，也许有违于直观理解，但它很容易实现，而且在不同方法上面得到的结果也是一致的。我们希望这些方法可以让研究者更清楚的理解深度学习是如何工作和为什么工作的。本文中，介绍三种可视化的方法：激活最大化、采样和线性组合法。
**一、概述**
一些深度架构（例如DBNs）与generative procedure生成过程存在密切的联系，所以我们可以利用这个生成过程来瞥见一个单独的隐层神经元所表示的是什么。在这里，我们研究其中一个这样的采样方法。然而，一方面有时候我们很难去获得可以完全覆盖波尔兹曼或者RBM分布的样本，另一方面这种基于采样的可视化方法没办法运用到其他基于自动编码器的深度架构模型或者在每个层嵌入保留相似性的半监督学习模型上。
一个典型的对深度架构第一层所提取的特征的定性分析方法是通过观察由模型所学习到的这些滤波器。这些滤波器是输入层到第一层的权值矩阵的权值。他们是由输入空间来表示。这样就非常方便了，因为输入是图像或者小波，他们是可以被可视化的。
一般来说，当在数字的数据集中训练的时候，这些滤波器可以被可视化为一些不同的数字的笔画的检测器。当在自然图像中训练时，这些滤波器就相当于不同的边缘检测器了（小波滤波器）。
本文的目标是研究一种可以可视化深度架构中任意层中的任意神经元所计算或者提取的特征的一种方法。为了达到这个目的，我们需要在输入空间（图像）中实现可视化，并且需要找到一种有效地计算方法去计算，然后使他具有通用性，也就是在不同的深度网络模型中都可以使用。在这里我们探究了几种方法。然后我们在两个数据集中对他们进行了定性的对比，研究他们之间的联系性。
在实验过程中，一个非常让人惊喜的地方是，每一个隐层的节点对输入图像的响应，也就是输入空间的函数，居然是单峰的，也就是说，不管你随机地在什么地方初始化，最终都可以可靠地找到这个最大值，这对于迭代寻优来说是非常爽的，而且它可以将每个节点做了什么公开于天下。一览无余。
**二、模型**
我们这里讨论两个模型，这两个模型都是在深度架构中比较常见的。第一个是DBNs，它是通过贪婪的堆叠多个RBM层得到的。我们先通过CD算法训练一个RBM，然后固定它，再将其输出做了另一个RBM的输入，去训练这一个隐层的RBM。这个过程可以通过不断重复，然后得到一个服从训练分布的非监督模型的深度架构。需要注意的一点是，它是一个数据的生成模型，可以很容易的通过一个已经训练好的模型去采集样本。
我们要讨论的第二个模型是降噪自动编码器，它是传统编码器的随机变种，它具有更强的能力。它不会学习到恒等函数（也就是h(x)=x，这个函数满足了零重构误差，但这是没有意义的）。它被强制去学习输入的本质表达。
它训练的关键是需要不断的提高生成模型的似然函数的下界。它比传统的编码机要牛逼，如果堆叠成一个深度监督架构的话，与RBMs的性能相当，甚至更牛。另一个在隐层单元比输入单元要多的情况下避免学习恒等函数的方式是对隐层code增加稀疏约束。
我们在这里概括下Stacked Denoising Auto-Encoders的训练方法。对于一个输入x，我们给它添加随机的污染或者噪声，然后训练让降噪自动编码机学习重构原始的输入x。每个自动编码机的输出都是一个码矢h(x)。在这里和传统的神经网络一样，h(x) = sigmoid(b + W x)。这里我们用C(x)表示x的一个随机污染，我们让Ci (x) = xi或0，换句话说，就是我们随机的在原来的x中挑选一个固定大小的子集，把他们设置为0。我们还可以添加椒盐噪声，随机的选择一定大小的子集，设置为Bernoulli(0.5)。
在实际的图像中，对于特定像素i的像素输入xi和它的重构像素xii都可以看成该像素的伯努利概率：该位置的像素被涂成黑色的概率。我们通过交叉熵来比较该像素位置i的原始输入xi和它的重构像素xii的分布的相似性。然后需要对所有像素求和。另外，只有当输入和重构的值都在[0,1]这个范围的时候，伯努利分布才有意义。另外的选择就是选择高斯分布，这时候对应的就是均方误差规则了。
**三、Maximizing the activation最大化激活值**
第一个思想是很简单的：我们寻找使一个给定的隐层单元的激活值最大的输入模式。因为第一层的每一个节点的激活函数都是输入的线性函数，所以对第一层来说，它的输入模式和滤波器本身是成比例的。
我们回顾下诺贝尔医学奖David Hubel和Torsten Wiesel的那个伟大的实验。他们发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。也就是说某个“特定方向神经细胞”只对这个特定方向的图像边缘存在激励或者兴奋。通俗点说就是如果我这个神经元是提取这个特征的，那么如果你这个图像满足这个特征（可以理解为和它很相似），那么神经元的输出就很大，会兴奋。（有资料表明，人的大脑高层会存在“祖母细胞”，这类细胞的某一个细胞只对特定一个目标兴奋，例如你大脑里面有个能记忆你女朋友的细胞，然后一旦你女朋友出现在你面前，你这个细胞就会兴奋，告诉大脑，啊，这个是我的女朋友！）我们如果了解过模板卷积，那么我们知道如果某个卷积模板与图像中模块越相似，那么响应就越大。相反，如果某个图像输入使得这个神经元输出激励值最大，那么我们就有理由相信，这个神经元就是提取了和这个输入差不多的特征。所以我们寻找可以使这个神经元输入最大的那个x就是我们可以可视化并且有意义的表达这个神经元学习到的特征了。
用数学属于来表述就是，一旦完成网络训练后，参数W是确定的了，那么我们就可以寻找使得这个神经元最大化的激活值对应的x了，也就是：
![](https://cc362.ikafan.com/static/L3Byb3h5L2h0dHAvaW1hZ2U2Ny4zNjBkb2MuY29tL0Rvd25sb2FkSW1nLzIwMTMvMTEvMjMwMi8zNjkxNDg0Ml8x.jpg)
但这个优化问题通常是一个非凸优化问题，也就是是存在很多局部最小值。最简单的方法就是通过梯度下降去寻找到一个局部最小值。这会出现两种场景：一是从不同的随机值初始化开始最后都迭代得到相同的最小值，二是得到两个或者更多的局部最小值。不管是哪种情况，该神经节点提取的特征都可以通过找到的一个或者多个最小值进行描述。如果有多个最小值，那么可以寻找使激活值最大的或者将所有的进行平均，或者把所有的都显示出来。
**四、Sampling from a unit of a Deep Belief Network从DBN的一个节点中采样**
我们这里用一个j层的Deep Belief Network来说明。这里层j和层j-1构成一个RBM，我们可以通过块Gibbs采样方法来对分布p(**h**j?1|**h**j )和p(**h**j |**h**j?1)进行连续采样（这里**h**j表示层j的所有的二值节点构成的向量）。在这个马尔科夫链中，我们限制一个节点**hij**为1，其他节点都是0。然后在DBN中，我们从层j-1一直执行top-down采样一直到输入层。这样就会产生一个分布pj(**x**|**hij**=1)。也就是说我们用分布pj(**x**|**hij**=1)来描述hij，和第三部分相似，我们可以通过从这个分布生成或者采样足够多的样本来描述这个隐层节点或者通过计算期望E[**x**|**hij**=1]来概括这个信息。这个方法只有一个参数需要确定，也就是我们要采样多少个样本来估计这个期望值。
在最大化激活值和计算期望E[**x**|**hij**=1]两种方法之间是存在一个非常微妙的联系的。由条件期望的定义我们可以知道：
![](https://cc362.ikafan.com/static/L3Byb3h5L2h0dHAvaW1hZ2U2Ny4zNjBkb2MuY29tL0Rvd25sb2FkSW1nLzIwMTMvMTEvMjMwMi8zNjkxNDg0Ml8y.jpg)
我们考虑一个极端的情况，就是这个分布全部集中在**x**+这一个点，这时候pj(**x**|**hij**=1)约等于δx+ (x)。所以其期望E[**x**|**hij**=1]=**x**+。
事实上，我们观测到的是，尽管采样得到的样本或者它们的平均可能看起来很像训练样本，但是由激活值最大化获得的图像反而看起来更像图像的部分。所以后者可能更能准确的表达一个特定的节点做了什么。
**五、Linear combination of previous layers’ filters上层滤波器的线性组合**
Lee等（2008）在他们的论文中展示了一种可视化第二层隐层的节点的特征的方法。他们是基于一个神经元节点可以由和其强连接的上一层的滤波器的组合来描述的假设的。该层的某个节点的可视化可以通过上一层滤波器的线性加权得到，每个滤波器的权值就是该滤波器与这个节点之间的连接权值。
他们用自然图像训练了一个具有激活值稀疏约束的DBNs，然后用这种方法显示它在第二层学习到的是一个角的检测器。Lee拓展了这个方法去可视化第三层学习到的东西：通过简单对第二层的滤波器进行加权，权值是第二层滤波器到第三层该节点的连接权值，而且选择的是最大的那个权值。
这种方法是简单而有效的。但是它的一个缺点就是，对在每一层如何自动的选择合适的滤波器个数没有一个清晰的准则。另外，如果只是选择少有的几个与第一次连接最强的滤波器的话，我们有可能得到的是失望的毫无意思的混杂图像，这是因为上面这种方法本质上忽略了上一层其他没有被选择的滤波器。另一方面，这种方法还忽略了层间的非线性，这种非线性在模型中确是非常重要的一部分。
但值得注意的是，实际上最大化一个节点的激活值的梯度更新方法与线性加权组合之前也存在微妙的联系。例如，对于第2层的第i个节点hi2= v’ sigmoid(W x)，这里v是该节点的权值，W是第一层的权值矩阵。然后?hi2/?x = v’diag(sigmoid(W x)?(**1**?sigmoid(W x)))W。这里*是逐元素相乘。diag是一个从一个向量创建一个对角矩阵的算子。**1**是个全一向量。如果第一层的节点没有饱和saturate，那么?hi2/?x就会大体的指向v’W的方向，可以用**v**i中的元素的绝对值的最大的那个来近似。（我也还没懂）
**六、实验**
**6.1、Data and setup**
我们在MINST手写体数据库和自然图像数据库中分别训练DBN和DSAE两种模型。然后再用三种方法来可视化其中一些层的一些节点所提取到的特征。
**6.2、Activation Maximization**
![](https://cc362.ikafan.com/static/L3Byb3h5L2h0dHAvaW1hZ2U2Ny4zNjBkb2MuY29tL0Rvd25sb2FkSW1nLzIwMTMvMTEvMjMwMi8zNjkxNDg0Ml8z.jpg)
在MNIST手写体数据库中用Activation maximization得到的可视化效果。左：分别是第一层（第一列）、第二层（第二列）和第三层（第三列）的36个节点所提取的特征，然后第一行是DBN训练得到的，第二行是SDAE训练得到的。右：对于DBN的第三层的一个节点，然后从9个随机初始化的值开始迭代，都可以得到同一个结果。
一般来说，在第三层的激活函数应该是一个关于它的输入的高度非凸的函数，但是不知道是我们一直很幸运还是说恰好我们在MNIST或者自然图像中训练网络是一种很特殊的情况，我们惊喜的发现，这些节点的激活函数既然还是趋向于更加的“单峰化”。
**6.3、Sampling a unit**
![](https://cc362.ikafan.com/static/L3Byb3h5L2h0dHAvaW1hZ2U2Ny4zNjBkb2MuY29tL0Rvd25sb2FkSW1nLzIwMTMvMTEvMjMwMi8zNjkxNDg0Ml80.jpg)
左：从MNIST数据库中训练DBN得到的，然后采样得到第二层的其中6个节点的可视化效果。右：从自然图像中训练的。每一行的样本是每一个节点的分布采样得到的，然后每行的均值在第一列。
值得注意的是，采样法和激活值最大化方法的结果不同，采样得到的（或者说分布生成的）样本更有可能是训练样本（数字或者patches）的潜在分布。激活值最大化方法是产生很多特征，然后要我们去决定哪些样本会匹配或者符合这些特征；采样方法是产生很多样本，然后由我们决定这些样本存在哪些共同的特征。从这个层面上面讲，这两种方法是互补的。
**6.4、Comparison of methods**
在这里，我们对比可视化的三种方法：激活最大化、采样和线性组合法。
![](https://cc362.ikafan.com/static/L3Byb3h5L2h0dHAvaW1hZ2U2Ny4zNjBkb2MuY29tL0Rvd25sb2FkSW1nLzIwMTMvMTEvMjMwMi8zNjkxNDg0Ml81.jpg)
我们这里展示了三种方法：左：采样；中：上次滤波器的线性组合；右：最大化节点激活值。在MINST数据库（top）和自然图像（down）上训练DBN模型。然后可视化第二层的36个节点。
在这三种方法中，线性组合方法是以前提出来的，其他两种是我们以现有的知识吗，集当前的智慧得到的。
**       附：**原论文中，提到了更多的内容，深入了解请参考原论文。另外，自己还没来得及去实现一些，等以后可以了再放上来和大家交流下。也希望大家已经实现了的也可以共享下，谢谢。
