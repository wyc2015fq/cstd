# ***K近邻Survey-Distance总结 - wishchinYang的专栏 - CSDN博客
2014年03月25日 13:45:26[wishchin](https://me.csdn.net/wishchin)阅读数：1219
个人分类：[图像检索																[计算机视觉](https://blog.csdn.net/wishchin/article/category/1871617)](https://blog.csdn.net/wishchin/article/category/1673923)
**（一）：漫谈knn：**原文链接：[http://www.kankanews.com/ICkengine/archives/103938.shtml](http://www.kankanews.com/ICkengine/archives/103938.shtml)   看引擎...有点对不起作者，不过没有办法，联系不到啊....
      什么是KNN算法呢？顾名思义，就是K-Nearest neighbors Algorithms的简称。我们可能都知道最近邻算法，它就是KNN算法在k=1时的特例，也就是寻找最近的邻居。
关于分类问题：KNN为空间类别判别提供了最终的原型...
      首先我要说的是为什么我们要寻找邻居啊，古话说的好，人以类聚，物以群分，要想知道一个人怎么样，去看看他的朋友就知道了，其实这个过程就蕴含了KNN的算法核心思想，我们如果要判断一个样本点的类别，去看看和它相似的样本点的类别就行了，If it walks like a duck, quacks like a duck, then it is probably a duck，如图1所示：
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/6ccdbc39d1_121120-1S8Q-1412321.png)
  好了，在深入了解KNN之前有必要了解一下分类算法的大致情况以及其完整定义。图2所示的是一般的分类模型建立的步骤，分类一般分为两种：
**  积极学习法** (决策树归纳)：先根据训练集构造出分类模型，根据分类模型对测试集分类。
**  消极学习****法** (基于实例的学习法):推迟建模， 当给定训练元组时，简单地存储训练数据 (或稍加处理)，一直等到给定一个测试元组。
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/bbff29e527_121207-ieym-1412321.png)
   消极学习法在提供训练元组时只做少量工作，而在分类或预测时做更多的工作。KNN就是一种简单的消极学习分类方法，它开始并不建立模型，而只是对于给定的训练实例点和输入实例点，基于给定的邻居度量方式以及结合经验选取合适的k值，计算并且查找出给定输入实例点的ｋ个最近邻训练实例点，然后基于某种给定的策略，利用这ｋ个训练实例点的类来预测输入实例点的类别。算法的过程如图3所示：
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/bbff29e527_121236-dlme-1412321.png)
   了解了KNN的主体思想以后，接下来我们就来逐一的探讨和回答我在第一章所提出的四个问题，第一个就是如何度量邻居之间的相识度，也就是如何选取邻居的问题，我们知道相似性的度量方式在很大程度上决定了选取邻居的准确性，也决定了分类的效果，因为判定一个样本点的类别是要利用到它的邻居的，如果邻居都没选好，准确性就无从谈起。因此我们需要用一个量来定量的描述邻居之间的距离，也可以形象的表述为邻居之间的相似度，具体的距离度量方式有很多，不同的场合使用哪种需要根据不同问题具体探讨，具体的我就不罗嗦，在这篇博文[http://www.cnblogs.com/v-July-v/archive/2012/11/20/3125419.html](http://www.cnblogs.com/v-July-v/archive/2012/11/20/3125419.html)中有详细的阐述。以下给出了使用三种距离（欧式距离，曼哈顿距离，还有切比雪夫距离）的对glass数据集测试的例子，测试结果如图4所示：红线指的是实验使用的距离度量方式，黄线指的是实验的结果，可以看出使用曼哈顿距离分类效果明显好于其他两种。
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/bbff29e527_121305-z4o9-1412321.png)
   在给定了度量方式以后，我们自然而然会遇到一个问题就是到底要找多少个邻居才合适了，如图5所示 ，X是待分类样本，‘，’和‘-’是样本类别属性，如果K选大了的话，可能求出来的k最近邻集合可能包含了太多隶属于其它类别的样本点，最极端的就是k取训练集的大小，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。如果K选小了的话，结果对噪音样本点很敏感。那么到底如何选取K值，其实我在前面也说了，其实完全靠经验或者交叉验证（一部分样本做训练
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/bbff29e527_121337-9uEc-1412321.png)
集，一部分做测试集）的方法，就是是K值初始取一个比较小的数值，之后不段来调整K值的大小来时的分类最优，得到的K值就是我们要的，但是这个K值也只是对这个样本集是最优的。一般采用k为奇数，跟投票表决一样，避免因两种票数相等而难以决策。下面我们可以通过交叉验证的方式求出最合适的K值，对iris数据（UCI
 Machine Learning Repository下载）用kNN算法进行分类，通过交叉验证（10次）的方式，对k取不同值时进行了实验，实验结果如图5所示，其中红线指的是实验选用的Ｋ值，黄线指的是实验的结果，我们发现在我所选取的k值中，当k=17时效果最好，在k=1时，即用最近邻来进行分类的效果也不错，实验结果呈现一个抛物线，与我们之前分析的结果相吻合。
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/abd1082f8e_121406-kuVx-1412321.png)
    好了，到这一步工作已经做了一半了，接下来就是如何去寻找这k个邻居了，因为对每一个待测样本点来说，我们都要对整个样本集逐一的计算其与待测点的距离，计算并存储好以后，接下来就是查找K近邻，这是最简单，也是最笨的方法，计算量太大了。因此KNN的一大缺点需要存储全部训练样本，以及繁重的距离计算量，有没有简单的一点的方法可以避免这种重复的运算啊，改进的方案有两个，一个是对样本集进行组织与整理，分群分层，尽可能将计算压缩到在接近测试样本邻域的小范围内，避免盲目地与训练样本集中每个样本进行距离计算。另一个就是在原有样本集中挑选出对分类计算有效的样说本，使样本总数合理地减少，以同时达到既减少计算量，又减少存储量的双重效果。KD树方法采用的就是第一个思路，关于KD树及其扩展可以参看博文[http://www.cnblogs.com/v-July-v/archive/2012/11/20/3125419.html](http://www.cnblogs.com/v-July-v/archive/2012/11/20/3125419.html)，它对其进行了详细的阐述，我就不啰嗦了。我想补充的是压缩近邻算法，它采用的思路是第二种方案，利用现有样本集，逐渐生成一个新的样本集，使该样本集在保留最少量样本的条件下，仍能对原有样本的全部用最近邻法正确分类，那么该样本集也就能对待识别样本进行分类，并保持正常识别率。它的步骤如下：
   首先定义两个存储器，一个用来存放即将生成的样本集，称为Store；另一存储器则存放原样本集，称为Grabbag。其算法是：
1.**初始化**。Store是空集，原样本集存入Grabbag；从Grabbag中任意选择一样本放入Store中作为新样本集的第一个样本。
2.**样本集生成**。在Grabbag中取出第i个样本用Store中的当前样本集按最近邻法分类。若分类错误，则将该样本从Grabbag转入Store中，若分类正确，则将该样本放回Grabbag中。
3.**结束过程**。若Grabbag中所有样本在执行第二步时没有发生转入Store的现象，或Grabbag已成空集，则算法终止，否则转入第二步。
   当然解决的方案很多，还有比如剪辑近邻法，快速搜索近邻法等等很多，就不一一介绍了。下面测试了一下不同最近邻搜索算法（线性扫描，kd树，Ball树，Cover树）所花费的时间，如表1所示：
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/abd1082f8e_121435-VdXx-1412321.png)
    到这一步基本上是万事俱备，只欠东风啦。K近邻（通俗的来说就是某人的k个最要好的朋友都找出来啦）都求出来啦，接下来就是要朋友们利用手中的投票器为其投票啦。一般的做法就是一人一票制，少数服从多数的选举原则，但是当和我测试对象离的近的数量少，而离得远的数量多时，这种方法可能就要出错啦，那咋办呢，看过歌唱选秀节目的人应该清楚，评审分为两种，一种是大众评审一人一票，一种是专家评审，一人可能有很多票，我们也可以借鉴这个思想，为每个邻居赋予一定的投票权重，通过它们与测试对象距离的远近来相应的分配投票的权重,最简单的就是取两者距离之间的倒数![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/abd1082f8e_121527-FQEi-1412321.png)，距离越小，越相似，权重越大，将权重累加，最后选择累加值最高类别属性作为该待测样本点的类别。我用不同的权重方式对UCI中的glass数据集进行测试，图7显示的是直接不采用权重的实验结果，图8显示的是权重为距离的倒数，图9显示的是权重为1减去归一化后的距离，红线指的是实验使用的权重赋值方式，“0”指的是不采用权重，“0
 -I”指的是取距离倒数，“0-F”指的是1减去归一化后的距离，深红线指的是实验的结果，我们可以看出采用了权重的总体上来说比不使用权重要好。
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/abd1082f8e_121556-Y9Wj-1412321.png)
![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/87b9908e75_121608-bGzm-1412321.png)![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/87b9908e75_121618-jP28-1412321.png)
    至此关于KNN算法的描述就到此结束了。可以看出算法的思想是十分简单的，我们自然而然的就会想这个算法的准确率到底是多少，有没有啥科学的证明，其实最初的近邻法是由Cover和Hart于1968年提出的，随后得到理论上深入的分析与研究，是非参数法中最重要的方法之一，它在论文Nearest Neighbor Pattern
 Classification中给出了算法准确率的相信描述。最近邻法的错误率是高于贝叶斯错误率的，![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/87b9908e75_121712-eZkW-1412321.png)其中代表的是贝叶斯误差率，由于一般情况下P*很小，因此又可粗略表示成：![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/87b9908e75_121803-5KW5-1412321.png)，对于kNN来说，当样本数量*N*→∞的条件下，*k*-近邻法的错误率要低于最近邻法，具体如图10所示：![](http://www.kankanews.com/ICkengine/wp-content/plugins/wp-o-matic/cache/87b9908e75_121908-FMfZ-1412321.png)
1.2 K值对训练的影响：
一个选择多少个邻居，即K值定义为多大的问题。不要小看了这个K值选择问题，因为它对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：
- 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“**学习”近似误差会减小**，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着**整体模型变得复杂，容易发生过拟合；**
- **如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大**。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
- K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。
    在实际应用中，K值一般取一个比较小的数值，例如采用[交叉验证](http://zh.wikipedia.org/zh/%E4%BA%A4%E5%8F%89%E9%A9%97%E8%AD%89)法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。
**（二）：[从K近邻算法、距离度量谈到KD树、SIFT+BBF算法](http://blog.csdn.net/v_july_v/article/details/8203674)**
原文链接：[http://blog.csdn.net/v_july_v/article/details/8203674/](http://blog.csdn.net/v_july_v/article/details/8203674/)
1.1一个人坚持自己的兴趣是比较难的，因为太多的人太容易为外界所动了，而尤其当你无法从中得到多少实际性的回报时，所幸，我能一直坚持下来。毕达哥拉斯学派有句名言：“万物皆数”，最近读完「微积分概念发展史」后也感受到了这一点。同时，从算法到数据挖掘、机器学习，再到数学，其中每一个领域任何一个细节都值得探索终生，或许，这就是“终生为学”的意思。
本文各部分内容分布如下：
- 第一部分讲K近邻算法，其中重点阐述了相关的距离度量表示法，
- 第二部分着重讲K近邻算法的实现--KD树，和KD树的插入，删除，最近邻查找等操作，及KD树的一系列相关改进(包括BBF，M树等)；
- 第三部分讲KD树的应用：SIFT+kd_BBF搜索算法。
    同时，你将看到，K近邻算法同本系列的前两篇文章所讲的决策树分类贝叶斯分类，及支持向量机SVM一样，也是用于解决分类问题的算法，
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/23/1353664561_7604.jpg)
    而本数据挖掘十大算法系列也会按照分类，聚类，关联分析，预测回归等问题依次展开阐述。
### 1.2、近邻的距离度量表示法
    上文第一节，我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。但有的读者可能就有疑问了，我是要找邻居，找相似性，怎么又跟距离扯上关系了？
    这是因为特征空间中两个实例点的距离和反应出两个实例点之间的相似性程度。K近邻模型的特征空间一般是n维实数向量空间，使用的距离可以使欧式距离，也是可以是其它距离，既然扯到了距离，下面就来具体阐述下都有哪些距离度量的表示法，权当扩展。
- **1. 欧氏距离**，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的距离为：![](https://img-my.csdn.net/uploads/201211/20/1353398777_7638.png)
> 
> 
(1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353399552_4107.png)
> 
(2)三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353399601_9675.png)
> 
(3)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353399644_3809.png)
> 
　　也可以用表示成向量运算的形式：![](https://img-my.csdn.net/uploads/201211/20/1353399664_2255.png)
其上，二维平面上两点欧式距离，代码可以如下编写：
```cpp
//unixfy：计算欧氏距离
double euclideanDistance(const vector<double>& v1, const vector<double>& v2)
{
     assert(v1.size() == v2.size());
     double ret = 0.0;
     for (vector<double>::size_type i = 0; i != v1.size(); ++i)
     {
         ret += (v1[i] - v2[i]) * (v1[i] - v2[i]);
     }
     return sqrt(ret);
 }
```
- **2. 曼哈顿距离**，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：![](https://img-my.csdn.net/uploads/201211/20/1353398955_7627.png)，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。 
     通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。
> 
> 
(1)二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353399908_7845.png)
> 
(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353399924_2304.png)
- **3. 切比雪夫距离，**若二个向量或二个点p 、and q，其座标分别为![](https://img-my.csdn.net/uploads/201211/20/1353399156_6169.png)及![](https://img-my.csdn.net/uploads/201211/20/1353399167_8768.png)，则两者之间的切比雪夫距离定义如下：![](https://img-my.csdn.net/uploads/201211/20/1353399181_4774.png)，
    这也等于以下Lp度量的极值：![](https://img-my.csdn.net/uploads/201211/20/1353399197_1359.png)，因此切比雪夫距离也称为L∞度量。
    以数学的观点来看，切比雪夫距离是由一致范数（uniform norm）（或称为上确界范数）所衍生的度量，也是超凸度量（injective metric space）的一种。
    在平面几何中，若二点p及q的直角坐标系坐标为![](https://img-my.csdn.net/uploads/201211/20/1353399234_6553.png)及![](https://img-my.csdn.net/uploads/201211/20/1353399245_9262.png)，则切比雪夫距离为：![](https://img-my.csdn.net/uploads/201211/20/1353399225_9083.png)。
    玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。
> 
> 
(1)二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353400142_8660.png)
> 
(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离 　　
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353400159_5706.png)
> 
这个公式的另一种等价形式是 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353400175_3976.png)
- **4. 闵可夫斯基距离(Minkowski Distance)，**闵氏距离不是一种距离，而是一组距离的定义。
> 
> 
(1) 闵氏距离的定义       
两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为： 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/20/1353400356_6225.png)
> 
其中p是一个变参数。
> 
当p=1时，就是曼哈顿距离
> 
当p=2时，就是欧氏距离
> 
当p→∞时，就是切比雪夫距离       
> 
根据变参数的不同，闵氏距离可以表示一类的距离。 
- 
**5. 标准化欧氏距离 (Standardized Euclidean distance )**
- 
标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。至于均值和方差标准化到多少，先复习点统计学知识。
假设样本集X的数学期望或均值(mean)为m，标准差(standard deviation，方差开根)为s，那么X的“标准化变量”X*表示为：(X-m）/s，而且标准化变量的数学期望为0，方差为1。
即，样本集的标准化过程(standardization)用公式描述就是：
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353468927_4645.png)
标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差　　
经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：　　
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353468944_5294.png)
如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。 
- 
**6. 马氏距离(Mahalanobis Distance)**
（1）马氏距离定义       
有M个样本向量X1~Xm，[协方差矩阵](http://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为： 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353469107_5540.png)
（协方差矩阵中每个元素是各个矢量元素之间的协方差Cov(X,Y)，Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，其中E为数学期望）
而其中向量Xi与Xj之间的马氏距离定义为：    
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353469122_3539.png)
若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：       
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353469133_1179.png)
也就是欧氏距离了。　　
若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。
(2)马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。 
「微博上的seafood高清版点评道：原来马氏距离是根据协方差矩阵演变，一直被老师误导了，怪不得看Killian在05年NIPS发表的LMNN论文时候老是看到协方差矩阵和半正定，原来是这回事」
- **7、巴氏距离（Bhattacharyya Distance）**
- 在统计中，Bhattacharyya距离测量两个离散或连续概率分布的相似性。它与衡量两个统计样品或种群之间的重叠量的Bhattacharyya系数密切相关。Bhattacharyya距离和Bhattacharyya系数以20世纪30年代曾在印度统计研究所工作的一个统计学家A. Bhattacharya命名。同时，Bhattacharyya系数可以被用来确定两个样本被认为相对接近的，它是用来测量中的类分类的可分离性。
> 
（1）巴氏距离的定义
对于离散概率分布 p和q在同一域 X，它被定义为：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353505211_7582.png)
> 
其中：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353505223_4832.png)
> 
是Bhattacharyya系数。
对于连续概率分布，Bhattacharyya系数被定义为：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353505232_5383.png)
> 
在![](https://img-my.csdn.net/uploads/201211/21/1353506460_1853.jpg)这两种情况下，巴氏距离![](https://img-my.csdn.net/uploads/201211/21/1353506510_1258.png)并没有服从三角不等式.（值得一提的是，Hellinger距离不服从三角不等式![](https://img-my.csdn.net/uploads/201211/21/1353505412_9942.jpg)）。 
对于多变量的高斯分布 ![](https://img-my.csdn.net/uploads/201211/21/1353505450_7450.png)，
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353505468_1004.png)，
> 
和是手段和协方差的分布![](https://img-my.csdn.net/uploads/201211/21/1353505482_9523.png)。
需要注意的是，在这种情况下，第一项中的Bhattacharyya距离与马氏距离有关联。 
（2）Bhattacharyya系数
Bhattacharyya系数是两个统计样本之间的重叠量的近似测量，可以被用于确定被考虑的两个样本的相对接近。
计算Bhattacharyya系数涉及集成的基本形式的两个样本的重叠的时间间隔的值的两个样本被分裂成一个选定的分区数，并且在每个分区中的每个样品的成员的数量，在下面的公式中使用
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353505511_9544.png)
> 
考虑样品a 和 b ，n是的分区数，并且![](https://img-my.csdn.net/uploads/201211/21/1353506721_2552.png)，![](https://img-my.csdn.net/uploads/201211/21/1353506738_9377.png)被一个 和 b i的日分区中的样本数量的成员。更多介绍请参看：[http://en.wikipedia.org/wiki/Bhattacharyya_coefficient](http://en.wikipedia.org/wiki/Bhattacharyya_coefficient)。
- **8. 汉明距离(Hamming distance)**， 两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。
> 
或许，你还没明白我再说什么，不急，看下[上篇blog](http://blog.csdn.net/v_july_v/article/details/7974418)中第78题的第3小题整理的一道面试题目，便一目了然了。如下图所示：
> 
![](https://img-my.csdn.net/uploads/201211/21/1353502733_2797.jpg)
```cpp
<span style="font-size:12px;">//<strong>动态规划：  </strong></span>
  
//f[i,j]表示s[0...i]与t[0...j]的最小编辑距离。  
f[i,j] = min { f[i-1,j]+1,  f[i,j-1]+1,  f[i-1,j-1]+(s[i]==t[j]?0:1) }  
  
//分别表示：添加1个，删除1个，替换1个（相同就不用替换）。
```
> 
    与此同时，面试官还可以继续问下去：那么，请问，如何设计一个比较两篇文章相似性的算法？（这个问题的讨论可以看看这里：[http://t.cn/zl82CAH](http://t.cn/zl82CAH)，及这里关于simhash算法的介绍：[http://www.cnblogs.com/linecong/archive/2010/08/28/simhash.html](http://www.cnblogs.com/linecong/archive/2010/08/28/simhash.html)），接下来，便引出了下文关于夹角余弦的讨论。
（[上篇blog](http://blog.csdn.net/v_july_v/article/details/7974418)中第78题的第3小题给出了多种方法，读者可以参看之。同时，程序员编程艺术系列第二十八章将详细阐述这个问题）
- **9. 夹角余弦(Cosine) **
，几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。
> 
(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353503598_9858.png)
> 
(2) 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353503607_7734.png)
> 
类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度，即：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353503631_6734.png)
> 
夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。 
- **10. 杰卡德相似系数(Jaccard similarity coefficient)**
> 
(1) 杰卡德相似系数       
两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。　
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353503768_8754.png)
> 
杰卡德相似系数是衡量两个集合的相似度一种指标。
(2) 杰卡德距离       
与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。
杰卡德距离可用如下公式表示：　　
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353503806_4754.png)
> 
杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。
(3) 杰卡德相似系数与杰卡德距离的应用      
可将杰卡德相似系数用在衡量样本的相似度上。
举例：样本A与样本B是两个n维向量，而且所有维度的取值都是0或1，例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。
M11 ：样本A与B都是1的维度的个数
M01：样本A是0，样本B是1的维度的个数
M10：样本A是1，样本B是0 的维度的个数
M00：样本A与B都是0的维度的个数
依据上文给的杰卡德相似系数及杰卡德距离的相关定义，样本A与B的杰卡德相似系数J可以表示为：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201212/14/1355467146_1157.png)
> 
这里M11+M01+M10可理解为A与B的并集的元素个数，而M11是A与B的交集的元素个数。而样本A与B的杰卡德距离表示为J'：
> 
> 
> 
> ![](https://img-my.csdn.net/uploads/201212/14/1355467157_8555.png)
- **11.皮尔逊系数(Pearson Correlation Coefficient)**
    在具体阐述皮尔逊相关系数之前，有必要解释下什么是相关系数 ( Correlation coefficient )与相关距离(Correlation distance)。
    相关系数 ( Correlation coefficient )的定义是：
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353513364_9506.png)
(其中，E为数学期望或均值，D为方差，D开根号为标准差，E{ [X-E(X)] [Y-E(Y)]}称为随机变量X与Y的协方差，记为Cov(X,Y)，即Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，而两个变量之间的协方差和标准差的商则称为随机变量X与Y的相关系数，记为![](https://img-my.csdn.net/uploads/201212/05/1354710691_8305.jpg))
   相关系数衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。
    具体的，如果有两个变量：X、Y，最终计算出的相关系数的含义可以有如下理解：
- 当相关系数为0时，X和Y两变量无关系。
- 当X的值增大（减小），Y值增大（减小），两个变量为正相关，相关系数在0.00与1.00之间。
- 当X的值增大（减小），Y值减小（增大），两个变量为负相关，相关系数在-1.00与0.00之间。
   相关距离的定义是：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353513387_4428.png)

> 
OK，接下来，咱们来重点了解下皮尔逊相关系数。
    在统计学中，皮尔逊积矩相关系数（英语：Pearson product-moment correlation coefficient，又称作 PPMCC或PCCs, 用r表示）用于度量两个变量X和Y之间的相关（线性相关），其值介于-1与1之间。
通常情况下通过以下取值范围判断变量的相关强度：
相关系数     0.8-1.0     极强相关
                 0.6-0.8     强相关
                 0.4-0.6     中等程度相关
                 0.2-0.4     弱相关
                 0.0-0.2     极弱相关或无相关
在自然科学领域中，该系数广泛用于度量两个变量之间的相关程度。它是由卡尔·皮尔逊从弗朗西斯·高尔顿在19世纪80年代提出的一个相似却又稍有不同的想法演变而来的。这个相关系数也称作“皮尔森相关系数r”。
**(1)皮尔逊系数的定义**：
两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商：
> 
> 
> 
> ![](https://img-my.csdn.net/uploads/201211/21/1353507664_9478.png)
> 
以上方程定义了总体相关系数, 一般表示成希腊字母ρ(rho)。基于样本对协方差和方差进行估计，可以得到样本标准差, 一般表示成r：
> 
> 
> 
> ![](https://img-my.csdn.net/uploads/201211/21/1353507674_8005.png)
> 
一种等价表达式的是表示成标准分的均值。基于(Xi, Yi)的样本点，样本皮尔逊系数是
> 
> 
> 
> ![](https://img-my.csdn.net/uploads/201211/21/1353507683_2154.png)
> 
               其中![](https://img-my.csdn.net/uploads/201211/21/1353507694_9044.png)、![](https://img-my.csdn.net/uploads/201211/21/1353507706_7395.png) 及 ![](https://img-my.csdn.net/uploads/201211/21/1353507716_2204.png)，分别是标准分、样本平均值和样本标准差。
或许上面的讲解令你头脑混乱不堪，没关系，我换一种方式讲解，如下：
假设有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算：
- 公式一：![](https://img-my.csdn.net/uploads/201211/22/1353579743_6051.jpg)
> 
> 
注：勿忘了上面说过，“皮尔逊相关系数定义为两个变量之间的协方差和标准差的商”，其中标准差的计算公式为：![](https://img-my.csdn.net/uploads/201211/22/1353574460_1284.jpg)
> 
- 公式二：![](https://img-my.csdn.net/uploads/201211/22/1353579754_6465.jpg)
- 公式三：![](https://img-my.csdn.net/uploads/201211/22/1353579761_5381.jpg)
- 公式四：![](https://img-my.csdn.net/uploads/201211/22/1353579768_6715.jpg)
以上列出的四个公式等价，其中E是[数学期望](http://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B)，cov表示[协方差](http://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE)，N表示变量取值的个数。
> **(2)皮尔逊相关系数的适用范围**
当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：
- 两个变量之间是线性关系，都是连续数据。
- 两个变量的总体是正态分布，或接近正态的单峰分布。
- 两个变量的观测值是成对的，每对观测值之间相互独立。
> **(3)如何理解皮尔逊相关系数**
> 
rubyist：皮尔逊相关系数理解有两个角度
其一, 按照高中数学水平来理解, 它很简单, 可以看做将**两组数据首先做Z分数处理之后, 然后两组数据的乘积和除以样本数，**Z分数一般代表正态分布中, 数据偏离中心点的距离.等于变量减掉平均数再除以标准差.(就是高考的标准分类似的处理)
样本标准差则等于变量减掉平均数的平方和，再除以样本数，最后再开方，也就是说，方差开方即为标准差，样本标准差计算公式为：
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/22/1353575338_2304.jpg)
> 
所以, 根据这个最朴素的理解,我们可以将公式依次精简为:
> 
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353511454_8956.png)
> 
其二, 按照大学的线性数学水平来理解, 它比较复杂一点,可以看做是两组数据的向量夹角的余弦。下面是关于此皮尔逊系数的几何学的解释，先来看一幅图，如下所示：
> 
> 
> 
> ![](https://img-my.csdn.net/uploads/201211/21/1353507876_1491.png)
> 
> 
> 
> 
> 
回归直线： y=gx(x) [红色] 和 x=gy(y) [蓝色]
> 
> 
> 
> 
如上图，对于没有中心化的数据, 相关系数与两条可能的回归线y=gx(x) 和 x=gy(y) 夹角的余弦值一致。
对于没有中心化的数据 (也就是说, 数据移动一个样本平均值以使其均值为0), 相关系数也可以被视作由两个随机变量 向量 夹角 的 余弦值（见下方）。
举个例子，例如，有5个国家的国民生产总值分别为 10, 20, 30, 50 和 80 亿美元。 假设这5个国家 (顺序相同) 的贫困百分比分别为 11%, 12%, 13%, 15%, and 18% 。 令 x 和 y 分别为包含上述5个数据的向量: x = (1, 2,
 3, 5, 8) 和 y = (0.11, 0.12, 0.13, 0.15, 0.18)。
利用通常的方法计算两个向量之间的夹角  (参见 数量积), 未中心化 的相关系数是:
> 
> 
> 
![](https://img-my.csdn.net/uploads/201211/21/1353507894_4766.png)
我们发现以上的数据特意选定为完全相关: y = 0.10 + 0.01 x。 于是，皮尔逊相关系数应该等于1。将数据中心化 (通过E(x) = 3.8移动 x 和通过 E(y) = 0.138 移动 y ) 得到 x = (−2.8, −1.8, −0.8, 1.2, 4.2) 和 y = (−0.028, −0.018, −0.008, 0.012, 0.042), 从中
> 
> 
> ![](https://img-my.csdn.net/uploads/201211/21/1353507911_5802.png)
**(4)皮尔逊相关的约束条件**
从以上解释, 也可以理解皮尔逊相关的约束条件:
- 1 两个变量间有线性关系
- 2 变量是连续变量
- 3 变量均符合正态分布,且二元分布也符合正态分布
- 4 两变量独立
在实践统计中,一般只输出两个系数,一个是相关系数,也就是计算出来的相关系数大小,在-1到1之间;另一个是独立样本检验系数,用来检验样本一致性。
     简单说来，各种“距离”的应用场景简单概括为，空间：欧氏距离，路径：曼哈顿距离，国际象棋国王：切比雪夫距离，以上三种的统一形式:闵可夫斯基距离，加权：标准化欧氏距离，排除量纲和依存：马氏距离，向量差距：夹角余弦，编码差别：汉明距离，集合近似度：杰卡德类似系数与距离，相关：相关系数与相关距离。

