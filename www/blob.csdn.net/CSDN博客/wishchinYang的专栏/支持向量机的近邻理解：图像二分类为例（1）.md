# 支持向量机的近邻理解：图像二分类为例（1） - wishchinYang的专栏 - CSDN博客
2016年10月19日 17:37:01[wishchin](https://me.csdn.net/wishchin)阅读数：1421
**前言：**
        机器学习在是否保留原始样本的层面划分为两类：参数学习和非参数学习。参数学习使用相对固定框架，把样本分布通过训练的方式回归到一个使用参数描述的数学模型里面，最终使用的是归纳方法；非参数模型保留了原始样本或者原始样本的其他空间转化形式，训练过程保留全部或者部分样本，最终使用的方法类似于演绎。
        svm方法偏向于非参数模型方法，使用的是精简—泛化模式。
**贝叶斯算法：**
        在类条件概率密度和先验概率已知（或可以估计）的情况下，通过贝叶斯公式比较样本属于两类的后验概率，将类别决策为后验概率大的一类。
        贝叶斯决策的的目的是 最小化总体误差率的平均期望。
        贝叶斯决策包含了大量不可探测（或刻意隐瞒）的事实，面对的是无穷的数据泛化空间，以最小化总体误差率的平均期望本质地实现强泛化性，是一个弱假设的决策方式。
**过拟合与欠拟合：**
         过拟合与欠拟合有多种表达方式。
**实验性的定义**： 模型在测试集上的性能普遍超过验证集合的性能，意味着模型的泛化能力较差，出现了过拟合。这是一个统计机器学习给出的普遍接受的定义，毕竟实验是检验真理的唯一标准。
        这个定义与教科书上的多项式拟合的阐述过拟合的方式没有任何特定的关系。强试验意味着弱假设，若把所有的事情交给试验，那么机器学习的专家还需要做些什么？把所有的东西都交给一个模型，那么这个强模型是从何而来？
        这就引入了机器学习的归纳学习本质，从数据中学习规律（规则），以规则集合来构建模型，进而验证集来验证规律是否合理。从数据中学习规律是机器学习的任务，学习到的规律是合理的吗？进行验证集试验是一个实验性的方法，从构建规则之初，已经暗含引入的一个模式：由数据和规则期望构建的假设空间。
        由数学期望构建的假设空间，来理解教科书上的解释多项式过拟合的阐述内容。
        从数据中学习规则，抽象为规则集合，构建为模型。若规则集合适用于验证集，则意味着泛化成功，若不适用于验证集，则出现了过拟合。
**假设空间:**
        一个古老的哲学原理：世界并不是以小包的形式来到我们面前，除非遍历整个空间，任何训练得到的模型都是过拟合的。面对学习问题，首先面对这一个空间的认知问题，对空间结构的认识来自于接口，而全面的认识来自于遍历。
        在认识一个未知空间之前，一般的套路是由接口获取的数据对这个空间进行简单假设，迭代修改理解规则，最后到遍历。
**图像二分类问题：**
         在图像识别领域，灰度图像被称为传说中的2维张量，任意图像为由所有二类图像构成的这个二维张量空间内的一个点。对于简单的二分类问题，学习需要做的是建立一个模型，可以把二维张量空间内的数据集塞入这个模型，压缩到一维张量空间内的一维向量之中的两个点上。这就是传说中的压缩映射，也叫压缩hash。
**假设空间**：所有能假设到的图像在二维张量空间可遍历到的范围。
**特征提取**：由人类专家完成图像属性归纳，把图像的特征显式的归结为一维张量空间的n维向量上，被称为特征提取。
**特征空间**：由所有可生成的n维向量可遍历到的向量空间，教科书称之为特征空间。
        SVM模型面对的问题：模型面对的问题是模型在特征空间中的泛化问题。
**一、线性可分性**
         函数集的VC维：https://en.wikipedia.org/wiki/VC_dimension
         VC维反映了函数集的学习能力，VC维越大则学习机器越复杂（容量越大），遗憾的是，目前尚没有通用的关于任意函数集VC维计算的理论，只对一些特殊的函数集知道其VC维。例如在N维空间中线性[分类器](http://baike.baidu.com/view/895803.htm)和线性实函数的VC维是N+1。
**线性可分**：根据模式识别教材理解，在二维空间中二分类线性可分意味着一条直线可以把两类样本完全分开。
                             在二维向量空间中，特征空间为集合S.<X,Y> 可遍历的二维向量任意位置。假设空间为假设的所有样本可遍历的二维向量空间。
                             线性可分即为存在一条直线 y=Ax+B 使 集合S.<X,Y> 可完全划分为两类。
**二、线性可分的近邻描述**
         近邻法在一维向量空间中用于分类有直观可验证效果。A和B为边界样本，分类超平面的理想形式为 一维点的坐标：（A+B）/2 。根据近邻原则，C为A类。
![](https://img-blog.csdn.net/20161019163722882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
         在二维向量空间中，线性可分即为存在一条直线 y= Ax+B 使 集合S.<X,Y> 可完全划分为两类。此直线为可划分集合的超平面。
![](https://img-blog.csdn.net/20161019163725994?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
红色直线为超平面：y= Ax+B
           对于一个完全可分的二维向量空间中的集合，分类超平面可以不止一条。根据近邻原则，~~最合适的超平面为中间的黑色直线，~~在黑色直线左侧的点离蓝色边界集合构成的直线较近，直觉上应该划分为蓝色集合。
![](https://img-blog.csdn.net/20161019163728725?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
                                       此图直接使用了SVM的论述介绍，直接借用了文章中的图；
                                       链接地址：[支持向量机（SVM）算法](http://www.cnblogs.com/end/p/3848740.html)
**三、压缩近邻法**
        模式识别教材里面阐述方法，压缩近邻法使用了1968年提出的CONDENSE算法，可以在不牺牲分类准确度的前提下大大压缩近邻法决策时的训练样本数目。通过启发式方法寻找用较少的样本代表集合中的分类信息。剔除离边界远的样本是迭代寻找边界的重要思想。
        压缩近邻法隐式或者显示地使用了样本间的欧式距离信息，在维度较高，欧式距离可表示性变差，是否可以使用一种新的近邻表示方式。
