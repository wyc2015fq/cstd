# IAAS: IT公司去IOE-Alibaba系统构架解读 - wishchinYang的专栏 - CSDN博客
2014年12月28日 18:02:59[wishchin](https://me.csdn.net/wishchin)阅读数：664
# 从Hadoop到自主研发，技术解读阿里去IOE后的系统架构
原地址：......................
[云计算](http://www.csdn.net/tag/%E4%BA%91%E8%AE%A1%E7%AE%97/news)[阿里](http://www.csdn.net/tag/%E9%98%BF%E9%87%8C/news)[飞天](http://www.csdn.net/tag/%E9%A3%9E%E5%A4%A9/news)
**摘要：**从IOE时代，到Hadoop与飞天并行，再到飞天单集群5000节点的实现，阿里一直摸索在技术衍变的前沿。这里，我们将从架构、性能、运维等多个方面深入了解阿里基础设施。
【导读】互联网的普及，智能终端的增加，大数据时代悄然而至。在这个数据为王的时代，数十倍、数百倍的数据给各个机构带来了无尽的机遇；然而，无可否认的是，数据体积的暴增同样前所未有的挑战着企业的基础设施。
        在这个大背景下，各个机构不得不在控制好成本支出的同时，不停摸索着时刻激增用户数据的解决之道，其中阿里的成绩无疑令人艳羡——单集群规模5000+的飞天，以及多集群跨机房计算的支持。本次我们将以飞天为例，为大家分享大规模分布式系统打造过程中的艰难坎坷及应对之道。
        本次分享共分为**视点、技术专题、应用实践**三大板块**：**“视点”从人物着手细分阿里当时所面临的形势及各个据测制定的依据；“技术专题”主要从实践出发剖析飞天5000节点扩展时所遭遇的艰难险阻及应对之道，涉及架构调整、性能优化、系统运维等多个领域；“应用实践”则更注重于云实践经验及用例分享。
## **目录**
**视点**
- [阿里云观察2014](http://www.csdn.net/article/2014-10-10/2822024)
- [阿里技术保障部：阿里云的幕后英雄](http://www.csdn.net/article/a/2014-09-27/15820215)
- [不期而遇的飞天之路](http://www.csdn.net/article/a/2014-09-27/15820216)
**技术专题**
**[探索5K巅峰，云梯架设的飞天之梦](http://www.csdn.net/article/a/2014-09-27/15820217)。**在3个月deadline的情况下，阿里却选择投入更多人力物力及时间的云梯1（以Hadoop为底层的集群）和云梯2（以飞天为底层的集群）并行扩容，阿里人选择背水一战的原因究竟是什么？在这个过程中，他们又会遭遇哪些挑战？目标实现后的惊喜又是什么？
**[优化无极限：盘古Master优化实践](http://www.csdn.net/article/2014-10-21/2822201)。**盘古，飞天的分布式文件系统，在内部架构上盘古采用Master/ChunkServer结构，Master管理元数据，ChunkServer负责实际数据读写，通过Client对外提供类POSIX的专有API。在集群扩展到5K规模后，相关问题纷至沓来，主要可分为两个部分：首先，盘古MasterIOPS问题；其次，盘古Master冷启动速度。那么究竟是什么造成了这些问题？阿里工程师又该如何应对？
**[走近伏羲，谈5000节点集群调度与性能优化](http://www.csdn.net/article/a/2014-09-27/15820219)。**伏羲，飞天平台的分布式调度系统。在5K攻坚中，从设计到实现每一步都可能存在性能“陷阱”，原因主要在三个方面：规模放大效应；木桶效应；长路径模块依赖。5000节点后这些方面究竟存在什么样的问题？阿里人又通过了什么方法保证了服务的性能与稳定性？
**[走近华佗，解析自动化故障处理系统背后的秘密](http://www.csdn.net/article/2014-10-14/2822089)。**5K后的运维模式究竟会产生什么样的变化？阿里人究竟为什么会开发华佗？上通飞天系统，下达运维各种系统，华佗健壮、简单和开放的架构究竟表现在什么方面？系统又是如何实现了自动化的运维？
**[ODPS技术架构及应用实践](http://www.csdn.net/article/2014-10-20/2822184)。**ODPS采用抽象的作业处理框架将不同场景的各种计算任务统一在同一个平台之上，共享安全、存储、数据管理和资源调度，为来自不同用户需求的各种数据处理任务提供统一的编程接口和界面。那么，在DT时代，不断扩大的数据规模又会给ODPS带来什么样的挑战？网站日志分析又该如何进行？
**[ODPS跨集群迁移与数据同步经验分享](http://www.csdn.net/article/a/2014-09-27/15820222)。**阿里各业务部门如淘宝、天猫、一淘、B2B等每天都会产生大量的数据，日均增量数百TB。2013年初，阿里内部的生产集群PA所在机房的存储量最多可扩容到数十PB，而当时已使用75%的存储量。存储容量告急，迫切需要将生产集群PA上的大量数据迁移到其他集群。那么阿里人该如何安全地跨集群迁移几十PB的数据和其上相关业务？数据迁移之后，两个集群间存在大量的数据依赖，需要互相访问最新的数据，如何安全快速地实现跨集群数据同步？
**[飞天5K实战经验：大规模分布式系统运维实践](http://www.csdn.net/article/2014-10-13/2822060)。**但短时间大规模快速膨胀的现状，给运维带来了巨大挑战，其中云梯2单集群规模更是从1500台升级到5000台。为此，运维需要做多个方向的调整，比如：提升全局掌控能力、实现系统的自我保护和自动化修复、大规模与精细化的平衡。那么，阿里又是通过什么途径完成这些工作的？
**应用实践**
- [数据生意背后的云计算](http://www.csdn.net/article/2014-10-10/2822031)
- [登月1号：支付宝演绎空中升级绝技](http://www.csdn.net/article/a/2014-09-27/15820225)
- [御膳房：构建大数据的美食厨房](http://www.csdn.net/article/a/2014-09-27/15820226)
## **节选**
**《不期而遇的飞天之路》——去IOE，飞天势在必行**
        翻开历史，淘宝曾启用全亚洲最大的OracleRAC集群，阿里更是购买过3年无限制的许可，阿里在IBM小型机以及EMC SAN存储上的投入也曾成为媒体争相报道的事件。但随着互联网爆发式发展，淘宝、支付宝和阿里巴巴B2B的注册用户数激增，阿里只能不停地通过水平和垂直扩展架构来应对新增用户生成的海量数据。而这种集中式数据库的架构，使得数据库成为了整个系统的瓶颈，越来越不适应海量数据对计算能力的巨大需求，更不用说越来越难以承受的高昂投入。阿里的“去IOE”已经势在必行：通过自主研发的分布式系统取代集中式数据库架构，使用MySQL+HBase取代Oracle，商用机取代小型机+SAN。
       选择自主研发，这也是阿里云在步入云计算之路上做出的最重要的抉择：坚持追求拥有自有的最有竞争力的核心技术。在唐洪看来，云计算是一门高技术门槛的生意，具备核心技术竞争力等于具备了在战场上可以正面抗衡竞争对手的实力，尽管这个技术攻关的历程非常之艰难。选择自主研发而非采用开源Hadoop优化，也是基于一定的考虑，尽管Hadoop在离线大数据处理上具备优势，但无法完全提供阿里云要求的大规模分布式计算与处理的能力，而目前基于飞天上线的云服务，已远远超出Hadoop的能力。开源可以说是一条先易后难的路，尽管一开始可以走一些捷径，但事后在版本升级、研发上都会受颇多限制；从核心知识产权角度来看，今天无论是微软、Amazon或者Google的云计算平台，都没有采用Hadoop且不开放代码开源，本质上都是在追求自有的核心竞争力。开源软件无法彻底成为一个云计算底层平台的基础，采用开源软件并非解决做分布式系统这个问题的一剂良方。发展自有技术，坚持底层自主研发，如今能够构建超级计算机的飞天已成为阿里拥抱云计算，以及对外提供云计算服务的坚实基础。
## 结语
       已经实现5000节点单集群的飞天5K拥有惊人的规模：10万核的计算能力；100PB存储空间 ；可处理15万并发任务数；可承载亿级别文件数目；100TB排序30分钟完成，远超今年7月1日Yahoo!在Sort Benchmark排序测试Daytona Gray Sort所创造的世界纪录——100TB排序完成时间约71分钟。
       优秀的产品背后，必定有优秀的基础设施支撑。在此，我们期望越来越多的团队打造出更加稳定、更具性能的底层平台，不管是自主研发，亦或是基于开源。（审校/魏伟）
