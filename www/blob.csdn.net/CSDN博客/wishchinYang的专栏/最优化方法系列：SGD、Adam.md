# 最优化方法系列：SGD、Adam - wishchinYang的专栏 - CSDN博客
2018年03月15日 17:58:08[wishchin](https://me.csdn.net/wishchin)阅读数：1117
整理一下资源，不过最好还是根据书上的理论好好推导一下.....
文章链接：[Deep Learning 最优化方法之SGD](http://blog.csdn.net/bvl10101111/article/details/72615436) 72615436
本文是[Deep Learning 之 最优化方法](http://blog.csdn.net/BVL10101111/article/details/72614711)系列文章
> 
 整个优化系列文章列表：
[Deep Learning 之 最优化方法](http://blog.csdn.net/BVL10101111/article/details/72614711)
[Deep Learning 最优化方法之SGD](http://blog.csdn.net/bvl10101111/article/details/72615436)
[Deep Learning 最优化方法之Momentum（动量）](http://blog.csdn.net/bvl10101111/article/details/72615621)
[Deep Learning 最优化方法之Nesterov(牛顿动量)](http://blog.csdn.net/bvl10101111/article/details/72615961)
[Deep Learning 最优化方法之AdaGrad](http://blog.csdn.net/bvl10101111/article/details/72616097)
[Deep Learning 最优化方法之RMSProp](http://blog.csdn.net/bvl10101111/article/details/72616378)
[Deep Learning 最优化方法之Adam](http://blog.csdn.net/bvl10101111/article/details/72616516)
在SVM里面已经讲习过SGD方法，这里重写一遍.....

