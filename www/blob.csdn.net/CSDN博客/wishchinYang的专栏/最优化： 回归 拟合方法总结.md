# 最优化：**回归/拟合方法总结 - wishchinYang的专栏 - CSDN博客
2015年08月24日 11:40:02[wishchin](https://me.csdn.net/wishchin)阅读数：24508
       回归分析是建模和分析数据的重要工具。本文解释了回归分析的内涵及其优势，重点总结了应该掌握的**线性回归**、**逻辑回归**、**多项式回归**、**逐步回归**、**岭回归**、**套索回归Lasso Regression**、**ElasticNet回归、SoftMax回归**等八种最常用的回归技术及其关键要素，最后介绍了选择正确的回归模型的关键因素。
       当然数据并非我想面对的，所谓处理数据的根本目的是为了重建模型的结构，即是根据数据回归到函数规则，生成函数规则集合，以此构建模型。
       本文做了少量修改，如有疑问，请访问原始文章链接。  原文链接：[http://news.csdn.net/article_preview.html?preview=1&reload=1&arcid=2825492](http://news.csdn.net/article_preview.html?preview=1&reload=1&arcid=2825492)
## **什么是回归分析？**
回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的[因果关系](http://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/)。例如，司机的鲁莽驾驶与道路交通事故数量之间的关系，最好的研究方法就是回归。
回归分析是建模和分析数据的重要工具。在这里，我们使用曲线/线来拟合这些数据点，在这种方式下，从曲线或线到数据点的距离差异最小。我会在接下来的部分详细解释这一点。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f54edbb07_middle.jpg?_=34626)
## **我们为什么使用回归分析？**
如上所述，回归分析估计了两个或多个变量之间的关系。下面，让我们举一个简单的例子来理解它：
比如说，在当前的经济条件下，你要估计一家公司的销售额增长情况。现在，你有公司最新的数据，这些数据显示出销售额增长大约是经济增长的2.5倍。那么使用回归分析，我们就可以根据当前和过去的信息来预测未来公司的销售情况。
使用回归分析的好处良多。具体如下：
- 它表明自变量和因变量之间的**显著关系**；
- 它表明多个自变量对一个因变量的**影响强度**。
回归分析也允许我们去比较那些衡量不同尺度的变量之间的相互影响，如价格变动与促销活动数量之间联系。这些有利于帮助市场研究人员，数据分析人员以及数据科学家排除并估计出一组最佳的变量，用来构建预测模型。
## **我们有多少种回归技术？**
有各种各样的回归技术用于预测。这些技术主要有三个度量（自变量的个数，因变量的类型以及回归线的形状）。我们将在下面的部分详细讨论它们。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f5b3903b4_middle.jpg?_=19921)
对于那些有创意的人，如果你觉得有必要使用上面这些参数的一个组合，你甚至可以创造出一个没有被使用过的回归模型。但在你开始之前，先了解如下最常用的回归方法：
### **1. Linear Regression线性回归**
它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。
线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。
用一个方程式来表示它，即Y=a+b*X + e，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f5dc33dc3.jpg)
一元线性回归和多元线性回归的区别在于，多元线性回归有（>1）个自变量，而一元线性回归通常只有1个自变量。现在的问题是“我们如何得到一个最佳的拟合线呢？”。
**如何获得最佳拟合线（a和b的值）？**
这个问题可以使用**最小二乘法**轻松地完成。最小二乘法也是用于拟合回归线最常用的方法。对于观测数据，它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f603272e0.jpg)
![](http://img.ptcms.csdn.net/article/201508/19/55d3f5e00506f.jpg)
我们可以使用R-square指标来评估模型性能。想了解这些指标的详细信息，可以阅读：模型性能指标[Part 1](http://www.analyticsvidhya.com/blog/2015/01/model-performance-metrics-classification/),[Part 2](http://www.analyticsvidhya.com/blog/2015/01/model-perform-part-2/) .
**要点：**
- 自变量与因变量之间必须有线性关系
- 多元回归存在多重共线性，自相关性和异方差性。
- 线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。
- 多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定
- 在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。
### **2.Logistic Regression逻辑回归**
逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。这里，Y的值从0到1，它可以用下方程表示。
```
odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk
```
上述式子中，p表述具有某个特征的概率。你应该会问这样一个问题：“我们为什么要在公式中使用对数log呢？”。
因为在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差（如在普通回归使用的）。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f6527f0aa.jpg)
**要点：**
- 它广泛的用于分类问题。
- 逻辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。
- 为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。
- 它需要大的样本量，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。
- 自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。
- 如果因变量的值是定序变量，则称它为序逻辑回归。
- 如果因变量是多类的话，则称它为多元逻辑回归。
### **3. Polynomial Regression多项式回归**
对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：
`    y=a+b*x^2`
在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f68b13771.jpg)
**重点：**
- 虽然会有一个诱导可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。下面是一个图例，可以帮助理解：
![](http://img.ptcms.csdn.net/article/201508/19/55d3f6a02a5c5.jpg)
- 明显地向两端寻找曲线点，看看这些形状和趋势是否有意义。更高次的多项式最后可能产生怪异的推断结果。
### **4. Stepwise Regression逐步回归**
在**处理多个自变量时**，我们可以使用这种形式的回归。在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。
这一壮举是通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。逐步回归通过同时添加/删除基于指定标准的协变量来拟合模型。下面列出了一些最常用的逐步回归方法：
- 标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。
- 向前选择法从模型中最显著的预测开始，然后为每一步添加变量。
- 向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显着性的变量。
这种建模技术的目的是使用最少的预测变量数来最大化预测能力。这也是处理高维数据集的方法之一。
### **5. Ridge Regression岭回归**
岭回归分析是一种用于存在**多重共线性**（自变量高度相关）数据的技术。在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。
上面，我们看到了线性回归方程。还记得吗？它可以表示为：
y=a+ b*x
这个方程也有一个误差项。完整的方程是：
`y=a+b*x+e (error term),  [error term is the value needed to correct for a prediction error between the observed and predicted value]``=> y=a+y= a+ b1x1+ b2x2+....+e, for multiple independent variables.`
在一个线性方程中，预测误差可以分解为2个子分量。一个是偏差，一个是方差。预测错误可能会由这两个分量或者这两个中的任何一个造成。在这里，我们将讨论由方差所造成的有关误差。
岭回归通过[收缩参数](https://en.wikipedia.org/wiki/Shrinkage_estimator)λ（lambda）解决多重共线性问题。看下面的公式
![](http://img.ptcms.csdn.net/article/201508/19/55d3f6f3add20.jpg)
在这个公式中，有两个组成部分。第一个是最小二乘项，另一个是β2（β-平方）的λ倍，其中β是相关系数。为了收缩参数把它添加到最小二乘项中以得到一个非常低的方差。
**要点：**
- 除常数项以外，这种回归的假设与最小二乘回归类似；
- 它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能
- 这是一个正则化方法，并且使用的是[L2正则化](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29)。
### **6. Lasso Regression套索回归**
它类似于岭回归，Lasso （Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。此外，它能够减少变化程度并提高线性回归模型的精度。看看下面的公式：
![](http://img.ptcms.csdn.net/article/201508/19/55d3f7141b25b.jpg)
Lasso 回归与Ridge回归有一点不同，它使用的惩罚函数是**绝对值，而不是平方**。这导致惩罚（或等于约束估计的绝对值之和）值使一些参数估计结果等于零。使用惩罚值越大，进一步估计会使得缩小值趋近于零。这将导致我们要从给定的n个变量中选择变量。
**要点：**
- 除常数项以外，这种回归的假设与最小二乘回归类似；
- 它收缩系数接近零（等于零），这确实有助于特征选择；
- 这是一个正则化方法，使用的是[L1正则化](http://news.csdn.net/l1%20regularization)；
· 如果预测的一组变量是高度相关的，Lasso 会选出其中一个变量并且将其它的收缩为零。
### **7.ElasticNet回归**
ElasticNet是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。
![](http://img.ptcms.csdn.net/article/201508/19/55d3f736bb158.jpg)
Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。
**要点：**
- 在高度相关变量的情况下，它会产生群体效应；
- 选择变量的数目没有限制；
- 它可以承受双重收缩。
除了这7个最常用的回归技术，你也可以看看其他模型，如[Bayesian](https://en.wikipedia.org/wiki/Bayesian_linear_regression)、[Ecological](https://en.wikipedia.org/wiki/Ecological_regression)和[Robust](https://en.wikipedia.org/wiki/Robust_regression)回归。
## 8.用于多类分类的SoftMax回归
参考链接：[http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
我们介绍Softmax回归模型，该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签 ![\textstyle y](http://ufldl.stanford.edu/wiki/images/math/c/8/1/c81e76c28ed991b22b8c1bb8fa392701.png) 可以取两个以上的值。 Softmax回归模型对于诸如MNIST手写数字分类等问题是很有用的，该问题的目的是辨识10个不同的单个数字。Softmax回归是有监督的，不过后面也会介绍它与深度学习/无监督学习方法的结合。（译者注： MNIST 是一个手写数字识别库，由NYU 的Yann LeCun 等人维护。[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) ）
回想一下在 logistic 回归中，我们的训练集由 ![\textstyle m](http://ufldl.stanford.edu/wiki/images/math/2/5/e/25e97e8a905fc2cb05d76cd4872a8567.png) 个已标记的样本构成：
![\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}](http://ufldl.stanford.edu/wiki/images/math/5/e/c/5ec89e9cf3712d45b80e93258352ea8f.png)
其中输入特征![x^{(i)} \in \Re^{n+1}](http://ufldl.stanford.edu/wiki/images/math/1/2/3/123c8ca74aa217158129b671fc7e75a8.png)。（我们对符号的约定如下：特征向量![\textstyle x](http://ufldl.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 的维度为![\textstyle n+1](http://ufldl.stanford.edu/wiki/images/math/c/e/9/ce988241166226ec379ecdfb009cc5c6.png)，其中![\textstyle x_0 = 1](http://ufldl.stanford.edu/wiki/images/math/c/5/8/c582053ce9cb63d69ae80acb53ded0d3.png) 对应截距项 。） 由于 logistic 回归是针对二分类问题的，因此类标记![y^{(i)} \in \{0,1\}](http://ufldl.stanford.edu/wiki/images/math/a/5/8/a589c252daed983404e6f9b3b1219954.png)。
![\begin{align}h_\theta(x) = \frac{1}{1+\exp(-\theta^Tx)},\end{align}](http://ufldl.stanford.edu/wiki/images/math/b/b/3/bb3791d463b832a88731b94f1d8e5279.png)
我们将训练模型参数 ![\textstyle \theta](http://ufldl.stanford.edu/wiki/images/math/6/9/d/69d920fe8e1da0543eb63d1097f21754.png)，使其能够最小化代价函数 ：
![\begin{align}J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right]\end{align}](http://ufldl.stanford.edu/wiki/images/math/f/a/6/fa6565f1e7b91831e306ec404ccc1156.png)
在 softmax回归中，我们解决的是多分类问题（相对于 logistic 回归解决的二分类问题），类标 ![\textstyle y](http://ufldl.stanford.edu/wiki/images/math/c/8/1/c81e76c28ed991b22b8c1bb8fa392701.png) 可以取![\textstyle k](http://ufldl.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个不同的值（而不是 2 个）。因此，对于训练集![\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}](http://ufldl.stanford.edu/wiki/images/math/5/e/c/5ec89e9cf3712d45b80e93258352ea8f.png)，我们有![y^{(i)} \in \{1, 2, \ldots, k\}](http://ufldl.stanford.edu/wiki/images/math/7/d/c/7dc095cfb7e3e1fc6bdbc358bd3e2888.png)。（注意此处的类别下标从 1 开始，而不是 0）。例如，在 MNIST 数字识别任务中，我们有![\textstyle k=10](http://ufldl.stanford.edu/wiki/images/math/1/b/8/1b84ec945b47439de6a73660b826df20.png) 个不同的类别。
对于给定的测试输入 ![\textstyle x](http://ufldl.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png)，我们想用假设函数针对每一个类别j估算出概率值![\textstyle p(y=j | x)](http://ufldl.stanford.edu/wiki/images/math/c/1/d/c1d5aaee0724f2183116cb8860f1b9e4.png)。也就是说，我们想估计![\textstyle x](http://ufldl.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 的每一种分类结果出现的概率。因此，我们的假设函数将要输出一个![\textstyle k](http://ufldl.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 维的向量（向量元素的和为1）来表示这![\textstyle k](http://ufldl.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个估计的概率值。 具体地说，我们的假设函数![\textstyle h_{\theta}(x)](http://ufldl.stanford.edu/wiki/images/math/8/8/7/887e72d0a7b7eb5083120e23a909a554.png) 形式如下：
![\begin{align}h_\theta(x^{(i)}) =\begin{bmatrix}p(y^{(i)} = 1 | x^{(i)}; \theta) \\p(y^{(i)} = 2 | x^{(i)}; \theta) \\\vdots \\p(y^{(i)} = k | x^{(i)}; \theta)\end{bmatrix}=\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }\begin{bmatrix}e^{ \theta_1^T x^{(i)} } \\e^{ \theta_2^T x^{(i)} } \\\vdots \\e^{ \theta_k^T x^{(i)} } \\\end{bmatrix}\end{align}](http://ufldl.stanford.edu/wiki/images/math/a/1/b/a1b0d7b40fe624cd8a24354792223a9d.png)
其中 ![\theta_1, \theta_2, \ldots, \theta_k \in \Re^{n+1}](http://ufldl.stanford.edu/wiki/images/math/f/d/9/fd93be6ab8e2b869691579202d7b4417.png) 是模型的参数。请注意![\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }](http://ufldl.stanford.edu/wiki/images/math/a/a/b/aab84964dbe1a2f77c9c91327ea0d6d6.png)这一项对概率分布进行归一化，使得所有概率之和为 1 。
为了方便起见，我们同样使用符号 ![\textstyle \theta](http://ufldl.stanford.edu/wiki/images/math/6/9/d/69d920fe8e1da0543eb63d1097f21754.png) 来表示全部的模型参数。在实现Softmax回归时，将![\textstyle \theta](http://ufldl.stanford.edu/wiki/images/math/6/9/d/69d920fe8e1da0543eb63d1097f21754.png) 用一个![\textstyle k \times(n+1)](http://ufldl.stanford.edu/wiki/images/math/8/d/7/8d75ffcaca20bce5c66ae0ffe2facfc3.png) 的矩阵来表示会很方便，该矩阵是将![\theta_1, \theta_2, \ldots, \theta_k](http://ufldl.stanford.edu/wiki/images/math/1/f/f/1ff687194349ee543cd4f1baa7bcaa58.png) 按行罗列起来得到的，如下所示：
![\theta = \begin{bmatrix}\mbox{---} \theta_1^T \mbox{---} \\\mbox{---} \theta_2^T \mbox{---} \\\vdots \\\mbox{---} \theta_k^T \mbox{---} \\\end{bmatrix}](http://ufldl.stanford.edu/wiki/images/math/a/b/4/ab4ba0d1df4b93696eec7d8bef86e9cd.png)
## 代价函数
现在我们来介绍 softmax 回归算法的代价函数。在下面的公式中，![\textstyle 1\{\cdot\}](http://ufldl.stanford.edu/wiki/images/math/b/2/7/b279688f53460dc80e6a81235beee14d.png) 是示性函数，其取值规则为：
` 值为真的表达式 `
， ![\textstyle 1\{](http://ufldl.stanford.edu/wiki/images/math/2/5/4/25481caeef48c5fa12aa22988d931716.png) 值为假的表达式![\textstyle \}=0](http://ufldl.stanford.edu/wiki/images/math/1/1/2/11274c6dbf36bae0b02acd07555f4ce7.png)。举例来说，表达式![\textstyle 1\{2+2=4\}](http://ufldl.stanford.edu/wiki/images/math/6/8/1/68118d4cdfdbe134b420cc4031f3c46e.png) 的值为1 ，![\textstyle 1\{1+1=5\}](http://ufldl.stanford.edu/wiki/images/math/4/5/f/45f90c8f2e9a8d2f00dff993e45c9dbd.png)的值为 0。我们的代价函数为：
![\begin{align}J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{k}  1\left\{y^{(i)} = j\right\} \log \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)} }}\right]\end{align}](http://ufldl.stanford.edu/wiki/images/math/7/6/3/7634eb3b08dc003aa4591a95824d4fbd.png)
值得注意的是，上述公式是logistic回归代价函数的推广。logistic回归代价函数可以改为：
![\begin{align}J(\theta) &= -\frac{1}{m} \left[ \sum_{i=1}^m   (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) + y^{(i)} \log h_\theta(x^{(i)}) \right] \\&= - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=0}^{1} 1\left\{y^{(i)} = j\right\} \log p(y^{(i)} = j | x^{(i)} ; \theta) \right]\end{align}](http://ufldl.stanford.edu/wiki/images/math/5/4/9/5491271f19161f8ea6a6b2a82c83fc3a.png)
可以看到，Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 ![\textstyle k](http://ufldl.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个可能值进行了累加。注意在Softmax回归中将![\textstyle x](http://ufldl.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 分类为类别![\textstyle j](http://ufldl.stanford.edu/wiki/images/math/2/3/5/235c5146ab110558897640c34dad7d97.png) 的概率为：
![p(y^{(i)} = j | x^{(i)} ; \theta) = \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}} }](http://ufldl.stanford.edu/wiki/images/math/a/2/e/a2e69ec139cdd4828130c175d990d4e3.png).
对于 ![\textstyle J(\theta)](http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png) 的**最小化问题，目前还没有闭式解法**。因此，我们使用迭代的优化算法（例如**梯度下降法，或 L-BFGS**）。经过求导，我们得到梯度公式如下：
![\begin{align}\nabla_{\theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = j\}  - p(y^{(i)} = j | x^{(i)}; \theta) \right) \right]  }\end{align}](http://ufldl.stanford.edu/wiki/images/math/5/9/e/59ef406cef112eb75e54808b560587c9.png)
让我们来回顾一下符号 "![\textstyle \nabla_{\theta_j}](http://ufldl.stanford.edu/wiki/images/math/b/8/2/b82b3c09d8bae5495cd4f9e6dedb8710.png)" 的含义。![\textstyle \nabla_{\theta_j} J(\theta)](http://ufldl.stanford.edu/wiki/images/math/5/1/4/514656111f9d351a9e2260d7630ea95b.png) 本身是一个向量，它的第 ![\textstyle l](http://ufldl.stanford.edu/wiki/images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 个元素![\textstyle \frac{\partial J(\theta)}{\partial \theta_{jl}}](http://ufldl.stanford.edu/wiki/images/math/3/f/8/3f886a15270b25ffd60003f2d037bcc4.png) 是![\textstyle J(\theta)](http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png)对![\textstyle \theta_j](http://ufldl.stanford.edu/wiki/images/math/3/7/e/37e2eaf89c7b1f26381f438a0367099a.png) 的第 ![\textstyle l](http://ufldl.stanford.edu/wiki/images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 个分量的偏导数。
有了上面的偏导数公式以后，我们就可以将它代入到梯度下降法等算法中，来最小化 ![\textstyle J(\theta)](http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png)。 例如，在梯度下降法的标准实现中，每一次迭代需要进行如下更新:![\textstyle \theta_j := \theta_j - \alpha \nabla_{\theta_j} J(\theta)](http://ufldl.stanford.edu/wiki/images/math/a/b/8/ab87ee11f99bda3dc7485ac1f009e5a4.png)(![\textstyle j=1,\ldots,k](http://ufldl.stanford.edu/wiki/images/math/8/3/a/83aaa94ba392e98b15d29ed67fdaae12.png)）。
当实现 softmax 回归算法时， 我们通常会使用上述代价函数的一个改进版本。具体来说，就是和权重衰减(weight decay)一起使用。我们接下来介绍使用它的动机和细节。
## Softmax回归模型参数化的特点
Softmax 回归有一个不寻常的特点：它有一个“冗余”的参数集。为了便于阐述这一特点，假设我们从参数向量 ![\textstyle \theta_j](http://ufldl.stanford.edu/wiki/images/math/3/7/e/37e2eaf89c7b1f26381f438a0367099a.png) 中减去了向量![\textstyle \psi](http://ufldl.stanford.edu/wiki/images/math/2/0/0/200d05b77b27ed6a0aa466165f660b64.png)，这时，每一个![\textstyle \theta_j](http://ufldl.stanford.edu/wiki/images/math/3/7/e/37e2eaf89c7b1f26381f438a0367099a.png) 都变成了![\textstyle \theta_j - \psi](http://ufldl.stanford.edu/wiki/images/math/9/b/8/9b8020b23b66a91888062b4a9d8902c5.png)(![\textstyle j=1, \ldots, k](http://ufldl.stanford.edu/wiki/images/math/8/3/a/83aaa94ba392e98b15d29ed67fdaae12.png))。此时假设函数变成了以下的式子：
![\begin{align}p(y^{(i)} = j | x^{(i)} ; \theta)&= \frac{e^{(\theta_j-\psi)^T x^{(i)}}}{\sum_{l=1}^k e^{ (\theta_l-\psi)^T x^{(i)}}}  \\&= \frac{e^{\theta_j^T x^{(i)}} e^{-\psi^Tx^{(i)}}}{\sum_{l=1}^k e^{\theta_l^T x^{(i)}} e^{-\psi^Tx^{(i)}}} \\&= \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}}}.\end{align}](http://ufldl.stanford.edu/wiki/images/math/d/8/0/d8076908fb40b49db821dc410b03700f.png)
换句话说，从 ![\textstyle \theta_j](http://ufldl.stanford.edu/wiki/images/math/3/7/e/37e2eaf89c7b1f26381f438a0367099a.png) 中减去![\textstyle \psi](http://ufldl.stanford.edu/wiki/images/math/2/0/0/200d05b77b27ed6a0aa466165f660b64.png) 完全不影响假设函数的预测结果！这表明前面的 softmax 回归模型中存在冗余的参数。更正式一点来说， Softmax 模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数![\textstyle h_\theta](http://ufldl.stanford.edu/wiki/images/math/3/2/b/32b8c2324fe254830c693b4e9a62cad6.png)。
进一步而言，如果参数 ![\textstyle (\theta_1, \theta_2,\ldots, \theta_k)](http://ufldl.stanford.edu/wiki/images/math/6/b/8/6b8d6eff611b23105ff5876c348e19b2.png) 是代价函数![\textstyle J(\theta)](http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png) 的极小值点，那么![\textstyle (\theta_1 - \psi, \theta_2 - \psi,\ldots,\theta_k - \psi)](http://ufldl.stanford.edu/wiki/images/math/e/d/3/ed3ccb28d5145b361d3396fca429a751.png) 同样也是它的极小值点，其中![\textstyle \psi](http://ufldl.stanford.edu/wiki/images/math/2/0/0/200d05b77b27ed6a0aa466165f660b64.png) 可以为任意向量。因此使![\textstyle J(\theta)](http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png) 最小化的解不是唯一的。（有趣的是，由于![\textstyle J(\theta)](http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png) 仍然是一个凸函数，因此梯度下降时不会遇到局部最优解的问题。但是 Hessian 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题）
注意，当 ![\textstyle \psi = \theta_1](http://ufldl.stanford.edu/wiki/images/math/5/0/6/5068a95b00ca4021de31c92d1b9265eb.png) 时，我们总是可以将![\textstyle \theta_1](http://ufldl.stanford.edu/wiki/images/math/f/5/3/f538938bbd1f7000bcc2c9d990d53632.png)替换为![\textstyle \theta_1 - \psi = \vec{0}](http://ufldl.stanford.edu/wiki/images/math/3/1/9/319e4c877c28268136c857140c74ac9b.png)（即替换为全零向量），并且这种变换不会影响假设函数。因此我们可以去掉参数向量![\textstyle \theta_1](http://ufldl.stanford.edu/wiki/images/math/f/5/3/f538938bbd1f7000bcc2c9d990d53632.png) （或者其他![\textstyle \theta_j](http://ufldl.stanford.edu/wiki/images/math/3/7/e/37e2eaf89c7b1f26381f438a0367099a.png) 中的任意一个）而不影响假设函数的表达能力。实际上，与其优化全部的![\textstyle k\times(n+1)](http://ufldl.stanford.edu/wiki/images/math/8/d/7/8d75ffcaca20bce5c66ae0ffe2facfc3.png) 个参数![\textstyle (\theta_1, \theta_2,\ldots, \theta_k)](http://ufldl.stanford.edu/wiki/images/math/6/b/8/6b8d6eff611b23105ff5876c348e19b2.png) （其中![\textstyle \theta_j \in \Re^{n+1}](http://ufldl.stanford.edu/wiki/images/math/8/6/6/8666911627e8b85075b73dc6a733370c.png)），我们可以令![\textstyle \theta_1 =\vec{0}](http://ufldl.stanford.edu/wiki/images/math/2/c/8/2c83f9d4b80eed256b1802eb58e42d7c.png)，只优化剩余的![\textstyle (k-1)\times(n+1)](http://ufldl.stanford.edu/wiki/images/math/9/f/8/9f8501030105bc8082adfcbee57888f1.png) 个参数，这样算法依然能够正常工作。
在实际应用中，为了使算法实现更简单清楚，往往保留所有参数 ![\textstyle (\theta_1, \theta_2,\ldots, \theta_n)](http://ufldl.stanford.edu/wiki/images/math/4/4/8/448d6ac6c1934448db184f3ef3c78623.png)，而不任意地将某一参数设置为 0。但此时我们需要对代价函数做一个改动：加入权重衰减。权重衰减可以解决 softmax 回归的参数冗余所带来的数值问题。
## 9.Softmax回归与Logistic 回归的关系
当类别数 ![\textstyle k = 2](http://ufldl.stanford.edu/wiki/images/math/4/0/9/409483805c4d8c79f734a131d859b9f7.png) 时，softmax 回归退化为 logistic 回归。这表明 softmax 回归是 logistic 回归的一般形式。具体地说，当![\textstyle k = 2](http://ufldl.stanford.edu/wiki/images/math/4/0/9/409483805c4d8c79f734a131d859b9f7.png) 时，softmax 回归的假设函数为：
![\begin{align}h_\theta(x) &=\frac{1}{ e^{\theta_1^Tx}  + e^{ \theta_2^T x^{(i)} } }\begin{bmatrix}e^{ \theta_1^T x } \\e^{ \theta_2^T x }\end{bmatrix}\end{align}](http://ufldl.stanford.edu/wiki/images/math/e/3/2/e32efab7bff7353e04775b030af0dae9.png)
利用softmax回归参数冗余的特点，我们令 ![\textstyle \psi = \theta_1](http://ufldl.stanford.edu/wiki/images/math/5/0/6/5068a95b00ca4021de31c92d1b9265eb.png)，并且从两个参数向量中都减去向量![\textstyle \theta_1](http://ufldl.stanford.edu/wiki/images/math/f/5/3/f538938bbd1f7000bcc2c9d990d53632.png)，得到:
![\begin{align}h(x) &=\frac{1}{ e^{\vec{0}^Tx}  + e^{ (\theta_2-\theta_1)^T x^{(i)} } }\begin{bmatrix}e^{ \vec{0}^T x } \\e^{ (\theta_2-\theta_1)^T x }\end{bmatrix} \\&=\begin{bmatrix}\frac{1}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\\frac{e^{ (\theta_2-\theta_1)^T x }}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } }\end{bmatrix} \\&=\begin{bmatrix}\frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\1 - \frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\\end{bmatrix}\end{align}](http://ufldl.stanford.edu/wiki/images/math/b/8/1/b81d6e553283fadddbe29fe55226fb38.png)
因此，用 ![\textstyle \theta'](http://ufldl.stanford.edu/wiki/images/math/4/1/8/418c4d9ed2c50151474385a534eb3537.png)来表示![\textstyle \theta_2-\theta_1](http://ufldl.stanford.edu/wiki/images/math/f/1/d/f1dd3fd83eddc4b396b62c33c8a637ac.png)，我们就会发现 softmax 回归器预测其中一个类别的概率为 ![\textstyle \frac{1}{ 1  + e^{ (\theta')^T x^{(i)} } }](http://ufldl.stanford.edu/wiki/images/math/b/5/6/b56e13a98d2fe77d4bc5fd0c004befed.png)，另一个类别概率的为![\textstyle 1 - \frac{1}{ 1 + e^{ (\theta')^T x^{(i)} } }](http://ufldl.stanford.edu/wiki/images/math/4/e/3/4e3deb50277418cc3eb445aa9aa7ac46.png)，这与 logistic回归是一致的。
## **如何获得回归解？**
回归问题是统计学里面最基础的问题。在统计学里面，一般采用最大似然和最小二乘法直接导出解析解。具体可以参考任何一般统计学的教材。其解析解里面有一个矩阵的逆。求逆和伪逆运算有一些快速算法可以利用。所以对于数据量小的回归问题，直接用解析解就可以快速的得到模型的参数。而对于数据挖掘，海量数据导致内存的开销巨大，这时候直接求解析解是不现实的。于是，在机器学习相关的教程里面，对于回归问题，描述的都是迭代算法。基于随机梯度下降的迭代算法的好处是，内存开销小。
## **如何正确选择回归模型？**
当你只知道一个或两个技术时，生活往往很简单。我知道的一个培训机构告诉他们的学生，如果结果是连续的，就使用线性回归。如果是二元的，就使用逻辑回归！然而，在我们的处理中，可选择的越多，选择正确的一个就越难。类似的情况下也发生在回归模型中。
在多类回归模型中，基于自变量和因变量的类型，数据的维数以及数据的其它基本特征的情况下，选择最合适的技术非常重要。以下是你要选择正确的回归模型的关键因素：
- 数据探索是构建预测模型的必然组成部分。在选择合适的模型时，比如识别变量的关系和影响时，它应该首选的一步。
- 比较适合于不同模型的优点，我们可以分析不同的指标参数，如统计意义的参数，R-square，Adjusted R-square，AIC，BIC以及误差项，另一个是[Mallows' Cp](http://support.minitab.com/en-us/minitab/17/topic-library/modeling-statistics/regression-and-correlation/goodness-of-fit-statistics/what-is-mallows-cp/)准则。这个主要是通过将模型与所有可能的子模型进行对比（或谨慎选择他们），检查在你的模型中可能出现的偏差。
- 交叉验证是评估预测模型最好额方法。在这里，将你的数据集分成两份（一份做训练和一份做验证）。使用观测值和预测值之间的一个简单均方差来衡量你的预测精度。
- 如果你的数据集是多个混合变量，那么你就不应该选择自动模型选择方法，因为你应该不想在同一时间把所有变量放在同一个模型中。
- 它也将取决于你的目的。可能会出现这样的情况，一个不太强大的模型与具有高度统计学意义的模型相比，更易于实现。
- 回归正则化方法（Lasso，Ridge和ElasticNet）在高维和数据集变量之间多重共线性情况下运行良好。
**原文链接：**[7 Types of Regression Techniques you should know!](http://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)（译者/刘帝伟 审校/刘翔宇、朱正贵 责编/周建丁） 
**译者简介：**[刘帝伟](http://blog.csdn.net/dream_angel_z)，中南大学软件学院在读研究生，关注机器学习、数据挖掘及生物信息领域。
