# 子空间：群论的角度解释无监督深度学习 - wishchinYang的专栏 - CSDN博客
2015年04月23日 22:24:10[wishchin](https://me.csdn.net/wishchin)阅读数：2632
参考：
        论文把DL的非监督学习映射为群，是为轨道——稳定集理论。
        DL的群映射：[轨道——稳定集理论](http://www.math.uchicago.edu/~may/VIGRE/VIGRE2011/REUPapers/MarkH.pdf)
《The Group theoretic  perspective on unSupervised- DeepLearning》
        参考：[http://en.wikipedia.org/wiki/Group_action](http://en.wikipedia.org/wiki/Group_action)
        一些现代的典型-特定神经网络，比如流行的深度学习，在信号——机器视觉、音频、文本 方面取得破纪录的成就。同时，对DL的原理的探索也更加让人感兴趣。这篇文章，通过群理论来解释深度学习。首先，我们先假设一层非监督与处理层可以 按照 orbit-stabilizer 原则 解释，然后我们 概括 如何好同样的原则适用于多层网络。
       我们聚焦于使神经网络复活的两条原则：
             Geoff Hinton 总结（[http://www.iro.umontreal.ca/~bengioy/dlbook/](http://www.iro.umontreal.ca/~bengioy/dlbook/) ）如下：在计算 图计算的基础上 做 机器视觉。换句话说，如果一个网络 产生了一个好的生成式模型，那么这个模型可以用于分类。
             每一次训练一层网络，而不是训练整个网络。
       在每一次训练过程中，训练层连接到一个暂时输出层、然后训练用于重新产生输出（**i.e to solve P1**)） 的权值。这种步骤—分层执行，从第一个隐含层开始，逐步深入到深层——通常被称为预训练( see Hintonet al. (2006); Hinton (2007); Salakhutdinov & Hinton (2009);
 Bengio et al. (in preparation) )，这个训练结果层被称为 自编码器。图标1 显示了自编码器的示意图，它的权值经过网络学习为W1. 随后，当授予一个输入f，网络将产生一个输出f'  约等于 f。此时 输出为W2 的权值同时被抛弃。
       这是P1 原理的另一/候补 描述。如上所述的一个自编码单元，映射到他自身的一个输入空间。进而，学习之后，他被定义为 输入* f* 的一个“稳定子/稳定集”。现在，输入信号一般分解为特征。满足“P1
 ” 理论意味着学习器（学习的配置）可以重生成这些特征，图标1(b)  阐述 预训练行为。若隐含层学习了特征f1, f2... ，其中之一，设fi   重回作为输入，输出 必定为自身 fi。换句话说，学习一个特征等同于 寻找一个 稳定自身的 变换函数（一个自身不动置换类）。
![](https://img-blog.csdn.net/20150423223423617?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lzaGNoaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
         Figure 1: (a) W1 is preserved, W2 discarded (b) Post-learning, each feature is stabilized(c)Alternate ways of decomposing a signal into simpler features. The neurons could potentially
 learn features in the top row, or the bottom row. Almost surely, **the simpler ones** (bottom row) are learned.
       稳定子/稳定集 的想法  启示了这一个在群作用原理中的 类似推理：**轨道自稳定**。
       假设G是一个作用在集合***X***上的群 通过 使自身的点集**环绕集合**（例如：一个 作用于一个欧式平面的 2X2 可逆矩阵）。考虑：x&X，令Ox 为一个 所有的可通过此群变化达到此种状态的x的集合，Ox被称为 **轨道**。其中的一个子集可以使k 保持不变性，这个子集Sx（也被成为**子群**），是x
 的一个**稳定子**。   若 可以定义一个群的量的概念，Sx 和Ox 存在 一个逆反关系，即使这秉持着： x 本质上其实是一个子集（而不是成为一个点）。比如：对于有限群，|Ox| 和 |Sx| 的生成群 为一个**序集**。
（参考：1.A transformation T is called a stabilizer of an input f , if f0 = T( f ) = f .
2.Mathematically, the orbit Ox of  an element x 2 X under the action of a group G, is defined as the set{![](https://img-blog.csdn.net/20150423223731497?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lzaGNoaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
 }.
**轨道** 和**稳定子/稳定集**的各层 之间的
 逆反关系 在我们 重返分析DL时占据中心角色。有很多可能的方法分解信号到更小的特征，图表1(c)阐述了这一点：一个矩形可以分解为 L形状特征 和 线特征。
         迄今为止 所有的实验 显示：一个神经网络 更倾向于 学习边特征。但是为什么？为了回答这个，假想一下来自于群的自编码器的空间（被视为输入变换）。一旦稳定子/稳定集
 产生 则批学习迭代停止。 大概的说：若搜索是一个马尔科夫链（或者一个启发链比如 蒙特卡洛马尔科夫），若稳定子/稳定集 越大，越容易碰撞。 群结构显示越大的稳定子/稳定集
 倾向于小的轨道。直觉显示拥有更简单的特征，就有更小的轨道。例如：线分割 在线性变化下产生更多可能的形状 而不是花状的形状。一个自编码器首先 应该学习这种简单特征，这 被大多数实验 证实（see Lee et al. (2009)).）。
         这个直觉**倾向于 一个多层方案**。每一层 用一个大稳定子/稳定集
 发现一个特征 。但是除了第一层，输入已经和样本层不在一个空间。一个 在这个新空间的“简单”特征 对应了一个 相对于原始空间更复杂的形状。这种过程随着层数增加而增加，从这种局部的模型表示 我们得到了已学习的高层表示。
引用文章：
         Bengio, Yoshua, Goodfellow, Ian, and Courville, Aaron. Deep learning. In Deep Learning. MIT Press, in preparation. URL        http://www.iro.umontreal.ca/~bengioy/dlbook/.
         Hinton, Geoffrey E. To recognize shapes, first learn to generate images. Progress in brain research,165:535–547, 2007.
         Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.
         Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning,
 pp. 609–616. ACM, 2009.
         Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In International Conference on Artificial Intelligence and Statistics, pp. 448–455, 2009.
**后记：**
         文章同态映射 形容的 很清晰，不过没有把 原因说出来啊...比如如何训练时生成稳定的小轨道，如何在轨道内划分轨道？
后记：
        此篇论文把DL的非监督学习映射为群，是为轨道——稳定集理论。
        DL的**群映射**：[轨道——稳定集理论](http://www.math.uchicago.edu/~may/VIGRE/VIGRE2011/REUPapers/MarkH.pdf)
**不动置换类**：设G是1,2,...,n 的置换群。若k是1…n中的某个元素，G中使 k 保持不变的置换的全体，记以 Zk，叫做 G 中使 k 保持不动的置换类，简称**k不动置换类**。
性质：        
        群G中关于 k 的不动置换类 Z*k* 是G的一个子群。
证明：
       封闭性：p1，p2分别是使k不动的两个置换，即p1，p2属于Zk，则p1p2属于Zk。
       结合律：对于群结合律成立，Zk属于G，故Zk中元素结合律成立。
       单位元：群G的单位元属于Zk，也是Z的单位元。
       逆元素：p属于Zk使得k保持不变，p的逆元属于G也使k不变，故逆元存在。
       因此 Z*k* 本身也是一个群，是群G的一个子群。
    若G是N={1,2,...,n}上的置换群，G在N上可以引出不同的等价类，则不同等价类的个数为

**参考：**
[Cayley](http://baike.baidu.com/view/482202.htm)定理又称凯莱定理，在群论中，以阿瑟·凯莱命名，声称所有群 G 同构于在 G 上的对称群的子群。这可以被理解为G在G的元素上的群作用的一个例子。集合 G 的置换是任何从 G 到 G 的双射函数；所有这种函数的集合形成了在函数复合下的一个群，叫做“G 上的对称群”并写为 Sym(G)。
        凯莱定理通过把任何[群](http://baike.baidu.com/view/48541.htm)(包括无限群比如 (R,+))都当作某个底层集合的置换群，把所有群都放在了同一个根基上。因此，对[置换群](http://baike.baidu.com/view/1879054.htm)成立的定理对于一般群也成立。
### 定理理解：
        此定理说明用n-1条边将n个一致的顶点连接起来的[连通图](http://baike.baidu.com/view/3148644.htm)的个数为n^(n-2)，也可以这样[理解](http://baike.baidu.com/view/58766.htm)，将n个城市连接起来的树状公路网络有n^(n-2)种[方案](http://baike.baidu.com/view/556358.htm)。所谓树状，指的是用n-1条边将n个顶点构成一个连通图。当然，建造一个树状的公路网络将n个城市连接起来，应求其中长度最短、造价最省的一种，或效益最大的一种。Cayley定理只是说明可能方案的数目。
       这对特征提取个数的理解呢？
       待续................
