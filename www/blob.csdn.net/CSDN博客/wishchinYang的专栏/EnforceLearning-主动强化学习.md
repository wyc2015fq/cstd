# EnforceLearning-主动强化学习 - wishchinYang的专栏 - CSDN博客
2016年06月04日 14:11:49[wishchin](https://me.csdn.net/wishchin)阅读数：1731
前言：
         被动学习Agent由固定的策略决定其行为。主动学习Agent必须自己决定采取什么行动。
        具体方法是：
             Agent将要学习一个包含所有行动结果概率的完整模型，而不仅仅是固定策略的模型；
            接下来，Agent自身要对行动做出选择（ 它需要学习的函数是由最优策略所决定的，这些效用遵循 Berman方程 ）；
            最后的问题是每一步要做什么（在获得了对于学习到的模型而言最优的效用函数U之后，Agent能够通过使期望最大化的单步前瞻提取一个最优行动；或者它使用迭代策略，最优策略已经得到，所以它应该简单的执行最优策略所建议的行动）。
           参考小品文：[强化学习的分类](https://blog.csdn.net/qq_30615903/article/details/80765106)。和上一篇：[EnforceLearning-在线学习-被动强化学习](https://blog.csdn.net/wishchin/article/details/51558372)/评价学习。画图挺好：[深度学习进阶之路-从迁移学习到强化学习](https://blog.csdn.net/linolzhang/article/details/72890618)。
![](https://img-blog.csdn.net/2018062117414024?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
一、完整模型
         Passive-ADP-Agent所使用的简单学习机制将做的很好。
二、探索
 1、贪婪算法
                   一个ADP Agent在搜索路径时，每一步都遵循所学习的模型的最优策略的建议，被称为 贪婪Agent。
                   一般问题：选择最优策略是如何导致非最优结果的？ 答案是：学习到的模型与真实环境中的模型并不相同；因而学习到的模型的最优可能不是真实环境中的最优。不幸的是，Agent并不知道真实环境是什么，所以它不能针对真实环境计算最优行动。
 2、如何改进？
                     折中：贪婪Agent忽略的事实是行动不仅根据当前学习到的模型提供回报，他们也通过影响所接受的感知信息对真实模型的学习做出贡献。通过改进模型，Agent将在未来得到更高的回报。
                     方法：Agent必须在充分利用信息exploitation 以最大化回报——反映在其当前效用估计上，和探索exploration以及最大化长期利益之间进行折中。
 单纯的充分利用信息要冒墨守成规的风险；那么单纯的探索对于提高一个人的知识是毫无用处的。
 3、GLIE Greedy in the limit of infinite exploration
                      寻找最优搜索策略，在统计决策理论领域得到了深入的研究。对精确求解最优策略并没有一个固定的方法，但是可以提出一个合理的方案最终导致Agent的最优行动。技术上，任何这样的方案在无穷探索的极限下都必然是贪婪的。
                      一个GLIE方案必须在每个状态下的每个行动进行无限制次数的尝试，以避免一系列不常见的糟糕结果而错过最优行动的概率。一个ADP Agent使用这样的方案最终将学习到真实的环境模型。 一个GLIE方案最终还必须变得贪婪，以使得Agent的行动对于学习到（此时等同于真实的）真实模型而言 是最优的。
4、几种尝试
                        一种最简单的方式是：让Agent在1/t的时间片段内选择一个随机行动，而其他时刻走遵循贪婪策略。简单时序片段脱离法
                      另一种更为有效的方法是：给Agent很少尝试的行动进行加权，同时避免那些已经确信的具有最低效用的行动，实现方法为 改变约束方程，以便给相对来说尚未探索的状态——行动分配更高的效用估计。   本质上，会得到一个关于可能环境的乐观先验估计，并导致Agent 最初的行动过如同整个区域到处散布者几号的回报一样。 
三、学习行动-效用函数
         1、为一个主动ADP Agent构建一个主动时序差分学习，与被动情况最明显的变化是Agent不再有固定的策略，它学习效用函数U时，就需要学习一个模型以便能够通过单步前瞻基于U采取一个行动。
               构建一个主动学习ADP Agent，随着训练序列的时间趋于无穷，TD算法与ADP算法收敛到相同的值。
         2、Q-Learn作为一种时序TD方法，它学习 一种行动-效用表示 而不是学习效用。
后续：
      参考：  [DeepMind用ReinforcementLearning玩游戏](http://blog.csdn.net/wishchin/article/details/42425145)
