# 神经网络激活函数总结 - Keith - CSDN博客





2017年03月29日 14:05:24[ke1th](https://me.csdn.net/u012436149)阅读数：1136
个人分类：[MachineLearning																[deeplearning](https://blog.csdn.net/u012436149/article/category/6416551)](https://blog.csdn.net/u012436149/article/category/6504450)








# 激活函数总结
- `sigmoid`
- `tanh`
- `ReLU`
- `Leaky ReLU`
- `Maxout`
- `ELU`

# sigmoid

数学表示： 


$y = \frac{1}{1+e^{-x}}$
**不建议使用,容易饱和**
# tanh

数学表示： 


$y = \frac{e^x-e^{-x}}{e^x+e^{-x}}$

# ReLU

数学表示： 


$y = max(0, x)$
**建议使用， 但是注意学习率，保证ReLU激活**
# Leaky ReLU

数学表示： 


$y = max(\alpha x, x)$

# Maxout

数学表示： 


$y = max(W_1^Tx, W_2^Tx)$

# ELU

数学表示： 


$y =\begin{cases}&x ,&\ \text{if}~x>0\\&\alpha(\exp(x)-1), &\text{if} ~x\leq0\end{cases}$






