# 两种交叉熵损失函数的异同 - Keith - CSDN博客





2017年04月07日 21:26:56[ke1th](https://me.csdn.net/u012436149)阅读数：4664








在学习机器学习的时候，我们会看到两个长的不一样的交叉熵损失函数。 
**假设我们现在有一个样本**$\{ x,t\}$，这两种损失函数分别是。
- 
$-t_j\text{log}(y_j)$， `t_j`说明样本的`ground-truth`是第`j`类。

- 
$-\sum_it_i\text{log}(y_i)+(1-t_i)\text{log}(1-y_i)$


这两个都是交叉熵损失函数，但是看起来长的却有天壤之别。为什么同是交叉熵损失函数，长的却不一样呢？

**因为这两个交叉熵损失函数对应不同的最后一层的输出。第一个对应的最后一层是softmax，第二个对应的最后一层是sigmoid。**

如果看到这个答案就明白了的话，就没必要往下看了，如果感觉云里雾里的话，请听细细分解。

首先来看信息论中交叉熵的定义： 


$-\int p(x)\text{log}g(x)dx$

交叉熵是用来描述两个分布的距离的，神经网络训练的目的就是使 $g(x)$ 逼近 $p(x)$。
现在来看`softmax`作为最后一层的情况。$g(x)$是什么呢？就是最后一层的输出 $y$ 。$p(x)$是什么呢？就是我们的`one-hot`标签。我们带入交叉熵的定义中算一下，就会得到第一个式子： 


$-t_j\text{log}(y_j)$
- $j$ : 样本$x$属于第$j$类。

再来看`sigmoid`作为最后一层的情况。`sigmoid`作为最后一层输出的话，那就不能吧最后一层的输出看作成一个分布了，因为加起来不为`1`。现在应该将最后一层的每个神经元看作一个分布，对应的 $target$ 属于二项分布(target的值代表是这个类的概率)，那么第 $i$ 个神经元交叉熵为：

$t_i\text{log}(y_i)+(1-t_i)\text{log}(1-y_i)$，所以最后一层总的交叉熵损失函数是

$-\sum_it_i\text{log}(y_i)+(1-t_i)\text{log}(1-y_i)$。

**解释完了，最后总结一下：这两个长的不一样的交叉熵损失函数实际上是对应的不同的输出层。**




