# 理解SVM（三）——扩展到多类 - bigfacesafdasgfewgf - CSDN博客





2014年12月02日 15:44:46[bigface1234fdfg](https://me.csdn.net/puqutogether)阅读数：3645








**理解SVM（三）——扩展到多类**

    前面两个系列分别讲诉了SVM的基本原理和代码实现，如何解决线性不可分情况。这一次我们讲解一下SVM的最后一篇：SVM解决多类分类问题。

# 1. one vs. other

    这种改进SVM的方法简单粗暴，也易于理解。详细来说，比如我们的数据有5个类别。

    Step 1：把类别1的样本定为正样本，其余2，3，4，5类的样本定为负样本。使用SVM分类器，得到一个二类分类器；

    Step 2：把类别2的样本定为正样本，其余1，3，4，5类的样本定为负样本。使用SVM分类器，得到一个二类分类器；


Step 3：把类别3的样本定为正样本，……

    Step 4：把类别4的样本定为正样本，……

    Step 5：把类别5的样本定为正样本，……


    Step 6：对测试样本，依次使用上面训练学习得到的5个分类器，每个分类器都可以得出结论，要么是第i类，要么就是其他（the other），直到我们发现有一个分类器说这样测试样本是我的，即可。

    这样改进方法比较简单，时间复杂度也不是很高。但是，仔细想一下问题来了~有时候会出现5个分类器都说这样测试样本是自己的，或者都是不是自己的……这个时候怎么办？

    如果说都是自己的，这叫分类重叠现象。这个可以选择SVM的间隔最大的那个分类器确定最终结果，或者使用投票的原则；

    如果说都不是自己的，这叫不可分类现象。这个时候就不好解决了，得不到正确的结果。

    此外，这种方法还有一个缺点：认为造成了类不平衡问题。而且，当数据类别很多的时候，other那一类的数据量会是one这一类的好多倍，SVM分类器会严重错误的把结果偏向于other的大类，这个时候就不是不可分类现象了，而是你SVM分类器就给直接分错了……对于类不平衡问题，我们后面的博客还会讲到。

# 2. one vs. one

    这种方法还是把多类问题转化为多个二类问题，只不过每个二类问题中都是“one vs. one”的方式。这个时候就没有类不平衡问题了。

    Step 1：把类别1的样本定为正样本，类别2定为负样本。使用SVM分类器，得到一个二类分类器；

    Step 2：把类别1的样本定为正样本，类别3定为负样本。使用SVM分类器，得到一个二类分类器；


        ……（一共有C(5, 2)=10个分类器）

   Step 11：让测试样本依次通过后面的分类器做判断，每次都会得到是否为第1类还是第2类，然后给5个类别投票，选择最高的作为最后的结果（一个类得票最高为4票）。

    这种方法也有分类重叠的现象。而且，当数据集类别增多的时候，我们需要学习的二类分类器就会很多，比刚才的那种one
 vs. other的方法多很多，时间复杂度受不了~

# 3. DAG方式

    类似于上面“one vs. one”的方法，我们可以把每个二类分类器作为结点，构建一个有向无环图。如下图所示：

![](https://img-blog.csdn.net/20141202161217063?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


图中很形象的描述了该方法的决策过程，即从上而下的分类，每次分类器的结果决定了往左右哪个方向。继续判断，直到到达叶子，每个叶子对应这一个类别。该方法不需要运行每个分类器，提高了测试速度，而且避免了投票数一样大的时候分类重叠现象。但是，该方法的缺点就是“误差累积”，假设第一个节点的分类器就直接分错了，那么后面就是一直跟着错。

# 4. 基于决策树的方法

    其实上面2，3方法需要训练的分类器的数量都是可怕的，基于决策树的方法可以减少学习二类分类器个数的问题。对于一个数据集，我们可以采用一些聚类的方法（例如Kmeas）把数据集分成两个子类，然后对两个子类进一步划分，如此循环，直到子类中只包含一个类别为止。这样，就得到了一个倒立的二叉树。最后，在二叉树各决策节点训练支持向量机分类器，这里我们就可以发现我们需要的分类器已经减少了很多了。这里，我们构造不同的树的结构（不一定是完全二叉树），就会得到不同的方法。但是使用完全二叉树结构时，需要学习的二类分类器数目是最少的。

# 5. 基于纠错输出编码的方法

    假设一个数据集一共有K类，我们使用L种两类分类器（不仅仅是SVM），就会得到L个分类结果，每个结果用+1和-1来表示。这样，对于K类数据集，我们就可以学习到一个K*L的矩阵。

    然后，来了一个测试样本，我们就用同一样的方法得到测试样本的长度为L的向量，拿这个向量和K*L矩阵中的每一行做Hamming distance，距离最小的即为该测试样本的分类结果。







    以上5类方法都各有优缺点，具体应用的时候可以选择效果最好的一种使用。不过现在也已经出现了很多直接面向多类分类的数据挖掘方法。后面我们再一一介绍。

    注意，上文中涉及到了类不平衡学习问题，后面会专门开了专栏来说这类问题的几大解决方法，请持续关注本博客~谢谢~~






