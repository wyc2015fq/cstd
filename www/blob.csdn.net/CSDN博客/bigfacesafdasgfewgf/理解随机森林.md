# 理解随机森林 - bigfacesafdasgfewgf - CSDN博客





2014年12月11日 11:23:59[bigface1234fdfg](https://me.csdn.net/puqutogether)阅读数：2095








**理解随机森林**



**随机森林利用随机的方式将许多决策树组合成一个森林，每个决策树在分类的时候投票决定测试样本的最终类别。**下面我们再详细说一下随机森林是如何构建的。




随机森林主要包括4个部分：随机选择样本；随机选择特征；构建决策树；随机森林投票分类。




# 1.随机选择样本




    给定一个训练样本集，数量为N，我们使用有放回采样到N个样本，构成一个新的训练集。注意这里是有放回的采样，所以会采样到重复的样本。详细来说，就是采样N次，每次采样一个，放回，继续采样。即得到了N个样本。

    然后我们把这个样本集作为训练集，进入下面的一步。




# 2. 随机选择特征




    在构建决策树的时候，我们前面已经讲过如何在一个节点上，计算所有特征的Information Gain（ID3） 或者 Gain Ratio（C4.5），然后选择一个最大增益的特征作为划分下一个子节点的走向。

    但是，在随机森林中，我们不计算所有特征的增益，而是从总量为M的特征向量中，随机选择m个特征，其中m可以等于sqrt(M)，然后计算m个特征的增益，选择最优特征（属性）。注意，这里的随机选择特征是无放回的选择！




** 所以，随机森林中包含两个随机的过程：随机选择样本，随机选择特征。**




# 3. 构建决策树




    有了上面随机产生的样本集，我们就可以使用一般决策树的构建方法，得到一棵分类（或者预测）的决策树。需要注意的是，在计算节点最优分类特征的时候，我们要使用上面的随机选择特征方法。而选择特征的标准可以是我们常见的Information Gain（ID3） 或者 Gain Ratio（C4.5）。




# 4. 随机森林投票分类




    通过上面的三步走，我们可以得到一棵决策树，我们可以重复这样的过程H次，就得到了H棵决策树。然后来了一个测试样本，我们就可以用每一棵决策树都对它分类一遍，得到了H个分类结果。这时，我们可以使用简单的投票机制，或者该测试样本的最终分类结果。




# 5. 优缺点分析




优点：


- 它能够处理很高维度（feature很多）的数据，并且不用做特征选择；

- 由于随机选择样本导致的每次学习决策树使用不同训练集，所以可以一定程度上避免过拟合；




缺点：


- 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合；
- 对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。




































