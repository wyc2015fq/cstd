# 复习机器学习算法：决策树 - bigfacesafdasgfewgf - CSDN博客





2015年03月02日 11:39:47[bigface1234fdfg](https://me.csdn.net/puqutogether)阅读数：924













决策树就是不断选择信息增益最大的属性，进行分类。

核心部分是使用信息增益判断属性的分类性能。信息增益计算如下：

信息熵：

![](https://img-blog.csdn.net/20150302113938854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



允许有多个类别。

![](https://img-blog.csdn.net/20150302114110645?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)







计算所有属性的信息增益，选择最大的作为决策树的根节点。然后，样本分枝，继续判断剩余的属性的信息增益。



信息增益有缺点：信息增益偏袒具有较多值的属性。分裂信息，用增益比率作为衡量标准，如下：





![](https://img-blog.csdn.net/20150302114135495?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

决策树的优点：对于有缺省特征的样本，也能够分类；允许样本特征有一定的错误，鲁棒性好。



缺点：容易过拟合，生成的树太大。（可以使用提前停止树的生长；剪枝；随机森林等方法避免过拟合）



剪枝的方法：先让决策树自由的生长，允许发生过拟合。然后把决策树转换为等价的规则集合，删除那些不对结果有影响的节点。如下：




![](https://img-blog.csdn.net/20150302114206586?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)




重复上面的过程，从底向上，遍历节点。



 参考：

《机器学习》

 http://blog.sina.com.cn/s/blog_4e4dec6c0101fdz6.html

http://www.cnblogs.com/tornadomeet/p/3395593.html








