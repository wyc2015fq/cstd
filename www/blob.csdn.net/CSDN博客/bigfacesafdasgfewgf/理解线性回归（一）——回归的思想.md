# 理解线性回归（一）——回归的思想 - bigfacesafdasgfewgf - CSDN博客





2014年10月09日 14:06:46[bigface1234fdfg](https://me.csdn.net/puqutogether)阅读数：3133








**理解线性回归（一）——回归的思想**



# **1. 经典的线性回归**



    之前介绍的LR回归和SVM算法本质上都和回归有写关联，尤其是LR回归算法。回归的目的是预测数值型的目标值，其核心部分和我们中小学时候学习到的线性拟合是一样的，就是说，假如我们能够建立了回归背后的数学模型，我们便可以根据输入变量来预测输出量。这儿的数学模型就是回归方程，里面的系数就是回归系数。求解这些回归系数的过程就是回归。

    那么如何在给定一组输入x和输出y的情况下，求出回归系数w呢？一个常用的方法就是找出使误差最小的w，这里的误差是指预测y和真实y之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消。所以我们一般使用平方误差。

![](https://img-blog.csdn.net/20141009142147941?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    那么根据上式，我们便可知：给定了数据集X和输出y，我们便可以求出回归系数w。Python代码如下。（注意：这里涉及到矩阵X的求逆运算，我们需要先判断X是否奇异，即如果X的行列式det(X)==0，那么我们是不可以求逆的，这个判断可以使用numpy模块中的linalg.det来计算行列式）



```python
def standRegres(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    xTx = xMat.T*xMat
    if linalg.det(xTx) == 0.0:  #计算矩阵X的行列式，判断X是否奇异，是否可以求逆
        print "This matrix is singular, cannot do inverse"
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws
```

有了回归系数w向量，我们便可以预测y值了。


    上面求w的公式是通过最小化均方误差得到的，所以这是一个无偏估计。如果模型欠拟合的话，我们将不能取得很好的预测效果。这里，我们先介绍一下什么是欠拟合和过拟合。

    给定一个假设空间H，一个假设h属于H，如果存在其他的假设h'也属于H，使得h'在训练集上的错误率比h的大，但是在整个实例分布上，h'的错误率更小，那么我们就说假设h过拟合了训练数据，反之则称为欠拟合。举例：给定7个样本，假如我们用6次的函数去拟合，那么在训练集中，该拟合曲线可能会非常精确的通过这7个点，但是一旦放到测试环节，发现预测的误差很大，这个就是过拟合。如果给定一组数据，我们只是简单的用一次的函数关系，那么就很有可能出现欠拟合。所以一般而言，对于特征集过小的情况，会出现欠拟合；特征集过大的情况，会出现过拟合。这里是理论解释，下面还有一个图可以形象地解释恰当拟合、欠拟合和过度拟合之间的差异。




# **2. 局部加权线性回归**



    那么在回归中，解决欠前文讲的欠拟合的一个有效方法就是：局部加权线性回归Locally Weighted Linear Regression。在LWLR中，我们给待预测点附近的每个点都赋予一定的权重，离预测点越近的点，权重越大；反之，越小。这儿的权重我们一般使用高斯核来做：

![](https://img-blog.csdn.net/20141009145759668?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


记住，以后在设置权重的时候要多多记得高斯核，高斯核中有个参数k需要设置，k控制了权重随着距离衰减的速度。

    引入权重矩阵之后，原先求回归系数w的公式就变成：

![](https://img-blog.csdn.net/20141009145943969?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中W是权重矩阵，用来给每个数据点赋予权重，它是一个方阵，阶数等于样本点的个数。用这样的w计算公式可以得到线性回归函数，Python代码如下：



```python
def lwlr(testPoint,xArr,yArr,k=1.0):
    xMat = mat(xArr); yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye((m)))
    for j in range(m):                      #next 2 lines create weights matrix
        diffMat = testPoint - xMat[j,:]     #
        weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))
    xTx = xMat.T * (weights * xMat)
    if linalg.det(xTx) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws
```

我们可以设置不同的k值，来观察欠拟合和过拟合的效果，如下：


![](https://img-blog.csdn.net/20141009150426704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


    从图中我们可以看出，k=0.01时拟合曲线最能够反映数据的规律信息，而k=0.003时的拟合曲线经过了许多的噪声点，这样是的拟合的模型过于复杂，进而导致了过拟合。于是，我们可以发现：随着模型复杂度的升高，训练误差会越来越小，而测试误差会有个抛物线的变化其趋势，如下图。所以，我们做出最好的预测，我们应该调整模型复杂度来达到测试误差的最小值。

![](https://img-blog.csdn.net/20141009153150729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


    局部加权线性回归也存在问题，计算量比较大。这是因为LWLR在对每个点预测的时候，都必须使用整个数据集。当数据集中的点很多的时候，这个计算量是很恐怖的。而实际上，在对一个点预测的时候，其余很多个点的权重都是为零的，也就是说权重矩阵W是稀疏sparse的，这时候改进点可以从这方面进入。




# **3. 样本矩阵X奇异的情况——小样本问题**



    当样本的特征维数大于样本总数的时候，样本矩阵X奇异，不可以求逆，回归系数w无法求解。这个问题称为小样本问题。解决这个问题有两种思路：从样本的角度和从回归方法的角度。


- 从样本的角度。既然样本的特征维数大于样本的个数，那么我们可以把样本降维，使用PCA方法把样本降到train_total-class_num维，这样就没有维数问题了；
- 从回归方法的角度。岭回归的方法，还有lasso法，前向逐步回归法。

![](https://img-blog.csdn.net/20141009152556374?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)







# **4. 非线性回归模型**



    实际运用中，很多数据集都是非线性的，此时上面的线性模型不能取得较好的预测效果。这里，我们介绍一种新的树构建算法，叫做CART，即分类回归树Classification And Regression Trees。注意，为了防止树的过拟合，我们需要对树进行剪枝。

    CART算法使用二元切分法处理数据，以及建立树。如果特征值大于给定的阈值，那么就放在左子树；反之，放在右子树。借鉴数据结构中的二叉树的构建，这儿的回归树在构建的时候应该也是一个递归的过程。首先，把数据集分成两部分，判断是否满足停止条件。如果满足，返回某类模型的常数；否则，继续在两份数据集上分别递归调用构建树的函数。

    在构建了一个回归树之后，我们还需要判断是否出现了过拟合。我们可以使用测试集上的交叉验证计数来判断是否出现了过拟合。一棵树如果节点过多，表明需要进行剪枝，即通过降低决策树的复杂度来避免过拟合的过程。剪枝有两种：预剪枝，提前终止建树；后剪枝，使用测试集和训练集。

    后剪枝需要用到训练集和测试集。首先，我们用训练集构建一个足够大的树，然后从上往下找到叶节点，用测试集来判断这些即诶但合并是否能降低测试误差，如果可以降低，就合并。




    我们先熟悉如何构建回归树，以及如何对树进行剪枝。具体代码实现日后再理解。




这样我们就理解了如何通过回归来拟合线性和非线性数据。















