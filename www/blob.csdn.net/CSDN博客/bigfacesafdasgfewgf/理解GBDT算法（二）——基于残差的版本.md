# 理解GBDT算法（二）——基于残差的版本 - bigfacesafdasgfewgf - CSDN博客





2015年03月30日 16:18:41[bigface1234fdfg](https://me.csdn.net/puqutogether)阅读数：7301








GBDT算法有两种描述思路，一个是基于残差的版本，一个是基于梯度gradient的版本。这篇我们先说说基于残差的版本。

前面一篇博文已经说了该版本的大致原理，请参考。 
[http://blog.csdn.net/puqutogether/article/details/41957089](http://blog.csdn.net/puqutogether/article/details/41957089)

这篇我们再总结一个几个注意点：
- 这个版本的核心思路：每个回归树学习前面树的残差，并且用shrinkage把学习到的结果大步变小步，不断迭代学习。其中的代价函数是常见的均方差。
- 其基本做法就是：先学习一个回归树，然后“真实值-预测值*shrinkage”求此时的残差，把这个残差作为目标值，学习下一个回归树，继续求残差……直到建立的回归树的数目达到一定要求或者残差能够容忍，停止学习。
- 我们知道，残差是预测值和目标值的差值，这个版本是把残差作为全局最优的绝对方向来学习。
- 这个版本更加适用于回归问题，线性和非线性的均可，而且在设定了阈值之后还可以有分类的功能。
- 当时该版本使用残差，很难处理纯回归以外的问题。版本二中使用梯度，只要建立的代价函数能够求导，那么就可以使用版本二的GBDT算法，例如LambdaMART学习排序算法。
- Shrinkage和梯度下降法中学习步长alpha的关系。shrinkage设小了只会让学习更慢，设大了就等于没设，它适用于所有增量迭代求解问题；而Gradient的步长设小了容易陷入局部最优点，设大了容易不收敛。它仅用于用梯度下降求解。这两者其实没太大关系。



