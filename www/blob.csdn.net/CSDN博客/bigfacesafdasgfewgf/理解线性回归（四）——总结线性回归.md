# 理解线性回归（四）——总结线性回归 - bigfacesafdasgfewgf - CSDN博客





2014年11月07日 10:50:13[bigface1234fdfg](https://me.csdn.net/puqutogether)阅读数：3403








# 理解线性回归（四）——总结线性回归



这一篇我们来总结几个常用的线性回归模型。

    首先，我们给出这么集中线性回归模型的list：




# 1）Ordinary Least Squares 




    这个线性模型是所有线性模型的基础，后面的线性模型要么就是在它的基础上加上更多的约束，一般是对表示系数的约束；要么就是基于它的构造思想。

    总的来说，ordinary least squares目标函数的思想就是让许多特征的线性表示之和与它们的结果（分类标记），之间的误差最小化。这个也是几乎所有线性模型建立目标函数的基础。

    其目标函数如下：

![](https://img-blog.csdn.net/20141110090842791?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


    关于ordinary least squares的模型，我们在上一篇中已经有了详细的介绍。

http://blog.csdn.net/puqutogether/article/details/40399545





# 2）Ridge Regression 




    岭回归的目标函数就是建立在上面的基础上的一种改进版本，但顶多算是个v 0.1版本。为什么呢？因为其几乎完全就是稍微修改了一下目标函数。在误差最小的基础上，又加入了对表示系数矩阵w的L2范数的约束。具体的表达的在前面的前面一篇也介绍过了。

    其目标函数如下：

![](https://img-blog.csdn.net/20141110090921791?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


http://blog.csdn.net/puqutogether/article/details/40863943





# 3）Lasso 




    Lasso也是一种线性模型，其对表示系数加入了稀疏的约束。稀疏的直观解释就是只存在少部分非零值，其余的大部分都是0. 需要说一下的是，系数矩阵（向量）有其独特的表示形式，就指明行号和列号，以及该位置具体的数值就可以，不需要把整个矩阵（向量）full出来。

    有了稀疏的认识，那么我们就可以推断出Lasso模型中对表示系数的约束形式：L0范数。L0范数就是统计向量的非零个数。但是这个约束是NP问题，我们一般不直接使用L0范数来对系数做要求，转换为L1范数来做。，所以Lasso模型的目标函数为：

![](https://img-blog.csdn.net/20141110091535859?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)





# 4）Elastic Net 




    Elastic Net（弹性网线性模型）结合了岭回归和Lasso回归的优点，即在目标函数中对系数的约束既有L1范数，也有L2范数。In other words, 这个模型求出来的表示系数，既有稀疏性，又有正则化约束的特性，继承了岭回归的健壮性。

Elastic Net比较适合多特征样本的预测问题，而且特征之间比较相关。

   其目标函数如下：

![](https://img-blog.csdn.net/20141110092121420?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)








# 5）Multi-task Lasso 




    该线性模型是基于Lasso的，适合一下子学习多个回归问题。所有此时的Y就表示样本矩阵，Multi-Lasso可以针对每个task学习一组表示系数，而且这种学习是jointly的。

    其目标函数如下：

![](https://img-blog.csdn.net/20141110092544967?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)





# 6）Least Angle Regression （LARS）




    这种线性模型适合高维数据。其主要有如下一个特性：

a. 当样本维数远大于样本个数的时候，该模型特别有效；

b. 计算复杂度和ordinary least squares差不多；

b. 当两个样本非常相似的时候，他们的系数学习的过程也很相似；

c. 对噪声敏感；




# 7）LARS Lasso 




    和上一个模型类似，只不过能够得到较为精确的解。




# 8）Orthogonal Matching Pursuit (OMP) 




    这是一个非常有名的求解稀疏表示系数的算法。它可以直接对系数的L0范数约束求解，只不过要给这个算法输入一个用户指定的tolerance。

    其模型如下：

![](https://img-blog.csdn.net/20141110094224040?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVxdXRvZ2V0aGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


    该算法是基于贪婪算法，字典中的原子都是和当前残差相关性最大。




# 9）Bayesian Regression 




    贝叶斯回归模型，就是把前面对系数的正则项约束用概率的形式展现出来。




# 10）Logistic Regression 




    LR回归更加适合分类这种task。该线性模型先通过系数线性表示特征，然后把特征映射到logistic模型中，使得其函数值可以非为0，1两类。

    更多的关于LR回归的介绍已经在前面一篇文章中给出了，参考：

http://blog.csdn.net/puqutogether/article/details/39613299













