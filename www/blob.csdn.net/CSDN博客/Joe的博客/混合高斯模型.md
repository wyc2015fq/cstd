# 混合高斯模型 - Joe的博客 - CSDN博客





2017年03月23日 09:11:46[Joe-Han](https://me.csdn.net/u010089444)阅读数：7402








## **1. 单高斯模型（SGM）**

多维高斯分布的概率密度函数如下式所示：
- $N(x|\mu,\Sigma )=\frac{1}{\sqrt{2\pi |\Sigma |}}e^{-\frac{1}{2}(x-\mu)^{\top }\Sigma^{-1} (x-\mu)}$

对于单高斯模型，由于可以明确训练样本是否属于该高斯模型，因此 $\mu$ 通常由**训练样本均值**代替，$\Sigma$ 通常由**样本方差**代替。单高斯模型可用于进行二分类问题，例如，对于任意输入$x$，$N(x;\mu,\Sigma )$表示该样本为正样本（负样本） 的概率。然后根据提前设置的阈值来进行分类。在几何图形上，单高斯分布在二维平面上近似于椭圆，在三维平面上近似于椭球状，下图为二维空间的例子： 

![图片名称](https://img-blog.csdn.net/20170322165404520?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


## **2. 混合高斯模型（GMM）**

虽然单⾼斯分布有⼀些重要的分析性质，但是当它遇到实际数据集时，也会有巨⼤的局限性。考虑下图给出的例⼦。这个数据集被称为“⽼忠实间歇喷泉”数据集，由美国黄⽯国家公园的⽼忠实间歇喷泉的272次喷发的测量数据组成。每条测量记录包括喷发持续了⼏分钟（横轴）和距离下次喷发间隔了⼏分钟（纵轴）。我们看到数据集主要聚集在两⼤堆中，⼀个简单的⾼斯分布不能描述这种结构，⽽两个⾼斯分布的线性叠加可以更好地描述这个数据集的特征。 

![图片名称](https://img-blog.csdn.net/20170322185000733?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


混合高斯模型假设**数据服从混合高斯分布**，通过将单个⾼斯分布进⾏线性组合而得到的模型，被称为混合高斯模型（Gaussian Mixture Model）。下图是⼀维混合⾼斯分布的例⼦，蓝⾊曲线给出了三个⾼斯分布，红⾊曲线表⽰它们的和。通过使⽤⾜够多的⾼斯分布，并且调节它们的均值和⽅差以及线性组合的系数，⼏乎所有的连续概率密度都能够以任意的精度近似。 

![图片名称](https://img-blog.csdn.net/20170322185401051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


混合高斯模型通常用于聚类，假设模型由 K 个 高斯分布组成（即数据包含K个类），每个高斯分布称为一个“Component”，这些 Component 线性加成在一起就组成了 GMM 的概率密度函数：
- $p(x)=\sum_{k=1}^{K}p(k)p(x|k)=\sum_{k=1}^{K}\pi_{k}N(x|\mu_{k},\Sigma_{k})$

其中，参数$\pi_{k}$被称为混合系数（mixing coefficients），$\sum_{k=1}^{K}\pi_{k}=1$ 且 $0 \leqslant \pi_{k}\leqslant 1$。我们把$\pi_{k} = p(k)$看成选择第k个分布的**先验概率**， 把密度$N(x|\mu_{k},\Sigma_{k}) = p(x|k)$看成给定第k个分布时x的概率。

下图给出了⼆维空间中3个⾼斯分布混合的例⼦。(a)每个混合分量的常数概率密度轮廓线，其中三个分量分别被标记为红⾊、蓝⾊和绿⾊，且混合系数的值在每个分量的下⽅给出。(b)混合分布的边缘概率密度p(x)的轮廓线。(c)概率分布p(x)的⼀个曲⾯图。


![图片名称](https://img-blog.csdn.net/20170322191234275?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


为了便于引入EM算法，我们通过隐变量来描述GMM。我们引⼊⼀个K维⼆值随机变量z(称为隐变量)，这个变量采⽤了“1-of-K”表⽰⽅法，其中⼀个特定的元素$z_{k} $等于1，其余所有的元素等于0。z的边缘概率分布根据混合系数k进⾏赋值，即:
- $p(z_{k}=1)=\pi_{k}$

类似地，给定z的⼀个特定的值，x的条件概率分布是⼀个⾼斯分布
- $p(x|z_{k}=1)=N(x|\mu_{k},\Sigma_{k})$

联合概率分布为$p(z)p(x | z)$，从⽽x的边缘概率分布可以通过将联合概率分布对所有可能的z求和的⽅式得到，求和式的各项的结果就分别代表样本x 属于各个类的概率，如下所示：
- $p(x)=\sum_{z}p(z)p(x|z)=\sum_{k=1}^{K}\pi_{k}N(x|\mu_{k},\Sigma_{k})$

现在假设我们有 N 个数据点，并假设它们服从某个分布（记作 $p(x)$），现在要确定里面的一些参数的值，.在 GMM 中，我们就需要确定 $\pi_k$、$\mu_k$ 和$ \Sigma_k$ 这些参数。 我们的想法是，找到这样一组参数，它所确定的概率分布生成这些给定的数据点的概率最大，而这个概率实际上就等于$ \prod_{i=1}^N p(x_i)$ ，这个乘积就是似然函数。通常单个点的概率都很小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，因此我们通常会对其取对数，把乘积变成加和 $\sum_{i=1}^N \ln p(x_i)$，得到 log-likelihood function ：
- $\sum_{i=1}^N \ln p(x_i) =\sum_{i=1}^N \ln \begin{Bmatrix} \sum_{k=1}^{K}\pi_k N(x_i|\mu_{k},\Sigma_{k}) \end{Bmatrix}$

## **3. EM算法求解参数**

本文不涉及EM算法的详细介绍，简单来讲，EM的意思是“Expectation Maximization”：该过程分为两步：第一步,假设知道各个高斯分布的参数（可以初始化一个，或者基于上一步迭代结果），去估计每个高斯模型的隐变量；第二步,基于估计的隐变量，回过头再去确定高斯分布的参数。重复这两个步骤，直到收敛。

接下来我们需要最大化如下似然函数（通常的做法是求导并令导数等于零，然后解方程），亦即找到这样一组参数值，它让似然函数取得最大值，我们就认为这是最合适的参数，这样就完成了参数估计的过程。
- $\sum_{i=1}^N \ln p(x_i) =\sum_{n=1}^N \ln \begin{Bmatrix} \sum_{k=1}^{K}\pi_k N(x_i|\mu_{k},\Sigma_{k}) \end{Bmatrix}$

由于在对数函数里面又有加和，我们没法直接用求导解方程的办法直接求得最大值。采用EM算法的思想，混合高斯模型的参数确定可分为以下两步：

**（1）E-step**

 估计数据由每个 Component 生成的概率（并不是每个 Component 被选中的概率）：对于每个数据 $x_i$ 来说，它由第 k 个 Component 生成的概率为
- $ \displaystyle \gamma(i, k) = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j\mathcal{N}(x_i|\mu_j, \Sigma_j)}$

由于式子里的 $\mu_k$ 和 $\Sigma_k $也是需要我们估计的值，我们采用迭代法，在计算 $\gamma(i, k)$ 的时候我们假定 $\mu_k$ 和 $\Sigma_k$ 均已知，我们将取上一次迭代所得的值（或者初始值）。 

**（2）M-step**

估计每个 Component 的参数：现在我们假设上一步中得到的 $\gamma(i, k)$ 就是正确的“数据 $x_i$ 由 Component k 生成的概率”，亦可以当做该 Component 在生成这个数据 $x_i$上所做的贡献，或者说，我们可以看作 $x_i$ 这个值其中有 $\gamma(i, k)x_i$ 这部分是由 Component k 所生成的。集中考虑所有的数据点，现在实际上可以看作 Component 生成了 $\gamma(1, k)x_1, \ldots, \gamma(N, k)x_N$ 这些点。由于每个 Component 都是一个标准的 Gaussian 分布，可以很容易分布求出最大似然所对应的参数值： 
- $\displaystyle  \begin{aligned}\mu_k & = \frac{1}{N_k}\sum_{i=1}^N\gamma(i, k)x_i \\\Sigma_k & = \frac{1}{N_k}\sum_{i=1}^N\gamma(i,k)(x_i-\mu_k)(x_i-\mu_k)^T  \\\pi_k  & =N_k/N \end{aligned}$

其中 $N_k = \sum_{i=1}^N \gamma(i, k) $

重复迭代前面两步，直到似然函数的值收敛为止。 

## **4. 参考资料**

[http://blog.pluskid.org/?p=39](http://blog.pluskid.org/?p=39)
[http://blog.csdn.net/linkin1005/article/details/41212085](http://blog.csdn.net/linkin1005/article/details/41212085)



