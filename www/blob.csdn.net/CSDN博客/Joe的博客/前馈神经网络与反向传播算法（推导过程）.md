# 前馈神经网络与反向传播算法（推导过程） - Joe的博客 - CSDN博客





2016年09月16日 14:49:35[Joe-Han](https://me.csdn.net/u010089444)阅读数：17811标签：[神经网络																[反向传播																[BP																[公式推导																[算法](https://so.csdn.net/so/search/s.do?q=算法&t=blog)
个人分类：[深度学习](https://blog.csdn.net/u010089444/article/category/6419961)





# **1. 符号说明**

$n_{l}$ ：表示网络的层数，第一层为输入层 
$s_{l}$ ：表示第l层神经元个数 

f(·) ：表示神经元的激活函数 
$W^{(l)} \in R^{ s_{_{l+1}}  \times  s_{_{l}} }$：表示第l层到第l+1层的权重矩阵 
$b^{(l)} \in  R^{ s_{_{l+1}} }$：表示第l层到第l+1层的偏置 
$z^{(l)} \in R^{ s_{_{l}} }$ ：表示第l层的输入，其中${z_{i}}^{(l)}$为第l层第i个神经元的输入 
$a^{(l)} \in R^{ s_{_{l}} }$ ：表示第l层的输出，其中${a_{i}}^{(l)}$为第l层第i个神经元d的输出
# **2. 向前传播**

下图直观解释了层神经网络模型向前传播的一个例子，圆圈表示神经网络的输入，“+1”的圆圈被称为偏置节点。神经网络最左边的一层叫做输入层，最右的一层叫做输出层。中间所有节点组成的一层叫做隐藏层。 
![这里写图片描述](https://img-blog.csdn.net/20160916155002721)
每个神经元的表达式如下： 
![这里写图片描述](https://img-blog.csdn.net/20160916155442973)
前向传播的步骤如下： 
$\left.\begin{matrix}\\z^{(l)}= W^{(l-1)}  a^{(l-1)}+b^{(l-1)}\\a^{(l)} = f(z^{(l)})\end{matrix}\right\}  \Rightarrow z^{(l)} = W^{(l-1)} f(z^{(l-1)})+b^{(l-1)}$

# **3. 反向传播算法推导过程**

## **(1)目标函数**

给定一个包含m个样本的训练集，目标函数为：

$J(W,b) =\frac{1}{m} \sum_{i=1}^{m}J(W,b;x^{(i)},y^{(i)})+\frac{\lambda }{2}\parallel W\parallel_{2}^{2}$

$ \qquad\quad\ =\frac{1}{m} \sum_{i=1}^{m}(\frac{1}{2}\parallel h(x^{(i)})-y^{(i)})\parallel_{2}^{2})+\frac{\lambda }{2} \sum_{l=1}^{n_{_{l}}-1} \sum_{i=1}^{s_{_{l}}} \sum_{j=1}^{s_{_{l+1}}} ( {W_{ji}}^{(l)} ) ^{2} $

采用梯度下降方法最小化J(W,b)， 参数更新方式如下：

${W_{new}}^{(l)}=W^{(l)}-\alpha \cdot \frac{\partial J(W,b))}{\partial W^{(l)}}$

$ \qquad\quad\ =W^{(l)}-\alpha\sum_{i=1}^{m}\frac{\partial J(W,b;x^{(i)},y^{(i)})}{\partial W^{(l)}}-\lambda W$

${b_{new}}^{(l)}=b^{(l)}-\alpha \cdot \frac{\partial J(W,b))}{\partial b^{(l)}}$

$\qquad\ \ \ =b^{(l)}-\alpha\sum_{i=1}^{m}\frac{\partial J(W,b;x^{(i)},y^{(i)})}{\partial b^{(l)}}$

因此，参数更新的关键在于计算 $\frac{\partial J(W,b;x,y）}{\partial W^{(l)}}$和$\frac{\partial J(W,b;x,y）}{\partial b^{(l)}}$

## **(2)计算$\frac{\partial J(W,b;x,y）}{\partial W^{(l)}}$**

根据链式法则可得：

$\frac{\partial J(W,b;x,y）}{\partial W^{(l)}}=(\frac{\partial J(W,b;x,y）}{\partial z^{(l+1)}})^\mathrm{ T }  \frac{\partial z^{(l+1)}}{\partial W^{(l)}}$

其中，$\frac{\partial z^{(l+1)}}{\partial W^{(l)}}=\frac{\partial  [W^{(l)}  \cdot a^{(l)}+b^{(l)}] }{\partial W^{(l)}}=a^{(l)} $

定义残差为： $\delta^{(l)} = \frac{\partial J(W,b;x,y）}{\partial z^{(l)}}$

对于输出层（第$n_{l}$层），残差的计算公式如下：（其中，$f(z^{(n_{l})})$是按位计算的向量函数，因此其导数是对角矩阵）

$\delta^{(n_{l})} = \frac{\partial J(W,b;x,y）}{\partial z^{(n_{l})}}$

$\quad\ \ \ = \frac{\partial \frac{1}{2} \parallel h(x)-y)\parallel_{2}^{2}}{\partial z^{(n_{l})}}  $

$\quad\ \ \ = \frac{\partial \frac{1}{2} \parallel f(z^{(n_{l})})-y)\parallel_{2}^{2}}{\partial z^{(n_{l})}}  $
$\quad\ \ \ = (a^{(n_{l})} -y)\cdot diag(f^{'}(z^{(n_{l})}))$

$\quad\ \ \ = (a^{(n_{l})} -y)\odot f^{'}(z^{(n_{l})})$

对于网络其它层，残差可通过如下递推公式计算： 
$\delta^{(l)} = \frac{\partial J(W,b;x,y）}{\partial z^{(l)}}$

$\quad\ \ = \frac{\partial a^{(l)} }{\partial z^{(l)}}  \frac{\partial z^{(l+1)} }{\partial a^{(l)}}  \frac{\partial J(W,b;x,y）}{\partial z^{(l+1)}}$

$\quad\ \ = \frac{\partial f(z^{(l)}) }{\partial z^{(l)}} \cdot \frac{\partial [W^{(l)} a^{(l)}+b^{(l)}] }{\partial a^{(l)}} \cdot \delta^{(l+1)}$
$\quad\ \ =   diag(f^{'}(z^{(l)})) \cdot (W^{(l)} )^\mathrm{ T } \cdot \delta^{(l+1)}$
$\quad\ \ =f^{'}(z^{(l)})  \odot  (W^{(l)} )^\mathrm{ T } \delta^{(l+1)}   $
## **(3)计算$\frac{\partial J(W,b;x,y）}{\partial b^{(l)}}$**

与(2)计算过程同理

$\frac{\partial J(W,b;x,y）}{\partial b^{(l)}}= \frac{\partial z^{(l+1)}}{\partial b^{(l)}} \frac{\partial J(W,b;x,y）}{\partial z^{(l+1)}}$

$\qquad\qquad \ = \frac{\partial [W^{(l)}\cdot a^{(l)}+b^{(l)}] }{\partial b^{(l)}}  \delta^{(l+1)} $

$\qquad\qquad \ = \delta^{(l+1)} $

综上所述：

$\frac{\partial J(W,b;x,y）}{\partial W^{(l)}}=(\delta^{(l+1)} )^\mathrm{ T }a^{(l)} $

$\frac{\partial J(W,b;x,y）}{\partial b^{(l)}}=\delta^{(l+1)}$

反向传播算法的含义是：第l 层的一个神经元的残差是所有与该神经元相连的第l+ 1 层的神经元的残差的权重和，然后在乘上该神经元激活函数的梯度。

# **4. 反向传播算法流程**

借网上一张图，反向传播算法可表示为以下几个步骤：

![图片名称](https://img-blog.csdn.net/20160916185141894)](https://so.csdn.net/so/search/s.do?q=公式推导&t=blog)](https://so.csdn.net/so/search/s.do?q=BP&t=blog)](https://so.csdn.net/so/search/s.do?q=反向传播&t=blog)](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)




