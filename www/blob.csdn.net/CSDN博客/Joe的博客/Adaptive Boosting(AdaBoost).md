# Adaptive Boosting(AdaBoost) - Joe的博客 - CSDN博客





2016年11月09日 15:19:11[Joe-Han](https://me.csdn.net/u010089444)阅读数：3165








# **1. Motivation**

bootstrap在训练集上通过有放回的采样构造出不同的训练数据，该过程可以看作在完整训练集上，为每一个样本赋予不同的权重。若该样本在本次抽样中没有被选中，则其权重为0。

![图片名称](https://img-blog.csdn.net/20161114100150433)

在采样得到的每一个数据集 $u_{n}^{(t)}$ 上训练得到模型 $g_{t}$  。为了增加不同 $g_{t}$ 之间的差异性，可采用如下思路：经过训练得到的模型 $g_{t}$ 在数据集  $u_{n}^{(t)}$ 上具有较好的表现，我们希望 $g_{t}$  在数据集  $u_{n}^{(t+1)}$ 上的表现较差，这样通过 $u_{n}^{(t+1)}$ 训练出的模型 $g_{t+1}$  与 $g_{t}$  就会存在差异。 

具体做法就是让 $g_{t}$  模型在构造出的 $u_{n}^{(t+1)}$ 数据集上的二分类准确率为50%，这样就等同于机选。

![图片名称](https://img-blog.csdn.net/20161114101656642)

具体例子如下：在数据集 $u_{n}^{(t)}$ 上，模型 $g_{t}$  分类正确的数据有6211个，分类错误的数据有1126个。因此在构造 $u_{n}^{(t+1)}$ 时，将分类错误的样本权重乘以 1126 ,将分类正确的样本权重乘以 6211 。即放大错误，减小正确的影响。

![图片名称](https://img-blog.csdn.net/20161114103915561)

将多个较弱的模型

# **2. Algorithm**

Adaptive Boosting 算法训练出多个不同的模型 $g_{t}$ ，并将其组合起来形成一个更强的模型 $G(x)$ 。Adaptive Boosting在训练模型和计算模型权重上是同时进行的。具体流程如下：

![图片名称](https://img-blog.csdn.net/20161114105936100)



