# 强化学习笔记(1)：Q-Learning - Joe的博客 - CSDN博客





2018年05月30日 19:01:24[Joe-Han](https://me.csdn.net/u010089444)阅读数：1289








# 1. 强化学习基本概念

考虑下面这个例子：假如我们想让一只老鼠学会走迷宫，往往会在迷宫的几个关键地点放上奶酪，老鼠每次走到关键点就会获得奖励，久而久之，老鼠就能学会快速找到迷宫出口，这就是强化学习的一个例子。 

![图片名称](https://img-blog.csdn.net/20180530124801246?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwODk0NDQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


强化学习的关键要素包括：环境（environment），回报（reward），动作（action），状态（state） 。在上述例子中，environment就是老鼠所处的迷宫，迷宫中的奶酪代表reward，action 为老鼠每一步可走的方向（即上下左右），state为老鼠在迷宫中所处的位置（可用二维坐标表示）。强化学习的目标是针对一个具体问题（训练老鼠走迷宫）得到一个最优的policy（在每个state下应该采取哪种action），使得在该策略下获得的reward最大（即老鼠可成功走出迷宫）。

强化学习的传统方法包括Q-Learning、Sarsa、Sarsa(lambda)，这类方法的的主要思路就是通过学习建立一张决策表，表中的行表示state，列表示在某一state下采取每种action的可能性。据此，在实际应用中可根据所处的state和决策表选择当下最优的action。

# 2. Q-Learning算法

下面同样以老鼠走迷宫为例，对Q-Learning的训练过程进行说明：
- 首先初始化决策表$Q$，令$Q=0$。$Q$是一个$27×4$的矩阵，行表示迷宫中的位置（即每个白格），列表示action（即上下左右），表中的值表示在某一state下采取特定action的评分。
- $For\ each \ episode:$
- 初始化状态s 为迷宫入口位置。
- 若当前状态s 不等于迷宫出口，则重复执行以下步骤： 
- 在当前状态s的所有可能动作中选取一个动作a：具体方法是以$\varepsilon $的概率选择Q表中对应状态s下评分最高的动作，以$1-\varepsilon $的概率随机选择动作。
- 执行动作a并获得下一个状态s’ 和回报r，这里可认为当老鼠拿到奶酪时，r为1，其余情况均为0。
- 更新Q表：$Q(s,a)=Q(s,a)+\alpha [r+\gamma\cdot  max_{a'}Q(s',a')-Q(s,a)]$，其中$Q(s,a)$和$r+\gamma\cdot  max_{a'}Q(s',a')$分别表示在状态s下采取动作a所带来的潜在回报的估计值和现实值，将现实值和估计值的差作为梯度对$Q$表进行更新。参数$\alpha$和$\gamma$分别为学习率和衰减因子。
- 更新状态：$s = s' $


Q表建立好后，可以很容易的找出从迷宫起点到达出口行为路径，具体步骤如下：
- 令初始状态$s=s_0$
- 确定动作$a$，使$Q(s,a)=max_\tilde a Q(s,\tilde a)$
- 令当前状态$s=\tilde s$（$\tilde s$为$a$对应的下一个状态）
- 重复2-3，直到到达迷宫出口

下面给出Q-Learning的算法流程： 

![这里写图片描述](https://img-blog.csdn.net/20180530162302148?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwODk0NDQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


从上图可看出，每次更新都用到了 Q 现实和 Q 估计，而且 Q-Learning 的迷人之处就是在 $Q(s, a)$ 现实中，也包含了一个$Q(s',a')$ 的最大估计值，将下一步衰减的最大估计和当前得到的回报当成这一步的现实。

在算法的参数方面，$\varepsilon $是用在决策上的一种策略，比如 $\varepsilon= 0.9 $ 时，就说明有90% 的概率我会按照 Q 表的最优值选择行为，有10% 的概率使用随机选行为。$\alpha$是学习率，来决定这次的误差有多少是要被学习的。$\gamma$是对未来 reward 的衰减因子，$\gamma=1$时，模型能清清楚楚看到之后所有步的全部价值；但是当 $\gamma=0$时，模型只能摸到眼前的 reward r，即只在乎最近的大奖励。

# 3. 强化学习，监督学习，无监督学习

**强化学习 v.s 监督学习**
- 监督学习就好比你在学习的时候，有一个老师在旁边指点，他知道怎么是对的怎么是错的，但在很多实际问题中，例如下围棋这种有成千上万种组合方式的情况，老师无法知道未来所有可能的结果。而强化学习会在没有任何标签的情况下，先尝试做出一些行为得到一个结果，通过这个结果的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么行为可以得到最好的结果。
- 强化学习的结果反馈有延时，有时候可能需要走很多步以后才知道以前的某一步的选择是好还是坏，而监督学习做了比较坏的选择会立刻反馈给算法。
- 两种学习方式都会学习输入到输出的一个映射，监督式学习得到的是输入和输出的关系；强化学习得到的是state和action对应的reward，即用来判断当下这个行为是好是坏。
- 监督学习的输入是独立同分布的，而强化学习当前的行为会影响下一次决策的输入。

**强化学习 v.s 无监督学习**
- 无监督学习的目标是发现数据中的模式。例如在向用户推荐新闻文章的任务中，无监督学习会找到用户先前已经阅读过的类似文章并向他们推荐，而强化学习将通过向用户先推荐少量的新闻，并不断获得来自用户的反馈，最后构建用户可能会喜欢的文章信息。




