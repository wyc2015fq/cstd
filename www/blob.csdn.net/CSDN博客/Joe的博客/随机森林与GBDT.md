# 随机森林与GBDT - Joe的博客 - CSDN博客





2017年04月06日 22:03:46[Joe-Han](https://me.csdn.net/u010089444)阅读数：10826








# **1. 随机森林**

### **1.1 决策树**

决策树分为两大类，分类树和回归树。分类树是我们比较熟悉的决策树，比如C4.5分类决策树。分类树用于分类标签值，如晴天/阴天、用户性别、网页是否是垃圾页面。而回归树用于预测实数值，如明天的温度、用户的年龄、网页的相关程度。也就是分类树的输出是定性的，而回归树的输出是定量的。
- 
分类树以C4.5算法为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature<=阈值，和feature>阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。

- 
回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化平方误差。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。


### **1.2 随机森林（Random Forest）**

随机森林（Random Forest，RF）有很多的优点，包括：
- 模型简单，易于实现
- 计算开销小，训练速度快 
- 能够处理feature很多的数据，并且不用做特征选择
- 在训练完后，它能够给出哪些feature比较重要
- 泛化能力较强

随机森林是[Bagging](http://blog.csdn.net/u010089444/article/details/52992745)的一个扩展变体，RF在以[CART决策树](http://blog.csdn.net/u010089444/article/details/53241218)为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了**随机属性选择**，具体来说，传统决策树在选择划分属性时是在当前节点的属性集合（假设有d个属性）中选择一个最优的属性；而在随机森林中，对决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程度。当k=d时，则基决策树的构建与传统决策树相同，若k=1，则是随机选择一个属性用于划分。一般情况下，推荐值k=$log_2d$ 或k=$\sqrt{d }$
- 
随机森林的训练过程可以总结如下：
- 给定训练集D，特征维数F。确定参数：随机森林中CART决策树的数量n，每棵树的深度d，每个节点使用到的特征数量k，终止条件：节点上最少样本数s，节点上最少的Gini Gain
- 对于第 i 棵树，从训练集D中有放回的抽取大小和D一样的训练集D(i)作为构造第 i 颗树的数据，并从根节点开始进行分支
- 如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的输出为当前节点样本集合中数量最多的那一类；如果是回归问题，输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。如果当前节点没有达到终止条件，则从F维特征中无放回的随机选取k维特征。选择k个特征中分类效果最好的一个特征
- 重复2，3步直到所有节点都训练过了或者被标记为叶子节点。
- 重复2，3，4步直到所有CART决策树都被训练过。

- 
利用随机森林的预测过程如下：
- 从当前第 i 棵树的根节点开始，根据输入样本的属性到达某个叶子节点，并输出预测值
- 重复执行步骤1直到每棵树都输出了预测值。如果是分类问题，则输出为所有树中预测类别数最多的那一个类；如果是回归问题，则输出为所有树的输出的平均值


按RF算法得到的每一棵都是较弱的模型，但是大家组合起来就很厉害了。可以将每一棵决策树比喻成一个精通于某个窄领域的专家（因为我们从F个feature中选择k个让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。关于随机森林的实现以及调参，可以参考：[scikit-learn随机森林类库概述](http://www.cnblogs.com/pinard/p/6160412.html)

# **2. GBDT**

梯度提升决策树(Gradient Boosting Decision Tree，GBDT) 又叫做MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，Tree Net等，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终结果。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。

GBDT中的树是回归树（不是分类树），调整后也可以用于分类。GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性；GBDT在淘宝的搜索及预测业务上也发挥了重要作用。

### **2.1  GBDT实例**

GBDT是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。参考一篇博客的例子，训练一个GBDT模型来预测年龄：训练集是4个人，A，B，C，D年龄分别是14，16，24，26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下：


![图片名称](https://img-blog.csdn.net/20170406154232637?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


预测值等于所有树值得累加：
- A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14 
- B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16 
- C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24 
- D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26

### **2.2 GBDT算法**

GBDT是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。Adaboost是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t−1}(x)$， 损失函数是$L(y,f_{t−1}(x))$ ,，我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y,f_{t}(x))=L(y,f_{t−1}(x))+h_t(x)$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。

GBDT利用加法模型和前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步的优化很简单。但对于一般的损失函数，往往每一步优化没那么容易，如下图中的绝对值损失函数和Huber损失函数。针对这一问题，Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。 

![图片名称](https://img-blog.csdn.net/20170406161052798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


第m轮的第i个样本的损失函数的负梯度可表示为：
- $r_{im} = - [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x_i)=f_{m-1}(x_i)}$

算法流程如下： 

![图片名称](https://img-blog.csdn.net/20170406170855407?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

- 初始化：估计使损失函数极小化的常数值$\gamma$，它是只有一个根节点的树。
- M轮迭代（生成M棵树） 
- 在第m轮，对于每一个样本$i$，计算损失函数的负梯度在当前模型的值$r_{im}$，将它作为残差的估计
- 利用样本集$（x，r_{im}）$生成一棵回归树，以拟合残差的近似值。其中$R_{jm}$为第m棵树的第j个叶子节点，$J_m$为回归树的叶子节点的个数
- 利用线性搜索估计叶节点区域的值，使损失函数极小化
- 更新模型

- 得到输出的最终GBDT模型 $f(x)$

GBDT算法的scikit-learn实现以及调参可参考：[scikit-learn GBDT类库概述](http://www.cnblogs.com/pinard/p/6143927.html)

### **2.3 GBDT分类算法**

GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。于对数似然损失函数，我们又有二元分类和多元分类的区别，具体介绍见：[梯度提升树(GBDT)原理小结](http://www.cnblogs.com/pinard/p/6140514.html)。

### **2.4 GBDT的正则化**

和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式：

第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为$ν$，对于前面的弱学习器的迭代
- $ f_k(x)=f_{k−1}(x)+h_k(x) $

如果我们加上了正则化项，则有:
- $ f_k(x)=f_{k−1}(x)+νh_k(x) $

$ν$ 的取值范围为$0<ν≤1$ 。对于同样的训练集学习效果，较小的$ν$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。

第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行迭代的弱点。

第三种是对于弱学习器即CART回归树进行正则化剪枝。

# **3. 随机森林与GBDT树的深度**

以下摘自知乎上的一个问答：[为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？](https://www.zhihu.com/question/45487317)
- 
**问：**

用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了0.0请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？

- 
**答：**

参见这个问题，[为什么说bagging是减少variance，而boosting是减少bias? - 机器学习。](https://www.zhihu.com/question/26760839)

这是一个非常好的问题，题主对各算法的学习非常细致透彻，问的问题也关系到这两个算法的本质。这个问题其实并不是一个很简单的问题，我尝试用我浅薄的机器学习知识对这个问题进行回答。 

  一句话的解释，来自周志华老师的机器学习教科书（ 机器学习-周志华）：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。 

  随机森林(random forest)和GBDT都是属于集成学习（ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。 

  Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。 

  其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X^2)-[E(X)]^2）。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。 
![这里写图片描述](https://img-blog.csdn.net/20170406173721882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。 

  当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。 
![这里写图片描述](https://img-blog.csdn.net/20170406173545647?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。 

  对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。 

  对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。


# **4. 参考资料**
- Breiman L. Random Forests[J]. Machine Learning, 2001, 45(1):5-32.
- 周志华. 机器学习 : = Machine learning[M]. 清华大学出版社, 2016.
- [GBDT：梯度提升决策树](http://www.jianshu.com/p/005a4e6ac775)





