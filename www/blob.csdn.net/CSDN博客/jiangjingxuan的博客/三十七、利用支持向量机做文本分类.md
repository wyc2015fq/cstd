# 三十七、利用支持向量机做文本分类 - jiangjingxuan的博客 - CSDN博客





2017年01月25日 11:55:51[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：744












![](http://www.shareditor.com/uploads/media/default/0001/01/thumb_141_default_big.jpeg)



从上一节提取出的全部特征中选取出关键的特征，并利用支持向量机对测试样本做回归计算，判断准确率

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

## 选取出关键特征

通过tf-idf计算出来的数值是某个特征（词）对于这篇文档的权重，不代表这个特征（词）在文本分类中的权重。这很容易理解，比如某一个特征（词）在多个分类中的tf-idf是不一样的，但是这个特征对于这个分类问题的权重肯定是一个定值。

选取重要的特征的方法可以是：1.）按tf-idf排序从大到小选topN；2）按特征的普遍性选取（在多个类别中出现过）；3）按特征在不同文档中tf-idf的差距选择；

我们这次采取结合的形式：在至少2个类别中tf-idf大于0，同时在多个类别中第一名高于第二名10%以上。这么选择的原因是：我的总类别一共5种比较少，所以2个类别以上就说明具有普遍性了，你可以根据你的类别数目调整，第一名高于第二名10%表示这个特征具有一定的区分度。

修改我们的feature_extract.py如下：

```python
def feature_dump():
    cursor = conn.cursor()
    category={}
    category[0] = 'isTec'
    category[1] = 'isSoup'
    category[2] = 'isMR'
    category[3] = 'isMath'
    category[4] = 'isNews'

    corpus=[]
    for index in range(0, 5):
        sql = "select segment from CrawlPage where " + category[index] + "=1"
        print sql
        cursor.execute(sql)
        line = ""
        for result in cursor.fetchall():
            segment = result[0]
            line = line + " " + segment
        corpus.append(line)

    conn.commit()
    conn.close()

    vectorizer=CountVectorizer()
    csr_mat = vectorizer.fit_transform(corpus)
    transformer=TfidfTransformer()
    tfidf=transformer.fit_transform(csr_mat)
    word=vectorizer.get_feature_names()
    print tfidf.toarray()

    for index in range(0, 5):
        f = file("tfidf_%d" % index, "wb")
        for i in np.argsort(-tfidf.toarray()[index]):
            if tfidf.toarray()[index][i] > 0:
                f.write("%f %s\n" % (tfidf.toarray()[index][i], word[i]))
        f.close()
def feature_extraction():
    d = {}
    for index in range(0, 5):
        f = file("tfidf_%d" % index, "r")
        lines = f.readlines()
        for line in lines:
            word = line.split(' ')[1][:-1]
            tfidf = line.split(' ')[0]
            if d.has_key(word):
                d[word] = np.append(d[word], tfidf)
            else:
                d[word] = np.array(tfidf)

        f.close();
    f = file("features.txt", "wb")
    for word in d:
        if d[word].size >= 2:
            index = np.argsort(d[word])
            if float(d[word][index[d[word].size-0-1]]) - float(d[word][index[d[word].size-1-1]]) > 0.01:
                f.write("%s %s\n" % (word, d[word]))
    f.close()

if __name__ == '__main__':
    #get_segment();
    feature_dump();
    feature_extraction();
```

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

最终输出的features.txt中有809个特征供我们使用，如下：

```bash
集群 ['0.027741' '0.014016' '0.010606']
分类器 ['0.002870' '0.045052' '0.000943']
中心 ['0.008167' '0.001647' '0.004274' '0.006954' '0.031360']
首席 ['0.003015' '0.017036']
fit ['0.016885' '0.000284']
懂得 ['0.035888' '0.001629' '0.003064']
密度 ['0.002414' '0.002106' '0.015073' '0.001586']
master ['0.021045' '0.002002' '0.000471']
对方 ['0.002414' '0.020451' '0.001684' '0.003569']
物品 ['0.020088' '0.001158' '0.001414']
……
```



## 研究scikit-learn SVM的使用

利用支持向量机模型可以直接使用scikit-learn，下一节我们再来研究一下scikit-learn的支持向量机怎么用




