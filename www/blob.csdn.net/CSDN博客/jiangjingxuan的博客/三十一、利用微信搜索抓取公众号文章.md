# 三十一、利用微信搜索抓取公众号文章 - jiangjingxuan的博客 - CSDN博客





2017年01月25日 11:53:04[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：10015
个人分类：[做一个全栈工程师](https://blog.csdn.net/jiangjingxuan/article/category/6694850)












![](http://www.shareditor.com/uploads/media/default/0001/01/thumb_126_default_big.png)



我喜欢看微信公众号里的技术文章，但是总是有一些鸡汤文阻碍我的实现，我是怎么让机器帮我自动摆脱鸡汤文的呢？接下来的几个章节讲述我的解决方案，让你感兴趣的文章扑面而来，无关的鸡汤文随风而去。本章节先将怎么利用搜狗微信搜索抓取公众号的文章

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

## 自动收集我关注的微信公众号文章



我的微信里关注了数十个有关大数据的公众号，每天都会出现那个小红点让我点进去看，但是点多了就会觉得烦了，所以我要做的第一步就是自动把公众号里的新文章都收集到一块，怎么做呢？scrapy！

对！scrapy抓取！但是scrapy顺着超链接抓取web网页容易，抓取微信app里的内容就有难度了，暂时还是做不到模拟一个收集app软件。庆幸的是，腾讯和搜狗搜索结婚啦！生出了一个小宝宝：搜狗微信搜索。下面我们就借助搜狗微信搜索来实现我的目的

举个例子，我关注了一个公众号叫：大数据文摘。打开[http://weixin.sogou.com/](http://weixin.sogou.com/)，输入“大数据文摘”，点“搜公众号”，搜索结果如下：

![](http://www.shareditor.com/uploads/media/my-context/0001/01/7ca6250a5be71c615cfeffebfdf7606adf46a9a6.png)

点击这个搜索结果，跳到了新页面

![](http://www.shareditor.com/uploads/media/my-context/0001/01/4dcd9d22bd44555f28c6906d8602796632e1a6b1.png)

这里面显示的都是最新发布的文章

好！我们就沿着这条路线来追踪公众号的新文章

下面我们来分析一下url

第一个搜索结果页的url是：[http://weixin.sogou.com/weixin?type=1&query=%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%96%87%E6%91%98&ie=utf8&_sug_=n&_sug_type_=](http://weixin.sogou.com/weixin?type=1&query=%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%96%87%E6%91%98&ie=utf8&_sug_=n&_sug_type_=)，我们去掉query以外的参数得到：[http://weixin.sogou.com/weixin?query=%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%96%87%E6%91%98](http://weixin.sogou.com/weixin?query=%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%96%87%E6%91%98)，打开之后结果是一样的，ok，这个就作为我们抓取的种子入口，如果搜索其他公众号就把query参数换掉

下面分析搜索结果里怎么提取第二章页面，也就是公众号profile页的链接，我们看下搜索结果页的部分html如下：

```
<div target="_blank" href="http://mp.weixin.qq.com/profile?src=3&timestamp=1463443372&ver=1&signature=lNY-ZbjfPHr40G-zyUe*Sdc9HIn2IisEo0vwpKEAV*Z*ALBYuYf2HaMUtEP*15rQzs47zSEiORN3BOWPNA2R*A==" class="wx-rb bg-blue wx-rb_v1 _item" uigs_exp_id="" onclick="gotourl('http://mp.weixin.qq.com/profile?src=3&timestamp=1463443372&ver=1&signature=lNY-ZbjfPHr40G-zyUe*Sdc9HIn2IisEo0vwpKEAV*Z*ALBYuYf2HaMUtEP*15rQzs47zSEiORN3BOWPNA2R*A==',event,this);return true;" id="sogou_vr_11002301_box_0" uigs="sogou_vr_11002301_box_0">
<div class="img-box">
<span class="ico-bg"></span><span class="ico-r"></span><img style="visibility: visible; height: 57px; margin-left: 0px;" src="http://img01.sogoucdn.com/app/a/100520090/oIWsFt58NVJTkYWvPtICKgg8ka60" onload="vrImgLoad(this, 'fit', 57, 57)" onerror="vrImgErr(this, '/wechat/images/account/def56-56.png')" extra="err:'http://img01.sogoucdn.com/net/a/04/link?appid=100520078&url=http://wx.qlogo.cn/mmhead/Q3auHgzwzM46WJlQ8GYRWPhThl25rSKJEYBm408fnEkYS9DUkiaSxGg/0/0'"></div>
<div class="txt-box">
<h3><em><!--red_beg-->大数据文摘<!--red_end--></em></h3>
<h4>
<span>微信号：<label name="em_weixinhao">BigDataDigest</label></span>
</h4>
<p class="s-p3">
<span class="sp-tit">功能介绍：</span><span class="sp-txt">普及<em><!--red_beg-->数据<!--red_end--></em>思维,传播<em><!--red_beg-->数据<!--red_end--></em>文化</span>
</p>
<p class="s-p3">
<span class="sp-tit"><script>authnamewrite('2')</script>微信认证：</span><span class="sp-txt">深圳大数据文摘科技有限公司</span>
</p>
<p class="s-p3">
<span class="sp-tit">最近文章：</span><span class="sp-txt"><a class="blue" target="_blank" id="sogou_vr_11002301_link_first_0" href="http://mp.weixin.qq.com/s?src=3&timestamp=1463443372&ver=1&signature=fZ5HsUYiytbTgb8SekmcI3g9oizZncGBgdipWihPFh2pPnAwAwO62nX9iXNILZx0XtQB3R*3PWcgqPh1YWL*LX3qxIOf0ZpkKyhZSUkAgPmH*w71dqIB2*wfNTpVDZx5G3nh31tctf*lNqXlfXzgfPO6E60vqoqB694bPMymy*I=" title="二项式与小苹果——看牛顿如何将灵感火花拓展成知识体系">二项式与小苹果——看牛顿如何将灵感火花拓展成知识体系</a><span class="hui"><script>vrTimeHandle552write('1463440604')</script>46分钟前</span></span>
</p>
……
```

看这里关键的href一行：
`<div target="_blank" href="http://mp.weixin.qq.com/profile?src=3&timestamp=1463443372&ver=1&signature=lNY-ZbjfPHr40G-zyUe*Sdc9HIn2IisEo0vwpKEAV*Z*ALBYuYf2HaMUtEP*15rQzs47zSEiORN3BOWPNA2R*A==" class="wx-rb bg-blue wx-rb_v1 _item" uigs_exp_id="" onclick="gotourl('http://mp.weixin.qq.com/profile?src=3&timestamp=1463443372&ver=1&signature=lNY-ZbjfPHr40G-zyUe*Sdc9HIn2IisEo0vwpKEAV*Z*ALBYuYf2HaMUtEP*15rQzs47zSEiORN3BOWPNA2R*A==',event,this);return true;" id="sogou_vr_11002301_box_0" uigs="sogou_vr_11002301_box_0">`
这就是我们要提取的profile页链接，提取方式可以直接通配成：“url里带http://mp.weixin.qq.com/profile?src=的href属性”



ps：找xpath的方便方法是利用浏览器的开发者工具，比如chrome界面如下：

![](http://www.shareditor.com/uploads/media/my-context/0001/01/46ab4bb2a02da6c4665c5bd6fd91f30372c32725.png)

在Elements的标签处点右键选择：Copy->Copy XPath，就自动把xpath路径拷贝到剪切板了



注意：在这里我突然想到一个问题，每个公众号对应的profile页面是不是永远不变的呢？经过我的实验，这条url里的timestamp参数和signature是有对应关系的，任意一个错了都无法打开，而且每次搜索生成的链接都是不同的，所以我断定在微信搜索内容是动态生成链接的，那么这个动态链接的生命周期就不可预测了，所以为了保险起见，我们每次都从搜索入口追溯，才是万全之策

下面我们分析profile页里的文章链接，我们看profile页的部分 html如下：

```
<h4 class="weui_media_title" hrefs="/s?timestamp=1463443165&src=3&ver=1&signature=dZCo9et5C6nyZfVAQAl416OW-eXJbi0VaS0QPQdvEv1tawqgsjlVYUd0oav0tUHAf38HOGU3Lskd7qqXbFg9D2mP8cv36CZ1dW0bGxbP4YyJcRdy*M*Mow6xD5YWDK8-82r9MX*4WqgbGqo4FAhZeiGTEl27YhIbaIxPiQgMbxc=">代理银行业务：通过监管列表对代理银行客户进行风险评级</h4>
<p class="weui_media_desc">为了确保银行积极的通过代理银行关系来连接美国金融市场,需要考虑如何根据现有电汇和监管列表信息,来提升可疑行为模型的成熟度。</p>
<p class="weui_media_extra_info">2016年5月17日</p>
```

这里面可以找到文章的内容了链接、标题、摘要、发布时间，简直太完美了

链接的提取方式可以直接通配成：h4.weui_media_title hrefs

标题的提取方式可以直接通配成：h4.weui_media_title text

摘要的提取方式可以直接通配成：p.weui_media_desc

发布时间的提取方式可以直接通配成：p.weui_media_extra_info



## 开发我的scrapy爬虫



如果还没有安装scrapy，请见《[教你成为全栈工程师(Full Stack Developer) 三十-十分钟掌握最强大的python爬虫](http://www.shareditor.com/blogshow/43)》

创建一个scrapy工程
`scrapy startproject weixin`
在weixin/spiders/中创建dashujuwenzhai.py内容如下：

```python
#!/usr/bin/python
# -*- coding: utf-8 -*-
import scrapy

class ShareditorSpider(scrapy.Spider):
    name = "dashujuwenzhai"
    allowed_domains = ["qq.com"]
    start_urls = [
        "http://weixin.sogou.com/weixin?query=大数据文摘"
    ]

    def parse(self, response):
        print response.body
        href = response.selector.xpath('//div[@id="sogou_vr_11002301_box_0"]/@href').extract()[0]
        yield scrapy.Request(href, callback=self.parse_profile)

    def parse_profile(self, response):
        print response.body
```



执行
`scrapy crawl dashujuwenzhai`
即可以抓到大数据文摘的profile页面内容

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

接下来来研究profile页，抓回的页面不是普通的html页面，而是通过js渲染出来的，也就是我们看到的每一条文章的标题、摘要等都是通过js计算出来的，代码里有这么一句：

```java
var msgList = '{"list":[{"comm_msg_info":{"id":410106318,"type":49,"datetime":1463528503,"fakeid":"2391437564","status":2,"content":""},"app_msg_ext_info":{"title":"机器人前传：达芬奇的机器狮和日耳曼装甲骑士","digest":"这是一篇描述阿尔法狗和Atlas机器人祖先的文章。远在500多年前的达芬奇时代，已经有了不少关于机器人的探索。这个大天才写了大量关于自动机描述，在他的个人笔记中也充斥着各种机械发明的构思，比如弹簧驱动的汽车和机器狮子。","content":"","fileid":504157567,"content_url":"\\/s?timestamp=1463529354&amp;src=3&amp;ver=1&amp;signature=cG*R8qc-PGKV-aZ4q9IlJQfIHtGp5I3H63xlK-h5mBO0W2FRAzCddav9cPf*GuwUBI4x0zJzmtcoOU7sQQeMf3CfNzaTEIq4C8YwnsZQGnqnauqr2wQYvEFvAooyecPF3H6bg8OiqpSZsd5LnY*fVrZOMINmQwV8Qup*D9qvUkw=","source_url":"https:\\/\\/mp.weixin.qq.com\\/s?__biz=MzA4OTYwNzk0NA==&amp;mid=401744027&amp;idx=1&amp;sn=43699667dca4438a49db51fb3700af4f&amp;scene=1&amp;srcid=0517MRoAk1EzgC5iSMtvoYC5&amp;pass_ticket=06ybKvJknob%2F5%2B%2FAmkUtnjcyCqWcuNxZTJapLW5QZyk7PWh1jD7ubwb5H1zXzMWB#rd","cover":"http:\\/\\/mmbiz.qpic.cn\\/mmbiz\\/wc7YNPm3YxXiajPXq2Y2PWQsic1SmjCxnTicHKtwItmARwkha1RI1gH1WwTfRvEUzauWJibjuJC9oJ8eibeVlDjRkwg\\/0?wx_fmt=jpeg","subtype":0,"is_multi":1,"multi_app_msg_item_list":[{"title":"清华论坛实录|刘瑞宝:洞见数据内涵，提升公共安全研判能力","digest":"本文为刘瑞宝先生于2016年3月24日在RONG—大数据与公共安全专场上所做的题为《洞见数据内涵，提升公共安全研判能力》的演讲实录。","content":"","fileid":504157565,"content_url":"\\/s?timestamp=1463529354&amp;src=3&amp;ver=1&amp;signature=cG*R8qc-PGKV-aZ4q9IlJQfIHtGp5I3H63xlK-h5mBO0W2FRAzCddav9cPf*GuwUBI4x0zJzmtcoOU7sQQeMf3CfNzaTEIq4C8YwnsZQGnrmdiX-aBZzJtqDGa76CoHH8gL7PEfN3ZQN5lNa4YgJUeUyE*SIna3B7W*zKWYskkU=","source_url":"https:\\/\\/mp.weixin.qq.com\\/s?__biz=MzAxMzA2MDYxMw==&amp;mid=2651555964&amp;idx=2&amp;sn=479aaf7f3b687b973ffa303d3d3be6b9&amp;scene=1&amp;srcid=0517C5DgLArlrdVAlQ9GIHOl&amp;pass_ticket=06ybKvJknob%2F5%2B
……
```

当然还没有截取全，这就是文章的全部内容，写到了一个js变量里，这样就无法通过scrapy原生的response.xpath拿到，这怎么办呢？

我们来利用phantomjs来渲染，这是一个强大的工具，它是无界面的浏览器，所以渲染js速度非常快，但是也有一些缺陷，有一些浏览器渲染功能不支持，所以如果再深入可以借助selenium工具，这又是一个强大的工具，它原本是用来做web应用程序自动化测试用的，也就是可以模拟各种点击浏览等动作，那么用他来做爬虫几乎就是一个真人，本节先来研究phantomjs，有关selenium的内容后面有需求了再研究



## 安装phantomjs

```
wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2
tar jxvf phantomjs-2.1.1-linux-x86_64.tar.bz2
cd phantomjs-2.1.1-linux-x86_64/
./bin/phantomjs examples/netlog.js http://www.shareditor.com/
```

以上输出了网路通信日志，说明没有问题



为了方便，可以把./bin/phantomjs拷贝到~/bin下



## 写一个phantomjs渲染脚本

```java
var page = require('webpage').create();
var system = require('system');
page.open(system.args[1], function(status) {
    var sc = page.evaluate(function() {
        return document.body.innerHTML;
    });
    window.setTimeout(function() {
        console.log(sc);
        phantom.exit();
    }, 100);
});
```

创建phantomjs渲染脚本getBody.js内容如下：



执行
`phantomjs getBody.js 'http://mp.weixin.qq.com/profile?src=3×tamp=1463529344&ver=1&signature=lNY-ZbjfPHr40G-zyUe*Sdc9HIn2IisEo0vwpKEAV*Z*ALBYuYf2HaMUtEP*15rQ7TpyhXFL52e8W929D4nd2g==' > profile.html`
这里的链接可能已经失效，请换成在搜狗微信搜索搜到某个公众号profile页面里的某一篇文章的url

打开profile.html会发现内容已经被渲染完成了，每篇文章的地方变成了：

```
<div id="WXAPPMSG410106318" class="weui_media_box appmsg" msgid="410106318">
                        <span class="weui_media_hd" style="background-image:url(http://mmbiz.qpic.cn/mmbiz/wc7YNPm3YxXiajPXq2Y2PWQsic1SmjCxnTicHKtwItmARwkha1RI1gH1WwTfRvEUzauWJibjuJC9oJ8eibeVlDjRkwg/0?wx_fmt=jpeg)" data-s="640" data-t="1463528503000" hrefs="/s?timestamp
                        <div class="weui_media_bd">
                            <h4 class="weui_media_title" hrefs="/s?timestamp=1463531541&src=3&ver=1&signature=n187YKNZjqgxyUtJ*yFEQGG7wJOH79RQeRrjQ0RGRdKEiZmR6iM0oNE5P0DPbQEwWTnShlZ4C3JIZr9PYThxbnhuCPl2UTc5NGE0ZkARKXEhTqCe7QvAGFf8vy2QWnPKqA9iSBBgBrocHKLBAuTM

                            机器人前传：达芬奇的机器狮和日耳曼装甲骑士
                            </h4>
                            <p class="weui_media_desc">这是一篇描述阿尔法狗和Atlas机器人祖先的文章。远在500多年前的达芬奇时代，已经有了不少关于机器人的探索。这个大天才写了大量关于自动机描述，在他的个人笔记中也充斥着各种机械发明的构思，比如弹簧驱动的汽车和机器狮子。</p>
                            <p class="weui_media_extra_info">2016年5月18日</p>
                        </div>
                    </div>
```

  ​

这便可以通过scrapy的request.xpath提取了



## 重新完善我们的scrapy爬虫脚本



```python
#!/usr/bin/python
# -*- coding: utf-8 -*-
import scrapy
import subprocess
from scrapy.http import HtmlResponse
from scrapy.selector import Selector

class ShareditorSpider(scrapy.Spider):
    name = "dashujuwenzhai"
    allowed_domains = ["qq.com"]
    start_urls = [
        "http://weixin.sogou.com/weixin?query=算法与数学之美"
    ]

    def parse(self, response):
        href = response.selector.xpath('//div[@id="sogou_vr_11002301_box_0"]/@href').extract()[0]
        cmd="~/bin/phantomjs ./getBody.js '%s'" % href
        stdout, stderr = subprocess.Popen(cmd, shell=True, stdout = subprocess.PIPE, stderr = subprocess.PIPE).communicate()
        response = HtmlResponse(url=href, body=stdout)

        for selector in Selector(response=response).xpath('//*[@id="history"]/div/div/div/div'):
            hrefs= selector.xpath('h4/@hrefs').extract()[0].strip()
            title = selector.xpath('h4/text()').extract()[0].strip()
            abstract = selector.xpath('//*[contains(@class, "weui_media_desc")]/text()').extract()[0].strip()
            pubtime = selector.xpath('//*[contains(@class, "weui_media_extra_info")]/text()').extract()[0].strip()
            print hrefs
            print title
            print abstract
            print pubtime

    def parse_profile(self, response):
        print response.body
```

这是一段我用了数天精力创造成功的一段代码，耗费了我很多体力值，所以重点讲解一下

```python
href = response.selector.xpath('//div[@id="sogou_vr_11002301_box_0"]/@href').extract()[0]
```

从公众号搜索结果页里提取profile页面的链接，这个id我怀疑不久后将失效，所以如果想做完美，还得不断完善，有关xpath的使用技巧可以参考[http://ejohn.org/blog/xpath-css-selectors/](http://ejohn.org/blog/xpath-css-selectors/)

```python
cmd="~/bin/phantomjs ./getBody.js '%s'" % href
stdout, stderr = subprocess.Popen(cmd, shell=True, stdout = subprocess.PIPE, stderr = subprocess.PIPE).communicate()
```

加载phantomjs脚本getBody.js来渲染profile页面，把里面的js渲染成html

```python
response = HtmlResponse(url=href, body=stdout)
```

用渲染后的html页面来创建一个HtmlResponse，用于 后面继续xpath提信息

```python
Selector(response=response).xpath('//*[@id="history"]/div/div/div/div')
```

找到每一条文章模块所在的div



```python
hrefs= selector.xpath('//h4/@hrefs').extract()[0].strip()
            title = selector.xpath('h4/text()').extract()[0].strip()
            abstract = selector.xpath('//*[contains(@class, "weui_media_desc")]/text()').extract()[0].strip()
            pubtime = selector.xpath('//*[contains(@class, "weui_media_extra_info")]/text()').extract()[0].strip()
```

根据这个div结构提取各个字段



基于这个爬虫脚本，想造就怎样的神奇，就看你之后的想象力了，没有做不到，只有想不到！




