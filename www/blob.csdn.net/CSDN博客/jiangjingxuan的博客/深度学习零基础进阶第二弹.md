# 深度学习零基础进阶第二弹 - jiangjingxuan的博客 - CSDN博客





2017年02月27日 08:18:45[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：476








![干货分享 | 深度学习零基础进阶第二弹](http://static.leiphone.com/uploads/new/article/740_740/201610/580a03f91c18b.png?imageMogr2/format/jpg/quality/90)

*图片来自*[*wiki*](https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Single-layer_feedforward_artificial_neural_network.png)

昨天，雷锋网编译了[《干货分享 | 深度学习零基础进阶大法！》](http://www.leiphone.com/news/201610/tgtcVePX2kdDlHfL.html)，相信读者一定对[深度学习](http://www.leiphone.com/news/201701/LqwiP7VUJO9DgBPi.html)的历史有了一个基本了解，其基本的模型架构（CNN/RNN/LSTM）与深度学习如何应用在图片和[语音识别](http://www.leiphone.com/news/201412/SPIrQG1uFa6jWMVZ.html)上肯定也不在话下了。今天这一部分，我们将通过新一批论文，让你对深度学习的方式与深度学习在不同领域的运用有个清晰的了解。由于[第二部分的论文开始向细化方向延展](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap/blob/master/README.md)，因此你可以根据自己的研究方向酌情进行选择。本文对每篇论文都增加了补充介绍，分上下两篇，由老吕IO及奕欣编译整理，未经雷锋网(公众号：雷锋网)许可不得转载。

**1.深度学习模型**

Hinton 与 Geoffrey 等技术专家合著的《Improving neural networks by preventing co-adaptation of feature detectors》也很有指导意义。论文提出，在训练神经网络模型时，如果训练样本较少，为了防止模型过拟合，Dropout 可以作为一种 trikc 供选择。

[1] [https://arxiv.org/pdf/1207.0580.pdf](https://arxiv.org/pdf/1207.0580.pdf)

关于 Dropout，Srivastava 与 Nitish 等技术专家也合著过《Dropout: a simple way to prevent neural networks from overfitting》一文。论文提出，拥有大量参数的深度神经网络是性能极其强大的[机器学习](http://www.leiphone.com/news/201609/SJGulTsdGcisR8Wz.html)系统，但过度拟合问题却成了系统中难以解决的一个大问题，而
 Dropout 是处理这一问题的技术捷径。

[2] [http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)

深度神经网络的训练是个复杂异常的活，因为训练中每一层参数的更改都会牵一发而动全身，而这一问题就造成训练效率低下。Ioffe、 Sergey 和 Christian Szegedy在《Batch normalization: Accelerating deep network training by reducing internal covariate shift》一文中着重介绍了解决这一问题的关键：内部协变量的转变。

[3] [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf)

深度神经网络的训练非常考验计算能力，而要想缩短训练时间，就必须让神经元的活动正常化，而最新引入的“批规范化”技术则是解决这一问题的突破口。完成技术突破的技术方式纠缠在多位专家合著的这份名为《Layer normalization》的论文中。

[4] [https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote)

《Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1》是今年2月份刚刚出炉的论文，论文的主要思想是通过二值化weights和activations，来提高NN的速度和减少其内存占用。由于二值网络只是将网络的参数和激活值二值化，并没有改变网络的结构，因此我们要关注如何二值化，以及二值化后参数如何更新。

[5] [https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf)

《Decoupled neural interfaces using synthetic gradients》是一篇来自Google DeepMind很有意思的神经网络论文，论文中用合成的梯度来分解backprop中的关联关系，五星推荐。

[6] [https://arxiv.org/pdf/1608.05343.pdf](https://arxiv.org/pdf/1608.05343.pdf)

**2. 深度学习优化**

《On the importance of initialization and momentum in deep learning》一文介绍了初始化和Momentum技术在深度学习方面的重要性，更多的着眼在实验分析上。

[7] [http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf)

Adam是一种基于梯度的优化方法，与SDG类似。其具体信息可以参阅论文《Adam: A method for stochastic optimization》。

[8] [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)

《Learning to learn by gradient descent by gradient descent》由 Andrychowicz 和 Marcin 等专家撰写而成，本文的思想是利用LSTM学习神经网络的更新策略，即利用梯度下降法学习一个优化器，然后用这个优化器去优化其他网络的参数。该文指导意义颇强，五星推荐。

[9] [https://arxiv.org/pdf/1606.04474.pdf](https://arxiv.org/pdf/1606.04474.pdf)

斯坦福大学的 Song Han 与 Huizi Mao 等专家撰写了一系列有关网络压缩的论文，《Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding》是其中一篇，论文题目已经概括了文中的三个重点，非常清晰明了。同时它也荣获了 ICLR 2016 最佳论文，五星推荐。

[10] [https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf)

《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size》由 Iandola 和 Forrest N 等专家撰写，开头论文先提了在相同精确度下，体积更小的深度神经网络有着3点好处。随后，提出了本文的创新 SqueezeNet 并给出了一个分类精度接近 AlexNet1 的网络，模型缩小 510 倍，还归纳了缩小模型尺寸时的设计思路。

[11] [https://arxiv.org/pdf/1602.07360.pdf](https://arxiv.org/pdf/1602.07360.pdf)

**3. 无监督学习/深层生成模型**

《Building high-level features using large scale unsupervised learning》讲述了 Google Brain 中特征学习的原理，通过使用未标记的图像学习人脸、猫脸特征，得到检测器。文章使用大数据构建了一个9层的局部连接稀疏自编码网络，使用模型并行化和异步 SGD 在 1000 个机器（16000核）上训练了 3 天，实验结果显示可以在未标记图像是否有人脸的情况下训练出一个人脸检测器。

[12] [https://arxiv.org/pdf/1112.6209.pdf&embed](https://arxiv.org/pdf/1112.6209.pdf&embed)

Kingma、 Diederik P 和 Max Welling 三位专家共同撰写了《Auto-encoding variational bayes》，该论文提出一个融合 Variational Bayes 方法和神经网络的方法，这个方法可以用来构造生成模型的自编码器。

[13] [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)

《Generative adversarial nets》是 Ian Goodfellow 大神的 2014 年的论文，中文应该叫做对抗网络，在许多教程中作为非监督深度学习的代表作给予推广。本文解决了非监督学习中的著名问题：给定一批样本，训练一个系统，能够生成类似的新样本。五星推荐。

[14] [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)

《Unsupervised representation learning with deep convolutional generative adversarial networks》是在 GAN 的论文中提出的对抗模型的原型，本文给出了基于卷机网的实现。同时还描述了实现过程中的细节，比如参数设置。也提到了解决 GAN 中训练不稳定的措施，但是并非完全解决。文中还提到利用对抗生成网络来做半监督学习。在训练结束后，识别网络可以用来提取图片特征，输入有标签的训练图片，可以将卷基层的输出特征作为 X ，标签作为
 Y 做训练。

[15] [https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)

《DRAW: A recurrent neural network for image generation》来自谷歌，描述了如何用 Deep Recurrent Attentive Writer (DRAW)神经网络框架自动生成图像，五星推荐。

[16] [http://jmlr.org/proceedings/papers/v37/gregor15.pdf](http://jmlr.org/proceedings/papers/v37/gregor15.pdf)

《[Pixel](http://www.leiphone.com/news/201609/Y27OMlcZK51DvvRf.html) recurrent neural networks》是谷歌 ICML 获奖论文，它解释了像素递归神经网络是如何帮图片“极致”建模的。在这篇文章中，作者在深度递归网络下建立了对自然图片的通用建模并显著提升了它的效率。此外，作者提出了一种新颖的二维 LSTM 层：ROW
 LSTM和 Diagonal BiLSTM，它能更容易扩展到其他数据上。

[17] [https://arxiv.org/pdf/1601.06759.pdf](https://arxiv.org/pdf/1601.06759.pdf)

《Conditional Image Generation with PixelCNN Decoders》来自谷歌DeepMind团队。他们研究一种基于PixelCNN（像素卷积神经网络）架构的模型，可以根据条件的变化生成新的图像。如果该模型输入ImageNet图像库的分类标签照片，该模型能生成多变的真实场景的照片，比如动物、风景等。如果该模型输入其他卷积神经生成的未见过的人脸照片，该模型能生成同一个人的不同表情、姿势的照片。

[18] [https://arxiv.org/pdf/1606.05328.pdf](https://arxiv.org/pdf/1606.05328.pdf)



