# 十九、机器人是怎么理解“日后再说”的 - jiangjingxuan的博客 - CSDN博客





2017年01月25日 13:50:32[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：551
个人分类：[动手做聊天机器人](https://blog.csdn.net/jiangjingxuan/article/category/6694788)













![](http://www.shareditor.com/uploads/media/default/0001/01/thumb_222_default_big.jpeg)



日后再说这个成语到了当代可以说含义十分深刻，你懂的，但是如何让计算机懂得可能有两种含义的一个词到底是想表达哪个含义呢？这在自然语言处理中叫做词义消歧，从本节开始我们从基本的结构分析跨入语义分析，开始让计算机对语言做深层次的理解

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

## 词义消歧

词义消歧是句子和篇章语义理解的基础，是必须解决的问题。任何一种语言都有大量具有多种含义的词汇，中文的“日”，英文的“bank”，法语的“prendre”……。

词义消歧可以通过机器学习的方法来解决。谈到机器学习就会分成有监督和无监督的机器学习。词义消歧有监督的机器学习方法也就是分类算法，即判断词义所属的分类。词义消歧无监督的机器学习方法也就是聚类算法，把词义聚成多类，每一类是一种含义。



## 有监督的词义消歧方法

### 基于互信息的词义消歧方法

这个方法的名字不好理解，但是原理却非常简单：用两种语言对照着看，比如：中文“打人”对应英文“beat a man”，而中文“打酱油”对应英文“buy some sauce”。这样就知道当上下文语境里有“人”的时候“打”的含义是beat，当上下文语境里有“酱油”的时候“打”的含义是buy。按照这种思路，基于大量中英文对照的语料库训练出来的模型就可以用来做词义消歧了，这种方法就叫做基于“互信息”的词义消歧方法。讲到“互信息”还要说一下它的起源，它来源于信息论，表达的是一个随机变量中包含另一个随机变量的信息量(也就是英文信息中包含中文信息的信息量)，假设两个随机变量X、Y的概率分别是p(x),
 p(y)，它们的联合分布概率是p(x,y)，那么互信息计算公式是：
`I(X; Y) = ∑∑p(x,y)log(p(x,y)/(p(x)p(y)))`
以上公式是怎么推导出来的呢？比较简单，“互信息”可以理解为一个随机变量由于已知另一个随机变量而减少的不确定性(也就是理解中文时由于已知了英文的含义而让中文理解更确定了)，因为“不确定性”就是熵所表达的含义，所以：
`I(X; Y) = H(X) - H(X|Y)`
等式后面经过不断推导就可以得出上面的公式，对具体推导过程感兴趣可以百度一下。

那么我们在对语料不断迭代训练过程中I(X; Y)是不断减小的，算法终止的条件就是I(X; Y)不再减小。

基于互信息的词义消歧方法自然对机器翻译系统的效果是最好的，但它的缺点是：双语语料有限，多种语言能识别出歧义的情况也是有限的(比如中英文同一个词都有歧义就不行了)。



### 基于贝叶斯分类器的消歧方法

提到贝叶斯那么一定少不了条件概率，这里的条件指的就是上下文语境这个条件，任何多义词的含义都是跟上下文语境相关的。假设语境(context)记作c，语义(semantic)记作s，多义词(word)记作w，那么我要计算的就是多义词w在语境c下具有语义s的概率，即：
`p(s|c)`
那么根据贝叶斯公式：
`p(s|c) = p(c|s)p(s)/p(c)`
我要计算的就是p(s|c)中s取某一个语义的最大概率，因为p(c)是既定的，所以只考虑分子的最大值：
`s的估计=max(p(c|s)p(s))`
因为语境c在自然语言处理中必须通过词来表达，也就是由多个v(词)组成，那么也就是计算：
`max(p(s)∏p(v|s))`
请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

下面就是训练的过程了：

p(s)表达的是多义词w的某个语义s的概率，可以统计大量语料通过最大似然估计求得：
`p(s) = N(s)/N(w)`
p(v|s)表达的是多义词w的某个语义s的条件下出现词v的概率，可以统计大量语料通过最大似然估计求得：
`p(v|s) = N(v, s)/N(s)`
训练出p(s)和p(v|s)之后我们对一个多义词w消歧的过程就是计算(p(c|s)p(s))的最大概率的过程



## 无监督的词义消歧方法

完全无监督的词义消歧是不可能的，因为没有标注是无法定义是什么词义的，但是可以通过无监督的方法来做词义辨识。无监督的词义辨识其实也是一种贝叶斯分类器，和上面讲到的贝叶斯分类器消歧方法不同在于：这里的参数估计不是基于有标注的训练预料，而是先随机初始化参数p(v|s)，然后根据EM算法重新估计这个概率值，也就是对w的每一个上下文c计算p(c|s)，这样可以得到真实数据的似然值，回过来再重新估计p(v|s)，重新计算似然值，这样不断迭代不断更新模型参数，最终得到分类模型，可以对词进行分类，那么有歧义的词在不同语境中会被分到不同的类别里。

仔细思考一下这种方法，其实是基于单语言的上下文向量的，那么我们进一步思考下一话题，如果一个新的语境没有训练模型中一样的向量怎么来识别语义？

这里就涉及到向量相似性的概念了，我们可以通过计算两个向量之间夹角余弦值来比较相似性，即：
`cos(a,b) = ∑ab/sqrt(∑a^2∑b^2)`


## 机器人是怎么理解“日后再说”的

回到最初的话题，怎么让机器人理解“日后再说”，这本质上是一个词义消歧的问题，假设我们利用无监督的方法来辨识这个词义，那么就让机器人“阅读”大量语料进行“学习”，生成语义辨识模型，这样当它听到这样一则对话时：
`有一位老嫖客去找小姐，小姐问他什么时候结账啊。嫖客说：“钱的事情日后再说。”就开始了，完事后，小姐对嫖客说：“给钱吧。”嫖客懵了，说：“不是说日后再说吗？”小姐说：“是啊，你现在不是已经日后了吗？”`
辨识了这里的“日后再说”的词义后，它会心的笑了




