# 逻辑回归的理解 - jiangjingxuan的博客 - CSDN博客





2019年02月22日 13:05:44[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：42








## 逻辑回归(Logistic Regression)

### 1. 回归(Regression)
- 回归，我的理解来说，其直观的理解就是拟合的意思。我们以线性回归为例子，在二维平面上有一系列红色的点，我们想用一条直线来尽量拟合这些红色的点，这就是线性回归。回归的本质就是我们的预测结果尽量贴近实际观测的结果，或者说我们的求得一些参数，经过计算之后的预测结果尽可能接近真实值。

![](https://i.imgur.com/la7bDLS.jpg)

### 2. 逻辑回归的由来
- 对于二类线性可分的数据集，使用线性感知器就可以很好的分类。如下图中红色和蓝色的点，我们使用一条直线x1+x2=3
- x1​+x2​=3就可以区分两种数据集，在直线上方的属于红色类，直线下方的属于蓝色类。

![](https://i.imgur.com/uPQ0xxk.jpg)
- 但是如果二类线性不可分的数据集，我们无法找到一条直线能够将两种类别很好的区分，即线性回归的分类法对于线性不可分的数据无法有效分类。例如下图中的红色点和蓝色点，我们无法使用一条直线很好的区分这两类，但是我们可以使用非线性分类器，如果我们使用x12+x22=1
- x1​2+x2​2=1，在圆外面的为红色类，在圆里面的一类为蓝色类。

![](https://i.imgur.com/TdJn8NO.jpg)
- 
诚然，数据线性可分可以使用线性分类器，如果数据线性不可分，可以使用非线性分类器，这里似乎没有逻辑回归什么事情。但是如果我们想知道对于一个二类分类问题，对于具体的一个样例，我们不仅想知道该类属于某一类，而且还想知道该类属于某一类的概率多大,有什么办法呢？

- 
线性回归和非线性回归的分类问题都不能给予解答，因为线性回归和非线性回归的问题，假设其分类函数如下：

	y=wx+b

- 
y=wx+b

- 
y的阈值处于（−∞，+∞）


（−∞，+∞），此时不能很好的给出属于某一类的概率，因为概率的范围是[0,1],我们需要一个更好的映射函数，能够将分类的结果很好的映射成为[0,1]之间的概率，并且这个函数能够具有很好的可微分性。在这种需求下，人们找到了这个映射函数，即逻辑斯谛函数，也就是我们常说的sigmoid函数，其形式如下：

11+e−z
- 
1+e−z1​

- 
sigmoid函数图像如下图所示


![](https://i.imgur.com/T53inQT.png)
- sigmoid函数完美的解决了上述需求，而且sigmoid函数连续可微分。
- 假设数据离散二类可分，分为0类和1类,如果概率值大于1/2，我们就将该类划分为1类，如果概率值低于1/2,我们就将该类划分为0类。当z取值为0的时候，概率值为1/2，这时候需要人为规定划分为哪一类。

### 3. 逻辑回归的损失函数(Loss Function)和成本函数(Cost Function)
- 在二类分类中，我们假定sigmoid输出结果表示属于1类的概率值，我们很容易想到用平方损失函数，即

![](https://i.imgur.com/3C1RzcD.jpg)
- 在这种情况下，我们φ(z(i))表示sigmoid对第i个值的预测结果，我们将sigmoid函数带入上述成本函数中，绘制其图像，发现这个成本函数的函数图像是一个非凸函数，如下图所示，这个函数里面有很多极小值，如果采用梯度下降法，则会导致陷入局部最优解中,有没有一个凸函数的成本函数呢？

![](https://i.imgur.com/R5r6LvC.jpg)
- 假设sigmoid函数φ(z)表示属于1类的概率，于是做出如下的定义：

![](https://i.imgur.com/IBlS1zQ.jpg)
- 将两个式子综合来，可以改写为下式：

![](https://i.imgur.com/KdpNVpf.jpg)
- 
上式将分类为0和分类和1的概率计算公式[合二为一](https://www.baidu.com/s?wd=%E5%90%88%E4%BA%8C%E4%B8%BA%E4%B8%80&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)。假设分类器分类足够准确，此时对于一个样本，如果它是属于1类，分类器求出的属于1类的概率应该尽可能大，即p(y=1lx)尽可能接近1；如果它是0类，分类器求出的属于0类的概率应该尽可能大，即p(y=0lx)尽可能接近1。

- 
通过上述公式对二类分类的情况分析，可知我们的目的是求取参数w和b，使得p(ylx)对0类和1类的分类结果尽可能取最大值，然而实际上我们定义的损失函数的是求最小值，于是，很自然的，我们想到对p(ylx)式子加一个负号，就变成了求最小值的问题，这就得到了逻辑回归中的损失函数。

- 
不过，为了计算方便，我们通常对上述式子取log，因而得到下式：


![](https://i.imgur.com/Swgjn66.jpg)
- 公式(1)是对概率公式取log，公式(2)是对公式(1)取相反数。上述公式的函数图像如下图所示。这是一个凸函数（斜率是非单调递减的函数即凸函数），因此可以用梯度下降法求其最小值。

![](https://i.imgur.com/CAHBRdg.jpg)
- 根据损失函数是单个样本的预测值和实际值的误差，而成本函数是全部样本的预测值和实际值之间的误差，于是对所有样本的损失值取平均数，得到我们的成本函数：

![](https://i.imgur.com/cNXT1Ev.jpg)
- 损失函数是凸函数，m个损失函数的和仍然是凸函数，因而可以用梯度下降法求最小值。

### 4. 极大似然法求解逻辑回归
- 还可以用我们熟知的统计学知识——极大似然法估计逻辑回归中的参数w和b，上述得到的logp(y|w,x),假设目前有m组样本，分别为(x1,y1),(x2,y2)...(xm,ym)
- (x1​,y1​),(x2​,y2​)...(xm​,ym​)，其中xi表示第i个样本的特征，yi表示第i个样本的类别，yi = 0或者1，利用极大似然法的原则，假设所有训练样本独立同分布，则联合概率为所有样本概率的乘积，即：

![](https://i.imgur.com/kaGRRZc.jpg)
- 对上述公式两边取对数，得到下述公式，是不是对这个公式优点熟悉呢？这个公式就是我们的成本函数的和，对于这个公式和成本函数来说，取平均值和不取平均值没有影响。

![](https://i.imgur.com/zHLnJJc.jpg)
- 按照极大似然法求极值的方法，分别对w的每个参数求偏导数使其为0，得到对数似然方程组，求解该方程，便可以到的w的参数。只是如果参数很多，求解方程组就会很复杂，此时可以考虑梯度下降法来求解。

### 5. 总结

- 逻辑回归最大的优势在于它的输出结果不仅可以用于分类，还可以表征某个样本属于某类别的概率。
- 
逻辑斯谛函数将原本输出结果从范围（−∞，+∞）

- 
（−∞，+∞） 映射到(0,1)，从而完成概率的估测。

- 
逻辑回归得判定的阈值能够映射为平面的一条判定边界，随着特征的复杂化，判定边界可能是多种多样的样貌，但是它能够较好地把两类样本点分隔开，解决分类问题。

- 
求解逻辑回归参数的传统方法是梯度下降，构造为凸函数的代价函数后，每次沿着偏导方向(下降速度最快方向)迈进一小部分，直至N次迭代后到达最低点。


原文地址：[https://blog.csdn.net/t46414704152abc/article/details/79574003](https://blog.csdn.net/t46414704152abc/article/details/79574003)



