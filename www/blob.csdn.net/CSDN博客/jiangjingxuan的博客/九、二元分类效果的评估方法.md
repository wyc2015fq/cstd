# 九、二元分类效果的评估方法 - jiangjingxuan的博客 - CSDN博客





2017年01月25日 10:22:07[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：391












![](http://www.shareditor.com/uploads/media/default/0001/01/thumb_136_default_big.jpeg)



效果评估是模型选择和算法设计的重要步骤，知道评估优劣才能选择最佳的模型和算法，本节介绍一些有关评估方法的定义，凡是在统计或大数据领域都用得到

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

## 真阳性

true positives, TP

## 真阴性

true negatives, TN

## 假阳性

false positives, FP

## 假阴性 

false negatives, FN)



## 准确率

分类器预测正确性的比例，可以通过LogisticRegression.score() 来计算准确率



## 精确率

分类器预测出的脏话中真的是脏话的比例

P=TP/(TP+FP)



## 召回率

也叫灵敏度。所有真的脏话被分类器正确找出来的比例。

R=TP/(TP+FN)



## 综合评价指标

F-measure，精确率和召回率的调和均值。精确率和召回率都不能从差的分类器中区分出好的分类器，综合评价指标平衡了精确率和召回率。

1/F+1/F=1/P+1/R即

F=2*PR/(P+R)



## 误警率

假阳性率，所有阴性样本中分类器识别为阳性的样本所占比例 

F=FP/(TN+FP)



## ROC(Receiver Operating Characteristic)

ROC曲线画的是分类器的召回率与误警率(fall-out)的曲线 





## AUC(Area Under Curve)

ROC曲线下方的面积,它把ROC曲线变成一个值,表示分类器随机预测的效果 

scikit-learn画ROC曲线和AUC值的方法如下：

```python
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
false_positive_rate, recall, thresholds = roc_curve(pred, predictions)
roc_auc = auc(false_positive_rate, recall)
plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)
plt.show()
```






