# 互联网金融爬虫怎么写－第三课 雪球网股票爬虫（ajax分析） - jiangjingxuan的博客 - CSDN博客





2017年02月27日 09:28:20[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：370








大家好啊，话说好久没有出来活动了，组织上安排写代码写了很久，终于又被放出来写教程了，感谢大家一直的支持和厚爱，我会一如既往的帮助大家完成爬虫工程师从入门到放弃的升华。


> 
工具要求与基础知识：


工具要求：

1).神箭手云爬虫框架--这个是爬虫的基础；

2).Chrome浏览器和Chrome的插件XpathHelper--这个用来测试Xpath写的是否正确;

3).Advanced REST Client--用来模拟提交请求。

基础知识：

本教程中设计到web的一些基础原理，如果不了解的，最好提前做一些功课。



> 
开始ajax分析


咱们废话不多说，接着上一课继续研究雪球网，雪球网通过狂拽的js请求，直接把我们打回原形，辛辛苦苦分析半天的页面前功尽弃，不过不要紧，咱们想爬别人数据，就不能害怕js渲染，前面的课程大多都通过种种方式绕过了js请求的方式来完成爬虫，那么这节课，就通过雪球网，来实实在在的面对一下我们的这个无法回避的敌人--ajax。

首先依然打开chrome的开发者工具，点击network的标签。

**注意事项：**

1).当你打开一个页面，再点开network标签时是不会有信息的，我们需要在打开的情况下，刷新一下页面；

2).为了防止页面突然的跳转而丢失信息，一定要勾上preserved单选框。



具体设置如下图：

![](http://images2015.cnblogs.com/blog/954944/201607/954944-20160729150456091-1690610401.png)



刷新页面之后，选中xhr小标签之后，可以清晰的看到一个ajax请求，我们点开预览看下：

![](http://images2015.cnblogs.com/blog/954944/201607/954944-20160729150511278-1998050320.png)



看到这个画面，是不是觉得祖国的天空又蓝了一点？

![](http://images2015.cnblogs.com/blog/954944/201607/954944-20160729150519653-2095340395.jpg)

**首先**，我们先确保我们可以通过直接访问拿到这个ajax请求，我们先把这个请求的地址复制出来：

> 
http://xueqiu.com/stock/cata/stocklist.json?page=1&size=30&order=desc&orderby=percent&type=11%2C12&_=1469588824728




**然后，**单独粘贴到浏览器的地址栏中，访问看看效果，为了保证实验的独立性，建议大家打开一个chrome的隐身窗口，这样可以防止之前的cookie的污染。

![](http://images2015.cnblogs.com/blog/954944/201607/954944-20160729150526528-1784733089.png)

Oh no~  雪球的码农，我们出来聊一下，我保证不打死你们~

这个时候，很多初学者甚至有一定经验的工程师都方了。不要紧，其实只要不需要登录，我们都还是来得及下班的。



一般来说，限制来自于常见的三个情况：

1).cookie ； 2).referer ; 3).url中的参数

由于2)和3)容易测试，我们一般先测试2)和3)，测试方法就是参照我们在浏览器中能正常访问到时的请求，删掉我们可能觉得不重要的参数，逐步测试。**这里非常强调的是**，我们必须使用控制变量法，首先我们需要重现能够成功获取数据的情况，然后在一个一个变量进行调整，最终将无关的参数全部去除，并找到最核心的参数，这里我们还需要使用一个模拟提交请求的工具。我们这里使用的是chrome的插件Advanced REST Client。同类型的工具很多，大家也可以根据自己的习惯挑选。



我们先将cookie，referer和url完整的复制到请求中去，点击访问看看能不能拿到数据：

![](http://images2015.cnblogs.com/blog/954944/201607/954944-20160729150534419-681130396.png)



下面结果部分被截断了，结果返回的是正常的数据。

那么我们先来确定下2)和3)是否影响，通过删除referer以及url中不相关的参数，重新点击访问我们可以知道，这些参数并不影响返回结果，那这个时候，就只剩下一个可能，就是cookie，当然这个可能是我们最不希望看到了，当然cookie的问题依然分为两种情况：

1).http response返回的cookie设置；2).js对cookie的设置。



如果是1)，那还没什么大不了的；如果是2)的话，那估计整个人都要不开心了。

第一件事，我们依然要把cookie中不相关的参数，特别是一些统计代码的cookie删除掉，他们通常很长，很干扰，但是毫无作用。常见的百度统计有这样一些cookie: Hm_Lpvt开头和Hm_lvt开头的，当然一般Hm_开头的大概率百度统计的，其他的大家自己在做的过程中去做总结，这里就不一一解释了。

删除之后发现，只要有xq_a_token这个cookie就可以返回正常的数据，那么我们现在就找找这个cookie是在哪里设置的。



最简单的，先访问一下首页，看看response：

![](http://images2015.cnblogs.com/blog/954944/201607/954944-20160729150542341-1574085831.png)



哈哈，可以看到reponse里面的set-cookie中已经有了xq_a_token这个参数，so easy!我们把这个cookie加入到请求中去，顺利的请求到了数据，可见，在爬取这个ajax之前，只需要先访问一下首页就可以获取我们需要的cookie值了。



最后我们再回头看一下ajax的url，经过筛选剩余的url如下：

> 
http://xueqiu.com/stock/cata/stocklist.json?page=1&size=30&order=desc&orderby=percent&type=11%2C12


很高兴的看到了page和size，另外还居然有order和orderby，只能说雪球实在是太贴心，当然最后还有一个type，这个我们多点几个分类就可以看出，这个是沪深一览的分类，相对应的还有美股一览和港股一览。这里我们就不做详细介绍了。



分析就到这里，下一课，我们会开始根据这一课的分析，完成整个代码的编写。



