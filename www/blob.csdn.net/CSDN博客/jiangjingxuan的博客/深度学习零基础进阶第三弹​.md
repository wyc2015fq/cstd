# 深度学习零基础进阶第三弹​ - jiangjingxuan的博客 - CSDN博客





2017年02月27日 08:20:14[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：586








![深度学习零基础进阶第三弹​｜干货分享](http://static.leiphone.com/uploads/new/article/740_740/201610/580a049ce23db.png?imageMogr2/format/jpg/quality/90)

雷锋网曾编译[《干货分享 | 深度学习零基础进阶大法！》](http://www.leiphone.com/news/201610/tgtcVePX2kdDlHfL.html)，相信读者一定对[深度学习](http://www.leiphone.com/news/201701/LqwiP7VUJO9DgBPi.html)的历史有了一个基本了解，其基本的模型架构（CNN/RNN/LSTM）与深度学习如何应用在图片和[语音识别](http://www.leiphone.com/news/201412/SPIrQG1uFa6jWMVZ.html)上肯定也不在话下了。今天这一部分，我们将通过新一批论文，让你对深度学习的方式与深度学习在不同领域的运用有个清晰的了解。由于第二部分的论文开始向细化方向延展，因此你可以根据自己的研究方向酌情进行选择。雷锋网对每篇论文都增加了补充介绍，分上下两篇，由老吕IO及奕欣编译整理，未经雷锋网(公众号：雷锋网)许可不得转载。


**4. 循环神经网络/序列到序列模式**

《Generating sequences with recurrent neural networks》一文由 Graves 和 Alex 两位专家合力撰写，这篇论文解释了用递归神经网络生成手写体的原理。

[19] [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf)

《Learning phrase representations using RNN encoder-decoder for statistical machine translation》完成了将英文转译为法文的任务，使用了一个 encoder-decoder 模型，在 encoder 的 RNN 模型中是将序列转化为一个向量。在 decoder 中是将向量转化为输出序列，使用 encoder-decoder 能够加入词语与词语之间的顺序信息。此外，还将序列表达为一个向量，利用向量能够清楚的看出那些语义上相近的词聚集在一起。

[20] [https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)

《Sequence to sequence learning with neural networks》是谷歌的 I. Sutskever 等人提出的一种序列到序列的学习方法, 最直接的应用就是机器翻译。

[21] [http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf](http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf)

Attention 机制最早是在视觉图像领域提出来的，随后 Bahdanau 等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，使用类似 attention 的机制在机器翻译任务上将翻译和对齐同时进行，他们算是第一个提出将 attention 机制应用到 NLP 领域中的团队。

[22] [https://arxiv.org/pdf/1409.0473v7.pdf](https://arxiv.org/pdf/1409.0473v7.pdf)

《A Neural Conversational Model》是最早应用于序列到序列框架建立对话模型的论文，即便其中使用的模型结构并不复杂，网络层数数量也不多，但效果是却很可观。

[23] [https://arxiv.org/pdf/1506.05869.pdf](https://arxiv.org/pdf/1506.05869.pdf)

**5.神经图灵机**

《Neural turing machines》一文介绍了神经图灵机，一种从生物可行内存和数字计算机的启发产生的神经网络架构。如同传统的神经网络，这个架构也是可微的端对端的并且可以通过梯度下降进行训练。我们的实验展示了它有能力从样本数据中学习简单的算法并且能够将这些算法推广到更多的超越了训练样本本身的数据上。绝对的五星推荐。

[24] [https://arxiv.org/pdf/1410.5401.pdf](https://arxiv.org/pdf/1410.5401.pdf)

神经图灵机是当前深度学习领域三大重要研究方向之一。论文《Reinforcement learning neural Turing machines》利用增强学习算法来对神经网络进行训练，从而使神经图灵机的界面变得表现力十足。

[25] [https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf](https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf)

《Memory networks》由四位专家撰写而成，实际上所谓的 Memory Network 是一个通用的框架而已，内部的输入映射、更新记忆映射、输出映射、响应映射都是可以更换的。

[26] [https://arxiv.org/pdf/1410.3916.pdf](https://arxiv.org/pdf/1410.3916.pdf)

《End-to-end memory networks》在算法层面解决了让记忆网络端对端进行训练的问题，在应用方面则解决了问题回答和语言建模等问题。

[27] [http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)

《Pointer networks》中提出了一种新型的网络架构，用来学习从一个序列输入到一个序列输出的推导。跟以往的成果不同之处在于，输入输出的长度都是可变的，输出的长度跟输入有关。

[28] [http://papers.nips.cc/paper/5866-pointer-networks.pdf](http://papers.nips.cc/paper/5866-pointer-networks.pdf)

《Hybrid computing using a neural network with dynamic external memory》是谷歌 DeepMind 首发于《自然》杂志的论文，它介绍了一种记忆增强式的神经网络形式，其被称为可微神经计算机（differentiable neural computer），研究表明它可以学习使用记忆来回答有关复杂的结构化数据的问题，其中包括人工生成的故事、家族树、甚至伦敦地铁的地图。研究还表明它还能使用强化学习解决拼图游戏问题。五星推荐。

[29] [https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf)

**6. 深度强化学习**

终于！我们来到了深度强化学习的门下。说到这个名词，怎么能不提第一篇提出深度强化学习的论文呢？Mnih 所写的《Playing atari with deep reinforcement learning》将卷积神经网络和 Q Learning 结合，使用同一个网络玩 Atari 2600（也就是打方块）这类只需要短时记忆的 7 种游戏。结果显示，这种算法无需人工提取特征，还能生成无限样本以实现监督训练。

[30] [http://arxiv.org/pdf/1312.5602.pdf](http://arxiv.org/pdf/1312.5602.pdf)

而至于深度强化学习的里程碑之作，同样要属同一作者的《Human-level control through deep reinforcement learning》，作者发明了一个名为DQN也就是深度Q网络的东西，让人工神经网络能直接从传感器的输入数据中获得物体分类，成功实现端到端的强化学习算法从高维的传感器输入中直接学习到成功策略。

[31] [http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)

而接下来这篇名为《Dueling network architectures for deep reinforcement learning》的文章则提出了一个新的网络——竞争架构网络。它包括状态价值函数和状态依存动作优势函数。这一架构在多种价值相似的动作面前能引发更好的政策评估。此文当选 ICML 2016最佳论文大奖。

[32] [http://arxiv.org/pdf/1511.06581](http://arxiv.org/pdf/1511.06581)

《Asynchronous methods for deep reinforcement learning》由 DeepMind 出品，主要增强了 Atari 2600 的游戏效果，也被视为通过多个实例采集样本进行异步更新的经典案例。

[33] [http://arxiv.org/pdf/1602.01783](http://arxiv.org/pdf/1602.01783)

比起传统的规划方法，《Continuous control with deep reinforcement learning》里提到的DQL方法能够应用于连续动作领域，鲁棒解决了  20 个仿真运动，采用的是基于ICML 2014的Deterministic policy gradient （DPG）的 actor-critic 算法，名为 DDPG。

[34] [http://arxiv.org/pdf/1509.02971](http://arxiv.org/pdf/1509.02971)

《Continuous Deep Q-Learning with Model-based Acceleration》采用了 Advantage Function 完成增强学习工作，但主要集中于变量连续行动空间。而就像标题所言，为了加快机器经验获取，研究还用卡尔曼滤波器加局部线性模型。实验结果显示，这种方法比前一篇论文提及的 DDPG 要好些。

[35] [http://arxiv.org/pdf/1603.00748](http://arxiv.org/pdf/1603.00748)

Schulman的《Trust region policy optimization》可谓是计算机玩游戏的一大突破，这个名为 TRPO 的算法所呈现的结果丝毫不逊色于 DeepMind 的研究成果，展示了一种广义的学习能力。除了叫[机器人](http://www.leiphone.com/category/robot)走路，我们还能让它成为游戏高手。

[36] [http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf)

接下来介绍的这篇论文就是鼎鼎大名的 AlphaGo 所运用的算法，《Mastering the game of Go with deep neural networks and tree search》里，谷歌运用了 13 层的策略网络，让计算机学会用蒙特卡罗搜索树玩围棋游戏。当然，五星推荐此篇，不服来辩。

[37]  [http://willamette.edu/~levenick/cs448/goNature.pdf](http://willamette.edu/~levenick/cs448/goNature.pdf)

**7. 无监督特征学习**

《Deep Learning of Representations for Unsupervised and Transfer Learning》可谓无监督特征学习的开山之作。

[38] [http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf](http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf)

而接下来的这篇《Lifelong Machine Learning Systems: Beyond Learning Algorithms》主要提到的观点是，如果一个具有Lifelong Machine Learning能力的[机器学习](http://www.leiphone.com/news/201609/SJGulTsdGcisR8Wz.html)系统，是否能够使用解决此前问题的相关知识帮助它解决新遇到的问题，也就是举一反三的能力。文章在
 2013 年的AAAI 春季研讨会上首次提出。

[39] [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf)

[人工智能](http://www.leiphone.com/category/ai)教父又来了，他这次和 Dean 合作带来的是《Distilling the knowledge in a neural network》，也就是压缩神经网络。不过核心创新貌似不多，所以给个四星吧。

[40] [http://arxiv.org/pdf/1503.02531](http://arxiv.org/pdf/1503.02531)

《Policy distillation》，文章由谷歌大神Andrei Alexandru Rusu 所写，同款文章还有 Parisotto 的《Actor-mimic: Deep multitask and transfer reinforcement learning》，都是在讲 RL 域的问题。

[41] [http://arxiv.org/pdf/1511.0629](http://arxiv.org/pdf/1511.0629)

[42] [http://arxiv.org/pdf/1511.06342](http://arxiv.org/pdf/1511.06342)

这里还有另外一篇 Andrei 的文章，名为《Progressive neural networks》，提出了一项名为“渐进式神经网络”的算法，即在仿真环境中训练机器学习，随后就能把知识迁移到真实环境中。无疑，这将大大加速机器人的学习速度。

[43] [https://arxiv.org/pdf/1606.04671](https://arxiv.org/pdf/1606.04671)

**8. 一步之遥**

以下五篇论文虽然并不是完全针对深度学习而推荐，但包含的一些基本思想还是具有借鉴意义的。

《Human-level concept learning through probabilistic program induction》五星推荐，文章主要介绍了贝叶斯学习程序（BPL）框架，“如何依靠简单的例子来对新概念进行学习和加工，学习主体是人类。”

[44] [http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf](http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf)

而读读 Koch 的《Siamese Neural Networks for One-shot Image Recognition》和这篇《One-shot Learning with Memory-Augmented Neural Networks》着实很有[必要](http://www.leiphone.com/news/201511/tgBJPsUzzzgvFHLp.html)。

[45] [http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf)

[46][http://arxiv.org/pdf/1605.06065](http://arxiv.org/pdf/1605.06065)

将重点放在大数据上的《Low-shot visual object recognition》则是走向图像识别的必要一步。 

[47][http://arxiv.org/pdf/1606.02819](http://arxiv.org/pdf/1606.02819)

以上便是第二阶段值得一读的论文，敬请期待后续更新。



