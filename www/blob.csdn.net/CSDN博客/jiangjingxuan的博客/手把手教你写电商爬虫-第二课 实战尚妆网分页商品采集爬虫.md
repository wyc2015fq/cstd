# 手把手教你写电商爬虫-第二课 实战尚妆网分页商品采集爬虫 - jiangjingxuan的博客 - CSDN博客





2017年02月27日 09:22:27[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：234
个人分类：[手把手教你写爬虫](https://blog.csdn.net/jiangjingxuan/article/category/6749897)









如果没有看过第一课的朋友，请先移步第一课，第一课讲了一些基础性的东西，通过软柿子"切糕王子"这个电商网站好好的练了一次手，相信大家都应该对写爬虫的流程有了一个大概的了解，那么这课咱们就话不多说，正式上战场，对垒尚妆网。

首先，向我们被爬网站致敬，没有他们提供数据，我们更是无从爬起，所以先安利一下尚妆网：

经营化妆品时尚购物，大数据为驱动，并依托智能首饰为入口的新一代智慧美妆正品电子商务平台。其创始团队来自天猫、支付宝、欧莱雅、薇姿等互联网公司和化妆品集团。


好吧，我很懒，直接从百度知道里抄过来的，不过不代表我没有诚意。OK，言归正传，我们先把我们的工具包拿出来：

**1、神箭手云爬虫框架，**

**2、Chrome浏览器**

**3、Chrome的插件XpathHelper 不知道是干嘛的同学请移步第一课**

古代战士上战场前，必须先好好的观察对手，所谓知己知彼，百战不殆。我们先来观察一下尚妆网

![](https://img-blog.csdn.net/20160512141211505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


从首页大家能看出什么？说美女很美的，还有说美女表情很到位的同学，你们可以先回家了。

![](https://img-blog.csdn.net/20160512154904952?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


剩下的同学，我们继续了：

可以看出，作为一个完善的电商网站，尚妆网有着普通电商网站所拥有的主要的元素，包括分类，分页，主题等等。首先我们要确定我们希望要爬取哪一类数据，当然作为爬虫来说，全部爬下来不是不行，不过对于做实验来说，就没必要了。好，我们假设：我们要爬护肤里的面膜品类所有商品，价格和销量，至于为什么是面膜，你们猜呢？

废话太多了，我们开始爬虫三步走，跟着我再背诵一遍：1、选入口Url 2、限定内容页和中间页 3、写内容页抽取规则

**1、选定入口url**

这个简单，找到面膜页面的地址：

```
http://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C
```

好，就是它了。

**2、区分内容页和中间页**

好，重点来了，尚妆网的列表页面，是通过ajax动态加载了，这个怎么实现呢？我们先不着急，先看下内容页

```
http://item.showjoy.com/sku/26551.html

http://item.showjoy.com/sku/100374.html
```

内容页很简单，我们直接提取成正则表达式

```
http://item\\.showjoy\\.com/sku/\\d+\\.html
```

那么列表页呢？首先，第一个当然是：

```
http://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C
```



下一页的链接是什么呢？这个时候就需要借助chrome浏览器的开发者工具，我们打开工具，切换到network选项卡，向下滑动加载下一页，可以看到展示出的连接地址：

![](https://img-blog.csdn.net/20160512142945418?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


注意，可以忽略掉png这些图片的文件，直接看到下一页的连接，我们将链接复制出来：

```
http://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C&stock=1&page=4&_synToken=59a6c555b0947486769f35d010353cd5
```

看着好像很复杂，不过page我认识，其他的可以去掉吗？我们试一下访问

```
http://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C&page=4
```

貌似正常打开，而且也可以显示不同的商品，就此我们可以看出来，这个ajax加载下一页不过是一个纸老虎，根本没什么可怕的。我们将这个提取成正则表达式，另外
 值得注意的是，由于我们第一页可能是没有page的，所以也需要考虑没有page参数的情况

```
http://list\\.showjoy\\.com/search/\\?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C(&page=\\d+)?
```



这里再次提醒大家，注意正则的点和问好都是要转义的，并且转义需要两个\，好，第二步大功告成。

第三步：就是写内容页的抽取规则了，我们就抽取商品名称，评价数和成交数这三项数据吧，有人要问了，为啥不要价格呢。我只能说，too
 young too native，你打开商品页面的时候，有没有注意到价格的地方也一个快速的异步加载。考虑到咱们毕竟才第二课，而且刚刚还没那个ajax搞得虎躯一震，差 一点把这节课改成第三课，所以咱们这里先降低点难度，下一课咱们用一节课的时间来探讨下这个价格该怎么提取。

![](https://img-blog.csdn.net/20160512150459991?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


根据前面课程教的方案，我们同样的方法，写出xpath：

标题：//h3[contains(@class,"choose-hd")]

评价：//div[contains(@class,"dtabs-hd")]/ul/li[2]

成交记录：//div[contains(@class,"dtabs-hd")]/ul/li[3]



通过xpath helper进行验证之后没有问题，这样我们可以组合代码得到下面的结果

```
![复制代码](http://common.cnblogs.com/images/copycode.gif)

    var configs = {  
        domains: ["www.showjoy.com","list.showjoy.com","item.showjoy.com"],  
        scanUrls: ["http://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C"],  
        contentUrlRegexes: ["http://item\\.showjoy\\.com/sku/\\d+\\.html"],  
        helperUrlRegexes: ["http://list\\.showjoy\\.com/search/\\?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C(\\&page=\\d+)?"],//可留空  
        fields: [  
            {  
                // 第一个抽取项  
                name: "title",  
                selector: "//h3[contains(@class,'choose-hd')]",//默认使用XPath  
                required: true //是否不能为空  
            },  
            {  
                // 第二个抽取项  
                name: "comment",  
                selector: "//div[contains(@class,'dtabs-hd')]/ul/li[2]",//使用正则的抽取规则  
                required: false //是否不能为空  
            },  
            {  
                // 第三个抽取项  
                name: "sales",  
                selector: "//div[contains(@class,'dtabs-hd')]/ul/li[3]",//使用正则的抽取规则  
                required: false //是否不能为空  
            }  
        ]  
    };  
      
    start(configs);  

![复制代码](http://common.cnblogs.com/images/copycode.gif)
```

可以看到在domains里 我填入了三个域名，这里是一定要注意的，因为他的列表页和详情页的域名都不一致，因此需要把每一个域名都写进去。

好了，代码运行正常，但是启动任务之后发现，怎么第二页的内容没有采集到呢？ 还有前面说的价格咱们也采集不到，到底还能不能愉快的玩耍了呢？ 我们第三课就给大家讲讲如何解决ajax页面的url发现和ajax加载内容的提取。



