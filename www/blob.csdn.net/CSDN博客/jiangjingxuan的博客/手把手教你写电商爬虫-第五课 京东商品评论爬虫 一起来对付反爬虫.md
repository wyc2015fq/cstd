# 手把手教你写电商爬虫-第五课 京东商品评论爬虫 一起来对付反爬虫 - jiangjingxuan的博客 - CSDN博客





2017年02月27日 09:24:34[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：296
个人分类：[手把手教你写爬虫](https://blog.csdn.net/jiangjingxuan/article/category/6749897)









四节课过去了，咱们在爬虫界也都算见过世面的人，现在再来一些什么ajax加载之类的小鱼小虾应该不在话下了，即使是淘宝这种大量的ajax，我们 祭上我们的核武器，也轻松应对了，这一课主要是来看看除了技术上的页面处理外，我们还会遇上更棘手的问题，就是反爬虫，当然现在有各种各样的反爬虫，今天 就先介绍最简单的一种：限制IP。

今天咱们的对手依然是业界大佬，马云最忌惮的男人，宅男心中爱恨交错的对象 - JD.COM

也不用我安利，特别是程序员，有几个没给京东送过钱的。废话不多说，先上工具：

**1、神箭手云爬虫，**

**2、Chrome浏览器 **

**3、Chrome的插件XpathHelper 不知道是干嘛的同学请移步第一课**

打开网站瞅一眼：

![](https://img-blog.csdn.net/20160513161851866?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

好了，相信我，截这张图绝对不是在虐你们这些单身狗。我们就是科学的研究一下这个页面，没啥特别的：大厂风，硬仗准备。

先来挑一个分类吧，这次挑一个大家都熟悉的互联网书类：




```
```java
http:
```

```java
//search.jd.com/Search?keyword=Python&enc=utf-8&book=y&wq=Python&pvid=33xo9lni.p4a1qb
```
```




你们的最爱，python从入门到放弃的全部资料。

和前面几节课类似的分析这节课就不做了，对于分页，ajax请求什么的，大家可以直接参考前面的四节课，这一刻主要特别的是，我们在采集商品的同时，会将京东的商品评价采集下来。同时呢，我们也探讨下该如何应对京东对IP的限制，OK，先直接上代码：

```
![复制代码](http://common.cnblogs.com/images/copycode.gif)

var configs = {  
    domains: ["search.jd.com","item.jd.com","club.jd.com"],  
    scanUrls: ["http://search.jd.com/Search?keyword=Python&enc=utf-8&qrst=1&rt=1&stop=1&book=y&vt=2&page=1&s=1&click=0"],  
    contentUrlRegexes: ["http://item\\.jd\\.com/\\d+.html"],  
    helperUrlRegexes: ["http://search\\.jd\\.com/Search\\?keyword=Python&enc=utf-8&qrst=1&rt=1&stop=1&book=y&vt=2&page=\\d+&s=1&click=0"],//可留空  
    fields: [  
        {  
            // 第一个抽取项  
            name: "title",  
            selector: "//div[@id='name']/h1",//默认使用XPath  
            required: true //是否不能为空  
        },  
        {  
            // 第一个抽取项  
            name: "productid",  
            selector: "//div[contains(@class,'fl')]/span[2]",//默认使用XPath  
            required: true //是否不能为空  
        },  
        {  
            name: "comments",  
            sourceType: SourceType.AttachedUrl,  
            attachedUrl: "http://club.jd.com/productpage/p-{productid}-s-0-t-3-p-0.html",  
            selectorType: SelectorType.JsonPath,  
            selector: "$.comments",  
            repeated: true,  
            children:[  
                {  
                    name: "com_content",  
                    selectorType: SelectorType.JsonPath,  
                    selector: "$.content"  
                },  
                {  
                    name: "com_nickname",  
                    selectorType: SelectorType.JsonPath,  
                    selector: "$.nickname"  
                },  
            ]  
        }  
    ]  
};  
configs.onProcessHelperUrl = function(url, content, site){  
    if(!content.indexOf("抱歉，没有找到")){  
        var currentPage = parseInt(url.substring(url.indexOf("&page=") + 6));  
        if(currentPage == 0){  
            currentPage = 1;  
        }  
        var page = currentPage + 2;  
        var nextUrl = url.replace("&page=" + currentPage, "&page=" + page);  
        site.addUrl(nextUrl);  
    }  
    return true;  
};  
var crawler = new Crawler(configs);  
crawler.start();  

![复制代码](http://common.cnblogs.com/images/copycode.gif)
```

这里主要给大家讲一下这个评论的配置，由于评论是多项，且评论还有子项，在框架中，是通过children关键字来配置的。具体参照代码既可，我们 可以在子项中在定义不同的字段，像这里的comments抽取项会有content和nickname两个子抽取项，分别对应的是评论的内容和昵称。

这里是一个简化的版本，由于京东页面相对很复杂，我们在抽取评论的时候，只抽取前一部分评论，当然我们还可以拿到更多的信息，包括评论数，评论人的等级等等，这里大家就自行探索吧。

最后，由于京东会对IP进行封锁，虽然说神箭手会自动分布式开启爬虫，不过依然扛不住京东大叔的封锁，因此这里需要通过接入代理IP解决这样的问题，类似开启js渲染，爬取速度会大大下降，需要大家耐心等待结果喽，代码如下：

```
configs.enableProxy = true;
```


大功告成，开启爬虫，喝杯咖啡，京东商品的评论就可以看到啦：

![](http://images2015.cnblogs.com/blog/954944/201605/954944-20160517172400560-1101260913.png)

评论因为是数字，因此会存储的时候，会直接存储成json格式：

![](http://images2015.cnblogs.com/blog/954944/201605/954944-20160517172409748-681183195.png)



