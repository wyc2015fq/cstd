# 二十八、脑洞大开：基于美剧字幕的聊天语料库建设方案 - jiangjingxuan的博客 - CSDN博客





2017年01月25日 13:56:17[jiangjingxuan](https://me.csdn.net/jiangjingxuan)阅读数：960












![](http://www.shareditor.com/uploads/media/default/0001/01/thumb_337_default_big.jpeg)



要让聊天机器人进行学习，需要海量的聊天语料库，但是网上的语料库基本上都是有各种标注的文章，并没有可用的对话语料，虽然有一些社区的帖子数据，但是既要花大把银子还不知道质量如何。笔者突然灵机一动，找到一个妙招能获取海量高质聊天语料，这下聊天机器人再也不愁语料数据了。

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址

## 美剧字幕

是的，你没有看错，我就是这样获取海量高质聊天语料的。外文电影或电视剧的字幕文件是一个天然的聊天语料，尤其是对话比较多的美剧最佳。为了能下载大量美剧字幕，我打算抓取字幕库网站www.zimuku.net，当然你也可以选择其他网站抓取。



## 自动抓取字幕

有关爬虫相关内容请见我的另一篇文章《[教你成为全栈工程师(Full Stack Developer) 三十-十分钟掌握最强大的python爬虫](http://www.shareditor.com/blogshow/?blogId=43)》。在这里我直接贴上我的抓取器重要代码(代码共享在了[https://github.com/warmheartli/ChatBotCourse](https://github.com/warmheartli/ChatBotCourse))：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import scrapy
from w3lib.html import remove_tags
from subtitle_crawler.items import SubtitleCrawlerItem

class SubTitleSpider(scrapy.Spider):
    name = "subtitle"
    allowed_domains = ["zimuku.net"]
    start_urls = [
            "http://www.zimuku.net/search?q=&t=onlyst&ad=1&p=20",
            "http://www.zimuku.net/search?q=&t=onlyst&ad=1&p=21",
            "http://www.zimuku.net/search?q=&t=onlyst&ad=1&p=22",
    ]

    def parse(self, response):
        hrefs = response.selector.xpath('//div[contains(@class, "persub")]/h1/a/@href').extract()
        for href in hrefs:
            url = response.urljoin(href)
            request = scrapy.Request(url, callback=self.parse_detail)
            yield request

    def parse_detail(self, response):
        url = response.selector.xpath('//li[contains(@class, "dlsub")]/div/a/@href').extract()[0]
        print "processing: ", url
        request = scrapy.Request(url, callback=self.parse_file)
        yield request

    def parse_file(self, response):
        body = response.body
        item = SubtitleCrawlerItem()
        item['url'] = response.url
        item['body'] = body
        return item
```

下面是pipeline.py代码：

```python
class SubtitleCrawlerPipeline(object):
    def process_item(self, item, spider):
        url = item['url']
        file_name = url.replace('/','_').replace(':','_')
        fp = open('result/'+file_name, 'w')
        fp.write(item['body'])
        fp.close()
        return item
```

看下我抓取的最终效果

```
[root@centos:~/Developer/ChatBotCourse/subtitle $] ls result/|head -1
http___shooter.zimuku.net_download_265300_Hick.2011.720p.BluRay.x264.YIFY.rar
[root@centos:~/Developer/ChatBotCourse/subtitle $] ls result/|wc -l
82575
[root@centos:~/Developer/ChatBotCourse/subtitle $] du -hs result/
16G   	result/
```

请尊重原创，转载请注明来源网站[www.shareditor.com](http://www.shareditor.com)以及原始链接地址



## 字幕文件的解压方法

### linux下怎么解压zip文件

直接执行unzip file.zip即可



### linux下怎么解压rar文件

[http://www.rarlab.com/download.htm](http://www.rarlab.com/download.htm)

wget http://www.rarlab.com/rar/rarlinux-x64-5.4.0.tar.gz

tar zxvf rarlinux-x64-5.4.0.tar.gz

./rar/unrar试试

解压命令：

unrar x file.rar



### linux下怎么解压7z文件

http://downloads.sourceforge.net/project/p7zip下载源文件，解压后执行make编译后bin/7za可用，用法

bin/7za x file.7z



## 最终字幕的处理方式

有关解压出来的文本字幕文件的处理，我后面的文章会详细讲解如何分词、如何组合，敬请期待。




