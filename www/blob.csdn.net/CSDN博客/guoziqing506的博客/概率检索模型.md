# 概率检索模型 - guoziqing506的博客 - CSDN博客





2018年08月13日 21:30:26[guoziqing506](https://me.csdn.net/guoziqing506)阅读数：1267
所属专栏：[信息检索学习笔记](https://blog.csdn.net/column/details/16835.html)









概率检索模型是当前信息检索领域效果最好的模型之一，它基于对已有反馈结果的分析，根据贝叶斯原理为当前查询排序。

我在之前的博客 [朴素贝叶斯分类](https://blog.csdn.net/guoziqing506/article/details/67638148) 中介绍了如何用朴素贝叶斯算法对数据进行分类，其实概率检索模型的基本原理与朴素贝叶斯分类是一样的。先回忆一下朴素贝叶斯算法的原理：对于测试元组$X$，最终目的是要计算对于不同的类$C_i$，计算后验概率$p(C_i|X)$，哪个类最大，就属于哪个类。而为了计算$p(C_i|X)$，则需要用贝叶斯公式做如下分解：



$\begin{equation}p(C_i|X) = \frac{p(X|C_i)p(C_i)}{p(X)}\end{equation}$

因为要比较大小，所以忽略$p(X)$，只需要考虑分子中的$p(X|C_i)p(C_i)$，其中$p(C_i)$可以通过抽样得到，那么问题转化为计算$p(X|C_i)$，$p(X|C_i)$代表$X$在类$C_i$中的概率。如果$X$由$n$个相互之间无关的属性组成，那么这个概率一般如下计算：



$\begin{equation}p(X|C_i) = \prod_{j = 1}^n p(X_j|C_i) \end{equation}$

其中$X_j$为测试元组的第$j$个属性值，如果属性是离散属性，那么$p(X_j|C_i) = \frac{|X_j|}{|C_i|}$，其中$\frac{|X_j|}{|C_i|}$表示类$C_i$的数据元组中拥有属性$X_j$的概率。如果属性是连续属性呢，你自己看上面那篇博文，我这里不说了。之所以说离散时的情况，是因为本文后面要用。以上就是朴素贝叶斯分类法的原理，我大概复述一遍，方便理解后面要说的东西。

## 1. 基本思想

概率检索模型与贝叶斯分类的思想非常接近，但还是有本质区别的。概率检索模型的根本目的不是分类，它不需要根据查询判断一个文档属于“相关”或者“不相关”，而是计算这个文档属于属于“相关”或者“不相关”的概率大小为文档排序。我将概率检索模型要解决的问题刻画如下。

**问题模型**：现在对于一个查询$q$，已知文档集中哪些与$q$是相关的（这类文档的类标号记为$C_1$），哪些与$q$是不相关的（这类文档的类标号记为$C_0$）。概率检索模型的核心是对于每一个文档$X$计算公式(1)，公式(1)计算出的$\alpha$代表了文档$X$属于“相关”类的概率与属于“不相关”类的概率的比值（也叫“优势比”）。显然，这个比值越大，代表该文档与查询的相关度越大，因此我们就把$\alpha$看做是相关度得分，最后通过$\alpha$将文档排序。



$\begin{equation}\alpha = \frac{p(C_1|X)}{p(C_0|X)} = \frac{p(X|C_1)p(C_1)}{p(X|C_0)p(C_0)}\end{equation}\tag{1}$

其中，$p(C_1|X)$和$p(C_0|X)$的计算过程如下：



$\begin{equation}\begin{aligned}&p(C_1|X) = \frac{p(X|C_1)p(C_1)}{p(X)}\\&p(C_0|X) = \frac{p(X|C_0)p(C_0)}{p(X)}\end{aligned}\end{equation}$

## 2. 推导过程

现在看看具体怎样计算公式(1)。首先，$p(C_1)$和$p(C_0)$其实对于所有的文档来说都是一样的，因为最终的目的是比较大小，所以我们忽略掉。$\alpha$的计算可以简写成如下形式：



$\begin{equation}\alpha = \frac{p(X|C_1)}{p(X|C_0)}\end{equation}\tag{2}$

接下来计算$p(X|C_i)$的方法就跟朴素贝叶斯中那个连乘的公式是一样的了，但是有一点不同，用朴素贝叶斯做数据分类的时候，一般默认所有数据元组的属性值都是存在的，而到了信息检索这就不一样了，我们知道文档由词项组成，而某一个词项可能在某一个文档中，也可能不在。

所以我们不妨记单词$w_j$在类$C_i$中随机选择的一篇文档中出现的概率为$p(w_j|C_i)$；那么单词$w_j$不在类$C_i$中随机选择的一篇文档中出现的概率就是$1 -  p(w_j|C_i)$，那么记为$p(\overline{w_j}|C_i)$好了。

现在就可以将$\alpha$的计算公式写成如下形式：



$\begin{equation}\alpha = \frac{\prod_{w_j \in X}p(w_j|C_1) \prod_{w_j \notin X}p(\overline{w_j}|C_1)}{\prod_{w_j \in X}p(w_j|C_0) \prod_{w_j \notin X}p(\overline{w_j}|C_0)}\end{equation}$

为了方便推导，将$p(w_j|C_1)$记为$p_j$，将$p(w_j|C_0)$记为$s_j$，则$\alpha$可以表示成下面的公式(3)：



$\begin{equation}\alpha = \frac{\prod_{w_j \in X}p_j \prod_{w_j \notin X} 1 - p_j}{\prod_{w_j \in X}s_j \prod_{w_j \notin X}1 - s_j}\end{equation}\tag{3}$

直接看公式(3)可能有点抽象，我举个例子尝试说明一下，假如文档集的词典为$\{w_1, w_2, w_3, w_4\}$，文档1拥有的词项为$\{w_1, w_3\}$，那么文档1的$\alpha$值可以如下计算：



$\begin{equation}\alpha = \frac{p_1p_3 \cdot (1 - p_2)(1 - p_4)}{s_1s_3 \cdot (1 - s_2)(1 - s_4)}\end{equation}$

现在，对公式(3)做一个数学上的等价变换，如下：



$\begin{equation}\begin{aligned}\alpha &= \frac{\prod_{w_j \in X}p_j \prod_{w_j \notin X}1 - p_j}{\prod_{w_j \in X}s_j \prod_{w_j \notin X}1 - s_j} = \frac{\prod_{w_j \in X}p_j}{\prod_{w_j \in X}s_j} \cdot \frac{\prod_{w_j \notin X}1 - p_j}{\prod_{w_j \notin X}1 - s_j}\\&= (\frac{\prod_{w_j \in X}p_j}{\prod_{w_j \in X}s_j} \cdot \frac{\prod_{w_j \in X}1 - s_j}{\prod_{w_j \in X}1 - p_j}) \cdot (\frac{\prod_{w_j \in X}1 - p_j}{\prod_{w_j \in X}1 - s_j} \frac{\prod_{w_j \notin X}1 - p_j}{\prod_{w_j \notin X}1 - s_j})\\&= \frac{\prod_{w_j \in X}p_j(1 - s_j)}{\prod_{w_j \in X}s_j(1 - p_j)} \cdot \frac{\prod 1 - p_j}{\prod 1 - s_j} \end{aligned}\end{equation}\tag{4}$

其中$p_j$和$s_j$对于任意文档来说都一样，所以公式(4)的第二部分可以忽略，这也是我上面经过这么复杂的公式计算的原因，就是要将文档排序的比较依据化简成



$\begin{equation}\alpha = \frac{\prod_{w_j \in X}p_j(1 - s_j)}{\prod_{w_j \in X}s_j(1 - p_j)}\end{equation}$

用$\log$函数进一步处理，得到：



$\begin{equation}\sum_{j = 1}^n \log \frac{p_j}{1 - p_j} + \log \frac{1 - s_j}{s_j}\end{equation}\tag{5}$

也就是说，现在只要能计算出$p_j$和$s_j$就成功了。在计算之前，我们先写出下面的索引项出现列联表：
|$~$|$相关文档数$|$不相关文档数$|$总文档数$|
|----|----|----|----|
|包含$w_j$的文档|$r_j$|$n_j - r_j$|$n_j$|
|不包含$w_j$的文档|$|C_1| - r_j$|$N - n_j - |C_1| + r_j$|$N - n_j$|
|所有文档|$|C_1|$|$|C_0|$|N|

根据这个表可以得到以下计算公式： 


$\begin{equation}\begin{aligned}& p_j = \frac{r_j}{|C_1|}\\& s_j = \frac{n_j - r_j}{|C_0|}\end{aligned}\end{equation}$

因为在公式(5)中，我们用$\log$函数进行了处理，所以我们在$p_j$和$s_j$的计算公式中分子加0.5，分母加1，做平滑计算：



$\begin{equation}\begin{aligned}& p_j = \frac{r_j + 0.5}{|C_1| + 1}\\& s_j = \frac{n_j - r_j + 0.5}{|C_0| + 1}\end{aligned}\end{equation}$

把上面的结果代入公式(5)，得到：



$\begin{equation}\sum_{j = 1}^n \log \frac{(r_j + 0.5)(|C_0| - n_j + r_j + 0.5)}{(|C_1| -  r_j + 0.5)(n_j -  r_j + 0.5)}\end{equation}\tag{6}$

这个公式(6)也叫做Robertson-Sparck Jones等式。

## 3. 无类别估值的解决方案

Robertson-Sparck Jones等式的计算条件是知道$|C_1|$，但是如果不知道呢？实际上，大多时候我们是不知道的。一种可行的方案是，初始时令$|C_1| = 0$，则公式(6)化简为：



$\begin{equation}\sum_{j = 1}^n \log \frac{N - n_j + 0.5}{n_j + 0.5}\end{equation}\tag{7}$

可以看到，公式(7)里面有一个IDF的成分，但是既没有TF成分，也没有文档长度归一化的处理过程。这些问题导致上面所说的概率检索模型并不实用。后来的BM25模型解决了这些问题，成为了商业搜索系统中非常成功的案例。

## 4. BM25模型

我们知道，在向量空间模型的经典权重算法$TF-IDF$中，好的索引权重模型应该考虑三方面的内容：（1）词频；（2）逆文档频率；（3）文档长度。而上面由概率检索模型推导出的公式(6)显然只是包含了逆文档频率的因素，而未考虑词频和文档长度。实验结果也证明Robertson-Sparck Jones等式直接应用的效果并不好。所以学者们考虑在Robertson-Sparck Jones等式中加入代表词频和文档长度的因子，重新计算文档排序。这就是经典的BM25模型。

BM25模型为文档$D_i$每个索引项$t_j$分配了一个系数$B_{i, j}$，由公式(8)计算生成：



$\begin{equation}B_{i, j} = \frac{(K_1 + 1)f_{i, j}}{K_1[(1 - b) + b\frac{len(D_i)}{avg\_doclen}] + f_{i, j}}\end{equation}\tag{8}$

其中，$K_1$和$b$为经验参数，用于调节词频和文档长度在权重计算中起到的作用，一般来讲，$K_1$取1，$b$取0.75已经被证明是合理的假设。而$f_{i, j}$则为词$w_j$在文档$D_i$中的词频，$avg\_doclen$为平均文档长度。

计算得到了系数$B_{i, j}$，就可以基于Robertson-Sparck Jones等式最终计算出文档关于查询的排序：



$\begin{equation}sim(D_j, q) = \sum_{t_j \in q} B_{i, j} \times \log \frac{(r_j + 0.5)(|C_0| - n_j + r_j + 0.5)}{(|C_1| -  r_j + 0.5)(n_j -  r_j + 0.5)}\end{equation}$

如果不知道哪些文档是相关的，那么根据公式(7)，还可以简化上式：



$\begin{equation}sim(D_j, q) = \sum_{t_i \in q} B_{i, j} \times \log \frac{N - n_j + 0.5}{n_j + 0.5}\end{equation}\tag{9}$

这个公式(9)，就是BM25模型最为经典的计算公式。



