# SVM解释：四、线性不可分的情况 - guoziqing506的博客 - CSDN博客





2018年07月23日 08:41:42[guoziqing506](https://me.csdn.net/guoziqing506)阅读数：2705
所属专栏：[机器学习经典算法研究](https://blog.csdn.net/column/details/25189.html)









之前的博客介绍了在数据为线性可分的情况下，如何用SVM对数据集训练，从而得到一个线性分类器，也就是超平面$WX + b = 0$. 但是我已经强调过多次，线性可分的情况有相当的局限，所以SVM的终极目标还是要解决数据线性不可分的情况。解决这种线性不可分的情况基本的思路有两种：
- 加入松弛变量和惩罚因子，找到相对“最好”超平面，这里的“最好”可以理解为尽可能地将数据正确分类；
- 使用核函数，将低维的数据映射到更高维的空间，使得在高维空间中数据是线性可分的，那么在高维空间使用线性分类模型即可；

本篇博客的全部内容就是介绍这两种思路如何在数据线性不可分的情况下训练分类器的。

## 1. 松弛变量和惩罚因子

### 1.1 定义

所谓线性不可分，说得形象一些，就是“你中有我，或者我中有你”，我们找不到一个超平面，恰好能把两类数据严格地分开。举个例子，Fig.1(a)中，白色数据元组中有一个点比较“另类”（我用文字标了出来），它跑到人家黑色元组的区域中去了，显然，如果这个点直接被忽略掉，那么分类效果是很好的（比如Fig.1(a)中的蓝色虚线），但是现在的情况就让人很难受，因为我们找不到一个超平面，恰好能把黑白两类数据分开。



![](https://img-blog.csdn.net/20180722095236538?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


还有一种情况也比较有意思，就是现在数据还是线性可分的，比如Fig.1(b)，但是存在一个“异常点”（SVM中称之为outlier），如果忽略它，最佳分离超平面的效果非常好，但是如果按正常的套路训练的话，我们只能得到一个“差强人意”的结果。如Fig.1(b)所示，有它或没它，超平面的训练效果是有很鲜明对比的。

针对上面这两种情况，最直接的解决思路就是能不能适当的放宽数据点和边缘侧面之间的关系？还记得之前线性可分情况下的约束条件是这样的：



$\begin{equation}y_i(WX_i + b) \geq 1, i = \{1, 2, \dots, n\}\end{equation}$

现在放宽一些，变成下面的形式：



$\begin{equation}y_i(WX_i + b) \geq 1 - \xi_i, i = \{1, 2, \dots, n\}\end{equation}$

其中$\xi_i$为元组$X_i$对应的松弛变量，它实际上是这个元组距离它原本的边缘所超出的权重距离，也就是Fig.1两个子图中黑色虚线的权重长度（权重是$W$）。这样做的结果相当于是放宽了约束要求。通过在约束条件中加入松弛变量，使得一些原本不在相应边缘外部的点，被“拉”回了相应的侧面上，我给出松弛变量形式化的定义如下：

**定义1-松弛变量**：指数据元组关于侧面的离差的长度。离差有两种类型：
- 大离差：即某个类的元组直接到超平面的另一侧去了，像Fig.1(a)中的异常点，这时有离差的数据被错误分类了；
- 小离差：即某个类的元组没有到超平面的另一侧，但是落在了超平面本侧的边缘内，像Fig.1(b)中的outlier，这时有离差的数据未被错误分类；

从大小离差的定义，我们也能确定松弛变量的取值范围：大离差$\xi_i \geq 1$；小离差$1 > \xi_i > 0$. 当然如果一个元组正确地落在它该在的位置上，那就是没有离差，即$\xi_i = 0$

离差的存在帮助我们在一定的容错率下获取最佳分离超平面，但问题在于目标函数中离差的大小如何确定？显然，如果太小了，分离超平面就计算不出来了，或者只能计算一个“不佳”的分离超平面；而如果太大了，那任何一个超平面都是分类器了，也不行。所以，一定要采取措施，“控制”松弛变量。

控制的方法是加入“惩罚因子”。定义惩罚因子之前，我们先定义“软误差”：

**定义2-软误差**：指所有数据元组的松弛变量的和，$\sum_{i = 1}^n \xi_i$

**定义3-惩罚因子**：是一个大于0的系数（记为$C$），软误差乘该系数后，加入目标函数，使之成为新的目标函数，如下：



$\begin{equation}\min \frac{1}{2} \|W\|^2 + C \sum_{i = 1}^n \xi_i\end{equation}$

其中$C$的取值用来在数据拟合之间找到平衡。显然，$C$越大，则分类器越不愿意分类错误，它要按“最正确”的来，那就会得到一个过拟合的分类器；而如果$C$越小，那么分类器就越“不在乎”错误，得到一个过于泛化的分类器，也就是欠拟合了。

### 1.2 求解新的优化问题

现在，加入松弛变量和惩罚因子后，新的优化问题诞生了，它是我在上一篇博客。。。的公式(1)的基础上加入松弛变量和惩罚因子的结果：



$\begin{equation}\begin{aligned}\min ~&\frac{1}{2} \|W\|^2 + C \sum_{i = 1}^n \xi_i\\&s.t. ~1 - \xi_i - y_i(WX_i + b) \leq 0\\&~~~~~~~\xi_i \geq 0\end{aligned}\end{equation}\tag{1}$

注意，这里$\xi_i \geq 0$也是约束条件

拉格朗日方程变为： 


$\begin{equation}L(W, b, \alpha, \xi, C, \mu) = \frac{1}{2} \|W\|^2 + C \sum_{i = 1}^n \xi_i + \sum_{i = 1}^n \alpha_i(1 - \xi_i - y_i(WX_i + b)) - \sum_{i = 1}^n \mu_i\xi_i\end{equation}$

其中，$\mu_i$为新加入的拉格朗日乘子。

得到拉格朗日方程后，就可以进入我们在上一篇博客 [SVM解释：三、线性可分的情况](https://blog.csdn.net/guoziqing506/article/details/81119449) 中解决此类问题的“套路”了。具体地说，

现在的优化问题相当于求解：



$\begin{equation}p^* = \min_{W, b, \xi} \max_{\alpha, \mu} L(W, b, \alpha, \xi, C, \mu)\end{equation}$

转换成对偶问题：



$\begin{equation}d^* = \max_{\alpha, \mu} \min_{W, b, \xi} L(W, b, \alpha, \xi, C, \mu)\end{equation}$

先求解$\min_{W, b, \xi} L(W, b, \alpha, \xi, C, \mu)$：



$\begin{equation}\begin{aligned}&\frac{\partial L}{\partial W} = 0 \Rightarrow W = \sum_{i = 1}^{n} \alpha_i y_i X_i^T\\&\frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i = 1}^{n} \alpha_i y_i = 0\\&\frac{\partial L}{\partial \xi_i} = 0 \Rightarrow C - \alpha_i - \mu_i = 0\end{aligned}\end{equation}\tag{2}$

将以上的结果带入拉格朗日函数，那么对偶问题$d^*$就是要计算：



$\begin{equation}\max_{\alpha, \mu} \sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j\end{equation}$

其实这里的$\mu$不用考虑了，因为现在的式子里没有$\mu$存在了。

最后来看看这个对偶问题的约束条件：
- $\alpha_i \geq 0$
- $\sum_{j = 1}^{m} \alpha_i y_i = 0$
- $\alpha_i \leq C$

前两个条件怎么来的，上一篇博客中推导过了（可以参考上一篇博客中的公式(3)），至于第3点，那是因为在上面求偏导的过程中（本篇博客的公式(2)），存在下式：



$\begin{equation}C - \alpha_i - \mu_i = 0\end{equation}$

综上，加入松弛变量和惩罚因子后的SVM问题最终转化为下面的带约束条件的凸优化问题：



$\begin{equation}\begin{aligned}\max_{\alpha} ~&\sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j\\&s.t. ~0 \leq \alpha_i \leq C\\&~~~~~~\sum_{j = 1}^{m} \alpha_i y_i = 0\end{aligned}\end{equation}\tag{3}$

上面这个优化问题，就是SVM中SMO算法主要要解决的问题。我在下一篇博客 [SVM解释：五、SMO算法](https://blog.csdn.net/guoziqing506/article/details/81155323) 中对SMO算法的介绍全部基于这个优化问题，所以这里大家注意一下，有个印象。

解得$\alpha$后进一步得到$W$和$b$，最终得到分类器：



$\begin{equation}f(X) = \sum_{i = 1}^{n} \alpha_i y_i X_i^TX + b\end{equation}$

分类器在形式上与没有加入松弛变量和惩罚因子时是一样的。

### 1.3 训练元组的位置分析

最后，我们分析一下训练元组$X_i$在分类器中的位置。上一篇博客中，我最后讲到了可以根据$\alpha_i$的取值判断训练元组是否为支持向量，再根据支持向量即可计算出分类器$WX + b = 0$的常数项$b$的值。先来简单回顾一下当时的分析：
- 如果 $\alpha_i > 0$，那么此时$X_i$是支持向量，满足$y_i(WX_i + b) = 1$；
- 如果 $\alpha_i = 0$，那么此时$X_i$是不是支持向量，满足$y_i(WX_i + b) \geq 1$；

与线性可分的情况相比，加入松弛变量和惩罚因子后，分析会变得更复杂。具体如下：

我们先看最原始的，公式(1)中的优化问题。当然，其最优解当然要满足KKT条件（参见我的第2篇博客 [SVM解释：二、SVM的数学基础](https://blog.csdn.net/guoziqing506/article/details/81117820)），因此根据KKT条件的第三项，可以得到以下结论：



$\begin{equation}\begin{aligned}&\alpha_i(1 - \xi_i - y_i(WX_i + b)) = 0\\&\mu_i\xi = 0\\\end{aligned}\end{equation}$

分下列情况讨论：
- 
$0 < \alpha_i < C$时： 

因为$\alpha_i > 0$，所以$1 - \xi_i - y_i(WX_i + b) = 0$；因为$\alpha_i + \mu_i = C$，且$\alpha_i < C$，所以$\mu_i > 0$；因为$\mu_i\xi = 0$，所以$\xi = 0$；因为$\xi = 0$，所以$\xi = 0$且$1 - \xi_i - y_i(WX_i + b) = 0$，所以$y_i(WX_i + b) = 1$；

- 
$\alpha_i = C$时： 

因为$\alpha_i > 0$，所以$1 - \xi_i - y_i(WX_i + b) = 0$；因为$\alpha_i + \mu_i = C$且$\alpha_i = C$，所以$\mu_i = 0$，$\xi \geq 0$且无其他限制；因为$1 - \xi_i - y_i(WX_i + b) = 0$，所以$y_i(WX_i + b) \leq 1$；

- 
$\alpha_i = 0$时： 

因为$\alpha_i = 0$且$\alpha_i + \mu_i = C$，所以$\mu_i = C$，因为$\mu_i\xi = 0$，所以$\xi = 0$，因为$1 - \xi_i - y_i(WX_i + b) \leq 0$，所以$y_i(WX_i + b) \geq 1$；


综上，在最优点处，所有样本必须满足公式(4)：



$\begin{equation}\left\{\begin{aligned}&if ~~\alpha_i = 0,  ~~~~~~~~~~~y_i(WX_i + b) \geq 1\\&if ~~\alpha_i = C,  ~~~~~~~~~~y_i(WX_i + b) \leq 1\\&if ~~0 < \alpha_i < C,  ~~~y_i(WX_i + b) = 1\\\end{aligned}\right.\end{equation}\tag{4}$

由公式(4)，我们还能推测出以下规律：
- 满足$0 < \alpha_i < C$的样本点就是支持向量，他们在侧面上；
- 满足$\alpha_i = 0$的样本点是被正确分类的点，他们在侧面以外；
- 满足$\alpha_i = C$的样本点是是未被正确分类的点，且不在侧面上（实际上就是可以被离差“拉”到侧面上的点）；

总结出这样的规律是很有用的，我们可以根据优化结果中$\alpha_i$的取值确定支持向量，再用支持向量解出$W$和$b$，最终确定一个有容错的SVM分类器。同时，这里的公式(4)大家注意一下，我在下一篇博客讲解SMO算法中，讲到选择优化变量的时候，也会用到这个公式。

## 2. 核函数

好了，到现在为止，我一共用了之前的2篇博客，以及本文的一半讲SVM，对于线性可分的情况和线性不可分的情况其实我们已经有所了解了。现在终于要介绍SVM的重头戏“核函数”了。跟它的名字一样，这一块的知识也是“核”级别的，它非常重要。

加入松弛变量和惩罚因子，使得我们可以解决在训练数据线性不可分的情况下计算分类器的问题。但是好像还是不那么令人满意，因为它对训练数据都有一定的容错率，接下来自然要思考的问题是，存不存在一种方法，能够学习得到更为复杂的一个曲面（而不是超平面），从而对于线性不可分数据训练效果更佳。这就要用到所谓的“核技巧”了，和之前的线性模型不同，它是一种非线性模型。

### 2.1 核技巧

**定义4-核技巧**：如果数据线性不可分，则可以对特征向量进行映射，将他们转化到更高维的空间，使得数据在高维空间是线性可分的。这种思路被称为“核技巧”。

我曾看到过一篇文章，作者对于核技巧有一个非常形象的描述：现在有一堆黑球，白球放在桌面上，线性不可分，所谓核技巧就是一拍桌子，将这些球拍到空中去，在空中我们插入一个板子，将两类球分开。其实通过这个描述你也大致能总结出非线性分类器学习步骤：
- 使用一个非线性映射将数据映射到一个特征空间$z$（拍桌子）；
- 在特征空间$z$使用线性学习器分类（插板子）；

假设现在有映射函数$\phi()$将数据映射到空间$z$：



$\begin{equation}X \rightarrow \phi(X)\end{equation}$

根据上一篇博客中的公式(4)，如果在$z$空间中数据是线性可分的，那么决策超平面的方程是： 


$\begin{equation}f(x) = \sum_{i = 1}^{n} \alpha_i y_i \phi(X_i)^T + b\end{equation}$

自然地，最终的决策规则可以用训练点和测试点的内积得到：



$\begin{equation}f(x) = \sum_{i = 1}^{n} \alpha_i y_i \phi(X_i)^T \phi(X^*) + b\end{equation}$

其中，$X^*$为测试元组。

想象一下，如果有一种方式可以在原始空间中直接计算映射后的高维空间的向量内积$\phi(X_i)^T \phi(X_j)$，就不需要把原始空间中的数据映射到新空间中了。那也就是说，可以将上面的两个步骤融合到一起建立一个非线性的学习器。这样直接计算法的方法称为核函数方法。

形式化地，核函数$K(,)$对于原始空间中的数据元组$X_i, X_j$，有下式成立：



$\begin{equation}K(X_i, X_j) = \phi(X_i)^T \phi(X_j)\end{equation}$

### 2.2 为什么要用核函数

在介绍几种常用的核函数之前，首先明确一个问题：为什么SVM的非线性分类模型要用核函数？

我在这里以July在博文 [支持向量机通俗导论](https://blog.csdn.net/v_july_v/article/details/7624837) 中的例子来说明一下。

Fig.2中，两类数据以圆环型分布，那我们当然可以预测，一个合理的决策面也是圆环型的（而不是平面），而且就在这两类数据之间。



![](https://img-blog.csdn.net/20180719205500694?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


如果以$x_1, x_2$为当前这个二维平面的两个坐标的话，二次曲线（圆环也是二次曲线）的方程可以写成如下形式：



$\begin{equation}a_1x_1^2 + a_2x_2^2 + a_3x_1 + a_4x_2 + a_5x_1x_2 + a_6 = 0\end{equation}$

但是和之前线性可分的情况下我们构造的超平面$WX + b = 0$相比，现在这种曲面无法用线性分类模型处理。那我们考虑能不能把原始空间映射到高维空间，再考虑当前这个曲面。方法是构造一个5维空间，空间的每个坐标这样设置：



$\begin{equation}z_1 = x_1^2, z_2 = x_2^2, z_3 = x_1, z_4 = x_2, z_5 = x_1x_2\end{equation}$

那么在新的高维空间中，之前的二次曲线变成了一个超平面：



$\begin{equation}\sum_{i = 1}^5 a_iz_i + a_6 = 0\end{equation}$

到这一步之后，似乎直接根据之前讲的SVM的线性分类模型直接处理就OK了。但是你会发现，直接这样做是有问题的，我刚才对二维空间内的曲面（其实是曲线）进行处理，把一阶、二阶的坐标组合后，映射到了5维空间，那如果对三维空间映射呢？就会映射到一个19维的空间上，这个空间维数的数量级是爆炸式增长的。会给高维空间中线性分类模型的计算造成很大困难。关于这个问题的解释，July在其博客中说的非常详细，读者可自行翻看其博客中的2.2.2节。

这个问题怎么解决呢？就要用到核函数了。

先观察一下训练分类器用到的计算和分类时用到的计算，可以参考上一篇博客中的公式(3)和公式(4)：
- 
训练时：就是解决如下的凸优化问题（这是经过转换之后最终得到的拉格朗日对偶问题）： 


$\begin{equation}\begin{aligned}\max ~&\sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j \\    &s.t. \sum_{j = 1}^{m} \alpha_i y_i = 0\\    &~~~~~~~\alpha_i \geq 0\end{aligned}\end{equation}$

- 
分类时，是计算如下函数的正负性： 


$\begin{equation}\sum \alpha_i y_i X_i^T X_j + b\end{equation}$


不难发现，不论是训练过程还是分类过程，主要涉及的关于数据的运算都是内积计算。而核函数的作用，就是在不把数据映射到高维空间的前提下（实际上是不显式地写出映射后的结果），直接在低维空间中计算映射后向量的内积。综上，训练和分类的计算在核函数的帮助下，可以整理成如下形式：
- 
训练 


$\begin{equation}\begin{aligned}\max ~&\sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j K(\phi(X_i), \phi(X_j))\\    &s.t. \sum_{j = 1}^{m} \alpha_i y_i = 0\\    &~~~~~~~\alpha_i \geq 0\end{aligned}\end{equation}$

- 
分类 


$\begin{equation}f(X_j) = \sum \alpha_i y_i K(\phi(X_i), \phi(X_j)) + b\end{equation}$


### 2.3 几种常见的核函数

对于任意一个映射，我们自己构造出对应的核函数是困难的，因此，一般会从几种常见的核函数中选择。常见的核函数大致有4种。我画了一个表格，展示出了他们的公式：
|核函数|公式|
|----|----|
|线性核|$K(X_1, X_2) = X_1^T X_2$|
|多项式核|$K(X_1, X_2) = (X_1^T X_2 + 1)^q$|
|高斯核（径向基函数）|$K(X_1, X_2) = \exp(-\frac{\|X_1 - X_2\|^2}{2s^2})$|
|sigmoid核（S形函数）|$K(X_1, X_2) = tanh(X_1^T X_2 + 1)$|

我简单说一下多项式核与高斯核：

**1. 多项式核**：$q$是由用户选择的，比如处理上面那两个圆环型分布的点集分类，因为在二维空间中，所以我们可以选择$q = 2$。向量$X_1, X_2$映射到5维空间后，其内积为：



$\begin{equation}\phi(X_i)^T \phi(X_j) = X_{11}^2X_{21}^2 + X_{12}^2X_{22}^2 + X_{11}X_{21} + X_{12}X_{22} + X_{11}X_{21}X_{12}X_{22}\end{equation}$

而如果我们选择$q = 2$时的多项式核函数$K(X_1, X_2) = (X_1^T X_2 + 1)^2$，计算结果是这样的：



$\begin{equation}K(X_1, X_2) = X_{11}^2X_{21}^2 + X_{12}^2X_{22}^2 + 2X_{11}X_{21} + 2X_{12}X_{22} + 2X_{11}X_{21}X_{12}X_{22} + 1\end{equation}$

你发现他俩个有点像，但又不太一样。实际上，只要把新的5维空间的几个维度缩放一下，再加上一个常数维，就是一样的了。也就是说，核函数计算的向量内积其实是原始空间中的向量被映射到下面的空间中的内积：



$\begin{equation}\phi(x_1, x_2) = \{x_1^2, x_2^2, \sqrt{2}x_1, \sqrt{2}x_2, \sqrt{2}x_1x_2, 1\}\end{equation}$

从核函数的公式你能清晰地感受到核函数的神奇，它就是直接在原始空间计算，达到在高维空间计算的效果。需要说明的是，当$q = 1$时，多项式核就是线性核。

**2. 高斯核**：参数$s$由用户设定，这个参数$s$实际上是平衡分类计算的因子。具体地说，如果$s$太大，那相当于高斯核只是将原始空间映射到一个不是那么高维的新空间中，不一定能执行线性分类；而如果$s$太小，理论上可以将任意的数据映射为线性可分的，但是这样一来就可能存在过拟合的问题。不过，只要参数得当，高斯核可以非常完美的解决线性不可分数据的SVM分类问题，所以也是应用最为广泛的。关于高斯核函数映射，有张非常形象的示意图，如下：



![](https://img-blog.csdn.net/20180723144936542?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


大多时候，面对线性不可分的数据，我们先用核函数将数据映射到高维空间中（其实并非真正映射，只是可以计算出映射后向量的内积），再通过加入松弛变量和惩罚因子让映射后的数据在线性模型下求解。这样，线性不可分情况下完整的SVM就展现在我们眼前了，它就是求解下面的凸优化问题：



$\begin{equation}\begin{aligned}\min ~&\frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j K(X_i, X_j) - \sum_{i = 1}^{n} \alpha_i\\&s.t. ~0 \leq \alpha_i \leq C\\&~~~~~~\sum_{j = 1}^{m} \alpha_i y_i = 0\end{aligned}\end{equation}\tag{5}$

上面的公式(5)其实是上一篇博客中的公式(4)在线性不可分的情况下，应用核函数，加入松弛变量和惩罚因子后的“终极”优化问题。

注意，因为这个优化问题从结构上比上一篇博客中的公式(3)要更复杂，但是他们的解法是一致的，所以在下一篇博客讲解SMO算法时，我会以这个优化问题举例，因为难的解决了，简单的不也就容易了吗。

## 3. SVM分类的效果

至此，如何处理数据线性不可分的情况下的SVM分类就算是基本说完了。最后我们看一个例子，这也是July在博文中写到的，我觉得很形象，直接抄在底下：

**“假设现在你是一个农场主，圈养了一批羊群，但为预防狼群袭击羊群，你需要搭建一个篱笆来把羊群围起来。但是篱笆应该建在哪里呢？你很可能需要依据牛群和狼群的位置建立一个“分类器”，比较下图这几种不同的分类器，我们可以看到SVM完成了一个很完美的解决方案。”**



![](https://img-blog.csdn.net/20180719210925202?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


## 4. 损失函数

其实，任何监督学习最终的目的都是要最小化预测结果与真实结果之间的误差。这种误差一般用“风险”来评估。很显然，风险对于一个监督学习算法来说是很重要的，它所评估的误差大小直接说明了算法的好坏。监督学习系统中有结构化风险和经验风险两种。既然说到了SVM的损失函数，就一并将两种风险做个简要介绍。

设决策函数为f()$f()$，对于给定的输入X$X$，它的预测结果（计算得到的类标记或者预测数值）为f(X)$f(X)$，真实结果为Y$Y$，那么常用的损失函数有如下几种：
- 
0-1损失函数： 
L(Y,f(X))={1,Y≠f(X)0,Y=f(X)$\begin{equation}L(Y, f(X)) = \left\{\begin{aligned}& 1, Y \neq f(X)\\& 0, Y = f(X)\end{aligned}\right.\end{equation}$

- 
平方损失函数： 
L(Y,f(X))=(Y−f(X))2$\begin{equation}L(Y, f(X)) = (Y - f(X))^2\end{equation}$

- 
绝对损失函数： 
L(Y,f(X))=|Y−f(X)|$\begin{equation}L(Y, f(X)) = |Y - f(X)|\end{equation}$

- 
对数损失函数： 
L(Y,p(Y|X))=−logp(Y|X)$\begin{equation}L(Y, p(Y|X)) = -\log p(Y|X)\end{equation}$


如果给定一个训练集{(X1,Y1),(X2,Y2),…,(Xn,Yn)}$\{(X_1, Y_1), (X_2, Y_2), \dots, (X_n, Y_n)\}$和损失函数L(Y,f(X))$L(Y, f(X))$，则经验风险被定义为模型f(X)$f(X)$关于训练集的平均损失：



$\begin{equation}R_{emp}(f) = \frac{1}{n} \sum_{i = 1}^n L(Y_i, f(X_i))\end{equation}$

所以，如果按照经验风险最小的策略设计学习模型，那么实际上是求解下面的优化问题：



$\begin{equation}\min_{f \in F} \frac{1}{n} \sum_{i = 1}^n L(Y_i, f(X_i))\end{equation}$

经验风险很直观，容易理解。但是有个缺点就是当样本容量较小时，容易产生过拟合。而依据结构风险的设计策略则可以避免这个问题。

结构风险在经验风险的基础上加入表示模型复杂度的正则化项或罚项，定义如下：



$\begin{equation}R_{srm}(f) = \frac{1}{n} \sum_{i = 1}^n L(Y_i, f(X_i)) + \lambda J(f)\end{equation}$

其中$J(f)$为模型复杂度，模型越复杂，$J(f)$越大，反之，$J(f)$越小。$\lambda \geq 0$为惩罚系数。显然，罚项的存在控制了模型的复杂度，能够解决过拟合的问题。按结构风险最小的策略设计的学习模型实际上是求解下面的优化问题：



$\begin{equation}\min_{f \in F} \frac{1}{n} \sum_{i = 1}^n L(Y_i, f(X_i)) + \lambda J(f)\end{equation}$

设计SVM时，我们加入松弛变量其实就是一种结构风险最小的策略，使得分类器不会过拟合。如果尝试从损失函数的角度理解SVM，逻辑回归(LR)，提升(Boosting)等算法的话，可能会有不同的收获。

## 5. 结语

这篇博文和之前的两篇放在一起，我已经解释了SVM中80%的内容，虽然很多数学方面的细节确实不尽详细，好在如果只是掌握SVM的基本思想的话，这点内容还算够。那剩下20%就是要介绍上面说的，解决对偶问题时需要用到的SMO算法了。SMO算法有点复杂，所占篇幅也不小，所以单独列在整个SVM学习笔记的最后（即我关于SVM的最后一篇博客），以方便大家在前面更容易掌握SVM的整体逻辑。



