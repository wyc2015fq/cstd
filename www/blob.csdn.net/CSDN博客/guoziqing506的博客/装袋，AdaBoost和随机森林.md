# 装袋，AdaBoost和随机森林 - guoziqing506的博客 - CSDN博客





2018年04月17日 20:44:30[guoziqing506](https://me.csdn.net/guoziqing506)阅读数：375
所属专栏：[机器学习经典算法研究](https://blog.csdn.net/column/details/25189.html)









装袋，AdaBoost和随机森林都属于组合分类方法的例子，用于改善单个分类模型的学习效果。

我们知道，在很多情况下，面对大量复杂的训练元组，如果只使用一种分类模型构造分类器，很可能对于某些元组是有“硬伤”的，预测结果很不准确。所以最直接的思路是将多种分类模型组合起来，通过得到的多个分类器投票判断。好比是有个消息你不知道，去问别人，如果只问一个人，那这个人说的也不一定对吧。但是如果问很多人，以多数人的答案为准，就不太容易出错了。

具体的组合分类方法的例子，一般我们接触较多的是三个：装袋，AdaBoost和随机森林

## 装袋

### 工作原理

装袋表示的含义是“自助聚集”，这个含义也比较形象：假设原始的训练集为$D$，我们采用有放回抽样，每次从$D$中随机取出一个元组，经过$N$次有放回抽样（$N$也是$D$的大小），生成训练集$D_i$（就像把训练元组装进一个袋中），注意这里$D$中的元组可能多次出现在$D_i$中，也可能不出现在$D_i$中。之后，采用事先设定好的学习方案，使用$D_i$中的训练元组生成分类器，记为$M_i$。经过$k$轮上述迭代，我们最终可以得到$k$个分类器$\{M_1, M_2, \dots, M_k\}$。那么，在对新元组（记为$X$）进行分类时，这$k$个分类器都被用来对$X$生成类号，根据“多数投票原则”，选择得票数最高的类作为最终分类结果。

### 效果分析

就准确率来说，装袋得到的组合分类器的准确率，通常显著高于直接用原始训练集$D$在学习模型上得到单个分类器的准确率，这是因为组合分类器降低了个体分类器的方差。而对于噪声数据和过分拟合的影响，装袋的性能也在可以接受的范围内。

## AdaBoost

AdaBoost是Adaptive Boosting的简称，翻译过来就是“适应性提升”，所谓“提升”其实跟上面说的“装袋”基本思想是一致的，都是构造组合分类器，解决单个学习模型对于个别元组误判的问题。与“装袋”不同的是，“提升”会对每个训练元组赋予一个权重，每一轮迭代学习之后，更新元组的权重，最后经过多轮迭代，获得组合分类器，最后加权投票，得到预测结果。而不像装袋那样：在每轮迭代中，训练元组的权重是等大的，不用加以考虑。

AdaBoost中，元组权重迭代更新的根本目的在于使得下一轮学习过程更加“关注”上一轮迭代中，未被正确分类的元组。（装袋则不同，它的每轮迭代对于所有元组的关注度是一样的）这样做的好处其实很明显：对于单个学习模型来说，有些元组是所谓“难分类”的，我们现在使这些“难分类”的元组在下一轮学习中被“重点照顾”，这样，得到的新分类器会对“难分类”元组效果相对较好，循环往复，最终得到的组合分类器会更加均衡，它对于所有元组的分类效果能够做到都比较好。

当前实现AdaBoost主流的方法有两种，一种是韩家炜教授《数据挖掘》中讲到的，每轮迭代采用“自助样本”（即多次有放回抽样生成的元组集合）作为训练集，利用学习方案生成分类器，其中，训练集的选择以元组权重大小为抽样概率依据；另一种是则不采取抽样，而是整个训练集参与每轮迭代，每轮迭代根据元组的权重分布设计新的分类器。其实无论是哪一种方法，其基本思想都是一样的：通过迭代学习，更新元组权重，再根据元组权重，生成下一轮迭代的分类器（韩家炜的方法实际上是通过元组权重抽样得到不同的训练数据集，从而实现生成不同的分类器）。最终，得到带权重的组合分类器，加权投票得到预测的分类结果。下面我分别就两种方法做一个简单的介绍。

### 整体迭代

采取整体迭代方式的AdaBoost的工作原理如下所示：

输入：训练元组集合$D$；迭代轮数$k$；学习方案$Learning()$； 

输出：复合模型（组合分类器）
- 
初始化D中所有训练元组的权重，$W_{i, 0} = 1/N$;

- 
对3-6步执行$k$轮迭代:

- 
计算得到一个弱分类器$M_j = Learning(D), j \in [1, 2, \dots, k]$

- 
计算弱分类器的误差：$W_{i, j - 1}$表示在第$j - 1$轮迭代后，元组$X_i$的权重 


$\begin{equation}err_j = \frac{1}{2} (\sum_{i = 1}^n W_{i, j - 1} |M(X_i) - y_i|) / \sum_{i = 1}^n W_{i, j - 1}\end{equation}$

- 
计算弱分类器的权重$\alpha_j$： 


$\begin{equation}\alpha_j = \frac{1}{2} \ln \frac{1 - err_j}{err_j} \end{equation}$

- 
计算当前元组的权重（如下式）：这里的$Z_j$是本轮迭代的归一化因子，它就是所有元组权重的加和 


$\begin{equation}W_{i, j} = W_{i, j - 1} \frac{\exp(-\alpha_j y_i M_j(X_i))}{Z_j}\\Z_j = \sum_{i = 1}^n W_{i, j - 1} \exp(-\alpha_j y_i  M_j(X_i))\end{equation}$

- 
分类：一共得到$k$个若分类器，构造组合分类器，再分类即可。方法是多数投票：其中$sgn(x)$是个符号判断函数，若$x > 0$，则$sgn(x) = 1$；反之，$sgn(x) = -1$


$\begin{equation}sgn(M(X)) = sgn(\sum_{i = 1}^k \alpha_j M_i(X))\end{equation}$


这里需要重点说明的是第5步。此处我默认数据元组只有两类，类标号分别为1和-1，所以计算元组权重的时候用$\exp(-\alpha_j y_i M_j(X_i))$，这样，当该元组被正确分类时，$y_i$与$M_j(X_i)$同号，$\exp(-\alpha_j  y_i M_j(X_i)) < 1$；反之，当该元组被错误分类时，$y_i$与$M_j(X_i)$异号，$\exp(-\alpha_j y_i M_j(X_i)) > 1$，使得被错误分类的元组在下一轮迭代中权重更大。

### 抽样方式

抽样方式与整体参与迭代的方法本质上是一样的。不同之处仅在于每轮迭代并非使用全体元组，而是根据上一轮迭代中元组的权重抽样选择参与这一轮迭代的元组（是整体元组的一个子集）。具体方法与上面类似，不赘述了。

通过对上面两种思路的学习，可以观察到，AdaBoost的核心思想在于对于每次没有正确被分类的元组，会在下一次迭代当中被“重点关注”（通过元组权重）。这样可以建立出一组“互补”的分类器，使得每个元组都能被尽可能地正确分类。

最后，说一下AdaBoost的缺点。由于关注误分类元组，所以可能导致复合模型对数据过分拟合。这和装袋相比，是一个缺点。但是AdaBoost的优点也是很明显的：在有些情况下，可以获得比装袋更高的准确率。所以具体实现时，也要根据具体情况具体分析。

## 随机森林

### 两种基本模式

随机森林，本质上讲就是“决策树+装袋”，当使用决策树作为学习模型时，我们进行$k$轮迭代，每轮通过决策树模型，使用有放回抽样生成的“自助样本”构建分类器，这样一共可以生成$k$棵决策树，就好像一个森林。决策树归纳的相关知识我在之前的博文：[决策树归纳](https://blog.csdn.net/guoziqing506/article/details/65633770)中已经有了相对详细的讲解。

随机森林一般情况下有两种最常用的模型：Forest-RI（随机输入选择），Forest-RC（随机线性组合）。我下面简单谈一下。
- 
Forest-RI（随机输入选择）：每轮迭代中，构建决策树时，在每个树节点随机选择$F$个属性作为该节点“分裂”的候选属性。我们知道，普通的决策树归纳，在分裂节点时，应该是考虑所有属性并针对每个属性，计算用于选择的标准。比如信息增益，增益率，基尼指数等等，来找到最佳的属性，并确定分裂值。但是随即森林每次却是在可用属性中，随即选择$F$个属性作为候选（当然，为了保证算法是可执行的，$F$一般远远小于可用属性数），并且一般采用CART算法来增长树（即采用基尼指数选择分裂属性），树增长到最大规模，且不剪枝。

- 
Forest-RC（随机线性组合）：对于属性较少的训练集而言，Forest-RI就显得不可用了。解决办法是Forest-RC，即分裂节点时，首先随机选择$L$个属性，从$[-1, 1]$中随机选取$L$个数，这样就得到了一个有$L$个属性线性组合生成的“新属性”，我们多次选取随机值，共生成$F$个新属性。最后根据基尼指数找到最佳分裂属性和分裂点。这种随机线性组合是思想是为了降低个体分类器之间的相似度。


### 效果分析
- 随机森林在准确率上与AdaBoost相近，但对于错误和离群点更鲁棒；
- 随着森林中树的增加，泛化误差收敛，也能尽量避免过拟合的问题；
- 属性选择时，一般$F$的值为$\log(d) + 1$。考虑较少属性的特点，也为随机森林应用于大型数据库提供了性能方面的优势。

此外，构造随机森林时，应尽量提高个体分类器的能力同时降低它们之间的相关性。



