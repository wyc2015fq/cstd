# SVM解释：一、SVM的整体框架 - guoziqing506的博客 - CSDN博客





2018年07月23日 08:40:36[guoziqing506](https://me.csdn.net/guoziqing506)阅读数：603
所属专栏：[机器学习经典算法研究](https://blog.csdn.net/column/details/25189.html)









支持向量机（Support Vector Machine）是一种非常重要的分类方法，大的范畴上讲，属于监督学习。它最早由Vapnik等人在1992年提出，已经发展了近30年。尽管它的训练速度偏慢，但是由于其对复杂非线性数据的强大的建模能力，依然在很多领域，包括手写数字识别，对象识别，基准时间序列预测检验等有着非常广泛的应用。可以说，任何一种二分类的问题都在理论上都可以用SVM解决。

当然，SVM如此强大的原因也要归功于它背后巧妙而又复杂的数学原理，如果没有相关的基础，学习SVM是一件费时费力的事情。和机器学习中那些相对简单的分类算法不同，学习SVM的过程是非常浩繁的。这一点从我写的这个SVM的学习笔记就能看出来：我前后一共用了5篇博文。因此也实在难免出错，如果有心的读者发现了我的错误，或者觉得内容有需要改进的地方，还望不吝赐教。

本文类似于一篇引言，我将简单介绍一下什么是SVM，它用来做什么，大致的框架是什么样的，以及我这5篇博文都大概说了些什么。以求在最大程度上给大家，也给未来健忘的我自己一个清晰的概念。

## 1. SVM是什么

支持向量机（简称SVM）是一种分类算法。它主要应用于二分类问题，通过定位两类数据在几何空间中的边缘，来确定分类器。一个简单的例子如Fig.1所示，黑点和白点分别代表两类数据，SVM的目的就是要找到一个超平面，比如Fig.1中的红色虚线（后面我们会知道，对于线性不可分数据来说，实际要找的是一个曲面），恰好能“最好地”将两类数据分开，这样的超平面（或曲面）就是SVM分类器，找到分类器后，我们自然可以依据测试数据与分类器的位置关系，预测它的分类。比如Fig.1中，如果测试数据在虚线以上，就属于黑点这个类；若在虚线以下，就属于白点这个类。



![](https://img-blog.csdn.net/20180719152530434?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


所以，核心的问题是，什么样的超平面（或曲面）是“最好的”呢？直观感受告诉我们，这样的超平面在两类数据之间，同时应该具备如下两个条件：
- 最近距离最远：两类数据中，每类数据都有一个点距离该超平面的距离是最近的，而“最好的”超平面要这个两个最近距离的和尽可能的远；
- 等距：超平面距离两类数据最近的点的距离是相等的；

上面这两个条件也是SVM最基本的原理，就是找到所谓的“最大间隔”。

你看，分类问题现在转换成了一个求“最大”的问题，这显然是一个优化问题，而怎么解这个优化问题（即怎么找到这样“最好的”平面或者曲面）就是SVM的全部内容了。

## 2. 二分类转多分类的策略

在介绍SVM的整体框架之前，我先在这里插一点别的东西，那就是二分类问题到多分类问题的转换策略。前面说SVM是典型的解决二分类问题的算法，那你肯定会产生疑问，能不能把它拓展到多分类问题呢？其实不仅是SVM，大多二分类问题都可以通过“一对一”和“一对多”两种策略转换到多分类问题中。

### 2.1 一对一

现有训练集$X$，其中的数据元组共属于$n$个类。所谓“一对一”策略就是将类别两两组合，构成$\frac{n(n - 1)}{2}$种组合。举个例子，现在数据元组共属于4个类$A, B, C, D$，那就有6种组合：$AB, AC, AD, BC, BD, CD$。

训练时，按照这6种组合分别挑出对应的训练数据，按照相应的算法（比如SVM）构造分类器。比方说训练$AB$分类器就挑出类标记为$A$和$B$的数据进行训练。这样，对于一个$n$分类问题，我们一共构造了$\frac{n(n - 1)}{2}$个分类器。测试时，我们将测试元组依次用这$\frac{n(n - 1)}{2}$个分类器分类，最后统计每个类别得到的票数，得票最多者为测试结果。

### 2.2 一对多

一对多策略与一对多不同，要求每次训练都是找到其中一个类别为正，其他类别为负。还是上面那个$A, B, C, D$的例子，我们如下训练分类器：
- $A$是一类，$B, C, D$合起来看做另一类，训练得到分类器；
- $B$是一类，$A, C, D$合起来看做另一类，训练得到分类器；
- $C$是一类，$A, B, D$合起来看做另一类，训练得到分类器；
- $D$是一类，$A, B, C$合起来看做另一类，训练得到分类器；

这样，对于有$n$类的数据集，一共训练得到$n$个分类器。测试时，如果某个分类器对测试元组的结果为单类（比如用$A$是一类，$B, C, D$合起来看做另一类训练得到的分类器时，测试结果属于类$A$），那么就按这个结果为最终结果；如果某个分类器对测试元组的结果为多类（比如用$A$是一类，$B, C, D$合起来看做另一类训练得到的分类器时，测试结果属于类$B, C, D$），那么$B, C, D$类分别得$1/3$分，如果所有的分类器结果都是多类，则统计最后的得分，得分最高的类为最终结果。

## 3. SVM的整体框架

可以看到，SVM虽然牵涉的内容极广，但是基本原理却非常简单，就是解有约束条件下的凸优化问题（约束条件在后面具体学习中会知道）。但是具体实施的时候面临的问题有很多。比如，到底如何解此类优化问题，有没有好用的方法？对于数据线性不可分的情况如何处理等等。所以SVM的学习就是这一个个的问题组成的，我按照顺序推理的思路，针对每类问题分别写有一篇文章，具体地说，目录如下：
- 一、SVM的整体框架
- 二、[SVM的数学基础](https://blog.csdn.net/guoziqing506/article/details/81117820)
- 三、[线性可分的情况](https://blog.csdn.net/guoziqing506/article/details/81119449)
- 四、[线性不可分的情况](https://blog.csdn.net/guoziqing506/article/details/81120354)
- 五、[SMO算法](https://blog.csdn.net/guoziqing506/article/details/81155323)

**一、SVM整体框架** 就是本篇博文，我将大致介绍SVM要解决的问题，以及整体框架。讲述框架时，我会简要写出每篇博文主要讲解的问题，以方便读者整体把握SVM。此外，我还在本文第二节顺带介绍了二分类问题到多分类问题的转换策略。

**二、SVM数学基础** 我将一些重要的，SVM用到的数学理论在这篇文章中补充，包括凸优化，拉格朗日乘子法和KKT条件，拉格朗日对偶问题等等。这些基础数学知识是理解SVM的必要条件，所以我放到最前面说。

**三、线性可分的情况** 介绍在数据线性可分的情况下，SVM算法是如何确定最佳分离超平面的。这篇文章中将展现利用拉格朗日对偶转换优化问题的详细过程，以及计算的结果。当然，具体求解拉格朗日对偶问题要用到SMO算法，这又是一大块内容，为了清晰把握SVM的主要脉络，我将SMO算法放到最后最后一篇博文去讲

**四、线性不可分的情况** 线性可分情况的拓展，通过线性模型和非线性模型两种方法解决了针对线性不可分数据的分类。具体地，这两种方法分别为：（1）加入松弛变量和惩罚因子；（2）核函数。

**五、SMO算法** 详细解释了SMO算法是如何求解SVM中最重要的拉格朗日对偶问题，这算是对之前博文的补充。

下面我用一张简图，大致表示出SVM的主体思路：



![](https://img-blog.csdn.net/20180722145436517?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


好了，从下一篇博客开始，我将正式开始介绍SVM的具体内容。



