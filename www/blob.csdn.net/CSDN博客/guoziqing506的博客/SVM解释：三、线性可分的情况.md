# SVM解释：三、线性可分的情况 - guoziqing506的博客 - CSDN博客





2018年07月23日 08:41:26[guoziqing506](https://me.csdn.net/guoziqing506)阅读数：1689
所属专栏：[机器学习经典算法研究](https://blog.csdn.net/column/details/25189.html)









在之前的博客 [SVM解释：二、SVM的数学基础](https://blog.csdn.net/guoziqing506/article/details/81117820) 中，我已经大致介绍了支持向量机(SVM)的数学理论基础。从本文开始，我将逐步推导SVM是如何运用于数据分类的。由简入难，我先来介绍比较简单的，通过训练线性可分的数据分类。

在我写的SVM的第一篇博客中，已经大致介绍了SVM是做什么的，大概是怎样一个思路，所以本文我们直接进入正题，从介绍最大边缘超平面的计算方法开始。

## 1. 最大边缘超平面

一个给定的数据集如Fig.1所示。数据集被标注为两类（黑点和白点），我们发现，此时数据集是“线性可分的”：即可以找到一个超平面，将数据集按类别分开，如图中虚线所示。当然，在二维空间内是一条线，那么在多维空间中，自然就是一个超平面了。



![](https://img-blog.csdn.net/20180719152530434?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


不难想到，我们自然可以把这样一个超平面当做分类器。对于新的未知分类的数据点，可以根据这个点和超平面的位置关系，预测其所在分类。但是现在的问题是，这样的超平面可以有很多，我们显然应该用“最好”的那个，“最好”的定义可以这样解释：**它能尽可能地区分属于不同类的数据**。那你说咋样算是能尽可能区分呢，直观来说就是为现在的两类数据找一个分离超平面，使得这个平面分别与两类数据的最近的数据点之间的距离之和最大。看下面的Fig.2(a)和Fig.2(b)就明白了：Fig.2(a)中两条蓝色虚线之间的距离显然小于Fig.2(b)中两条蓝色虚线之间的距离。我们把蓝色虚线之间的距离定义为“边缘”，它就是两类数据距离分离超平面的距离之和，而分离超平面，就是与蓝色虚线平行且等距的超平面。这里，显然Fig.2(b)中红色实线是更好地分离超平面。



![](https://img-blog.csdn.net/20180720103916242?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ppcWluZzUwNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


下面定义几个概念：
- 最大边缘超平面(MMH)：即我上面说的“最佳”的分离超平面。它距离两类数据中最近的元组的距离之和最大，且与相应元组等距，比如Fig2.(b)中的红色实线；
- 侧面：与MMH平行，且正好经过类1和类2数据中距离MMH最近的数据点的超平面，记为$H_1, H_2$，比如Fig2.(b)中的蓝色虚线；
- 支持向量：在$H_1$和$H_2$上的训练元组（即数据点）被称为“支持向量”，他们离MMH一样近，比如我在Fig.2(b)中用红色标出的数据元组；

综上所述，SVM在训练数据线性可分的情况下，要解决的问题可以用一句话概括：**根据训练数据，找到最大边缘超平面(MMH).**

## 2. 计算过程推导

设训练数据集为$X = \{X_1, X_2, \dots, X_n\}$，每个元组$X_i$都被标注了一个类别，类别号记为1或者-1（后面我会解释类标号的选取不影响算法，这样标只是为了方便推导）。

假设找到的MMH的方程为： 


$\begin{equation}WX + b = 0\end{equation}$

其中$W = \{w_1, w_2, \dots, w_m\}$为权重向量，其维度$m$也是训练数据的属性数；$b$为标量。显然，根据这个方程，我们可以找到这两个不同类的数据元组满足的特征，即：
- 在MMH上方的数据元组（即属于类1的数据元组），满足$WX + b \geq 0$；
- 在MMH下方的数据元组（即属于类-1的数据元组），满足$WX + b \leq 0$；

据此，我们可以接着写出侧面$H_1, H_2$的方程：



$\begin{equation}\begin{aligned}&H_1: WX + b = 1\\&H_2: WX + b = -1\end{aligned}\end{equation}$

**注1**：解释一下$H_1$和$H_2$的方程右侧为什么要用1和-1，其实任意两个绝对值相等的正负数都是可以的。比如我们设为$k$和$-k$，等式两边同时除$k$后，和上式就是一样的了，那能不能设置成任意两个数呢，比如2和3，也是可以的，但是你的MMH的方程就要发生变化了，所以没有这个必要，为了方便计算推导，就直接设为1和-1就行了。

上面说过，两个类的类标号为1和-1，那根据$H_1$和$H_2$的方程，对数据集中任意的元组$X_i$而言，下面的不等式一定成立： 


$\begin{equation}y_i(WX_i + b) \geq 0, y_i \in \{1, -1\}\end{equation}$

其中$y_i$是元组$X_i$的类标号，即1或-1.

**注2**：解释一下我之前说的类标号的取值问题。首先名称类的类标号可以用数值来替代，比如类buy和no_buy，可以标记为1和-1；其次，数值也不一定非要取1和-1，比如你也可以取成2和3，但是这样一来，如果还要写成一个式子大于等于0这种形式，上面的不等式就要发生变化，变成



$\begin{equation}(y_i - 2.5)(WX_i + b) \geq 0\end{equation}$

所以说本质上是一样的。不管类标号怎么取，我们的目的就是要构造一个约束条件出来，而且一定是个不等式的约束条件，因为SVM的核心就是在有不等式的约束条件下解决一个凸优化无问题，这一点你慢慢看后面会明白。此外，因为两个类分别在超平面的上下两侧，对于超平面的方程而言，他们在数值上是正负相反的，所以一般情况下，我们设类标号为1和-1. 总而言之，一句话：**y的取值是可以任意选两个值的，只是出于计算推导的方便和解释直观性，选取了1和-1.**

好了，现在关于超平面的方程以及数据元组和这个方程的关系我们已经列出来了，接着思考，我们说距离它的两个侧面$H_1, H_2$间距最大的MMH是要求解的对象。那先来看看这个距离怎么算，我们再次列出三者的方程如下：



$\begin{equation}\begin{aligned}&H_1: WX + b = 1\\&MMH: WX + b = 0\\&H_2: WX + b = -1\end{aligned}\end{equation}$

根据计算几何的知识，我们知道$H_1$和MMH的间距与$H_2$和MMH的间距是相等的，都是$\frac{1}{\|W\|}$。那现在的任务很明确了，求出使得$\frac{1}{\|W\|}$最大的$W$即可。

这是典型的凸优化问题，形式化的表示如下：



$\begin{equation}\begin{aligned}\max ~&\frac{1}{\|W\|}\\    &s.t. ~ y_i(WX_i + b) \geq 1, i = \{1, 2, \dots, n\}\end{aligned}\end{equation}$

等价转换一下，变成：



$\begin{equation}\begin{aligned}\min ~&\frac{1}{2} \|W\|^2\\    &s.t. ~ 1 - y_i(WX_i + b) \leq 0,  i = \{1, 2, \dots, n\}\end{aligned}\end{equation}\tag{1}$

这就是简单的将求最大值的问题转换成求最小值的问题，很好理解。至于为什么要用$\frac{1}{2} \|W\|^2$，那是为了后面的推导方便。

OK，现在的目标函数是二次的，约束条件是线性的，它是一个凸二次规划问题。根据July的博客 [支持向量机通俗导论](https://blog.csdn.net/v_july_v/article/details/7624837)，我们知道直接用现成的QP (Quadratic Programming) 优化包求解就行。其实到这里，关于线性可分的SVM算法就算是说完了。但是现在的问题是，如果遇到了线性不可分的情况（我将在下一篇博客中讲解），为了找到合适的分类器，我们需要将数据通过核函数映射到更高维的空间中，再寻找MMH。为了更清楚地理解核函数的相关概念，这里我们做一个转换：将公式(1)中带不等式约束的凸优化问题转换成拉格朗日对偶，通过求解拉格朗日对偶问题，计算分类器。

## 3. 拉格朗日对偶的计算步骤

关于拉格朗日对偶的预备知识我在上一篇博文 [SVM解释：二、SVM的数学基础](https://blog.csdn.net/guoziqing506/article/details/81117820) 的小节4中有详细的说明，我们已知两点：
- 优化问题(1)可以转换成一个无约束条件的原问题$p^*$；
- 因为满足Slater条件，原问题$p^*$与对偶问题$d^*$是同解的；

这里，我们把拉格朗日对偶直接用于解决优化问题(1)。具体的求解步骤如下。

首先，写出拉格朗日函数：



$\begin{equation}L(W, b, \alpha) = \frac{1}{2} \|W\|^2 + \sum_{i = 1}^{n} \alpha_i (1 - y_i(WX_i + b))\end{equation}$

令原问题为： 


$\begin{equation}p^* = \min_{W, b} \max_{\alpha} L(W, b, \alpha)\end{equation}$

其实这里的$W$和$b$合起来相当于是上一篇博客4.1节中的x，我们发现这个函数是个开口朝上的抛物线，所以先计算最大值不好求解，于是进一步找到它的对偶问题：



$\begin{equation}d^* = \max_{\alpha} \min_{W, b} L(W, b, \alpha)\end{equation}$

我们已知对偶问题与原问题的最优解是一样的，所以解决$d^*$就是解决$p^*$。

对偶问题$d^*$本质上是无约束的最优化问题，我们先计算$\min_{W, b} L(W, b, \alpha)$。固定$\alpha$，然后令$L(W, b, \alpha)$分别对$W$和$b$计算偏导，得到如下结果：



$\begin{equation}\begin{aligned}&\frac{\partial L(W, b, \alpha)}{\partial W} = 0 \Rightarrow W = \sum_{i = 1}^{n} \alpha_i y_i X_i^T\\&\frac{\partial L(W, b, \alpha)}{\partial b} = 0 \Rightarrow \sum_{i = 1}^{n} \alpha_i y_i = 0\end{aligned}\end{equation}\tag{2}$

**注意一下，上面这个公式(2)非常重要，后面我们要多次用到它**。

将以上的结果带入$L(W, b, \alpha)$，得到： 


$\begin{equation}\begin{aligned}L(W, b, \alpha) &= \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j - \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j - b \sum_{i = 1}^{n} \alpha_i y_i + \sum_{i = 1}^{n} \alpha_j\\&= \sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j\end{aligned}\end{equation}$

那你看看，现在没有了$W$和$b$，只有$\alpha$，而且问题变成了下面的带约束凸优化问题：



$\begin{equation}\begin{aligned}\max ~&\sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_i \alpha_j y_i y_j X_i^T X_j\\    &s.t. \sum_{j = 1}^{m} \alpha_i y_i = 0\\    &~~~~~~~\alpha_i \geq 0\end{aligned}\end{equation}\tag{3}$

**注3**：上面的公式(3)在后面的讲解中简称为“SVM对偶问题”，这也是SVM的核心公式。

求解SVM对偶问题一般用SMO算法，关于SMO算法的细节我会在第5篇博客 [SVM解释：五、SMO算法](https://blog.csdn.net/guoziqing506/article/details/81155323) 中详细说明，这里暂且省略，大家知道这个问题不难求解就可以了。解出$\alpha_i$后，进而可以根据公式(2)得到$W$。实验时，我们会发现$\alpha_i$中有很多是等于0的，只有少数是大于0的，而大于0的这些$\alpha_i$所对应的$X_i$则就是支持向量。想想为什么？因为大于0的$\alpha_i$对应的$X_i$满足：



$\begin{equation}1 - y_i(WX_i + b) = 1\end{equation}$

如果不满足上式的话，则无法满足KKT条件的第三项。这样，我们可以得到一个结论：大于0的$\alpha_i$对应的$X_i$就是支持向量（因为它恰好在侧面上）。而同时，那些$\alpha_i = 0$的所对应的元组$X_i$则不在侧面上，他们在侧面以外，而且对于超平面没有影响。从这个角度来讲，SVM其实只与少量的支持向量有关，与其他远离超平面的大量的训练元组没有关系。所以在实际应用中，可以通过这种思想筛掉大量的训练元组，以提高效率。这也是我在第本系列第5篇博文中，讲到的求解优化问题的高效SMO算法最本质的思想。

随后，我们可以依据这个式子求解出$b$。从数值的稳定性上讲，建议用所有的支持向量计算$b$的值，并求取平均值，这样找出的$W$和$b$合起来就是所谓的支持向量机（SVM）。

形式化地，支持向量机（SVM）可以用下面的公式(4)表示：



$\begin{equation}f(X) = \sum_{i = 1}^{n} \alpha_i y_i X_i^T X + b\end{equation}\tag{4}$

分类时，将测试元组$X^*$带入公式(4)，判断计算结果的正负性就可以预测分类了。

好了，对于线性可分情况下的SVM的求解过程到这里就算是全部完成了。但是，这种算法只能适用于数据线性可分的情况，其实现实中用处不大，因为现实中我们遇到的数据大多时候是线性不可分的。对于线性不可分的情况处理起来会更加复杂，我将在下一篇博客中详细阐述。



