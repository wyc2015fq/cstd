# 朴素贝叶斯分类 - guoziqing506的博客 - CSDN博客





2017年03月28日 20:02:55[guoziqing506](https://me.csdn.net/guoziqing506)阅读数：516标签：[分类																[机器学习																[数据挖掘](https://so.csdn.net/so/search/s.do?q=数据挖掘&t=blog)
个人分类：[机器学习																[数据挖掘](https://blog.csdn.net/guoziqing506/article/category/6289196)](https://blog.csdn.net/guoziqing506/article/category/6776278)

所属专栏：[机器学习经典算法研究](https://blog.csdn.net/column/details/25189.html)





之前，我探讨过“决策树归纳”的分类方法（详见：[决策树归纳](http://blog.csdn.net/guoziqing506/article/details/65633770)），本文我将介绍另一中比决策树更加简单的，用概率方法分类的技术——“朴素贝叶斯分类”。

## 贝叶斯定理

贝叶斯定理是概率论中非常简单基础的定理，其解决的核心点在于根据已有信息，对未知事物发生结果的概率计算。就拿分类这件事来说（分类的相关概念我已经在上一篇博文：[决策树归纳](http://blog.csdn.net/guoziqing506/article/details/65633770)中说得相当清楚，故不再赘述），如果现在有一个数据对象$X$，$X$形式上可以看做一个向量，每个维度代表了某一种属性的属性值。我们假设$X$属于某个类$C_i$，并把这个假设记为$H$。那么根据概率学的标记，可以用符号$P(H|X)$表示在数据对象为$X$的条件下，发生假设$H$的后验概率。也就是说，在已知一个数据对象所有属性的前提下，这个数据对象的分类情况满足假设$H$的概率。

与此同时，用符号$P(H)$可以表示假设$H$的先验概率，意思是在完全对数据对象无所知的情况下，这个数据对象的分类情况满足假设$H$的概率。根据字面意思，其实就不难理解，所谓“后验”是说已经得到一部分信息，有个判断的依据，再去做概率判断；而所谓“先验”是说完全没有任何信息，是纯粹的猜测概率。同理，后面还可以写出$P(X|H)$以及$P(X)$，相关的解释类似。

### 条件概率

至此，明白了这些概念，可以引入贝叶斯定理了。首先先提一下概率论中著名的条件概率公式。设$A$和$B$是样本空间$\Omega$中的两个事件，则：



$\begin{equation}P(A|B) = \frac{P(AB)}{P(B)}\end{equation}$

意思非常容易理解，在已知$B$发生的前提下，发生$A$的概率，等于$A, B$同时发生的概率除$B$发生的概率。举个简单的例子，一家人生了2个孩子，已知其中至少有一个是男孩，判断至少有一个是女孩的概率。
- $A$：至少有一个是女孩
- $B$：至少有一个是男孩



$\begin{equation}P(A|B) = \frac{P(AB)}{P(B)} = \frac{1}{2} / \frac{3}{4} = \frac{2}{3}\end{equation}$

而如果直接计算$P(A)$得到的结果是$P(A) = 3/4$.

### 乘法公式

我们可以将条件概率公式变形，这就得到了乘法公式：



$\begin{equation}P(AB) = P(A|B)P(B)\end{equation}$

### 全概率公式

再拓展一步，假设样本空间$\Omega$中有$n$个事件$B_1, B_2, \dots, B_n$互不相容，且$\bigcup_{i = 1}^{n}B_i = \Omega$，则根据乘法公式，存在：



$\begin{equation}\sum_{i = 1}^{n}{P(AB_i)} = \sum_{i = 1}^{n}{P(A|B_i)P(B_i)} = P(A)\end{equation}$

上面这个公式也就是所谓的全概率公式。

### 贝叶斯公式

最后根据乘法公式和全概率公式，可以最终得到贝叶斯公式：



$\begin{equation}P(B_i|A) = \frac{P(AB_i)}{P(A)} = \frac{P(A|B_i)P(B_i)}{\sum_{i = 1}^{n}{P(A|B_i)P(B_i)}}\end{equation}$

实际上，贝叶斯公式是做了一个转换，将$P(B_i|A)$转换成由$P(B_i)$和$P(A|B_i)$作为参数的函数。当然，在朴素贝叶斯分类中，并没有用到上面这么复杂的公式，回到刚才的$P(H|X)$的话题，贝叶斯定理帮助我们将$P(H|X)$的计算转换为由$P(H), P(X)$和$P(X|H)$为参数的函数，如下：



$\begin{equation}P(H|X) = \frac{P(HX)}{P(X)} = \frac{P(X|H)P(H)}{P(X)}\end{equation}$

## 朴素贝叶斯分类

在介绍具体的分类算法前，先给出符号说明：
- $D$：训练集，由训练元组构成，对于其中一个训练元组$X$来说，$X$由$m$个属性值构成：$X = \{x_1, x_2, \dots, x_m\}$；
- $A_k$：元组中第$k$个属性的属性名；
- $C_i$：第$i$个类的类标记，这里记类的总数为$n$；

具体算法其实只有一步，如下：

输入：训练集，新的数据对象$X$

输出：预测$X$应该属于的类的类标记

过程：对于所有类别，依次计算概率$P(C_i|X)$，选择使得该值最高的类$C_i$，并返回。

这个道理太过明显了，就是找分类中，概率最大的。但是计算$P(C_i|X)$却是得费一番心思。回到上面最后我提到的贝叶斯定理，实际上，这么写的目的就在于用$P(X|C_i)$（对应$P(X|H)$），$P(C_i)$（对应$P(H)$），$P(X)$这三个我们容易计算得到的概率来计算$P(C_i|X)$。依次分析如下；
- 
$P(X)$表示选择一个数据对象，恰好是$X$的概率，这个概率对于所有类别都是一样的，所以，既然我们是要比较大小，就可以不用考虑了。

- 
$P(C_i)$表示假设$H$成立的先验概率，即一无所知的情况下，一个数据对象属于类$C_i$的概率。若当前已有训练数据集，则用$\frac{|C_i|}{|D|}$来估计$P(H)$；若当前还没有训练集，则令$P(C_i) = 1/n$，其中，$n$为所有类别的的个数，即认为分配到每个类的概率都是相等的。

- 
$P(X|C_i)$的计算相对要复杂一点，它表示被归类为$C_i$的元组，就是要预测的元组$X$的概率。所以从含义也能看出$P(X|C_i)$可以用$X$在类$C_i$中的比例来估计。当需要处理的是属性值非常多的数据集时，直接比较计算会导致运算效率较低。因此，一般情况下会默认属性之间是相互独立的，然后用下面的公式计算：



$\begin{equation} P(X|C_i) = \prod_{k = 1}^{m}{P(X_k|C_i)} = P(X_1|C_i)P(X_2|C_i) \cdots P(X_m|C_i) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(1) \end{equation}$

其中，$P(X_k|C_i)$表示类$C_i$的所有元组中，第$k$个属性的值是$X_k$的元组出现的概率。

注意：显然，做出这种属性相互独立的假设可能会在某些情况下是不那么合理的。之所以还要用这样连乘的形式处理，而不是直接统计$X$在类$C_i$中的比例来用作估计的概率$P(X|C_i)$，据韩家炜《数据挖掘》中的说法，是为了降低计算开销，然而我个人认为这两种运算的方法至少在属性都是“离散”的情况下是一致的，都是$O(m|C_i|)$，其中$m$为属性总数。所以，我猜测这样处理的理由在于以下两点：
- 解决了元组中含有连续型属性时，概率估计问题（这一点下面会说）
- 解决了在训练集中不存在$X$时的概率估计问题。这一点比较好理解，可以想象这样一种情形：$X$在$C_i$中并不存在，但是$X$的每个属性值在$C_i$中都存在，总不能说$P(X|C_i) = 0$吧，所以可以用上式估计。

好了，现在可以计算$P(X_k|C_i)$了，正如上面分析的那样，分两种情况考虑：
- $X_k$是离散属性，这种情况简单，直接使用比例$\frac{|X_k|}{|C_i|}$估计即可。$|X_k|$表示类$C_i$的元组中，属性$A_k$的属性值为$X_k$的元组个数；
- $X_k$是连续属性，这种情况复杂一点，我们首先假设$X_k$在$C_i$中是符合均值为$\mu$，标准差为$\sigma$的正态分布，用其概率密度函数的值，估计概率，具体如下： 


$\begin{equation} P(X_k|C_i) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(X_k - \mu)^2}{2\sigma^2}} \end{equation}$

其中，$\mu$，$\sigma$表示$C_i$的元组中，属性$A_k$的的均值和标准差。
这样，整个计算过程完成。从计算过程能够看出，这样设计的$P(X|C_i)$的计算还是有瑕疵的，主要在于属性独立分布的假设和连续属性正态分布的假设并不是在所有情况下都成立。但是即便如此，朴素贝叶斯也是相当出色的，他在某些领域是可以与决策树以及神经网络相媲美的，而且理论上具有最小的错误率。

## 实例计算

我用上一篇博文（详见：[决策树归纳](http://blog.csdn.net/guoziqing506/article/details/65633770)）中的例子来展示一下上面所说的计算过程。这个例子说的是对用户是否买电脑的类别判断，训练集（属性及类标记）如下表所示：
|RID|student|income|age|credit_rating|class|
|----|----|----|----|----|----|
|1|no|high|youth|fair|no|
|2|no|high|youth|excellent|no|
|3|no|high|middle_aged|fair|yes|
|4|no|medium|senior|fair|yes|
|5|yes|low|senior|fair|yes|
|6|yes|low|senior|excellent|no|
|7|yes|low|middle_aged|excellent|yes|
|8|no|medium|youth|fair|no|
|9|yes|low|youth|fair|yes|
|10|yes|medium|senior|fair|yes|
|11|yes|medium|youth|excellent|yes|
|12|no|medium|middle_aged|excellent|yes|
|13|yes|high|middle_aged|fair|yes|
|14|no|low|senior|excellent|no|

假设现在要判断一个新的数据对象X = {age=youth; income=medium; student=yes; credit_rating=fair}属于哪一类。

根据上面的步骤：
- $P(X)$可忽略，不考虑；
- $P(class=yes) = 9/14 = 0.643$；$P(class=no) = 5/14 = 0.357$；
- 
既然是两类，那么需要计算两个概率，进行比较：

（1）计算：$P(X|class=yes)$



$\begin{equation} \begin{aligned} & ~~~P(X|class=yes) \\& = P(age=youth|class=yes) \times P(income=medium|class=yes) \times  P(student=yes|class=yes) \times P(credit\_rating=fair|class=yes) \\& = 2/9 \times 4/9  \times 6/9 \times 6/9\\& = 0.044 \end{aligned} \end{equation}$

（2）计算：$P(X|class=no)$

具体过程不写出来了，与上面类似。最终计算得到$P(X|class=no) = 0.019$

- 
比较大小： 
$P(X|class=yes)P(class=yes) = 0.044 \times 0.643 = 0.028$
$P(X|class=no)P(class=no) = 0.019 \times 0.357 = 0.007$
所以，$X$应该归类为买电脑的一类（class=yes）。

## 拉普拉斯校准

在上面介绍的朴素贝叶斯分类算法中，存在一种极端的情况：那就是在计算概率$P(X|C_i)$的过程中，可能出现$X$的某个属性$A_k$所对应的属性值$X_k$在类$C_i$中并没有出现，这样，根据公式(1)，必然会导致$P(X|C_i)$为0，但是这种情况显然不合理，因为一个0概率将消除所有其他属性对于类$C_i$的后验概率。

为了避免这种极端的不合理的情况，我们可以采取“拉普拉斯校准”的方法。基本思路是对每个属性值对应的元组个数加1，假设现在有$q$个属性值，那么要记得在元组整体的个数上加$q$。举个例子，类$C_i$有100个元组，属性$A_1$有3个属性值，记为”0, 1, 2”，其中，$C_i$中$A_1 = 0$的元组有80个，$A_1 = 1$的元组有20个，$A_1 = 2$的元组有0个，则相关比例分别为：$\frac{81}{103}$，$\frac{21}{103}$，$\frac{1}{103}$。

这样，既不会太过大地影响比例计算的最终结果，同时，也避免了0概率的情况发生。

这就是朴素贝叶斯分类的基本介绍了。不足之处，还请指正！](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)](https://so.csdn.net/so/search/s.do?q=分类&t=blog)




