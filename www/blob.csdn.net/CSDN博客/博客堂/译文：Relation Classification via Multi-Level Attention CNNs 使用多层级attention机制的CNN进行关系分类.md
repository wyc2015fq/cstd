# 译文：Relation Classification via Multi-Level Attention CNNs 使用多层级attention机制的CNN进行关系分类 - 博客堂 - CSDN博客





2019年02月22日 10:59:23[最小森林](https://me.csdn.net/u012052268)阅读数：171标签：[关系分类																[注意力机制																[attention																[cnn																[深度学习](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)
个人分类：[自然语言处理](https://blog.csdn.net/u012052268/article/category/6830884)





# 通过多层面的attention CNN的关系分类

原文：Relation Classification via Multi-Level Attention CNNs    [http://eprints.bimcoordinator.co.uk/14/](http://eprints.bimcoordinator.co.uk/14/)

## 摘要：

关系分类是众多从文本中挖掘结构化事实的信息抽取系统中的一个重要组成部分。我们提出了一种新的卷积神经网络体系结构，针对这一任务，依赖于两个层次的attention，以便更好地识别异构上下文中的模式。这种结构使端到端的任务特异性标记数据的学习，放弃对外部知识如显式依赖结构的需要。实验表明，我们的模型优于以前的最先进的方法，包括那些依赖于更丰富的先验知识形式。

## 1介绍

关系分类是识别文本中两个名义实体之间的语义关系的任务。它是自然语言处理的系统，需要从文本挖掘事实清楚的一个重要组成部分，例如各种信息提取中的应用以及答疑知识库完成（Tandon et al.，2011；陈等，2015）。例如，给定示例输入

“ Fizzy [drinks] and meat cause heart disease and [diabetes].”

带注释的目标实体提到E 1和E 2 =“饮料”=“糖尿病”，目标会自动识别这个句子表达了E 1和E 2之间的因果关系，而我们使用符号的因果（E 1，E 2）。准确的关系分类有利于精确句子的解释，语篇处理，和更高一级的NLP任务（亨德克斯et al.，2010）。因此，关系分类吸引了过去几十年的课程研究者的广泛关注（张，2004；钱et al.，2009；溜冰场和harabagiu，2010）。

在上面给出的例子中，动词与所期望的目标关系相当接近。然而，在野外，我们遇到了许多表达同一种关系的不同方式。这种具有挑战性的可变性本质上可以是词汇、句法甚至语用。一个有效的解决方案需要能够解释有用的语义和句法特征，不仅是词汇层面上的目标实体的意义，还包括它们的直接上下文和整个句子结构。

因此，众多特征和基于核的方法已被提出，这并不奇怪，许多依赖于一个成熟的NLP栈，包括词性标注、形态学分析、句法分析和语义分析，偶尔，以及对知识资源的获取词汇语义特征（kambhatla，2004；周等，2005。；Suchanek et al.，2006；钱et al.，2008；穆尼，2005；Bunescu bunescu，和穆尼，2005）。近年来，我们已经看到了一个深入的体系结构，它能够学习相关的表示和特性，而不需要进行大量的手工功能工程或使用外部资源。一些卷积神经网络（美国有线电视新闻网），递归神经网络（RNN），和其他神经的架构已经被提出关系分类（Zeng et al.，2014；多斯桑托斯等人，2015；Xu et al.，2015b）。尽管如此，这些模型常常无法识别关键线索，其中许多仍然需要外部依赖解析器。

我们提出了一个新的cnn架构，解决了以往方法的不足。我们的主要贡献如下：- 我们的cnn体系结构依赖于一种新的多层次注意机制，来捕获实体特定的注意（在输入级别上，针对目标实体的主要注意）和特定关系的集合注意（关于目标关系的次级注意）。这使得它能够检测到更多的微妙线索，尽管输入语句结构不一，使它能够自动学习哪些部分与给定的分类相关。
- 我们引入了一个新颖的基于边缘的边际目标函数，证明其优于标准损失函数。
- 在SemEval 2010 Task 8数据集中，我们获得了新的最先进的关系分类结果，F1评分为88.0％，优于依赖于更丰富的先验知识的方法。

## 2相关工作

除了几种无监督的聚类方法（长谷川等，2004; Chen等人，2005），大多数关于关系分类的工作已经被监督，通常被称为标准的多类或多标签分类任务。传统的基于特征的方法依赖于从显式语言预处理步骤的输出计算出的一组特征（Kambhatla，2004; Zhou等，2005; Boschee et al。，2005; Suchanek et al。，2006; Chan and Roth ，2010; Nguyen和Grishman，2014），而基于内核的方法利用卷积树内核（Qian et al。，2008），子序列内核（Mooney和Bunescu，2005）或依赖关系树内核（Bunescu和Mooney，2005） ）。因此，这些方法都取决于经常以试错法选择的精心设计的特征，或精心设计的内核，反过来又经常来自其他预先训练的NLP工具或词汇和语义资源。虽然这种方法可以从外部NLP工具中受益，以发现句子的离散结构，但语法解析容易出错，并且依赖于其成功也可能妨碍性能（Bach和Badaskar，2007）。进一步的缺点包括其对于不可见的词语的有限的词汇泛化能力，以及当应用于新的领域，类型，或语言。

近年来，深层神经网络已经取得了有希望的成果。 Socher等人的递归矩阵维基模型（MV-RNN） （2012）试图通过利用句法树来捕捉句子语义的组成方面。曾等（2014）提出了一种具有softmax分类的深卷积神经网络，提取词法和句子级特征。然而，这些方法仍然依赖于词汇资源和NLP工具包的附加功能。 Yu et al。 （2014）提出了基于因子的组合嵌入模型，它使用语法依赖树和句子级嵌入。除了Santos等人（2015），他们提出了具有类嵌入矩阵Miwa和Bansal（2016）的排名CNN（CR-CNN）模型，类似地观察到，由于网络中捕获的语言结构有限，基于LSTM的RNN优于使用CNN的模型建筑。已经提出了一些更复杂的变体来解决这个问题，包括双向LSTM（Zhang et al。，2015），深层复发神经网络（Xu et al。，2016）和双向结构化的LSTM-RNN（Miwa和Bansal，2016）。最近的几个作品还重新引入了基于依赖树的设计，例如在句法树（Hashimoto等，2013）上运行的RNN，基于最短依赖路径的CNN（Xu et al。，2015a）和SDP-LSTM模型Xu等，2015b）。最后，Nguyen和Grishman（2015）对CNN和RNN进行了训练，并使用投票，堆叠或对数线性建模来不同地整合其产出（Nguyen和Grishman，2015）。虽然这些最近的模型实现了很好的结果，但理想情况下，我们将需要一个简单而有效的架构，不需要依赖关系解析或训练多个模型。我们在第4节中的实验表明，我们确实可以实现这一点，同时在获得的F1分数方面也获得了实质性的改进。

## 3模型

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222104825564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIwNTIyNjg=,size_16,color_FFFFFF,t_70)

给定具有实体提及e 1和e 2的标签对的句子S（如在我们的示例中从第1节），关系分类是在一组候选关系类型中识别在e 1和e 2之间保持的语义关系的任务 （Hendrickx等人，2010）。 由于唯一的输入是具有两个明确提及的原始句子，因此获得准确预测所需的所有词汇，语义和句法提示是不重要的。
为此，我们提出了一种新颖的多层次关注卷积神经网络模型。我们的架构的示意图在图1中给出。输入句首先使用字矢量表示进行编码，利用上下文和位置编码来更好地捕获单词顺序。**使用基于对角矩阵的主要注意机制来捕获单词相对于目标实体的相关性**。对于所得到的输出矩阵，然后应用卷积运算以便捕获诸如相关的n-gram之类的上下文信息，然后是max-pooling。次要注意池层用于基于注意池矩阵从输出中确定关系分类中最有用的卷积特征。本节的其余部分将提供有关此架构的更多详细信息。表1提供了我们将为此使用的符号的概述。最终输出由新的目标函数给出，如下所述。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222104922297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIwNTIyNjg=,size_16,color_FFFFFF,t_70)
### 3.1分类目标

我们从关系分类架构的自上而下的设计考虑开始。 对于给定的句子S，我们的网络将最终输出一些w O。 对于每个输出关系y∈Y，我们假设有一个相应的输出嵌入W L y，它将自动被网络学习（dos Santos等，2015）。

我们提出一种新的距离函数δθ（S），其衡量方式为：网络的预测输出w O与候选关系y的接近度。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222104949152.png)
使用L 2范数（注意W L y已经被归一化）。 基于该距离函数，我们设计了基于边缘的成对损失函数L
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105009977.png)

其中1是余量，β是参数，δθ（S，y）是预测标签嵌入WL和地面真值标签y之间的距离，δθ（S，y-）是指w O 并选择不正确的关系标签y - 。 后者被选为所有不正确类别中最高分的人（Weston等人，2011; dos Santos等人，2015），即
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105030476.png)

这种基于边际的目标具有与经验损失函数相比具有强大的可解释性和有效性的优点，例如由dos Santos等人在CR-CNN方法中的排序损失函数。（2015年）。 基于由词类比推动的距离函数（Mikolov等，2013b），我们将预测输出和地面真值标签之间的差距最小化，同时使所选择的不正确类最大化距离。 通过迭代地最小化该成对损失函数（见第3.5节），δθ（S，y）被增加，而δθ（S，y-）增加。
### 3.2输入表示

给定具有标记实体的句子S =（w 1，w 2，…，wn）e 1（= wp）和e 2（= wt），（p，t∈[1，n]，p 6 = t），我们首先将每个单词变换成实值向量，以提供词汇特征。 给定维数d w×| V |的单词嵌入矩阵W V ，其中V是输入词汇，d w是词向量维度（超参数），我们将每个w i映射到列向量w d i∈R d w。

为了另外捕获与目标实体的关系的信息，我们结合了词位置嵌入（WPE）来反映第i个词与两个标记的实体提及之间的相对距离（Zeng et al。，2014; dos Santos et al。 ，2015）。 对于图1中的给定句子，单词“和”与实体e 1“饮料”和e 2“糖尿病”的相对距离分别为-1和6。 每个相对距离映射到R d p中随机初始化的位置向量，其中d p是超参数。 对于给定的词i，我们分别获得关于实体e 1和e 2的两个位置向量w p i，1和w p i，2。 第i个词的整体词嵌入是w M i = [（w d i）|） ，（w p i，1）| ，（w p i，2）| ] |。

使用以第i个字为中心的大小为k的滑动窗口，我们将k个连续词编码为向量z i∈R（d w + 2d p）k，以将上下文信息合并为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105049553.png)

在输入的开始和结束处，为了明确定义，重复多次填充令牌。
### 3.3 Input Attention Mechanism

虽然基于位置的编码是有用的，但我们推测，它们不足以充分捕捉特定词语与目标实体的关系以及它们对目标关系关系的影响。我们设计我们的模型，以便自动识别与关系分类相关的输入句子的部分。

注意机制已成功应用于序列学习任务，如机器翻译（Bahdanau等，2015; Meng等，2015）和抽象句子摘要（Rush等，2015），以及 诸如建模句子对（Yin et al。，2015）和问答（Santos等，2016）等任务。 迄今为止，这些机制通常被用于允许输入和输出序列的对准，例如， 机器翻译中的来源和目标句子，或者在句子相似性评分和问题回答中的两个输入句子之间的对齐。

在我们的工作中，我们将注意力建模的想法应用于涉及异构对象的一种不同类型的场景，即一个句子和两个实体。 为此，我们试图让我们的模型有能力确定哪一部分句子对两个感兴趣的实体最有影响力。 考虑到在具有多个子句的长句中，也许只有单个动词或名词可能与给定目标实体有相关关系。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105117544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIwNTIyNjg=,size_16,color_FFFFFF,t_70)

如图2所示，输入表示层与对角关注矩阵和卷积输入组合一起使用。
**内容相关矩阵** 考虑图1中的例子，其中非实体词“原因”在确定关系中具有特殊意义。 幸运的是，我们可以利用这样一个事实，即在语料同时出现之间，“cause”和“糖尿病”之间也有显着的联系。 **我们引入具有值A j ii = f（e j，w i）的两个对角关注矩阵A j来表征实体e j和单词w i之间的语境相关性和连接的强度。** 评分函数f被计算为词w i和实体e j的相应嵌入之间的内积，并且被参数化到网络中并且在训练过程期间被更新。 给定A j矩阵，我们定义
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105211416.png)
量化第i个词相对于第j个实体的相对程度（j∈{1,2}）

**输入注意组成**。 接下来，我们采用两个相关因子α1 i和α2 i，并通过简单平均来模拟其联合影响来识别关系
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105234675.png)

除了这个默认选择，我们还会评估另外两个变体。 第一个（Variant-1）将字向量连接为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105253667.png)

获得包含与实体1和实体2的关系相关性的该特定单词的信息丰富输入注意组件。第二变体（Variant-2）将关系解释为两个实体之间的映射，并且将两个实体特定权重为 （8）来捕捉它们之间的关系。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105416391.png)

基于这些r i，输入关注组件的最终输出是矩阵R = [r 1，r 2，…，r n]，其中n是句子长度。

### 3.4 Convolutional Max-Pooling with Secondary Attention

在此操作之后，我们将卷积最大值池与另一个次要关注模型应用于从上一层输出矩阵R中提取更多抽象的较高级别特征。

Convolution Layer例如，卷积层可以学习识别诸如三元组之类的短语。 给定我们新生成的输入注意表示R，因此，我们应用尺寸d c的滤波器作为尺寸d c×k（d w + 2d p）的权重矩阵W f。 然后我们添加一个线性偏差B f，接着是非线性双曲正切变换，以表示特征如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105454942.png)

Attention-Based Pooling. 而不是常规池，我们依赖于基于注意的池策略来确定单个窗口在R *中的重要性，由卷积核心编码。 这些窗口中的一些可以在输入中表示有意义的n-gram。 这里的目标是选择与第3.1节相关的R *相关的部分，该部分基本上要求关系编码过程，而忽略与此过程无关的句子部分。

我们继续首先创建一个相关建模矩阵G，其捕获来自句子的卷积上下文窗口和3.1节之前介绍的关系类嵌入W L之间的相关连接：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105513410.png)

其中U是由网络学习的加权矩阵。

然后我们采用softmax函数来处理这个相关建模矩阵G，得到一个注意集合矩阵A p
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105533856.png)

其中Gij是G和Apij的（i，j）条目，j是Ap的（i，j）条目。

最后，我们将这个注意集合矩阵与卷积输出R *相乘，以突出重要的单个短语级组件，并应用最大运算来选择最显着的组合（Yin et al。，2015; Santos et al。，2016） 输出的给定维度。 更准确地说，我们得到如下的输出表达式w O。（12）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105549597.png)

其中w O i是w O的第i个条目，（R * A p）i，j是R * A p的（i，j）条目
### 3.5训练过程

我们依靠随机梯度下降（SGD）来更新关于方程式中的损失函数的参数。 （2）如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105615104.png)

其中λ和λ1是学习速率，并且包含来自等式的β参数。（2）
## 4实验

### 4.1实验设置

Dataset and Metric 我们对常用的SemEval-2010任务8数据集（Hendrickx等，2010）进行实验，其中包含10,717个句子，其中包含九种类型的注释关系，以及另外的“其他”类型。九种类型是：因果，组件 - 整体，内容容器，实体授权，实体 - 原产地，仪器机构，会员收集，消息主题和ProductProducer，而关系类型“其他”表示关系表达在句子中不是九种类型。然而，对于上述关系类型中的每一个，两个实体也可以以相反的顺序出现，这意味着该句子需要被视为表示不同的关系，即相应的倒数。例如，原因效应（e 1，e 2）和CauseEffect（e 2，e 1）可以被认为是两个不同的关系，所以总数| Y |的关系类型是19. SemEval-2010任务8数据集包括一个8000个示例的训练集，以及其余的示例的测试集。我们使用官方得分手对九个关系对（不包括其他）中的Macro-F1分数进行评估。

Settings. 我们使用word2vec skip-gram模型（Mikolov等人，2013a）来学习维基百科上的初始词表示。 利用高斯分布之后的随机值对其他矩阵进行初始化。 我们对训练数据应用交叉验证过程，以选择合适的超参数。 该过程产生的选择在表2中给出。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222105631253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIwNTIyNjg=,size_16,color_FFFFFF,t_70)4.2 Experimental Results

表3提供了我们的多层次注意CNN模型与以前的方法的详细比较。 我们观察到，我们的新颖的注意力架构在这种关系分类数据集上实现了新的最先进的结果。 AttInputNN仅依赖于输入级别的初始注意，在卷积层之后执行标准最大池生成网络输出w O，其中使用新的目标函数。 使用Att-Input-CNN，我们获得了87.5％的F1分数，因此已经不仅胜过了SemEval任务的原始获胜者，基于SVM的方法（82.2％），而且还有着名的CR-CNN模型（84.1 ％），相对改善为4.04％，新发布的DRNN（85.8％）相对提高了2.0％，尽管后一种方法取决于斯坦福解析器获得依赖解析信息。 我们全面的双重关注模式Att-PoolingCNN实现了更优惠的88％的F1得分。

表4提供了由方程式给出的模型的两个变体的实验结果。 （7）和（8）。 我们的主要模式优于此数据集上的其他变体，尽管当应用于其他任务时，变体仍然可以被证明是有用的。 为了更好地量化我们模型的不同组成部分的贡献，我们还进行了评估几个简化模型的消融研究。 第一个简化是使用我们的模型没有输入注意机制，但与集中注意力层。 第二个消除了注意机制。 第三部分消除了两种形式的关注，并且另外使用基于内积s = r·w的常规目标函数来进行句子表示r和关系类嵌入w。 我们观察到，我们所有这三个组件都导致了这些基线的显着改进。4.3 Detailed Analysis

Primary Attention. 为了检查我们模型的内在作用，我们考虑了我们的多层次关注模型的主要关注矩阵，用于以下随机选择的句子：令人厌恶的场景是对她租用[房间] e 1的兄弟菲利普进行报复 在这个公寓[房子] e 2在伦巴第街。

图3绘制了输入关注层的单词级注意力值，作为一个例子，使用该语句中每个单词的计算注意力值。 我们发现“内在”一词被赋予了最高的关注度，而“房间”和“房子”等词也被认为是重要的。 根据作为ComponentWhole（e 1，e 2）关系的地面真相标签，这显然是明智的。 此外，我们观察到，与“目标”关系相关的“这个”这样的词语确实有较低的关注度。

关系最重要的特征 表5列出了每个关系类别y中对于确定关系分类的分数的贡献的排名最高的三元组。 回想方程中δθ（x，y）的定义。（1）。 在网络中，我们追踪对于每个句子S i的δθ（S i，y）最有助于正确分类的三角形。 然后，我们根据其总贡献对测试集中的句子中的所有这样的三元组进行排序，并列出顶级的三元组。 *在表5中，我们看到，推论这些关系确实非常有用。 例如，因果效应（e 2，e 1）的顶部三元组是“由…引起的”，这强烈地意味着第一个实体是由后者引起的效应。 类似地，Entity-Origin（e 1，e 2）的顶部三元组是“从e 2”，这表明e 2可以是原始位置，实体e 1可能位于该原始位置。

错误分析。 此外，我们研究了我们的模型产生的一些错误分类。 以下是错误分类句子的典型例子：该句子被错误地归类为属于“其他”类别，而实际标签则为Message-Topic（e 1，e 2）。 短语“旋转”并不出现在训练数据中，而是以隐喻的方式使用，而不是原来的转身感觉，使得模型难以识别语义连接。 另一个常见的问题源于“… e 1 e 2 …”形式的句子，例如：

它们分别属于三个不同的关系类别，分别是全部（e 2，e 1），实体 - 原始（e 2，e 1）和工具 - 机构（e 1，e 2） 文本，并且上下文不是特别有用。 在这种情况下，可以想象到更多的信息词嵌入可以帮助收敛。 最后，我们研究了两种主要方法的收敛行为。 我们绘制图4中Att-Input-CNN和Att-Pooling-CNN模型中的每个迭代的性能。可以看出，Att-Input-CNN非常顺利地收敛到其最终的F1分数，而对于AttPooling- CNN模型，其中包括一个额外的关注层，这两个注意层的共同作用引起更强的反向传播效应。 一方面，这导致结果曲线中的跷跷板现象，但另一方面，它使我们能够获得具有稍高的F1分数的更适合的模型。5 Conclusion

我们提出了一种新的目标和新形式的注意机制，应用于两个不同层次的CNN架构。 我们的研究结果表明，这种简单但有效的模型能够以结构化模型和NLP资源的形式，胜过先前基于更丰富的先验知识的工作。 我们期望这种架构也超出了关系分类的具体任务，我们打算在未来的工作中探索](https://so.csdn.net/so/search/s.do?q=cnn&t=blog)](https://so.csdn.net/so/search/s.do?q=attention&t=blog)](https://so.csdn.net/so/search/s.do?q=注意力机制&t=blog)](https://so.csdn.net/so/search/s.do?q=关系分类&t=blog)




