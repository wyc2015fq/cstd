# 2.6 实战天猫数据爬取 - 博客堂 - CSDN博客





2017年05月19日 18:49:15[最小森林](https://me.csdn.net/u012052268)阅读数：4282










- [6 实战天猫数据爬取](#26-实战天猫数据爬取)- [主要用到的知识点](#主要用到的知识点)- [实用技巧1多级页面的抓取-callback函数](#1-实用技巧1多级页面的抓取-callback函数)
- [实用技巧2图片的抓取-](#2-实用技巧2图片的抓取)
- [抓取过程中的常见问题cookie的处理cookie模拟登录](#3-抓取过程中的常见问题cookie的处理cookie模拟登录)
- [分页](#4-分页)
- [完整代码参见](#完整代码参见)






# 2.6 实战天猫数据爬取

## 主要用到的知识点
- 实用技巧1—多级页面的抓取-callback函数
- 实用技巧2—图片的抓取-
- 抓取过程中的常见问题—cookie的处理,cookie模拟登录
- 分页

### 1. 实用技巧1—多级页面的抓取-callback函数

```php
yield scrapy.Request(url=item["GOODS_URL"], meta={'item': item}, callback=self.parse_detail,dont_filter=True)
```

返回的是一个请求，参数为：

```
url为进一步处理的地址。
meta为了进一步把对象传进去
callback是指处理的函数
```

### 2. 实用技巧2—图片的抓取-

1.首先把图片地址获取

```
# 图片链接
            try:
                file_urls = div.xpath('div[@class="productImg-wrap"]/a[1]/img/@src|'
                                      'div[@class="productImg-wrap"]/a[1]/img/@data-ks-lazyload').extract()[0]
                item['file_urls'] = ["http:" + file_urls]
            except Exception as e:
                print("Error:",e)
                import pdb;pdb.set_trace()
```

2.在settings中引入引擎

```python
# 以下三行引入默认的图片下载器，想改可以重写它
ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}
# 引入items的连接属性
IMAGES_URLS_FIELD = 'file_urls'
# 设置存入本地的地址，当前目录。
IMAGES_STORE = r'./images'
```

### 3. 抓取过程中的常见问题—cookie的处理,cookie模拟登录

自己登录后，查看cookie，把cookie里面的所有参数都输入。

```python
def start_requests(self):  # 循环页码，就在这个函数中实现。
        reqs = []   # 每个页面的request
        cookies = {
            'miid':'1279809970704864021',
            'thw':'cn',
            't':'7349beda1fac2771e1b07173a388c1a7',
            'cookie2':'169e58df275871365bf763a04f83945d',
            '_tb_token_':'f5836335bbbed',
            'l':'As7Ol7pcpNOglmJtnYezXP/Fnq6RuZJB',
            'isg':'AuTkU7_eYUo5n5WHgkykUP1IteI6RAjnXtEpK_4Ehq96qYZzJ431dp1BH7ZL',
            'cna':'xxqjEU4BaTMCAXLV6R/2cfxq',
            'sca':'49d5174e',
            'atpsida':'b8147f8d3acd3709988ab26d_1495089785_1',
            'aimx':'xxqjEYvEdQcCAXLV6R9iOoQn_1495089785',
            'cad':'k95WugY3Sgew+2KIuDSUxTOnySH07xok1SSfrDICn3k=0001',
            'cap':'41cf',
            '_med':'dw:1366&dh:768&pw:1366&ph:768&ist:0',
            'res':'scroll%3A1349*6611-client%3A1349*637-offset%3A1349*6611-screen%3A1366*768',
            'pnm_cku822':'043UW5TcyMNYQwiAiwQRHhBfEF8QXtHcklnMWc%3D%7CUm5Ockt%2FR3pPe0F5QndJdCI%3D%7CU2xMHDJ7G2AHYg8hAS8XIgwsAl4%2FWTVSLFZ4Lng%3D%7CVGhXd1llXGhQbVhsVm5VYF5jVGlLcEx2SHxBf0F0QH5AekF%2FQG44%7CVWldfS0RMQ01DDQUKBMzHWxSPAIrFioSKhI4Az0YLlV7LXs%3D%7CVmhIGCUFOBgkGiMXNww3CzcXKxUuFTUPNAEhHSMYIwM5BjNlMw%3D%3D%7CV25Tbk5zU2xMcEl1VWtTaUlwJg%3D%3D',
            'cq':'ccp%3D1'
        }
        for i in range(0, 2): # 代表从0到1页
            req = scrapy.Request("https://list.tmall.com/search_product.htm?spm=a220m.1000858.0.0.wH40GN&s="+str(i*60)+"&q=%C4%D0%D7%B0&sort=d&style=g&from=nanzhuang..pc_1_suggest&suggest=0_1&type=pc#J_Filter",cookies=cookies )
            reqs.append(req)
        return reqs
```

### 4. 分页

```python
def start_requests(self):  # 循环页码，就在这个函数中实现。
        reqs = []   # 每个页面的request
        cookies = {
            'miid':'1279809970704864021',
        }
        for i in range(0, 2): # 代表从0到1页
            req = scrapy.Request("https://list.tmall.com/search_product.htm?spm=a220m.1000858.0.0.wH40GN&s="+str(i*60)+"&q=%C4%D0%D7%B0&sort=d&style=g&from=nanzhuang..pc_1_suggest&suggest=0_1&type=pc#J_Filter",cookies=cookies )
            reqs.append(req)
        return reqs
```

### 完整代码参见 :

[github_xqtbox_实战天猫数据爬取](https://github.com/xqtbox/python3-tmtopgoods-spider/tree/master)



