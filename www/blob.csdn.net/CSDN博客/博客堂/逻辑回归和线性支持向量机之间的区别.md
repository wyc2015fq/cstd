# 逻辑回归和线性支持向量机之间的区别 - 博客堂 - CSDN博客





2017年12月16日 18:32:56[最小森林](https://me.csdn.net/u012052268)阅读数：2875








# 逻辑回归和线性支持向量机之间的区别



- [逻辑回归和线性支持向量机之间的区别](#逻辑回归和线性支持向量机之间的区别)- [1区别](#1区别)- [1损失函数](#11损失函数)
- [2总结一下](#12总结一下)

- [2两种模型使用选择](#2两种模型使用选择)





![这里写图片描述](http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-25_092026.png)

## 1区别

逻辑回归和支持向量机之间的区别也是面试经常会问的一道题。下面主要讨论逻辑回归（LR）与线性支持向量机（linear SVM）的区别。

lr 和 svm本质不同在于loss function的不同，lr的损失函数是 cross entropy loss，  ,svm是hinge loss。

### 1.1损失函数

我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：



$% <![CDATA[\begin{align*}\text{SVM} : & \frac{1}{n}\sum_{i=1}^n(1-y_i[w_0+\mathbf{x}_i^T\mathbf{w}_1])^+ + \lambda \lVert \mathbf{w}_1 \rVert/2 \tag{1}\\\text{Logistic} : & \frac{1}{n}\sum_{i=1}^n \overbrace{-\log g(y_i[w_0+\mathbf{x}_i^T\mathbf{w}_1])}^{-\log P(y_i|\mathbf{x},\mathbf{W})} + \lambda \lVert \mathbf{w}_1 \rVert/2 \tag{2}\\\end{align*} %]]>$

其中，$g(z)=(1+\exp(-z))^{-1}$

可以将两者统一起来， 


$% <![CDATA[\begin{align*}\text{Both}: & \frac{1}{n}\sum_{i=1}^n Loss(\overbrace{y_i[w_0+\mathbf{x}_i^T\mathbf{w}_1]}^{z}) + \lambda \lVert \mathbf{w}_1 \rVert/2 \tag{3}\end{align*} %]]>$
> 
也就是说，它们的区别就在于逻辑回归采用的是 log loss（对数损失函数），svm采用的是hinge loss →E(z)=max(0,1−z)→E(z)=max(0,1−z)。

- SVM 损失函数 :$Loss(z) = (1-z)^+$
- LR 损失函数:$Loss(z) = \log(1+\exp(-z))$

![这里写图片描述](http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-22_195233.png?imageView2/2/w/400)

这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。

svm考虑局部（支持向量），而logistic回归考虑全局，就像大学里的辅导员和教师间的区别。

> 
辅导员关心的是挂科边缘的人，常常找他们谈话，告诫他们一定得好好学习，不要浪费大好青春，挂科了会拿不到毕业证、学位证等等，相反，对于那些相对优秀或者良好的学生，他们却很少去问，因为辅导员相信他们一定会按部就班的做好分内的事；而大学里的教师却不是这样的，他们关心的是班里的整体情况，大家是不是基本都理解了，平均分怎么样，至于某个人的分数是59还是61，他们倒不是很在意。


### 1.2总结一下
- 
Linear SVM和LR都是线性分类器

- 
Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。

- 
Linear SVM依赖数据表达的距离测度，所以需要对数据先做normalization；LR不受其影响

- 
Linear SVM依赖penalty的系数，实验中需要做validation

- 
Linear SVM和LR的performance都会收到outlier的影响，其敏感程度而言，谁更好很难下明确结论。


## 2两种模型使用选择
- 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
- 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
- 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况




