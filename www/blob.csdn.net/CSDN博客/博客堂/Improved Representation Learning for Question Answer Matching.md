# Improved Representation Learning for Question Answer Matching - 博客堂 - CSDN博客





2018年04月11日 11:11:43[最小森林](https://me.csdn.net/u012052268)阅读数：387








# Improved Representation Learning for Question Answer Matching

## 1 引言：

Passage-level答案选择是典型问答（QA）系统中的重要组成部分之一。

它需要有效的表示来捕捉问题和答案之间复杂的语义关系。其中： 答案文本选择步骤也被称为文本评分（passage scoring）。
|问题|Medicare是否覆盖我的配偶？|
|----|----|
|Ground-truth答案：|如果你的配偶已经工作并支付全部所需的40个季度的医疗保险税，或者由于残疾或其他原因有资格享受Medicare，你的配偶可以获得他/她自己的医疗保险福利。 如果你的配偶没有达到这些资格，而你达到了他们，并且如果你的配偶是65岁，他/她可以根据你的资格获得Medicare。|
|另一个候选答案：|如果您已与Medicare合格配偶结婚至少10年，您可能有资格享受Medicare。 如果你是丧偶的，并且没有再婚，并且在配偶去世前至少9个月你已经与你的配偶结婚，那么你可能有资格享受配偶条款下的Medicare福利。|

这项工作的核心挑战之一在于问题与答案之间复杂而多样的语义关系“
- 一个好的答案可能会以不同的形式出现：有时候一个正确的答案是用问题缺失的信息来完成。例如，表1中的问题仅包含五个单词，而最佳答案用60个单词进行阐述。
- 另外，虽然答案必须与问题相关，但它们通常不会共用通用的词汇单元。例如，在示例问题中，答案中没有直接提到“cover”。这个问题可能会混淆简单的单词匹配系统。

## 2相关工作：

与基于深度学习的方法相比，这些挑战因此使得手工特征不理想。此外，还要求系统能够捕捉最佳答案和可接受答案之间的细微差别。

例如，表1中的第二个答案适用于提问者，其配偶符合Medicare资格，询问他/她自己的保险，而示例问题更可能由符合Medicare资格的人员询问，询问他/她的配偶覆盖面。

虽然这项任务通常被视为一对一排序的问题，但捕捉问题与答案之间关联的最佳策略仍然是一个悬而未决的问题。目前已确立的方法通常存在两个弱点：
- 首先，诸如（Feng等人2015）之前的工作分别采用卷积神经网络（CNN）或递归神经网络（RNN）。但是，每个结构只描述文本的一个语义视角。CNN强调n-gram内的本地交互，而RNN旨在捕获长距离信息并且忽略不重要的本地信息。如何结合两者的优点还没有得到充分的探索。
- 其次，以前的方法通常基于独立生成的问答词向量; 这种表示的质量通常随着答案序列变长而降低。

多以相对应，本文提出解决办法如下：
- 我们提出了混合神经网络，通过结合RNN和CNN的优点，可以更好地表达问题和答案。
- 我们证明了注意力机制对答案选择任务的有效性，这在以前的工作中还没有得到充分的探讨。

## 3 框架

### 3.1 基础的QA-LSTM:

![](http://oqy7bjehk.bkt.clouddn.com/18-4-11/80044377.jpg)

我们的基本答案选择框架如图1所示。
- 给定输入对(q,a)， q是一个问题，a是候选答案，首先我们检索q和a的词向量，然后分别在两个WEs序列上应用一个biLSTM。
- 接下来,我们使用对所有输出向量进行最大汇聚maxpooling。生成一个固定大小的分布式矢量表示
- 最后，我们使用余弦相似的sim(q,a)来评分输入(q,a)对。

将目标函数定义为hinge loss：

```
L = max{0,M−sim(q,a+ )+sim(q,a− )}
```

a +是真实的答案，a- 是一个从整个答案空间中随机抽取的错误答案，M是一个margin。在训练期间，对于每个问题，我们随机抽取K个否定答案，但只使用最高的L来更新。

### 3.2 改进1：结合了cnn与lstm：

在QA-LSTM在处理长应答序列时，使用的pooling策略在过滤重要本地信息的能力低下。从根本上说，由于CNN与LSTM的拓扑结构不同，RNN和卷积神经网络有各自的优点和缺点。我们研究保持两者的优点，提出以下两种改进方法：

#### 1. Convolutional-pooling LSTMs

![](http://oqy7bjehk.bkt.clouddn.com/18-4-11/68575498.jpg)

通过对LSTM输出向量的序列应用卷积来捕获更丰富的本地信息。

#### 2. Convolution-based LSTMs

![](http://oqy7bjehk.bkt.clouddn.com/18-4-11/78822144.jpg)

这种方法的目的是利用卷积在较低的层次捕获本地的n-gram交互。在更高的层次上，构建了双向LSTMs基于复杂的n-gram提取了长期的依赖关系。

### 3.3 改进2：加入注意力机制：

QA-LSTM和两种混合模型，这些结构忽略了另一个潜在的问题。答案可能是非常长的，并且包含许多与当前问题无关的单词。

不管在回答的问题上使用什么先进的神经网络，产生的表示可能仍然会被无用的信息所干扰。

例子是表1中的第二个候选答案。如果答案表示的构建没有注意到输入问题，那么答案表示可能会受到n-gram的强烈影响：比如“你的配偶的死亡。。保险”，如果我们只看候选人的答案，这是有意义的，但是对于输入问题来说并不是那么重要。

我们通过开发一个简单的注意力模型，通过动态地调整问题的答案，从而缓解这个问题。

![](http://oqy7bjehk.bkt.clouddn.com/18-4-11/22263978.jpg)

让每个答案的biLSTM输出向量，乘以一个softmax权重。（权重由biLSTM的问题表示决定）

从概念上讲，注意机制更重视候选答案的某些单词，其中权重是通过考虑来自问题的信息来计算的。预期的是，在输入问题上，候选人回答中更重要的单词应该得到更大的权重。

## 4 实验
- 使用word2vec：“嵌入”的训练数据是维基百科.单词向量的大小设为100
- 随机梯度下降法(SGD)是一种优化策略。学习速率的λ是1.1
- 我们在hinge loss损失函数中也尝试了不同的边距，最终确定了M=0.2的边距
- 问题和答案的最大长度L是200。此范围外的任何分词都将被丢弃
- 我们以小批量的方式训练我们的模型(批量大小为20)
- LSTM输出向量的维数是141 x2 ，CNN的输出固定为282维

最后模型精度：68%



