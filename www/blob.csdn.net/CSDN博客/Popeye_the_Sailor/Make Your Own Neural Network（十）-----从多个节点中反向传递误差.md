# Make Your Own Neural Network（十）-----从多个节点中反向传递误差 - Popeye_the_Sailor - CSDN博客
2018年05月02日 21:52:38[_Sailor_](https://me.csdn.net/lz0499)阅读数：309

**Make Your Own Neural Network**
**构建你自己的神经网络**
[https://blog.csdn.net/lz0499](https://blog.csdn.net/lz0499)
**作者：lz0499**
**声明：**
1）**Make Your Own Neural Network**翻译自[Tariq Rashid](https://book.douban.com/search/Tariq%20Rashid)编写的神经网络入门书籍。作者的目的是尽可能的少用术语和高深的数学知识，以图文并茂的方式讲解神经网络是如何工作的。任何拥有高中数学水平的人就能够理解神经网络的工作方式。强烈推荐初学者以这本书作为神经网络入门书籍。
2）本文仅供学术交流，非商用。翻译的初衷是一边翻译一边加深对神经网络的理解。
3）由于刚刚接触神经网络这方面的知识，翻译过程中难免有些错误。若发现错误，还请各位前辈指正。谢谢！
4）由于工作原因，我将有选择的按照原文的章节不定期的进行翻译更新。
5）此属于第一版本，若有错误，还需继续修正与增删。
目录：
第一部分：神经网络是如何工作的
[一种简单的预测机](https://mp.csdn.net/postedit/80069089)
[分类即是预测](https://blog.csdn.net/lz0499/article/details/80072948)
[训练一个简单的分类器](https://blog.csdn.net/lz0499/article/details/80086402)
[单个分类器似乎远不够](https://blog.csdn.net/lz0499/article/details/80099968)
[神经元，自然界的计算机](https://blog.csdn.net/lz0499/article/details/80138584)
[通过神经网络的信号](https://blog.csdn.net/lz0499/article/details/80138955)
[矩阵很有用](https://blog.csdn.net/lz0499/article/details/80160354)
[利用矩阵计算三层神经网络的输出结果](https://blog.csdn.net/lz0499/article/details/80160449)
[从多个节点更新权重](https://blog.csdn.net/lz0499/article/details/80172534)
[从多个节点中反向传递误差](https://blog.csdn.net/lz0499/article/details/80172568)
[多层神经网络层反向传输误差](https://blog.csdn.net/lz0499/article/details/80185692)
[利用矩阵乘法计算反向传输误差](https://blog.csdn.net/lz0499/article/details/80185923)
[实际上是如何更新权重（一）](https://blog.csdn.net/lz0499/article/details/80209928)
[实际上是如何更新权重（二）](https://blog.csdn.net/lz0499/article/details/80210590)
[权重更新实例](https://blog.csdn.net/lz0499/article/details/80212695)
**从多个节点中反向传递误差**
下图表示的是一个由两个输入节点和两个输出节点组成神经网络。
![](https://img-blog.csdn.net/20180502214532140)
每一个输出节点都有一个误差，实际上当我们没有对神经网络进行训练的时候，输出节点肯定是会有误差的。你可以看到每一个误差都被反馈到神经网络中的权重，用于对权重值的更新。我们可以使用上一小节中的方法，我们把误差按照其贡献节点的权重比例进行划分。
事实上，当神经网络不止两个输出节点的时候，其权重更新的计算方法依旧是一样的。我们只需要验证第一个输出节点的计算方法，更新后续节点的误差对其贡献节点的权重。为什么如此简单呢？因为，在神经网络中每一个输出节点的贡献节点是相互独立的，也即是说，某一个节点的贡献节点并不依赖与其他输出节点的贡献节点。
仔细看上述图，我们把第一个输出节点的误差标记位e1。记住这是实际输出结果o1和我们期望输出的训练值t1之间的差值，即e1=t1-o1。第二个输出节点的误差我们标记为e2。
从图中我们可以观察到，误差e1按照其相连贡献节点的权重比例进行划分，其贡献节点的权重值分别为w11,w21；同理，对于误差e2将按照权重w12,w22进行划分。
让我们写出误差分割值大小，这样我们就不会出现误解。误差e1用来更新权重。所以用更新权重w11所使用的误差比例应该为：
![](https://img-blog.csdn.net/20180502214633929)
同样的，e1用更新权重w21的比例为：
![](https://img-blog.csdn.net/20180502214703163)
初看起来，这个比例有点令人疑惑。让我们实际计算其大小。
如果w11比w21大两倍，比如，w11=6,w21=3因此，用于更新w11的比例为6/6+3=2/3,而用于更新w21的比例为1/3。我们可以实际计算下究竟用于更新w21的比例是多少？3/6+3=1/3,确实是1/3的比例进行更新。
如果贡献节点的权重相等，那么利用误差更新相应权重的比例应该也是相等的。假设，w11=w21=4,那么权贡献节点的权重都将以4/4+4=1/2的比例进行更新。
在继续下一步计算之前，让我们停下来思考一下，看看我们具体做了些什么。我们知道我们需要利用误差更新神经网络中的某些参数，比如说相连节点之间的权重值。我们已经知道如何利用神经网络的输出结果更新节点之间的权重值。我们同样知道，当神经网络中不止一个输出节点的时候，是如何利用输出节点更新贡献节点之间的权重值：我们只是用同样的方式分别计算每一个输出节点如何更新其贡献节点之间的权重值。很好！
当神经网络中不止两层的时候，将会发生什么呢？我们该如何从输出节点更新其贡献节点之间的权重值呢？

