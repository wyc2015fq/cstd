# Make Your Own Neural Network（十四）-----实际上是如何更新权重（二） - Popeye_the_Sailor - CSDN博客
2018年05月06日 11:07:26[_Sailor_](https://me.csdn.net/lz0499)阅读数：5480

**Make Your Own Neural Network**
**构建你自己的神经网络**
[https://blog.csdn.net/lz0499](https://blog.csdn.net/lz0499)
**作者：lz0499**
**声明：**
1）**Make Your Own Neural Network**翻译自[Tariq Rashid](https://book.douban.com/search/Tariq%20Rashid)编写的神经网络入门书籍。作者的目的是尽可能的少用术语和高深的数学知识，以图文并茂的方式讲解神经网络是如何工作的。任何拥有高中数学水平的人就能够理解神经网络的工作方式。强烈推荐初学者以这本书作为神经网络入门书籍。
2）本文仅供学术交流，非商用。翻译的初衷是一边翻译一边加深对神经网络的理解。
3）由于刚刚接触神经网络这方面的知识，翻译过程中难免有些错误。若发现错误，还请各位前辈指正。谢谢！
4）由于工作原因，我将有选择的按照原文的章节不定期的进行翻译更新。
5）此属于第一版本，若有错误，还需继续修正与增删。
目录：
第一部分：神经网络是如何工作的
[一种简单的预测机](https://mp.csdn.net/postedit/80069089)
[分类即是预测](https://blog.csdn.net/lz0499/article/details/80072948)
[训练一个简单的分类器](https://blog.csdn.net/lz0499/article/details/80086402)
[单个分类器似乎远不够](https://blog.csdn.net/lz0499/article/details/80099968)
[神经元，自然界的计算机](https://blog.csdn.net/lz0499/article/details/80138584)
[通过神经网络的信号](https://blog.csdn.net/lz0499/article/details/80138955)
[矩阵很有用](https://blog.csdn.net/lz0499/article/details/80160354)
[利用矩阵计算三层神经网络的输出结果](https://blog.csdn.net/lz0499/article/details/80160449)
[从多个节点更新权重](https://blog.csdn.net/lz0499/article/details/80172534)
[从多个节点中反向传递误差](https://blog.csdn.net/lz0499/article/details/80172568)
[多层神经网络层反向传输误差](https://blog.csdn.net/lz0499/article/details/80185692)
[利用矩阵乘法计算反向传输误差](https://blog.csdn.net/lz0499/article/details/80185923)
[实际上是如何更新权重（一）](https://blog.csdn.net/lz0499/article/details/80209928)
[实际上是如何更新权重（二）](https://blog.csdn.net/lz0499/article/details/80210590)
[权重更新实例](https://blog.csdn.net/lz0499/article/details/80212695)
**实际上是如何更新权重（二）**
神经网络的输出是一个包含非常多权重参数的复杂函数。我们能够使用梯度下降法求得其权重吗？只要我们能够使用正确的误差函数就可以求得出神经网络中权重的参数值。
神经网络的输出本身并不是一个误差函数。但是我们能够轻松的将它转换为误差函数，因为误差是训练样本目标值与神经网络输出值之间的差值。
如下表所示。下表标注了三个输出节点的训练值和实际的输出值，以及它们之间的误差值。
![](https://img-blog.csdn.net/20180506105701491)
第一个候选误差函数非常简单，即目标值-实际输出值。这看起来很合理不是？如果我们以所有节点的误差和这个角度来度量神经网络训练的是否足够好，你将观察到其误差和为0！！
怎么回事？很明显，神经网络并没有被很好的训练，因为两个输出节点的值与目标值不一样，并且误差和为0意味着没有误差！！由于正负误差刚刚抵消了所以出现了误差和为0。即使误差没有完全的抵消，我们也可以观察到这是一种比较差的误差测量法。
让我们用实际输出值和目标值之间的绝对值来测量误差。这意味着我们将忽略符号，并即为|目标值-实际值|。最后误差求和时并没有完全的抵消。但是实际我们并不采用这种方法测量误差，因为在梯度下降法中靠近最小值的时候，其斜率并不是连续的，导致梯度下降法并不能很好的发挥作用。
第三种误差测量函数使用的是(目标值-实际值)的平方。我们采用这种函数测量误差主要是基于如下几点：
1.梯度下降法中求解斜率的时候比较容易。
2.这个误差函数时平滑而连续的，能够很好的发挥梯度下降法的优势
3.当靠近最小值的时候，梯度值越来越小。这意味着我们将使用平滑的步长靠近最小值，从而避免越过最小值的风险。
下面小节中我们将采用第三种误差测量函数。有人会问，有没有第四种误差测量函数呢？是的，当然有。你可以构建其他误差测量函数。有些不能很好的适用，有些只是针对特定的问题适用。
好了！在开始计算自变量是权重的误差函数之前，我们有些预备知识需要了解---微积分。你可能已经很熟悉微积分了，如果你不是很熟悉或者只是大概了解一些，本书的附录将包含一些这方面的介绍。微积分在数学上描述的是当一个变量变化时，另外一个变量将做如何变化。比如说，弹簧的长度改变时，弹簧的弹力将如何改变。对于神经网络而言，我们关心的是误差函数将随着权重的改变做何种改变。
让我们从一幅图画开始。
![](https://img-blog.csdn.net/20180506105910576)
就像上一小节那样，图例中我们并没有做过多的修改。但是这次我们想获得的最小值指的是神经网络中误差的最小值。我们需要修改的参数是神经网络中的斜率。在这简单的例子中，我们只是标示了一个权重值，但是你要知道的是神经网络中将会有很多的权重值。
下图表示的是两个权重值，并且误差函数中包含两个权重参数。我们试图求得最小误差值的过程有点像之前复杂地形中找到最低山谷的过程。
![](https://img-blog.csdn.net/20180506105946268)
对于有很多参数得误差函数来说，我们很难直观的用可视化的方式呈现。但是我们在这个多维平面中找最小值所使用的梯度下降法和之前是一样的。
让我们用数学公式写出下面的误差函数的相关梯度结果：
![](https://img-blog.csdn.net/20180506110004946)
这个偏导数表示的是当权重Wjk变化时，误差E将怎样变化。这个偏导数表示的是误差函数靠近最小值的斜率。
在我们详细解释这个表达式之前，让我们专注于隐藏层和最后输出层之间一个节点的情况。如下图高亮度标注。我们将会回到这张图，阐述输出层与隐藏层之间权重的计算方法。
![](https://img-blog.csdn.net/20180506110056546)
在后续计算微积分的时候，我们将会提到上面这幅图，确保我们不会忘记每一个符号所代表的含义。别太紧张，后续计算步骤并不难并且会一一解释。所有的概念在之前的小节中已经阐述过了。
首先，让我们扩展下神经网络的误差函数：神经网络输出层n个输出节点的目标值和实际值之间差值的平方和，即
![](https://img-blog.csdn.net/20180506110125631)
现在我们做的只是把实际的误差函数写出来。
我们可以简化上述公式，第n个输出节点的输出结果为On,只取决于前向神经网络层与它相连的节点。这意味着对于节点K，其输出结果Ok只取决于权重Wjk。因为只有这些节点与节点k相连。
从另外一个角度来看，节点k的输出结果并不取决于权重wjb,b不等于k。因为节点k与前向节点b之间并无连接。权重Wjb表示的是与输出节点b相连的权重而不是与输出节点k相连的权重。
这意味着我们可以把上述公式中n个输出结果中只与输出节点k的输出结果，因为ok只取决于Wjk。
至此，你可能意识到对于某一个节点来说，并不需要计算所有节点的误差平方和。因为输出节点的输出误差只取决于与之相连节点的权重值。
总之，我们有了一个更为简单的表达式：
![](https://img-blog.csdn.net/20180506110228768)
现在我们可以开始微积分计算了。
tk是一个常量，它不会随着Wjk的变化而变化。即tk并不是Wjk的函数。公式中的Ok这部分取决于权重wjk。我们将用到微积分的链式规则把上述公式中分几步进行微积分运算。即：
![](https://img-blog.csdn.net/2018050611032259)
现在我们可以分别计算每一个部分。第一部分是对一个简单的二次方函数求导。即：
![](https://img-blog.csdn.net/20180506110354211)
第二部分需要稍微思考下，但是也并不难。如果你还记得前面小节的话，节点k的输出结果Ok是与前一层相连节点输出结果加权和应用于sigmoid激活函数之后的输出结果。让我们写下如下公式：
![](https://img-blog.csdn.net/20180506110438398)
其中Oj是隐藏层j节点的输出结果，而不是最后一层的输出结果Ok。
我们该如何对sigmoid函数求导呢？用链式法则和e的指数求导法则，可得到如下公式
![](https://img-blog.csdn.net/20180506110506542)
有些函数求导之后，求导结果会非常复杂。但是sigmoid激活函数却是相对简单的一个结果。这也是为什么sigmoid函数在神经网络中大量使用的原因。
让我们继续计算最后的结果：
![](https://img-blog.csdn.net/20180506110530726)
这是一个神奇的表达式。我们将使用这个表达式对神经网络进行训练。
我们计算出了求斜率的关键表达式。我们可以利用这个表达式对每一个训练样本进行训练，之后根据训练样本进行更新权重。
记住权重是按照梯度的反方向进行改变。对一些特定问题，我们为了平滑改变权重将使用学习率。使用学习率避免在最小值附近的时候，超过最小值，来回震荡。我们用数学表示为：
![](https://img-blog.csdn.net/20180506110558500)
需要更新的权重值Wjk是旧的权重值减去误差函数斜率值。之所以减去误差函数的梯度值是因为当斜率是正值的时候，我们想减小权重；当斜率是负值的时候我们想增加权重。符号æ是保证权重值平滑变化的一个因子，避免在靠近最小值的时候越过最小值。这个因子常常被称为学习率。
这个表达式也是可以应用于输出层和隐藏层，而不仅仅限于隐藏层和输出层之间。在我们计算微积分之前，我们试着用矩阵的形式表示上述表达式：
![](https://img-blog.csdn.net/20180506110625598)
矩阵权重改变的值包括：某一层的节点j与下一层的节点k之间的权重。我们可以观察到表达式的第一部分使用的是下一层的值，而表达式最后一部分使用的是前一层的输出值。
上式中最后一部分是一个行向量，也即是上一层输出结果的转置。因此我们可以使用下面公式表示最终的结果：
![](https://img-blog.csdn.net/20180506110649703)
这个表达式其实不是很复杂。那些sigmoid激活函数消失了，因为我们用节点的输出Ok替代了它。


