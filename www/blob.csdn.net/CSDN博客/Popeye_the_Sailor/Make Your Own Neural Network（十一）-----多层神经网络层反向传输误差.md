# Make Your Own Neural Network（十一）-----多层神经网络层反向传输误差 - Popeye_the_Sailor - CSDN博客
2018年05月03日 20:20:26[_Sailor_](https://me.csdn.net/lz0499)阅读数：291
**Make Your Own Neural Network**
**构建你自己的神经网络**
[https://blog.csdn.net/lz0499](https://blog.csdn.net/lz0499)
**作者：lz0499**
**声明：**
1）**Make Your Own Neural Network**翻译自[Tariq Rashid](https://book.douban.com/search/Tariq%20Rashid)编写的神经网络入门书籍。作者的目的是尽可能的少用术语和高深的数学知识，以图文并茂的方式讲解神经网络是如何工作的。任何拥有高中数学水平的人就能够理解神经网络的工作方式。强烈推荐初学者以这本书作为神经网络入门书籍。
2）本文仅供学术交流，非商用。翻译的初衷是一边翻译一边加深对神经网络的理解。
3）由于刚刚接触神经网络这方面的知识，翻译过程中难免有些错误。若发现错误，还请各位前辈指正。谢谢！
4）由于工作原因，我将有选择的按照原文的章节不定期的进行翻译更新。
5）此属于第一版本，若有错误，还需继续修正与增删。
目录：
第一部分：神经网络是如何工作的
[一种简单的预测机](https://mp.csdn.net/postedit/80069089)
[分类即是预测](https://blog.csdn.net/lz0499/article/details/80072948)
[训练一个简单的分类器](https://blog.csdn.net/lz0499/article/details/80086402)
[单个分类器似乎远不够](https://blog.csdn.net/lz0499/article/details/80099968)
[神经元，自然界的计算机](https://blog.csdn.net/lz0499/article/details/80138584)
[通过神经网络的信号](https://blog.csdn.net/lz0499/article/details/80138955)
[矩阵很有用](https://blog.csdn.net/lz0499/article/details/80160354)
[利用矩阵计算三层神经网络的输出结果](https://blog.csdn.net/lz0499/article/details/80160449)
[从多个节点更新权重](https://blog.csdn.net/lz0499/article/details/80172534)
[从多个节点中反向传递误差](https://blog.csdn.net/lz0499/article/details/80172568)
[多层神经网络层反向传输误差](https://blog.csdn.net/lz0499/article/details/80185692)
[利用矩阵乘法计算反向传输误差](https://blog.csdn.net/lz0499/article/details/80185923)
[实际上是如何更新权重（一）](https://blog.csdn.net/lz0499/article/details/80209928)
[实际上是如何更新权重（二）](https://blog.csdn.net/lz0499/article/details/80210590)
[权重更新实例](https://blog.csdn.net/lz0499/article/details/80212695)
**多层神经网络层反向传输误差**
下图所示的是包含一个输出层、一个隐藏层和一个输出层的简单的神经网络。
![](https://img-blog.csdn.net/20180503201117111)
从上图最右侧的输出层开始，我们利用输出层的误差修正与输出层相连的隐藏层节点的权重值。我们把输出层的误差记为eout，并把与之相连的隐藏层节点对应的权重记为who。我们把输出层节点的误差按照与之相连隐藏层节点的权重比例分割。
如下图所示，我们按照同样的方法，对上述的神经网络的输入层进行相应的处理。
![](https://img-blog.csdn.net/20180503201240340)
我们简单的把隐藏层节点的输出误差按照与之相连的输入节点之间的权重比例进行划分。
如果神经网络中有更多神经网络层，我们只需按照这种方法，重复进行相应的处理即可。从这种方法的计算过程中，我们更加能够理解为什么此方法称之为反向传输法。
如果输出层节点的输出误差为eout，那么，对于隐藏层的而言，其输出误差ehidden应该如何计算呢？
这个问题问得好，对于神经网络隐藏层的误差值，我们并不能立即从上图中知晓其数值。但是，我们知道对于隐藏层确实将会有一个输出信号，我们还记得是把输入信号的加权和应用于sigmoid激活函数进行相应的处理之后输出其输出信号，但是，我们该如何计算隐藏层的误差呢？
对于隐藏层而言，我们并没有一个目标或者期望输出值。我们只有对输出层有一个输出目标值，这些目标值来源于训练样本数据。让我们再看看上述图例，隐藏层的第一个节点分别与输出层的每个节点相连，我们可以把输出层的每个节点的输出误差按照权重比例进行划分，就像我们之前做的一样。这意味着上述图例中隐藏层的每个节点都有来自两个输出节点的误差，我们可以把这两个来自输出层不同节点的误差相加，作为间接求得隐藏层的输出误差值，而不再纠结与隐藏层输出结果与其预期值之间的差值。通过这种间接的方法求得每个隐藏层的输出误差值。其求解过程如下图所示：
![](https://img-blog.csdn.net/20180503201410275)
你可以从上图中明确的看出其计算过程，但是我们还是按照之前的方法实际计算看看。我们需要一个隐藏层节点的误差以方便我们更新输入节点与隐藏层节点相连的权重值。我们标记隐藏层节点的误差为ehidden，但是我们还不知道其确切值。我们不能说这个误差值是隐藏层输出结果与其目标值之间的差值，因为，只是对神经网络中最后的输出层才有目标值。
训练样本值只是告诉我们神经网络输出层每个节点的输出应该是多少。而并没有告诉我们神经网络中其他神经网络层各个节点的应该输出多少。这是个令人困惑的问题。我们究竟需不需要知道每一个层的输出目标值呢？
实际上，我们并不需要知道其他神经网络层的输出结果。我们可以使用输出层每个节点的误差反向传输至与之相连的隐藏节点，就如我们之前描述的那样。所以，对于隐藏层而言，我们可以将输出层每个节点误差按照与之相连隐藏层节点的权重比例划分，并把每个输出节点划分给隐藏层第一个节点的误差相加，即可得到隐藏层第一个节点的最终误差值。如上图所示，我们从第一个输出节点获得的误差为eout,1，从第二个输出节点获得的误差为eout,2。
让我们写下其计算公式：
![](https://img-blog.csdn.net/20180503201359502)
下图展示了输出误差反向传输至一个简单三层神经网络的计算过程.
![](https://img-blog.csdn.net/20180503201925616)
让我们计算其中的一个误差值。我们可以看到输出层中第二个节点的误差0.5按照与之相连节点的权重（权重值为1.0和4.0）的比例被划分为0.1和0.4。隐藏层的第二个节点的误差值最终是输出层第一个节点划分的误差值与输出层第二个节点划分的误差值的和，即0.4+0.48=0.88。
下图表示的是隐藏层到输入层之间的误差反向传输过程。
![](https://img-blog.csdn.net/20180503201952100)

