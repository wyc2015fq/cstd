# 《机器学习》周志华-CH2 模型评估与选择 - PeterBishop - CSDN博客





2018年12月21日 23:52:54[PeterBishop0](https://me.csdn.net/qq_40061421)阅读数：20
个人分类：[机器学习](https://blog.csdn.net/qq_40061421/article/category/8552662)









### 写在前面



CH1讲述了机器学习领域的基本概念，包括算法、模型、数据集等重要的专有名词。在算法的帮助下，能从数据集中提取了模型；对于同一数据集而言，给定不同的算法，会提取不同的模型，甚至给定同一算法的不同参数，也能得到不同的模型。那么，在这诸多模型当中，如何选择最佳的模型呢？

这个过程，称之为模型选择。模型选择，会遵循一定的标准：首先，要将数据集分为若干部分，其中一部分用于训练模型，另一部分用于测试模型的泛化能力；然后，对于测试的结果，我们给定一个性能度量参数，从数值上比较不同模型的泛化能力；再然后，通过复杂的比较检验方法，对不同的模型性能进行比较；最后，对于模型的性能，尝试给出一个解释。

以上就是CH2的内容。主要涉及的要点，还是机器学习领域的通识性知识；内容较多，比较晦涩，和人的常识略有不同。；例如：比较不同模型的泛化能力，往往不能直接比较错误率/精度的大小，要通过复杂的比较检验来进行。需要反复查阅、体会。

### 2.1 经验误差与过拟合



先来明确几个概念。 

错误率error rate：分类错误的样本数占总数的比例； 

精度accuracy：分类正确的样本数占总数的比例，显然，。错误率+精度=1； 

误差error：学习器的实际预测输出与样本的真实输出之间的差异； 

训练误差training error/经验误差empirical error：学习器在训练集上的误差； 

泛化误差generalization error：学习器在新样本上的误差；

一般来说，即使得到了一个这样的模型：对于所有训练数据集，其精度为100%。这样的学习器在多数情况下的表现都不好。

过拟合overfitting：学习器把训练样本学得“太好”，把训练样本自身的一些特点，当成了所有潜在样本都会具有的一般性质，导致泛化能力下降； 

欠拟合underfitting：对于训练样本的一般性质都没有学习好。

欠拟合易于克服；过拟合难以克服，且无法彻底避免。

### 2.2 评估方法



在使用不同的算法（例如十大经典算法），可以从数据集得到不同的模型。从这些模型中，选择最恰当的一个，就是机器学习领域的“模型选择”问题。通常，用户会评估候选模型的泛化误差，然后选择泛化误差最小的那一个模型。

因此，除了给定一个训练集用于学习模型，还要给定一个测试集用于测试学习器。一般来说，通过对数据集采用一定的划分方法，可以将之分为训练集和测试集。对于深度学习而言，还有一个验证集。

留出法。

交叉验证法。

自助法。

### 2.3 性能度量



上面讲到了，用测试集的方法，来评估一个模型的泛化能力；而衡量泛化能力的标准，就是性能度量performance measure。

不同的性能度量标准，会带来不同的评判结果。因此，什么样的模型是好的，不仅取决于算法和模型，也要取决于任务需求。

错误率与精度，是分类任务中最常见的两种性能度量。

查准率，查全率，F1，PR曲线。

阈值，ROC曲线，AOC曲线。




