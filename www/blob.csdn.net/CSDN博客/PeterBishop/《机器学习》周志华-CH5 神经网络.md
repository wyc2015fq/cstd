# 《机器学习》周志华-CH5 神经网络 - PeterBishop - CSDN博客





2019年01月11日 16:42:14[PeterBishop0](https://me.csdn.net/qq_40061421)阅读数：40
个人分类：[机器学习](https://blog.csdn.net/qq_40061421/article/category/8552662)









神经网络

1、生物学上的神经网络是是目前发现的神经元细胞间相互组合链接组成一个互连的网络，通过对外界的刺激，神经元产生发送化学物质进而内部发生电位变化，下图是生物神经元结构。

![](https://img-blog.csdn.net/20170823111214569?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGluZ3lhaHVpMTIz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

        生物神经元的具体工作我们不需要了解，只需要知道互连神精元接受刺激产生物质通过比较阈值最终是兴奋还是抑制。在算法结构上，想要的就是他的这种网络结构模型。Hebb认为神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。那么生物神经网络的特点有哪些昵？1） 每个神经元都是一个多输入单输出的信息处理单元； 2） 神经元输入分兴奋性输入和抑制性输入两种类型； 3）神经元具有空间整合特性和阈值特性； 4）神经元输入与输出间有固定的时滞，主要取决于突触延搁。

         2、从感知到神经网路：心理学家Warren Mcculloch和数理逻辑学家Walter Pitts提出并给出了人工神经网络的概念及人工神神经元的数学模型，从而开创了人类神经网络研究的时代。人类的好多发明都来源与仿生学，最初的人工神经网络模型可以说感知机模型，简单的模仿神经元构造，其逻辑结构如下图。与书中的M-P模型大致相似，差别在于这里的激活函数输出结果兴奋对于1，抑制对应于-1（书中为0），物理模型仿照与生物神经元的反应是通过阶跃函数来实现，理想的阶跃函数的结果只有1和0对应于兴奋与抑制。理想的函数到了数学理论模型上经常不选用，不具有连续和不光滑的性质（不具有这些性质个人认为是不利于参数的训练，容量空间也小，结果只能1和0，数据之间的关系难以表达）。

![](https://img-blog.csdn.net/20170823112645403?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGluZ3lhaHVpMTIz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

感知机的数学模型为![](https://img-blog.csdn.net/20170823135059139?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGluZ3lhaHVpMTIz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)。单层感知机最主要的用途就是线性分类，分类边界是y=0，类似与用一条直线划分0和类似与用一条直线划分0和1，其实就用一个超平面划分两个类别单层两个类别，他的目的就是计算出恰当的权值系数（W1，W2....Wn），如在上图中的理论模型中，算法的步骤是我们对训练样本输入，在初始化参数W矩阵，通过期望输出与实际输出的误差，调节权值W矩阵，最终结果使得W矩阵对于训练样本的误差最小，这样在相似的输入情况下，其输出结果也会相似根据一定判定准则就可以对新输入的测试数据分类。这里的权值更新准则为。

![](https://img-blog.csdnimg.cn/20190111164503967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMDYxNDIx,size_16,color_FFFFFF,t_70)

多层感知机：多层感知机的出现是为了解决单层感知机的局限性，单层感知机的局限性在于他对线性问题的划分是可以的，线性问题的逻辑或、与、非，但是用于解决非线性问题像异或这种非线性可分的问题，他无法用一条直线来划分解决。既然单层解决不了那就多层，所谓的多层就是在单层的基础上多了隐含层结构。目的就是通过凹凸区域划分样本，从线划分平面到区域划分。多因层结构模型如下图所示


![](https://img-blog.csdn.net/20170823141933017?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGluZ3lhaHVpMTIz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

多因层结构增加了隐层神经元增加了参数数目，增加了模型的表达能力，其划分数据的能力如下图所示。

![](https://img-blog.csdn.net/20170823142109387?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGluZ3lhaHVpMTIz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

### BP算法

随着隐层数目的增加，凸区域的形状也变得任意，就可以用来解决复杂的分类问题。虽然多层结构可以解决复杂的分类问题但是它也有自己的问题需要解决，那就是如何训练调节隐层权值，多隐层已不能再用单层结构的调节模式调节权值，这种瓶颈也是阻碍感知发展的一大难题，直到BP神经网络的出现才使得人工神经网络重新崛起。BP神经网络，非线性连续变换函数的多层感知器的反向传播算法，通过输出与实际误差间接调节隐层的权值，简单理解和但因层的调节思路类似，详细的BP调节及技巧请查看转载博客http://blog.csdn.net/dingyahui123/article/details/54098174

BP算法是由正向传播和误差反向传播两部分组成。1）正向传播样本从输入层传入,经各隐层逐层处理后,传向输出层。若输出层的实际输出与期望的输出不符,则转入误差的反向传播阶段。2）反向传播将输出以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元，从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。

BP一般使用梯度下降的方式更新权值大小，基于梯度的方式容易出小梯度弥散，出现局部最优值而不是全局最优值。如下图一个局部最优与全局最优的事例，局部最优是在参数空间的某个点，其邻域点的误差函数值均不大于该点的值，全局最优点是参数空间所有点都不大于此点的值。全部最优一定是局部最优，反之不一定，我们想要找的点就是全局最优点。

![](https://img-blog.csdn.net/20170823145135458?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGluZ3lhaHVpMTIz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

### RBF网络：

径向基函数（Radical Basis Function，RBF）网络，一种单隐层前馈神经网络，使用径向基函数作为隐层的激活函数。http://blog.csdn.net/zouxy09/article/category/1291222

### ART网络：

（Adaptive Resonance Theory,自适应谐振理论)网络，竞争型学习的重要代表，竞争型学习是网络常用的无监督学习一种策略，网络的输出是神经元相互竞争，最后只有一个竞争获胜被激活其他为抑制，这种机制胜者通吃原则。ART网络有比较层、识别层、识别阈值和重置模块构成。比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元书目可以在训练过程中动态增加以增加新的模式。

### SOM网络：

自组织映射网络，一种竞争学习型的无监督神经网络，能将高维输入映射到低维空间，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层的临近神经元。

级联相关网络：一般的神经网络是固定好拓扑结构，然后训练权重和阈值。级联相关神经网络是从一个小网络开始，自动训练和添加隐含单元，最终形成一个多层的结构，他的重点在于级联和相关，此网络的优点在于学习速度快且自己决定神经元个数和深度，并且训练集变化之后还能保持原有的结构(这个是缺点还是优点)。最后他不需要后向传播错误信号。

### Elman网络：

一种典型的局部回归网络( global feed forward local recurrent)。Elman网络可以看作是一个具有局部记忆单元和局部反馈连接的递归神经网络。递归神经网络区别于前馈神经网络的是他允许网络中出现环形结构，从而可以使一些神经网络的输出反馈回来作为输入信号。

### Boltzmann机：

随机神经网络。http://blog.csdn.net/zouxy09/article/details/8781396/

###  深度学习：

神经网络之后的又一次突破。生物学上发现生物视觉是一个层级结构，是一个不断抽象迭代的过程。仿照这种结构构建多层次网络模型就形成了所谓的深度学习。深度学习的深度有一种理解是层次结构多的神经网络就是深度学习，但是多地多少层才算深度没有硬性规定，起码3层以上，他是相对于简单学习而言的，目前的多数分类、回归算法都属于简单学习或者浅层学习，包含1-2层隐层结构。典型的浅层结构有高斯混合模型(GMM)、隐马尔科夫模型(HMM)、条件随机域(CRF)、最大熵模型(MEM)、逻辑回归(LR)、支持向量机(SVM)和多层感知器(MLP)。（其中，最成功的分类模型是SVM，SVM使用一个浅层线性模式分离模型，当不同类别的数据向量在低维空间无法划分时，SVM会将它们通过核函数映射到高维空间中并寻找分类最优超平面）。浅层结构学习模型的相同点是采用一层简单结构将原始输入信号或特征转换到特定问题的特征空间中。浅层模型的局限性对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到一定的制约，比较难解决一些更加复杂的自然信号处理问题，例如人类语音和自然图像等。而深度学习可通过学习一种深层非线性网络结构，表征输入数据，实现复杂函数逼近，并展现了强大的从少数样本集中学习数据集本质特征的能力。在深度学习提出之初，多层次结构的深度学习权值调节使用BP算法，但是层次结构越多给BP的调节带来困难，梯度弥散和误差值随层数的增加带来的反向调节微弱影响使得反向调节对深度学习在深度的发展上带来了瓶颈。通过多层结构可以发现层次越多就可以处理越复杂的问题，从逻辑上将，深度架构的使用学习到了数据内部复杂的数据联系或者数据最本质的特征，良好的特征对于数据表达起到关键的作用。后来针对BP的调节问题，Hinton提出逐层初始化逐层训练的方法。将上层训练好的结果作为下层训练过程的初始化参数。典型的是Hinton提出的深度置信网络DBN，是由一系列的受限玻尔兹曼机（RBM）组成，提出非贪心逐层训练算法，取得了不错的效果。

深度学习是一列在信息处理阶段利用非监督特征学习和模型分析分类功能的，具有多层分层体系结构的机器学习技术。深度学习的本质是对观察数据进行分层特征表示，实现将低级特征进一步抽象成高级特征表示。目前深度学习可以分为3大结构。1）生成型深度结构：生成型深度结构旨在模式分析过程中描述观察到的课件数据的高阶相关属性，或者描述课件数据和其相关类别的联合概率分布。不关心数据的标签经常使用非监督特征学习。他的一个重要的任务就是预训练。但是当训练数据有限时学习较低层的网络是困难的。因此一般采用先学习每一个较低层，然后在学习较高层的方式，通过贪婪地逐层训练，实现从底向上分层学习。属于生成型深度结构的深度学习模型有：自编码器、受限玻尔兹曼机、深度置信网络等。2）判别型深度结构：判别型深度结构的目的是通过描述可见数据的类别的后验概率分布为模式分类提供辨别力。属于判别型深度结构的深度学习模型主要有卷积神经网络和深凸网络等。（3）混合型深度结构：混合型深度结构的目的是对数据进行判别，是一种包含了生成和判别两部分结构的模型。现有的生成型结构大多数都是用于对数据的判别，可以结合判别型模型在预训练阶段对网络的所有权值进行优化。如通过深度置信网络进行预训练后的深度神经网络。

深度学习应用：从结构上是多了许多隐层结构的人工神经网络，按照容量越大表示能力越强的趋势，会想到有足够的层次足够的参数就可以做任何事，但事实上做到万能是有一定距离的，先不说你可以用逐层训练来讲解决参数训练的问题。深度学习需要结合特定领域的先验知识，需要和其他模型结合才能得到最好的结果。此外，类似于神经网络，深度学习的另一局限性是可解释性不强，像个“黑箱子”一样不知为什么能取得好的效果，以及不知如何有针对性地去具体改进，而这有可能成为产品升级过程中的阻碍。但是因为深度学习特有的结构，可以逐层提取特征从而学习到本质更具有代表性的特征，所以在图像、语音、自然语言处理等方面远超于其他手工特征。特别是在图像识别领域，判别式的卷积神经网络的准确度远远高于其他算法。





