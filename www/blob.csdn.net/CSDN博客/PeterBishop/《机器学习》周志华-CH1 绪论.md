# 《机器学习》周志华-CH1 绪论 - PeterBishop - CSDN博客





2018年12月21日 23:48:47[PeterBishop0](https://me.csdn.net/qq_40061421)阅读数：15
个人分类：[机器学习](https://blog.csdn.net/qq_40061421/article/category/8552662)









### 1.1 引言



机器学习(machine learning)的定义：它致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。

在计算机系统中，“经验”通常以“数据”的形式存在。

ML研究的主要内容：在计算机上、从数据中产生“模型model”的算法。即是：如何通过数据集产生模型？因此机器学习本质上，研究的是算法；而这种算法的作用是，从数据集中产生模型；而模型的作用是，当面对新的数据时，模型会给我们提供一定的判断，即是数据预测。

模型，可以看做是：从数据集中学得的结果。

### 1.2 基本术语（极其重要）



本节讲述了ML领域诸多经典的基本术语，如果不明白这些术语的含义，那么ML的学习，将会寸步难行。下面，将这些入门术语都做个笔记，用浅显易懂的例子将它表述出来，从而加深自己的理解。

机器学习的根基，是数据，而且是大量的数据；通过将一系列的数据，提取它的规律，那么就能得到模型。注意，ML领域的“模型”，和三维建模的这个“模型”，是有本质上的区别的。后者是一种几何实体，而前者可以理解为一组方程。

本节的基本术语有：

数据集data set：机器学习的基础是数据，数据的集合；

示例instance/样本sample：每条数据描述了一个对象的信息，该对象称之为示例，一般用x表示；

属性attribute/特征feature：数据描述的是样本在某些方面的性质，称之为属性；

属性值attribute value：属性的取值；

属性空间attribute space/样本空间sample space/输入空间input space：对于一个样本而言，假如它有n种属性，则组成了一个n维空间，称之为样本空间；

特征向量feature vector：示例的别名；

学习learning/训练training：从数据集中学得模型的过程；

训练数据training data：学习过程中使用的数据；

训练样本training sample：训练数据中的样本；

训练集training set：数据集分为两部分，一部分用于训练模型；

假设hypothesis：学得的模型对应了数据集中某种潜在的规律，称之为假设；

真相/真实ground-truth：数据集本身的潜在的规律。学习的过程就是逼近真相的过程；

学习器learner：模型的别称；

标记label：有关示例结果的信息，一般用y表示；

样例example：具有标记信息的示例；

标记空间label space/输出空间：所有标记的集合构成的空间；

分类classification：一种典型的学习任务，将数据集按一定规律分为若干类；

回归regression：一种典型的学习任务，预测数据集对应的结果；

二分类binary classification：将数据集分为两类；

正类positive class：二分类任务其中的一类数据；

反类negative class：同上；

多分类multi-class classification：将数据集分为多类；

测试testing：学得模型后，对其进行预测的过程。机器学习是一个反复的过程，需要重复多次学习、测试、调整，才能得到准确率最高的模型；

测试样本testing sample：被预测的样本；

聚类clustering：无监督学习的一种，将训练集的数据分为若干组，而这些组事先是不知道的；

簇cluster：聚类得到的数据分类；

监督学习supervised learning：训练数据拥有标记信息；

无监督学习unsupervised learning：训练数据没有标记信息；

泛化generalization能力：学得模型适用于新样本的能力。或者说，模型预测数据的精准度；

独立同分布independent and identically distributed：简称i,i,d。假设样本是从一个很大的数据空间中，独立的从其内在分布上得到的；

大概20多个专有名词，一开始看的时候，不可能全部都理解的很透彻。因此，需要反复、多次的观看和理解。这些专有名词，是ML领域不可避免的重要内容。

### 1.3 假设空间



学习的目的是泛化，即通过训练，得到一个模型，而这个模型可以对新样例的标签进行精准的预测。

学习的过程，也可以看做，在所有假设组成的空间中，进行搜索的过程。假设，就是说该数据集对应的潜在规律；这个规律可能有很多种，学习的过程，就是找到最适合它的那一种。

### 1.4 归纳偏好



很多情况下，通过现有的有限的数据集，可以得到多个假设空间；但是我们必须得到一个最好的模型。这时候，就要从这若干个假设空间中，选择其中的一个，从这个空间中提取ML的模型。

尽管数据集无法从这若干个假设空间中选择最佳的那一个，但是我们可以使用另一个法宝：归纳偏好。机器学习算法在学习的过程中，对某种类型的假设的偏好，称之为归纳偏好。可以简单的理解为，对于上述不同的假设空间，在选择最优模型时，其权重不同。

对于归纳偏好，我们使用奥卡姆剃刀来作为一般的原则，用于引导算法确立“正确”的偏好。奥卡姆梯度是自然科学中最常见的法则之一：若有多个假设与观察一致，则选最简单的那个。




