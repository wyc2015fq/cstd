# （PCA与信息）方差和熵 - Machine Learning with Peppa - CSDN博客





2017年12月24日 21:50:09[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：401
所属专栏：[机器学习与数据挖掘](https://blog.csdn.net/column/details/18961.html)









# 方差和熵


最近在看主成分分析(PCA)时，在对数据进行压缩时，要求方差最大化，目的是保留数据的更多信息。根据信息论，“[信息熵](https://zh.wikipedia.org/wiki/%E7%86%B5_%28%E4%BF%A1%E6%81%AF%E8%AE%BA%29)”用于量化信息，那么这样看来方差和信息熵都可以用于量化信息，那它们有是什么不同呢？为什么它们可以量化信息呢？

> 

一条信息的信息量与其不确定性有着直接的诶关系。比如说，我们要搞清楚意见非常不确定的事，或者我们一无所知的事情，就需要了解大量的信息。所以，从这个角度来看，可以认为信息量就等于不确定性的多少(uncertainty)[1]



因此，方差和熵都是通过描述不确定性的多少来量化信息。

## 方差


在统计学和金融学上，大家通常用方差或者标准差用来描述不确定度(风险)，这很符合直观的解释：方差越大数据的波动也就越大，不确定性和风险当然也就越大。方差公式：




描述了输出值在平均值周围的偏差。方差描述不确定度在某些情况下会失效，因为它要求数据均匀分布并且忽略极端事件的发生。

## 熵


信息熵是信息论中概念，它是信息不确定性的度量，熵越大表示信息的不确定性越大，信息量越大：




可以发现公式中没有数据量级大小的表达，也就是说数据的大小不会直接影响熵的大小。熵的大小只是由样本数据概率大小决定。

## 方差和熵比较


先放结论：相比于方差，熵更适合描述信息的不确定度(废话，这就是熵的定义)，方差在某些前提下是可以描述信息的不确定性

下图是某股票数据的熵和对数标准差关系[2]：

![](http://images2015.cnblogs.com/blog/824175/201510/824175-20151018120752944-1286140214.png)


可以看出熵和有很强的正相关的关系。

对于常见的分布可以很容易推导出他们的熵和方差。[3]

|分布|熵|方差|
|----|----|----|
|伯努利|||
|二项分布|||
|均匀分布|||
|正态分布|||
|指数分布|||



可以从表中发现，在这些分布的情况下，方差和熵描述不去定性都是等价的。

但是当数据的分布有“多峰”（也可以理解为非凸）时方差描述信息不确定度的能力降低，这个时候应该用熵来描述不确定度，这种时候可能熵增大时方差减小。大概就是这样吧。恩！

![](http://images2015.cnblogs.com/blog/824175/201510/824175-20151018120811507-1157391029.png)

# 参考文献：


[1] 《数学之美》

[2] Andreia Dionisio, Entropy and Uncertainty Analysis in Financial Markets.

[3] Yuan Wei, Variance, Entropy and Uncertainty Measure.



