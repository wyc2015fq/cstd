# 距离，范数与相似度 - Machine Learning with Peppa - CSDN博客





2018年02月14日 20:04:04[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：645
所属专栏：[机器学习与数据挖掘](https://blog.csdn.net/column/details/18961.html)









在数据分析和数据挖掘的过程中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如K最近邻（KNN）和K均值（K-Means）。当然衡量个体差异的方法有很多，最近查阅了相关的资料，这里整理罗列下。

为了方便下面的解释和举例，先设定我们要比较X个体和Y个体间的差异，它们都包含了N个维的特征，即X=（x1， x2， x3， … xn），Y=（y1， y2， y3， … yn）。下面来看看主要可以用哪些方法来衡量两者的差异，主要分为距离度量和相似度度量。

**距离度量**

距离度量（Distance）用于衡量个体在空间上存在的距离，距离越远说明个体间的差异越大。

**欧几里得距离（Euclidean Distance）**

欧氏距离是最常见的距离度量，衡量的是多维空间中各个点之间的**绝对距离**。公式如下：

![Euclidean Distance](http://upload.chinaz.com/2011/1008/1318060559520.png)

因为计算是基于各维度特征的绝对数值，所以欧氏度量需要保证各维度指标在相同的刻度级别，比如对身高（cm）和体重（kg）两个单位不同的指标使用欧式距离可能使结果失效。

**明可夫斯基距离（Minkowski Distance）**

明氏距离是欧氏距离的推广，是对多个距离度量公式的概括性的表述。公式如下：

![Minkowski Distance](http://upload.chinaz.com/2011/1008/1318060559119.png)

这里的p值是一个变量，当p=2的时候就得到了上面的欧氏距离。

**曼哈顿距离（Manhattan Distance）**

曼哈顿距离来源于城市区块距离，是将多个维度上的距离进行求和后的结果，即当上面的明氏距离中p=1时得到的距离度量公式，如下：

![Manhattan Distance](http://upload.chinaz.com/2011/1008/1318060559389.png)

**切比雪夫距离（Chebyshev Distance）**

切比雪夫距离起源于国际象棋中国王的走法，我们知道国际象棋国王每次只能往周围的8格中走一步，那么如果要从棋盘中A格（x1， y1）走到B格（x2， y2）最少需要走几步？扩展到多维空间，其实切比雪夫距离就是当p趋向于无穷大时的明氏距离：

![Chebyshev Distance](http://upload.chinaz.com/2011/1008/1318060559321.png)

其实上面的曼哈顿距离、欧氏距离和切比雪夫距离都是明可夫斯基距离在特殊条件下的应用。

**马哈拉诺比斯距离（Mahalanobis Distance）**

既然欧几里得距离无法忽略指标度量的差异，所以在使用欧氏距离之前需要对底层指标进行数据的标准化，而基于各指标维度进行标准化后再使用欧氏距离就衍生出来另外一个距离度量——马哈拉诺比斯距离（Mahalanobis Distance），简称马氏距离。

**一范数：**

![= /sum_{i=1}^n /left| x_i - y_i /right|](http://upload.wikimedia.org/math/9/c/4/9c489f83338217f66bbf246b0afbeb7f.png)

**二范数**

![= /left( /sum_{i=1}^n /left| x_i - y_i /right|^2 /right)^{1/2}](http://upload.wikimedia.org/math/3/0/b/30b830ad4a100723381248b238199690.png)

**p范数(p-norm)**

![= /left( /sum_{i=1}^n /left| x_i - y_i /right|^p /right)^{1/p}](http://upload.wikimedia.org/math/a/5/b/a5b6afef3f40793dc95f28a9fe1dce7a.png)




**Hamming距离**

两个码字的对应比特质不同的比特数。一个有效编码集中,任意两个码字的海明距离的最小值称为该编码集的海明距离。举例如下：10101和00110从第一位开始依次有第一位、第四、第五位不同，则海明距离为3.

n位的码字可以用n维空间的超立方体的一个顶点来表示。两个码字之间的海明距离就是超立方体两个顶点之间的一条边，而且是这两个顶点之间的最短距离。

**相似度度量**

相似度度量（Similarity），即计算个体间的相似程度，与距离度量相反，相似度度量的值越小，说明个体间相似度越小，差异越大。

**向量空间余弦相似度（Cosine Similarity）**

余弦相似度用向量空间中**两个向量夹角的余弦值**作为衡量两个个体间差异的大小。相比距离度量，余弦相似度**更加注重两个向量在方向上的差异，而非距离或长度上**。公式如下：

![Cosine Similarity](http://upload.chinaz.com/2011/1008/1318060559877.png)

**皮尔森相关系数（Pearson Correlation Coefficient）**

即相关分析中的相关系数r，分别对X和Y基于自身总体标准化后计算空间向量的余弦夹角。公式如下：

![Pearson Correlation Coefficient](http://upload.chinaz.com/2011/1008/1318060559622.png)

**Jaccard相似系数（Jaccard Coefficient）**

Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以**Jaccard系数只关心个体间共同具有的特征是否一致这个问题。**Jacard最好是应用在离散的变量几何上。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数，公式如下：

![Jaccard Coefficient](http://upload.chinaz.com/2011/1008/1318060559590.png)

和Jacard index 相似的一个公式是Dice‘ coefficient, 它也很直观，

**![s = /frac{2 | X /cap Y |}{| X | + | Y |}](http://upload.wikimedia.org/math/2/3/5/2354a9c697d2bf4ae114b8f1f72d5090.png)**

**皮尔逊积差系数（Pearson correlation coefficient ）**

学过概率论的人都知道，有均值，反差，还有相关系数，相关系数就是就是描述两组变量是否线性相关的那个东西。相关系数的优点是，它跟变量的长度无关，这个都点像cosine相似性。有一个应用是，比如一个商品推荐系统，要给用户A推荐相应的产品，首先要通过对商品的打分，找到与A相似k个用户。但是有些人，可能天生喜欢打高分，有些人偏向于打低分，为了消除这个问题相关系数是一个很好的度量方法。列公式，

**![/rho_{X,Y}={/mathrm{cov}(X,Y) /over /sigma_X /sigma_Y} ={E[(X-/mu_X)(Y-/mu_Y)] /over /sigma_X/sigma_Y},](http://upload.wikimedia.org/math/1/7/7/17709e96782a6a8bcd39904c5f2383e6.png)**

E是数学期望，cov表示协方差。

**编辑距离或者Levenshtein distance**

编辑距离说的两个字符串的相似程度。串A通过删除，增加，和修改变成串B的可以度量函数（一般是是通过多少步能将A变成B 也可以对每一个编辑步骤加权）wikipedia上有非常好的描述，这里就不再赘述。Levenshtein Distance. 与之相关的字符串相似性方法还有Jaro-Winkler distance。

**SimRank 相似**

SimRank来自图论，说两个变量相似，因为他们链接了同一个或相似的节点。这个算法需要需要重点讨论。下次在说。调整余弦相似度（Adjusted Cosine Similarity）

虽然余弦相似度对个体间存在的偏见可以进行一定的修正，但是因为只能分辨个体在维之间的差异，没法衡量每个维数值的差异，会导致这样一个情况：比如用户对内容评分，5分制，X和Y两个用户对两个内容的评分分别为（1，2）和（4，5），使用余弦相似度得出的结果是0.98，两者极为相似，但从评分上看X似乎不喜欢这2个内容，而Y比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3，那么调整后为（-2，-1）和（1，2），再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。

**Tonimoto系数**

Tonimoto系数是Cosine Similarity的一个扩展，

![T(A,B) = {A /cdot B /over /|A/|^2 +/|B/|^2 - A /cdot B}.](http://upload.wikimedia.org/math/8/4/7/84726b0961f860e4be320367fcdc3db1.png)

T（A,B）的分母是大于等于cosine similarity的分母，当且仅当A,B长度一样是才相等。这就意味着，Tonimoto系数考虑了两个向量的长度差异，长度差异越大相似性约小。
欧氏距离与余弦相似度
欧氏距离是最常见的距离度量，而余弦相似度则是最常见的相似度度量，很多的距离度量和相似度度量都是基于这两者的变形和衍生，所以下面重点比较下两者在衡量个体差异时实现方式和应用环境上的区别。

借助三维坐标系来看下欧氏距离和余弦相似度的区别：

![distance and similarity](http://upload.chinaz.com/2011/1008/1318060559123.png)

从图上可以看出距离度量衡量的是空间各点间的绝对距离，跟各个点所在的位置坐标（即个体特征维度的数值）直接相关；而余弦相似度衡量的是空间向量的夹角，更加的是体现在方向上的差异，而不是位置。如果保持A点的位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦相似度cosθ是保持不变的，因为夹角不变，而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦相似度的不同之处。

根据欧氏距离和余弦相似度各自的计算方式和衡量特征，分别适用于不同的数据分析模型：**欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异；而余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感）。**



