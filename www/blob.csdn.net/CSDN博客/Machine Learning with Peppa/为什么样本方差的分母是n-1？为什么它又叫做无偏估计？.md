# 为什么样本方差的分母是n-1？为什么它又叫做无偏估计？ - Machine Learning with Peppa - CSDN博客





2018年03月20日 22:38:49[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：8347








简单的回答，是因为因为均值你已经用了n个数的平均来做估计在求方差时，只有(n-1)个数　和　均值信息　是不相关的。而你的第ｎ个数已经可以由前(n-1)个数和均值　来唯一确定，实际上没有**信息量**。所以在计算方差时，只除以(n-1)。

那么更严格的证明呢？请耐心的看下去。

样本方差计算公式里分母为![n-1](https://www.zhihu.com/equation?tex=n-1)的目的是为了让方差的估计是**无偏**的。无偏的估计(unbiased estimator)比有偏估计(biased estimator)更好是符合直觉的，尽管有的统计学家认为让mean square error即MSE最小才更有意义，这个问题我们不在这里探讨；**不符合直觉的是，为什么分母必须得是![n-1](https://www.zhihu.com/equation?tex=n-1)而不是![n](https://www.zhihu.com/equation?tex=n)才能使得该估计无偏。**


首先，我们假定随机变量![X](https://www.zhihu.com/equation?tex=X)的数学期望![\mu](https://www.zhihu.com/equation?tex=%5Cmu)是已知的，然而方差![\sigma^2](https://www.zhihu.com/equation?tex=%5Csigma%5E2)未知。在这个条件下，根据方差的定义我们有
![\mathbb{E}\Big[\big(X_i -\mu\big)^2 \Big]=\sigma^2, \quad\forall i=1,\ldots,n,](https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5CBig%5B%5Cbig%28X_i+-%5Cmu%5Cbig%29%5E2+%5CBig%5D%3D%5Csigma%5E2%2C+%5Cquad%5Cforall+i%3D1%2C%5Cldots%2Cn%2C)

由此可得
**![\mathbb{E}\Big[\frac{1}{n} \sum_{i=1}^n\Big(X_i -\mu\Big)^2 \Big]=\sigma^2](https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5CBig%5B%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cmu%5CBig%29%5E2+%5CBig%5D%3D%5Csigma%5E2).因此![\frac{1}{n} \sum_{i=1}^n\Big(X_i -\mu\Big)^2](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cmu%5CBig%29%5E2+)是方差![\sigma^2](https://www.zhihu.com/equation?tex=%5Csigma%5E2)的一个无偏估计，注意式中的分母不偏不倚正好是![n](https://www.zhihu.com/equation?tex=n)！**
这个结果符合直觉，并且在数学上也是显而易见的。

现在，我们考虑随机变量![X](https://www.zhihu.com/equation?tex=X)的数学期望![\mu](https://www.zhihu.com/equation?tex=%5Cmu)是未知的情形。这时，我们会倾向于无脑直接用样本均值![\bar{X}](https://www.zhihu.com/equation?tex=%5Cbar%7BX%7D)替换掉上面式子中的![\mu](https://www.zhihu.com/equation?tex=%5Cmu)。这样做有什么后果呢？后果就是，
**如果直接使用![\frac{1}{n} \sum_{i=1}^n\Big(X_i -\bar{X}\Big)^2](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cbar%7BX%7D%5CBig%29%5E2+)作为估计，那么你会倾向于低估方差！**
这是因为：
![\begin{eqnarray}\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2 &=&\frac{1}{n}\sum_{i=1}^n\Big[(X_i-\mu) + (\mu -\bar{X}) \Big]^2\\&=&\frac{1}{n}\sum_{i=1}^n(X_i-\mu)^2 +\frac{2}{n}\sum_{i=1}^n(X_i-\mu)(\mu -\bar{X})+\frac{1}{n}\sum_{i=1}^n(\mu -\bar{X})^2 \\&=&\frac{1}{n}\sum_{i=1}^n(X_i-\mu)^2 +2(\bar{X}-\mu)(\mu -\bar{X})+(\mu -\bar{X})^2 \\&=&\frac{1}{n}\sum_{i=1}^n(X_i-\mu)^2 -(\mu -\bar{X})^2 \end{eqnarray}](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7D%0A%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cbar%7BX%7D%29%5E2+%26%3D%26%0A%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%5CBig%5B%28X_i-%5Cmu%29+%2B+%28%5Cmu+-%5Cbar%7BX%7D%29+%5CBig%5D%5E2%5C%5C%0A%26%3D%26%0A%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cmu%29%5E2+%0A%2B%5Cfrac%7B2%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cmu%29%28%5Cmu+-%5Cbar%7BX%7D%29%0A%2B%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28%5Cmu+-%5Cbar%7BX%7D%29%5E2+%5C%5C%0A%26%3D%26%0A%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cmu%29%5E2+%0A%2B2%28%5Cbar%7BX%7D-%5Cmu%29%28%5Cmu+-%5Cbar%7BX%7D%29%0A%2B%28%5Cmu+-%5Cbar%7BX%7D%29%5E2+%5C%5C%0A%26%3D%26%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cmu%29%5E2+%0A-%28%5Cmu+-%5Cbar%7BX%7D%29%5E2+%0A%5Cend%7Beqnarray%7D)
换言之，除非正好![\bar{X}=\mu](https://www.zhihu.com/equation?tex=%5Cbar%7BX%7D%3D%5Cmu)，否则我们一定有
![\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2 <\frac{1}{n}\sum_{i=1}^n(X_i-\mu)^2](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cbar%7BX%7D%29%5E2+%3C%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cmu%29%5E2+),
而不等式右边的那位才是的对方差的“正确”估计！
这个不等式说明了，为什么直接使用![\frac{1}{n} \sum_{i=1}^n\Big(X_i -\bar{X}\Big)^2](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cbar%7BX%7D%5CBig%29%5E2+)会导致对方差的低估。

那么，在不知道随机变量真实数学期望的前提下，如何“正确”的估计方差呢？答案是把上式中的分母![n](https://www.zhihu.com/equation?tex=n)换成![n-1](https://www.zhihu.com/equation?tex=n-1)，通过这种方法把原来的偏小的估计“放大”一点点，我们就能获得对方差的正确估计了：
![\mathbb{E}\Big[\frac{1}{n-1} \sum_{i=1}^n\Big(X_i -\bar{X}\Big)^2\Big]=\mathbb{E}\Big[\frac{1}{n} \sum_{i=1}^n\Big(X_i -\mu\Big)^2 \Big]=\sigma^2.](https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5CBig%5B%5Cfrac%7B1%7D%7Bn-1%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cbar%7BX%7D%5CBig%29%5E2%5CBig%5D%3D%5Cmathbb%7BE%7D%5CBig%5B%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cmu%5CBig%29%5E2+%5CBig%5D%3D%5Csigma%5E2.)
至于为什么分母是![n-1](https://www.zhihu.com/equation?tex=n-1%0A)而不是![n-2](https://www.zhihu.com/equation?tex=n-2)或者别的什么数，最好还是去看真正的数学证明，因为数学证明的根本目的就是告诉人们“为什么”；暂时我没有办法给出更“初等”的解释了。            


