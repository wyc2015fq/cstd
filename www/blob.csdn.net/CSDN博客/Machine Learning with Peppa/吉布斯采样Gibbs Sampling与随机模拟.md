# 吉布斯采样Gibbs Sampling与随机模拟 - Machine Learning with Peppa - CSDN博客





2018年01月16日 20:44:04[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：274
所属专栏：[机器学习与数据挖掘](https://blog.csdn.net/column/details/18961.html)[有趣的算法](https://blog.csdn.net/column/details/19022.html)









## 为什么要用吉布斯采样

### 什么是sampling? 


sampling就是以一定的概率分布，看发生什么事件。举一个例子。甲只能E：吃饭、学习、打球，时间T：上午、下午、晚上，天气W：晴朗、刮风、下雨。现在要一个sample，这个sample可以是：打球+下午+晴朗。


### 吉布斯采样的通俗解释？


问题是我们不知道p(E,T,W)，或者说，不知道三件事的联合分布joint distribution。当然，如果知道的话，就没有必要用gibbs sampling了。但是，我们知道三件事的conditional distribution。也就是说，p(E|T,W),p(T|E,W),p(W|E,T)。现在要做的就是通过这三个已知的条件分布，再用gibbs
 sampling的方法，得到联合分布。

具体方法。首先随便初始化一个组合,i.e. 学习+晚上+刮风，然后依条件概率改变其中的一个变量。具体说，假设我们知道晚上+刮风，我们给E生成一个变量，比如，学习-》吃饭。我们再依条件概率改下一个变量，根据学习+刮风，把晚上变成上午。类似地，把刮风变成刮风（当然可以变成相同的变量）。这样学习+晚上+刮风-》吃饭+上午+刮风。同样的方法，得到一个序列，每个单元包含三个变量，也就是一个马尔可夫链。然后跳过初始的一定数量的单元（比如100个），然后隔一定的数量取一个单元（比如隔20个取1个）。这样sample到的单元，是逼近联合分布的。







# 马氏链收敛定理


马氏链定理： 如果一个非周期马氏链具有转移概率矩阵,且它的任何两个状态是连通的，那么 存在且与无关，记,
 我们有
- 
- 
-  是方程  的唯一非负解


其中,  
            称为马氏链的平稳分布。


所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的。 定理的证明相对复杂。

### 定理内容的一些解释说明
- 该定理中马氏链的状态不要求有限，可以是有无穷多个的；
- 定理中的“非周期“这个概念不解释，因为我们遇到的绝大多数马氏链都是非周期的；
- 两个状态 
    是连通并非指 可以直接一步转移到 
  (),而是指 
  可以通过有限的步转移到达()。马氏链的任何两个状态是连通的含义是指存在一个,
 使得矩阵 中的任何一个元素的数值都大于零。
- 我们用  表示在马氏链上跳转第步后所处的状态，如果 存在，很容易证明以上定理的第二个结论。由于




上式两边取极限就得到 

[[马尔科夫模型](http://blog.csdn.net/pipisorry/article/details/46618991)]



## Monte Carlo随机采样方法



**产生独立样本**

基本方法:概率积分变换(第一部分已讲)

接受—拒绝(舍选)采样

重要性采样
**产生相关样本:Markov Chain Monte Carlo**

Metropolis-Hastings算法

Gibbs Sampler

这里主要讲马尔科夫链蒙特卡罗( Markov chain Monte Carlo, MCMC )，它使得我们可以从一大类概率分布中进行采样，并且可以很好地应对样本空间维度的增长。


[皮皮blog](http://blog.csdn.net/pipisorry)







# Metropolis-Hastings采样算法

## Idea


对于给定的概率分布p(x),我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移序列 x0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第n步已经收敛了，于是我们就得到了 π(x) 的样本xn,xn+1⋯。


这个绝妙的想法在1953年被 Metropolis想到了，为了研究粒子系统的平稳性质， Metropolis 考虑了物理学中常见的波尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡罗方法，即Metropolis算法，并在最早的计算机上编程实现。Metropolis 算法是首个普适的采样方法，并启发了一系列 MCMC方法，所以人们把它视为随机模拟技术腾飞的起点。 Metropolis的这篇论文被收录在《统计学中的重大突破》中， Metropolis算法也被遴选为二十世纪的十个最重要的算法之一。


我们接下来介绍的MCMC 算法是 Metropolis 算法的一个改进变种，即常用的 Metropolis-Hastings 算法。由上一节的例子和定理我们看到了，马氏链的收敛性质主要由转移矩阵P 决定, 所以基于马氏链做采样的关键问题是如何构造转移矩阵P,使得平稳分布恰好是我们要的分布p(x)。如何能做到这一点呢？

## 细致平稳条件

**定理：[细致平稳条件] **如果非周期马氏链的转移矩阵和分布 
    满足




则 是马氏链的平稳分布，上式被称为细致平稳条件(detailed
 balance condition)。



其实这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态,
 从  转移出去到 而丢失的概率质量，恰好会被从  转移回 的概率质量补充回来，所以状态上的概率质量 
      是稳定的，从而 
    是马氏链的平稳分布。数学上的证明也很简单，由细致平稳条件可得





由于 是方程    
 的解，所以是平稳分布。


Note: 细致平稳条件是达到稳态的充分条件，并不是必要条件（也就是说要想达到稳态，必须要满足细致平衡条件，但也不是说满足细致平衡条件时的点就是稳态，而是说细致平衡条件是达到稳态的必经之路）。如在[[马尔科夫模型](http://blog.csdn.net/pipisorry/article/details/46618991)]示例中0.28*0.286=0.08008，0.15*0.489=0.07335不相等，并不符合细致平稳条件。


## 构建满足条件的马氏链


假设我们已经有一个转移矩阵为 的马氏链
 (表示从状态转移到状态的概率，也可以写为或者),
 显然，通常情况下


也就是细致平稳条件不成立，所以  不太可能是这个马氏链的平稳分布。我们可否对马氏链做一个改造，使得细致平稳条件成立呢？譬如，我们引入一个 ,
 我们希望



取什么样的  以上等式能成立呢？最简单的，按照对称性，我们可以取





于是(*)式就成立了。所以有

![](https://img-blog.csdn.net/20170919093058067?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGlwaXNvcnJ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


于是我们把原来具有转移矩阵的一个很普通的马氏链，改造为了具有转移矩阵的马氏链，而恰好满足细致平稳条件，由此马氏链的平稳分布就是！




在改造  的过程中引入的 称为接受率，物理意义可以理解为在原来的马氏链上，从状态 以 
    的概率转跳转到状态 的时候，我们以 
      的概率接受这个转移，于是得到新的马氏链的转移概率为。



Note：当按照上面介绍的构造方法把Q–>Q’后，就不能保证Q’是一个转移矩阵（转移矩阵每一行加和为1）了。这时应该在 j != i 的时候转移概率Q'(i, j) 如上处理， 当j = i 的时候， Q'(i, i) 应该设为Q'(i, i) = 1- 其它概率之和，从而归一化概率转移矩阵。


**马氏链转移和接受概率：**


![mcmc-transition](http://cos.name/wp-content/uploads/2013/01/mcmc-transition.jpg)

## MCMC采样算法


采样概率分布p(x)的算法，假设我们已经有一个转移矩阵Q(对应元素为q(i,j))，lz: 马氏链的初始状态及状态指的是概率分布


![mcmc-algo-1](http://cos.name/wp-content/uploads/2013/01/mcmc-algo-1.jpg)


Note: 一开始的采样还没有收敛，并不是平稳分布（p(x)）的样本（概率分布中的其中一个概率分布），只有采样了多次（如20次）可能收敛了，其样本才算是平稳分布的样本。




上述过程中  
    说的都是离散的情形，事实上即便这两个分布是连续的，以上算法仍然是有效，于是就得到更一般的连续概率分布  的采样算法，而  
 就是任意一个连续二元概率分布对应的条件分布。

## Metropolis-Hastings算法




{提高接受率alpha}


以上的 MCMC 采样算法已经能很漂亮的工作了，不过它有一个小的问题：马氏链在转移的过程中的接受率 可能偏小，这样采样过程中马氏链容易原地踏步，拒绝大量的跳转，这使得马氏链遍历所有的状态空间要花费太长的时间，收敛到平稳分布 的速度太慢。有没有办法提升一些接受率呢?


假设      
 , 此时满足细致平稳条件，于是




上式两边扩大5倍，我们改写为



看，我们提高了接受率，而细致平稳条件并没有打破！这启发我们可以把细致平稳条件(**) 式中的 
              同比例放大，使得两数中最大的一个放大到1，这样我们就提高了采样中的跳转接受率。所以我们可以取



于是，经过对上述MCMC 采样算法中接受率的微小改造，我们就得到了如下教科书中最常见的 Metropolis-Hastings 算法。

![mcmc-algo-2](http://cos.name/wp-content/uploads/2013/01/mcmc-algo-2.jpg)


对于分布 ,我们构造转移矩阵 使其满足细致平稳条件




此处  并不要求是一维的，对于高维空间的 ，如果满足细致平稳条件



那么以上的 Metropolis-Hastings 算法一样有效。



[皮皮blog](http://blog.csdn.net/pipisorry)






# Gibbs Sampling算法


{提高高维随机变量采样接受率alpha}


MCMC：Markov链通过转移概率矩阵可以收敛到稳定的概率分布。这意味着MCMC可以借助Markov链的平稳分布特性模拟高维概率分布``；当Markov链经过*burn-in*阶段，消除初始参数的影响，到达平稳状态后，每一*次*状态转移都可以生成待模拟分布的一个样本。


而Gibbs抽样是MCMC的一个特例，它交替的固定某一维度``，然后通过其他维度``的值来抽样该维度的值，注意，gibbs采样只对z是高维（2维以上）（Gibbs
 sampling is applicable in situations where Z has at least two dimensions）情况有效。


基本算法如下：
- 选择一个维度``，可以随机选择；
- 根据分布`` 抽样``。

## 吉布斯采样满足细致平稳条件的转移矩阵的构造


对于高维的情形，由于接受率 的存在(通常α<1),
 以上 Metropolis-Hastings 算法的效率不够高。能否找到一个转移矩阵Q使得接受率 α=1呢？我们先看看二维的情形，假设有一个概率分布 p(x,y), 考察坐标相同的两个点，我们发现


lz: 从下式可看出，M-H方法是从p(xi)出发构建细致平稳条件，而gibbs是从p(xi, yj)出发构建细致平稳条件。




所以得到



即



基于以上等式，我们发现，在  这条平行于 轴的直线上，如果使用条件分布 
 作为任何两个点之间的转移概率，那么任何两个点之间的转移满足细致平稳条件。



同样的，如果我们在  这条直线上任意取两个点 ,也有如下等式







![gibbs-transition](http://cos.name/wp-content/uploads/2013/01/gibbs-transition.png)


于是我们可以如下构造平面上任意两点之间的转移概率矩阵Q







有了如上的转移矩阵 Q, 我们很容易验证对平面上任意两点 ,
 满足细致平稳条件






于是这个二维空间上的马氏链将收敛到平稳分布   
 。而这个算法就称为 Gibbs Sampling 算法,是 Stuart Geman 和Donald Geman 这两兄弟于1984年提出来的，之所以叫做Gibbs Sampling 是因为他们研究了Gibbs random field, 这个算法在现代贝叶斯分析中占据重要位置。


lz: 从这可以看出，gibbs采样和M-H采样的不同在于，gibbs采样从更大角度考虑了每次采样间的联系，也就是M-H只是单独考虑某个pi_i的采样（相当于直接从A -> D），而gibbs考虑了pi_j和其它如pi_j间的关系，从而从固定pi_j的角度出发，对pi_i采样加速（采样都是沿着坐标轴进行的）。


## 二维吉布斯采样算法

![gibbs-algo-1](http://cos.name/wp-content/uploads/2013/01/gibbs-algo-1.jpg)![two-stage-gibbs](http://cos.name/wp-content/uploads/2013/01/two-stage-gibbs.png)

Note: 采样算法中右边的概率我们是知道的，例如你要采样的是二维高斯分布，那么固定xt后就是二维高斯分布固定xt后的一维高斯分布，且每次采样的坐标不同，这样这个一维高斯分布概率密度函数也就不一样了。


### Gibbs Sampling 算法中的马氏链转移


    Note：2.1步是从 (x0,y0)转移到(x0,y1)，满足Q(A→B)=p(yB|x1)的细致平稳条件，所以会收敛到平稳分布；同样2.2步是从(x0,y1)转移到(x1,y1)，也会收敛到平稳分布。也就是整个第2步是从 (x0,y0)转移到(x1,y1)，满足细致平稳条件，在循环多次后会收敛于平稳分布，采样得到的(xn,yn),(xn+1,yn+1)...就是平稳分布的样本（lz:不是(xn,yn),(xn,yn+1),(xn+1,yn+1)...，因为需要归一化）。


    以上采样过程中，如图所示，马氏链的转移只是轮换的沿着坐标轴 轴和轴做转移，于是得到样本(x0,y0),(x1,y1),(x2,y2),⋯马氏链收敛后，最终得到的样本就是  
 的样本，而收敛之前的阶段称为 burn-in period。也就是说马氏链跳转过程就是采样的过程(采样的意思就是说在xt下，我们先计算出p(y|xt)的概率分布，并依这个概率分布采样某个yt+1)， 马氏链任何一个时刻 i 到达的状态 x_i 都是一个样本。 只是要等到 i 足够大( i > K) ， 马氏链收敛到平稳分布后, 那么 x_K, x_{K+1}, …. 这些样本就都是平稳分布的样本。


lz：最终的近似平稳分布就可以通过采样结果近似表达出来。

### 坐标轴轮换采样


    一般地，Gibbs Sampling 算法大都是坐标轴轮换采样的，但是这其实是不强制要求的。最一般的情形可以是，在 
 时刻，可以在轴和轴之间随机的选一个坐标轴，然后按条件概率做转移，马氏链也是一样收敛的。轮换两个坐标轴只是一种方便的形式。


lz: 但是不能只固定一个采样的，否则样本都是从某个固定维度采样，并不具有整个分布的代表性。


## n维吉布斯采样算法


以上的过程我们很容易推广到高维的情形，对于(***) 式，如果变为多维情形，可以看出推导过程不变，所以细致平稳条件同样是成立的






此时转移矩阵 Q 由条件分布  定义。上式只是说明了一根坐标轴的情形，和二维情形类似，很容易验证对所有坐标轴都有类似的结论。


所以维空间中对于概率分布 
 可以如下定义转移矩阵
- 如果当前状态为，马氏链转移的过程中，只能沿着坐标轴做转移。沿着 这根坐标轴做转移的时候，转移概率由条件概率  
   定义；
-  其它无法沿着单根坐标轴进行的跳转，转移概率都设置为 0。


于是我们可以把Gibbs Sampling 算法从采样二维的   
 推广到采样 维的 


![gibbs-algo-2](http://cos.name/wp-content/uploads/2013/01/gibbs-algo-2.jpg)![](https://img-blog.csdn.net/20160528113718325?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**Note**: 



1 Algorithm 8中的第2步的第6小步，x2(t)应为x2(t+1)。


2 要完成Gibbs抽样，需要知道如下条件概率：``



lz这也就是说，gibbs采样是通过条件分布采样模拟联合分布，再通过模拟的联合分布直接推导出条件分布，以此循环。



Gibbs sampling makes it possible to obtain samples from probability distributions without having to explicitly calculate the values for their marginalizing integrals, e.g. computing expected values, by defining a conceptually straightforward approximation.


![](https://img-blog.csdn.net/20160528114307601?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


以上算法收敛后，每次完整的循环完成后得到的(xn+1,yn+1)...就是概率分布p(x1,x2,⋯,xn)的一个样本。（lz: g(.)是归一化）

![](https://img-blog.csdn.net/20160528114441319?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


同样的，在以上算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵 Q 中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定时刻t，在一根固定的坐标轴上转移的概率是1。




转自：http://[blog.csdn.net/pipisorry/article/details/51373090](http://blog.csdn.net/pipisorry/article/details/51373090)












