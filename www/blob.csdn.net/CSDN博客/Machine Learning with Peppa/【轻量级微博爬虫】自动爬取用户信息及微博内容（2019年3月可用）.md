# 【轻量级微博爬虫】自动爬取用户信息及微博内容（2019年3月可用） - Machine Learning with Peppa - CSDN博客





2019年02月26日 18:20:18[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：313标签：[爬虫																[Python																[网页爬虫](https://so.csdn.net/so/search/s.do?q=网页爬虫&t=blog)
个人分类：[编程之美：Python](https://blog.csdn.net/qq_39521554/article/category/7392111)





## 前言

为什么写这个博客，主要是CSDN上有几个比较热的微博爬虫，基本在今年都挂掉了用不了。。比如 [微博爬虫，每日百万级数据](https://blog.csdn.net/nghuyong/article/details/78415066)，博主写的比较全，不过因为今年微博查的更严了，所以每日百万级的基本不太可能（除非有很多账号，然而淘宝上的微博账号也涨价了，要达到这么大的数据量都够买一块RTX2080了）。

所以基于一些爬虫框架，这篇博客给出的是更加简单易懂的轻量级微博爬虫，对于学校实验、数据测试、NLP模型训练是大概够用的，代码在我的github：[Weibo_LightSpyder_2019](https://github.com/Y1ran/Weibo_Light_Spyder_2019)，下载前记得star哦。



说明：这个轻量级爬虫每次爬取内容大概在每个ID100条左右（可以设置的更大），只需要建立超过一个较大的微博ID列表（这个表可以自己爬或者网上找）然后随机选取一部分用户进行爬取即可。同一IP每日建议不超过10万条，以免被封。

此外有条件的可以用分布式爬虫扩展一下，如果对于数据量需求不高的则此版本即可。



## 步骤简介

### 1.选取爬取目标网址



首先，在准备开始爬虫之前，得想好要爬取哪个网址。新浪微博的网址分为网页端和手机端两个，大部分爬取微博数据都会选择爬取手机端，因为对比起来，手机端基本上包括了所有你要的数据，并且手机端相对于PC端是轻量级的。

下面是GUCCI的手机端和PC端的网页展示。

![](https://img-blog.csdnimg.cn/20190226180849642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0,size_16,color_FFFFFF,t_70)

![å¾®åææºç«¯](https://img-blog.csdn.net/20180920154555989?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FzaGVyMTE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 2.模拟登陆



定好爬取微博手机端数据之后，接下来就该模拟登陆了。

模拟登陆的网址

登陆的网页下面的样子

![æ¨¡æç»éç½é¡µ](https://img-blog.csdn.net/20180920155751614?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FzaGVyMTE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### **3.获取用户微博页码**



在登录之后可以进入想要爬取的商户信息，因为每个商户的微博量不一样，因此对应的微博页码也不一样，这里首先将商户的微博页码爬下来。与此同时，将那些公用信息爬取下来，比如用户uid，用户名称，微博数量，关注人数，粉丝数目

![è¿éæå¤§é¡µç ä¸º361](https://img-blog.csdn.net/20180920172114463?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FzaGVyMTE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**4.根据爬取的最大页码，循环爬取所有数据**

在得到最大页码之后，直接通过循环来爬取每一页数据。抓取的数据包括，微博内容，转发数量，评论数量，点赞数量，发微博的时间，微博来源，以及是原创还是转发。



**4.在得到所有数据之后，可以写到csv文件，或者excel**
![](https://img-blog.csdn.net/20180920153432868?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FzaGVyMTE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)](https://so.csdn.net/so/search/s.do?q=Python&t=blog)](https://so.csdn.net/so/search/s.do?q=爬虫&t=blog)




