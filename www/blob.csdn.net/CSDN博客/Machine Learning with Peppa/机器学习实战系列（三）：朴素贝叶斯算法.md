# 机器学习实战系列（三）：朴素贝叶斯算法 - Machine Learning with Peppa - CSDN博客





2018年07月07日 12:55:55[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：239








课程的所有数据和代码在我的Github：[Machine learning in Action](https://github.com/Y1ran/Machine-Learning-in-Action-Python3)，目前刚开始做，有不对的欢迎指正，也欢迎大家star。除了 版本差异，代码里的部分函数以及代码范式也和原书不一样（因为作者的代码实在让人看的别扭，我改过后看起来舒服多了）。在这个系列之后，我还会写一个scikit-learn机器学习系列，因为在实现了源码之后，带大家看看SKT框架如何使用也是非常重要的。    

## 算法简介

    朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。

根据贝叶斯定理，对一个分类问题，给定样本特征x，样本属于类别y的概率是：

![](https://img-blog.csdn.net/20180707123357777?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

在这里，x 是一个特征向量，设 x 维度为 M。因为朴素的假设，即特征条件独立，根据全概率公式展开，上式可以表达为：

![](https://img-blog.csdn.net/20180707123430844?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



## 算法特点

优点：在数据较少的情况下仍然有效，可以处理多类别问题。

缺点：对于输入数据的准备方式较为敏感。

适用数据类型：标称型数据



## 算法本质

我们把P(A)称为”先验概率”（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。

P(A|B)称为”后验概率”（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。

P(B|A)/P(B)称为”可能性函数”（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

所以，条件概率可以理解成下面的式子：
`后验概率　＝　先验概率 ｘ 调整因子`- 1

**这就是贝叶斯推断的含义。我们先预估一个”先验概率”，然后加入实验结果，看这个实验到底是增强还是削弱了”先验概率”，由此得到更接近事实的”后验概率”。**

   在这里，如果”可能性函数”P(B|A)/P(B)>1，意味着”先验概率”被增强，事件A的发生的可能性变大；如果”可能性函数”=1，意味着B事件无助于判断事件A的可能性；如果”可能性函数”<1，意味着”先验概率”被削弱，事件A的可能性变小。

   这样我们就可以进行计算了。如果有些迷糊，让我们从一个例子开始讲起，你会看到贝叶斯分类器很好懂，一点都不难。

   某个医院早上来了六个门诊的病人，他们的情况如下表所示：
|症状|职业|疾病|
|----|----|----|
|打喷嚏|护士|感冒|
|打喷嚏|农夫|过敏|
|头痛|建筑工人|脑震荡|
|头痛|建筑工人|感冒|
|打喷嚏|教师|感冒|
|头痛|教师|脑震荡|

    现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？

    根据贝叶斯定理：

![](https://img-blog.csdn.net/20170817204750272?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    可得：

![](https://img-blog.csdn.net/20170817204812471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    根据朴素贝叶斯条件独立性的假设可知，”打喷嚏”和”建筑工人”这两个特征是独立的，因此，上面的等式就变成了

![](https://img-blog.csdn.net/20170817204834377?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    这里可以计算：

![](https://img-blog.csdn.net/20170817204857275?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。

    这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。

    同样，在编程的时候，如果不需要求出所属类别的具体概率，P(打喷嚏) = 0.5和P(建筑工人) = 0.33的概率是可以不用求的。





## 加载数据、词向量转换

```python
from numpy import *

def loadDataSet():
    '''
    postingList: 进行词条切分后的文档集合
    classVec:类别标签   
    使用伯努利模型的贝叶斯分类器只考虑单词出现与否（0，1）
    '''
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0,1,0,1,0,1]    #1代表侮辱性文字，0代表正常言论
    return postingList,classVec

def Create_wordVec(dataset):
    word_set = set([])
    for doc in dataset:
        word_set = word_set | set(doc) # 通过对两个集合取并，找出所有非重复的单词
    return list(word_set)

def Words2Vec(wordList, input_set):
    '''
    @wordList：为前一个函数的输出值（包含单词）
    @input_set：输入需要分类的集合
    函数输出：包含0，1的布尔型向量（对应Wordlist中的单词出现与否）
    '''
    return_vec = [0] * len(wordList) 
    # 创建与词汇表等长的列表向量
    for word in input_set:
        if word in wordList:
            return_vec[wordList.index(word)] = 1 # 出现的单词赋1
        else: print("the word %s is not in list" % word)
    return return_vec
```

## 贝叶斯训练

```python
from numpy import *


def train_bayes(train_matrix, train_class):
    '''
    利用NumPy数组计算p(wi|c1)
    词条 属于类1的概率Prob_positive = p(c1) 
    因为是二分类所以属于类0概率 =1-p(c1)
    '''
    num_docs = len(train_matrix)
    num_word = len(train_matrix[0])
    # 获取输入文档（句子）数以及向量的长度
    Prob_positive = sum(train_class)/ float(num_docs)
    Prob_num_0 = ones(num_word) # 创建一个长度为词条向量等长的列表
    Prob_num_1 = ones(num_word) 
    # 为避免0概率使得最终乘积为0，使用拉普拉斯平滑（加入常数lamda，此处为1）
    Prob_denom_0 = 2.0
    Prob_denom_1 = 2.0
    
    for i in range(num_docs):
    # 统计类别为1的词条向量中出现的所有词条的总数
            # 即统计类1所有文档中出现单词的数目
        if train_class[i] == 1:
            Prob_num_1 += train_matrix[i]
            Prob_denom_1 += sum(train_matrix[i])
        else:
            Prob_num_0 += train_matrix[i]
            Prob_denom_0 += sum(train_matrix[i])         
    p1_vec = log(Prob_num_1 / Prob_denom_1)
    p0_vec = log(Prob_num_0 / Prob_denom_0)
    # 将结果取自然对数，避免下溢出，即太多很小的数相乘造成的影响
    return p1_vec, p0_vec, Prob_positive
    # p1概率实际上等于[p(w1|c1),p(w2|c1), ... p(wn|c1)]组成的向量
    # 后续计算中，会基于条件独立性求出p(W|c1)
```

## 贝叶斯分类

```python
def classify_bayes(test_vec, p0, p1, p_pos):
    '''
    @vec2Classify:待测试分类的词条向量
    @p0:类别0所有文档中各个词条出现的频数p(wi|c0)
    @p1:类别1所有文档中各个词条出现的频数p(wi|c1)
    @p_pos:类别为1的文档占文档总数比例
    '''
    p1 = sum(test_vec * p1) + log( p_pos)
    p0 = sum(test_vec * p0) + log( 1.0 - p_pos)
    # 原公式为乘积P(w|c)P(c)，log取对数后乘积变为相加
    # print(p1,p0)
    if p1 > p0:
        return 1
    else:
        return 0
    
#分类测试整体函数        
def Test_classify():
    #由数据集获取文档矩阵和类标签向量
    listOPosts,listClasses=loadDataSet()
    #统计所有文档中出现的词条，存入词条列表
    myVocabList=Create_wordVec(listOPosts)
    #创建新的列表
    trainMat=[]
    for postinDoc in listOPosts:
        #将每篇文档利用words2Vec函数转为词条向量，存入文档矩阵中
        trainMat.append(Words2Vec(myVocabList,postinDoc))\
    #将文档矩阵和类标签向量转为NumPy的数组形式，方便接下来的概率计算
    #调用训练函数，得到相应概率值
    p1V,p0V,pAb=train_bayes(array(trainMat),array(listClasses))
    #测试文档
    testEntry=['love','my','dalmation']
    #将测试文档转为词条向量，并转为NumPy数组的形式
    thisDoc=array(Words2Vec(myVocabList,testEntry))
    #利用贝叶斯分类函数对测试文档进行分类并打印
    print(testEntry,'classified as:',classify_bayes(thisDoc,p0V,p1V,pAb))
    #第二个测试文档
    testEntry1=['stupid','garbage']
    #同样转为词条向量，并转为NumPy数组的形式
    thisDoc1=array(Words2Vec(myVocabList,testEntry1))
    print(testEntry1,'classified as:',classify_bayes(thisDoc1,p0V,p1V,pAb))
```

## 垃圾邮件过滤器

```python
def text_parser(string_inp):
    tokens = re.split(r'\W*', string_inp)
    return [tok.lower() for tok in tokens if len(tok) > 2]

def Spam_filter(filename):
    '''
    处理数据长字符串
    对长字符串进行分割，分隔符为除单词和数字之外的任意符号串
    # 将分割后的字符串中所有的大些字母变成小写lower(),并且只
    # 保留单词长度大于3的单词
    '''
    #新建三个列表
    docList = [];classList = [];fullTest = []
    #i 由1到26
    for i in range(1,26):
        #打开并读取指定目录下的本文中的长字符串，并进行处理返回
        wordList = text_parser(open(filename + '/spam/%d.txt' % i).read())
        #将得到的字符串列表添加到docList
        docList.append(wordList)
        #将字符串列表中的元素添加到fullTest
        fullTest.extend(wordList)
        #类列表添加标签1
        classList.append(1)
        #打开并取得另外一个类别为0的文件，然后进行处理
        wordList = text_parser(open(filename + '/ham/%d.txt' % i).read())
        docList.append(wordList)
        fullTest.extend(wordList)
        classList.append(0)
    #将所有邮件中出现的字符串构建成字符串列表
    vocabList=Create_wordVec(docList)
    #构建一个大小为50的整数列表和一个空列表
    trainingSet = list(range(50)); testSet = []
    #随机选取1~50中的10个数，作为索引，构建测试集
    for i in range(10):
        #随机选取1~50中的一个整型数
        randIndex=int(random.uniform(0,len(trainingSet)))
        #将选出的数的列表索引值添加到testSet列表中
        testSet.append(trainingSet[randIndex])
        #从整数列表中删除选出的数，防止下次再次选出
        #同时将剩下的作为训练集
        del(trainingSet[randIndex])
    #新建两个列表
    trainMat=[];trainClasses=[]
    #遍历训练集中的吗每个字符串列表

    for docIndex in trainingSet:
        #将字符串列表转为词条向量，然后添加到训练矩阵中
        trainMat.append(Word2Vec_bag(vocabList, docList[docIndex]))
        #将该邮件的类标签存入训练类标签列表中
        trainClasses.append(classList[docIndex])
    #计算贝叶斯函数需要的概率值并返回
    p1V,p0V,pSpam = train_bayes(array(trainMat), array(trainClasses))
    errorCount = 0
    #遍历测试集中的字符串列表
   
    for docIndex in testSet:
        #同样将测试集中的字符串列表转为词条向量
            
        wordVector = Word2Vec_bag(vocabList,docList[docIndex])
        # print(wordVector)
        #对测试集中字符串向量进行预测分类，分类结果不等于实际结果
        if classify_bayes(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount += 1
        print('the error rate is:',float(errorCount)/ len(testSet))
```

## RSS源解析

```python
#实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向
#RSS源分类器及高频词去除函数

import operator

def calMostFreq(vocabList,fullTest):
    #导入操作符
    import operator
    #创建新的字典
    freqDict={}
    #遍历词条列表中的每一个词
    for token in vocabList:
        #将单词/单词出现的次数作为键值对存入字典
        freqDict[token]=fullTest.count(token)
    #按照键值value(词条出现的次数)对字典进行排序，由大到小
    sortedFreq=sorted(freqDict.items(),key=operator.itemgetter(1),reverse=True)
    
    #返回出现次数最多的前30个单词
    return sortedFreq[:30]

def localWords(feed1,feed0):
    import feedparser
    #新建三个列表
    docList=[];classList=[];fullTest=[]
    #获取条目较少的RSS源的条目数
    minLen = 100
    #遍历每一个条目
    for i in range(minLen):
        #解析和处理获取的相应数据
        wordList = text_parser(feed1['entries'][i]['summary'])
        #添加词条列表到docList
        docList.append(wordList)
        #添加词条元素到fullTest
        fullTest.extend(wordList)
        #类标签列表添加类1
        classList.append(1)
        #同上
        wordList = text_parser(feed0['entries'][i]['summary'])
        docList.append(wordList)
        fullTest.extend(wordList)
        #此时添加类标签0
        classList.append(0)
    #构建出现的所有词条列表
    vocabList=Create_wordVec(docList)
    #找到出现的单词中频率最高的30个单词
    top30Words=calMostFreq(vocabList,fullTest)
    #遍历每一个高频词，并将其在词条列表中移除
    #这里移除高频词后错误率下降，如果继续移除结构上的辅助词
    #错误率很可能会继续下降
    for pairW in top30Words:
        if pairW[0] in vocabList:
            vocabList.remove(pairW[0])
    #下面内容与函数spaTest完全相同
    trainingSet=list(range(2*minLen));testSet=[]
    for i in range(20):
        randIndex=int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(Word2Vec_bag(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p1V,p0V,pSpam=train_bayes(array(trainMat),array(trainClasses))
    errorCount=0
    for docIndex in testSet:
        wordVector=Word2Vec_bag(vocabList,docList[docIndex])
        if classify_bayes(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount+=1 
    print('the error rate is:',float(errorCount)/len(testSet))
    return vocabList,p0V,p1V
```

```python
def getTopWords(ny,sf):
    import operator
    #利用RSS源分类器获取所有出现的词条列表，以及每个分类中每个单词出现的概率
    vocabList,p0V,p1V=localWords(ny,sf)
    #创建两个元组列表
    topNY=[];topSF=[]
    #遍历每个类中各个单词的概率值
    for i in range(len(p0V)):
        #往相应元组列表中添加概率值大于阈值的单词及其概率值组成的二元列表
        if(p0V[i]>-6.0):topSF.append((vocabList[i],p0V[i]))
        if(p1V[i]>-6.0):topNY.append((vocabList[i],p1V[i]))
    # 对列表按照每个二元列表中的概率值项进行排序，排序规则由大到小
    sortedSF=sorted(topSF,key=lambda pair:pair[1],reverse=true)
    print('SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**')
    #遍历列表中的每一个二元条目列表
    for item in sortedSF:
        #打印每个二元列表中的单词字符串元素
        print(item[0])
    #解析同上
    sortedNY=sorted(topNY,key=lambda pair:pair[1],reverse=true)
    print('SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**')
    for item in sortedNY:
        print(item[0])
```













