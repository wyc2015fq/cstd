# 正规方程与梯度下降法的选择 - Machine Learning with Peppa - CSDN博客





2018年05月25日 16:27:43[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：412








从微积分来说我们可以利用费马定理直接令方程导数为0，从而解出最优解。然而现实的例子都是很多参数的，我们需要做的就是对于这些参数都求偏导数，从而就得到各个参数的最优解，也就是全局最优解，但是困难在于，如果按照上面这么做将会非常费时间，所以有更好的办法。 
在线性回归中。为了求得參数的最优值，一般採用梯度下降和本文将要介绍的正规方程（normal equation）。相比較梯度下降採用多次迭代逼近的方式。normal equation採用矩阵运算能够直接求解出參数。先介绍下什么是normal equation，如果一个数据集X有m个样本，n个特征。则


![](http://latex.codecogs.com/gif.latex?%3C/p%3E%3Cp%3EH_%7B%5Ctheta%20%7D%28X%29%20%3D%20%5Ctheta%20_%7B0%7D%20&plus;%20%5Ctheta%20_%7B1%7Dx_%7B1%7D%20&plus;%20%5Ctheta%20_%7B2%7Dx_%7B2%7D%20&plus;...%20&plus;%20%5Ctheta%20_%7Bn%7Dx_%7Bn%7D)

![](https://img-blog.csdn.net/20160401140555820)


对于一个训练样本的所有特征参数可以用x（i）向量来表示（注意x0（i）要添加上），而设计矩阵就可以表示为X，是所有样本向量的转置，y还是观测结果的向量，这样表示之后就可以用上面那个公式直接计算出θ的最优解。


![](http://latex.codecogs.com/gif.latex?x%5E%7B%28i%29%7D)表示第i个训练样本，![](http://latex.codecogs.com/gif.latex?x%5E%7B%28i%29%7D_%7Bj%7D)表示第i个训练样本的第j个特征。之所以在X中加了第一列全为1，是为了让![](http://latex.codecogs.com/gif.latex?%5Ctheta%20_%7B0%7D*1%20%3D%20%5Ctheta%20_%7B0%7D)





若希望如果函数可以拟合Y，则![](http://latex.codecogs.com/gif.latex?H_%7B%5Ctheta%20%7D%28X%29%20%3D%20Y)。又由于 ![](http://latex.codecogs.com/gif.latex?H_%7B%5Ctheta%7D%28X%29%20%3D%20X%20*%20%5Ctheta%20%3D%20Y) ，所以可以通过矩阵运算求出參数![](http://latex.codecogs.com/gif.latex?%5Ctheta)。

熟悉线性代数的同学应该知道怎么求出參数![](http://latex.codecogs.com/gif.latex?%5Ctheta)。可是前提是矩阵X存在逆矩阵![](http://latex.codecogs.com/gif.latex?X%5E%7B-1%7D)。




介绍完normal equation求解參数，我们已经知道了两种求解參数的方法。normal equation和梯度下降。如今来对照下这两种方法的优缺点以及什么场景选择什么方法。







![](https://img-blog.csdn.net/20160401151513113)








回到上面说的![](http://latex.codecogs.com/gif.latex?%28X%20%5E%7BT%7DX%29%5E%7B-1%7D)不一定存在，这样的情况是极少存在的。假设![](http://latex.codecogs.com/gif.latex?%3C/p%3E%3Cp%3E%28X%20%5E%7BT%7DX%29%5E%7B-1%7D)不可逆了，一般要考虑一下两者情况：

（1） 移除冗余特征。一些特征存在线性依赖。

（2） 特征太多时，要删除一些特征。比如（m<n)，对于小样本数据使用正则化。






