# 机器学习：维度灾难（Curse of Dimensionality） - Machine Learning with Peppa - CSDN博客





2018年06月11日 16:08:11[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：1460








## 一.引言

这里我们将要讨论所谓的“维数灾难”，同时结合过拟合现象来解释它在分类器学习中的重要性。举一个分类应用的简单例子，假设我们有一系列的图片，每张图片的内容可能是猫也可能是狗。

我们需要构造一个分类器能够对猫、狗自动的分类。首先，要寻找到一些能够描述猫和狗的特征，这样我们的分类算法就可以利用这些特征去识别物体。猫和狗的皮毛颜色可能是一个很好的特征，考虑到红绿蓝构成图像的三基色，因此用图片三基色各自的平均值称得上方便直观。这样就有了一个简单的Fisher分类器：这里我们将要讨论所谓的“维数灾难”，同时结合过拟合现象来解释它在分类器学习中的重要性。


```python
If  0.5*red + 0.3*green + 0.2*blue > 0.6 : return cat;
    else return dog;
```

但是，使用颜色特征可能无法得到一个足够准确的分类器，如果是这样的话，我们不妨加入一些诸如图像纹理(图像灰度值在其X、Y方向的导数dx、dy)，我们就有5个特征(Red、Blue、Green、dx、dy)来设计我们的分类器了。


接下来，也许分类器准确率依然无法达到要求，我们可以加入更多的特征，比如颜色、纹理的统计信息等等，如此下去，我们也许可能会得到上百个特征。那是不是我们的分类器性能会随着特征数量的增加而逐步提高呢？答案也许有些让人沮丧，事实上，当特征数量达到一定规模后，分类器的性能是在下降的。随着维度(特征数量)的增加，分类器的性能可以用下图来描述：

![](https://img-blog.csdn.net/20180611160230981?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)




Figure 1 随着维度的增加，分类器性能逐步上升，到达某点之后，其性能便逐渐下降




接下来，我们简要介绍这种现象发生的原因，进一步讨论如何避免维数灾难的发生：

维数灾难与过拟合：在上面这个分类的例子中，我们假设猫和狗图片的数量是有限的(实际上也确实如此，样本数量总是有限的)，就假设有10张图片吧，接下来我们就用这仅有的10张图片来训练我们的分类器。

首先从一个最为简单的线性分类器开始，这里我们仅仅使用单一特征(1维)，比如红色，来进行训练

![](https://img-blog.csdn.net/20180611160312932?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


Figure 2    单一特征的分类器，在训练集上表现并不好


接下来，我们增加一个特征，比如绿色，这样特征维数扩展到了2维：

![](https://img-blog.csdn.net/20180611160348705?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)




Figure 3  增加一个特征后，我们依然无法找到一条简单的直线将它们有效分类


为此，我们再增加一个特征，比如蓝色，扩展到3维特征空间后：



![](https://img-blog.csdn.net/20140829193406292)


Figure 4  增加一个特征形成的3维特征空间及样本分布



在3维特征空间中，我们很容易找到一个分类平面，能够在训练集上有效的将猫和狗进行分类。从1维到3维，给我们的感觉是：维数越高，分类性能越优。然而，在Figure 1中，我们说维数过高将导致一定的问题：

具体来说，在一维特征空间下，我们假设一个维度的宽度为5个单位，这样样本密度为10/5=2;在2维特征空间下，10个样本所分布的空间大小5*5=25，这样样本密度为10/25=0.4;在3维特征空间下，10个样本分布的空间大小为5*5*5=125，样本密度就为10/125=0.08。
如果我们继续增加特征数量，随着维度的增加，样本将变得越来越稀疏，在这种情况下，也更容易找到一个超平面将目标分开。然而，如果我们将高维空间向低维空间投影，高维空间隐藏的问题将会显现出来：




![](https://img-blog.csdn.net/20180611160506334?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


Figure 6  过多的特征导致的过拟合现象：训练集上表现良好，但是对新数据缺乏泛化能力




高维空间训练形成的分类器，相当于在低维空间的一个复杂的非线性分类器，这种分类器过多的强调了训练集的准确率甚至于对一些错误/异常的数据。
也进行了学习，而正确的数据却无法覆盖整个特征空间。为此，这样得到的分类器在对新数据进行预测时将会出现错误。这种现象称之为过拟合，同时也是维灾难的直接体现。

下图展示了用2个特征代替三个特征进行分类器的学习：




![](https://img-blog.csdn.net/2018061116052826?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)




Figure 7  尽管训练集上分类准确率不如3维下的高，但是具备更好的泛化能力


尽管如图7中所示，一个简单的线性分类器在训练数据上的表现不如非线性分类器，但由于线性分类器的学习过程中对噪声没有对非线性分类器敏感，因此对新数据具备更优的泛化能力。换句话说，通过使用更少的特征，避免了维数灾难的发生(也即避免了高维情况下的过拟合)。
在换个角度来解释维数灾难，图8展示了由于高维而带来的数据稀疏性问题：假设有一个特征，它的取值范围D在0到1之间均匀分布，并且对狗和猫来说其值都是唯一的，我们现在利用这个特征来设计分类器。如果我们的训练数据覆盖了取值范围的20%(e.g 0到0.2)，那么所使用的训练数据就占总样本量的20%。

上升到二维情况下，覆盖二维特征空间20%的面积，则需要在每个维度上取得45%的取值范围。在三维情况下，要覆盖特征空间20%的体积，则需要在每个维度上取得58%的取值范围...在维度接近一定程度时，要取得同样的训练样本数量，则几乎要在每个维度上取得接近100%的取值范围，或者增加总样本数量，但样本数量也总是有限的。换句话说，如果一直增加特征维数，由于样本分布越来越稀疏，如果要避免过拟合的出现，就不得不持续增加样本数量。


![](https://img-blog.csdn.net/20180611160607866?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)




Figure 8  取得相同数量样本需要的空间大小







