# 在机器学习、大数据等领域工作，该学Hadoop还是Spark？ - Machine Learning with Peppa - CSDN博客





2018年03月19日 12:10:19[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：5828








                相信看这篇文章的你们，都和我一样对Hadoop和Apache Spark的选择有一定的疑惑，今天查了不少资料，我们就来谈谈这两种 平台的比较与选择吧，看看对于工作和发展，到底哪个更好。


## 一、Hadoop与Spark


### 1.Spark

Spark是一个用来实现快速而通用的集群计算的平台。速度方面，Spark扩展了广泛使用的MapReduce计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。

Spark项目包含多个紧密集成的组件。Spark的核心是一个对由很多计算任务组成的、运行在多个工作机器或者是一个计算集群上的应用进行调度、分发以及监控的计算引擎。

![](https://img-blog.csdn.net/20180319205914915?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)




### 2.Hadoop
Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。
![](http://img-blog.csdn.net/20180319120430327?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


## 二、异与同

- 解决问题的层面不一样
首先，Hadoop和Apache Spark两者都是大数据框架，但是各自存在的目的不尽相同。Hadoop实质上更多是一个分布式数据基础设施: 它将巨大的数据集分派到一个由普通计算机组成的集群中的多个节点进行存储，意味着您不需要购买和维护昂贵的服务器硬件。同时，Hadoop还会索引和跟踪这些数据，让大数据处理和分析效率达到前所未有的高度。Spark，则是那么一个专门用来对那些分布式存储的大数据进行处理的工具，它并不会进行分布式数据的存储。


- 两者可合可分
Hadoop除了提供为大家所共识的HDFS分布式数据存储功能之外，还提供了叫做MapReduce的数据处理功能。所以这里我们完全可以抛开Spark，使用Hadoop自身的MapReduce来完成数据的处理。

相反，Spark也不是非要依附在Hadoop身上才能生存。但如上所述，毕竟它没有提供文件管理系统，所以，它必须和其他的分布式文件系统进行集成才能运作。这里我们可以选择Hadoop的HDFS,也可以选择其他的基于云的数据系统平台。但Spark默认来说还是被用在Hadoop上面的，毕竟，大家都认为它们的结合是最好的。

顺带说一下什么是mapreduce：我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。
- Spark数据处理速度秒杀MapReduce
Spark因为其处理数据的方式不一样，会比MapReduce快上很多。MapReduce是分步对数据进行处理的: ”从集群中读取数据，进行一次处理，将结果写到集群，从集群中读取更新后的数据，进行下一次的处理，将结果写到集群，等等…“ Booz Allen Hamilton的数据科学家Kirk Borne如此解析。
反观Spark，它会在内存中以接近“实时”的时间完成所有的数据分析：“从集群中读取数据，完成所有必须的分析处理，将结果写回集群，完成，” Born说道。Spark的批处理速度比MapReduce快近10倍，内存中的数据分析速度则快近100倍。如果需要处理的数据和结果需求大部分情况下是静态的，且你也有耐心等待批处理的完成的话，MapReduce的处理方式也是完全可以接受的。
但如果你需要对流数据进行分析，比如那些来自于工厂的传感器收集回来的数据，又或者说你的应用是需要多重数据处理的，那么你也许更应该使用Spark进行处理。大部分机器学习算法都是需要多重数据处理的。此外，通常会用到Spark的应用场景有以下方面：实时的市场活动，在线产品推荐，网络安全分析，机器日记监控等。

- Recovery & 恢复
两者的灾难恢复方式迥异，但是都很不错。因为Hadoop将每次处理后的数据都写入到磁盘上，所以其天生就能很有弹性的对系统错误进行处理。Spark的数据对象存储在分布于数据集群中的叫做弹性分布式数据集(RDD: Resilient Distributed Dataset)中。“这些数据对象既可以放在内存，也可以放在磁盘，所以RDD同样也可以提供完成的灾难恢复功能”



## 三、学哪个？

其实，正如所了解的那样，Spark的确是大数据行业中的后起之秀，与Hadoop相比，Spark有很多的优势。Hadoop之所以在大数据行业能够得到充分的认同主要是因为：

·Hadoop解决了大数据的可靠存储和处理问题；
·Hadoop的开源性，这能让很多大数据从业人员在里面找到灵感，方便实用；
·Hadoop经过了多年的开发，拥有完整的生态系统。
·HDFS在由普通PC组成的集群上提供高可靠的文件存储，通过将块保存多个副本的办法解决服务器或硬板坏掉的问题。
·MapReduce通过简单的Mapper和Reducer的抽象提供一个变成模型，可以在一个由几十台至上百台的PC组成的不可靠集群上并发地，分布式地处理大量的数据集，而把并发、分布式和故障恢复等计算细节隐藏起来。

Hadoop也有许多局限和不足，笼统的讲，在数据量不断扩大的情况下，Hadoop的运算速度会越发显得吃力。虽然现阶段，Hadoop在大数据行业内仍然有很高频率的应用，但不难想象在若干年后，数据量又上升几个数量级时，Hadoop所面临的窘境。而Spark的运算速度是Hadoop的百分之一甚至更快，因此，在未来，Spark必然会取代Hadoop，主宰大数据行业。

那是不是就可以跳过Hadoop，只学Spark呢？当然不是，有以下原因：

·现阶段，Hadoop仍然主导着大数据领域，我们可以学习先进的技术，但更是为了现阶段的就业，就目前阶段而言，学大数据必学Hadoop。
·MapReduce中有许多经典的思想，值得我们学习，这对我们理解大数据十分有帮助。
·确切的讲，Spark要替换的是Hadoop中的MapReduce，而不是Hadoop，Hadoop是一个工具包，而Spark和MapReduce一样，只是一种工具而已。

**结论：**

如果你是往业界的算法工程方面发展，那么两个都要学，Hadoop要了解，Spark要熟悉。如果你是大数据研究人员，那么要精通这两种。所以，这里的建议是，对于有志于在ML和大数据等领域发展的各位，可以按照Java -> Hadoop -> Spark这样的路径，如果你有C++和SQL的基础，那么学习曲线将不会特别陡峭，对于spark来说，学一点Scala则会更有帮助

最后推荐一个答案，下面有很多干货，帮助大家构建自己的学习路径：

# [零基础学习 Hadoop 该如何下手？](https://www.zhihu.com/question/19795366)




