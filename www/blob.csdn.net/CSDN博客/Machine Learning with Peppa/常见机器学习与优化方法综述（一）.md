# 常见机器学习与优化方法综述（一） - Machine Learning with Peppa - CSDN博客





2018年04月19日 17:26:57[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：571








**1.Introduction**

随着大数据时代到来，尽管计算机硬件条件的改善，对于机器学习算法效率的要求并不会降低，而机器学习算法效率更多地依赖于数值优化方法的改进，因而有必要对近年来关于大规模机器学习中的优化算法做一个总结，以便更好地理清思路，确定未来算法的改进方向。


本文尝试解答以下问题：


1.  优化问题怎样在机器学习应用产生，以及面对怎样的挑战？


2.  在大规模数据应用中最广泛使用的优化方法？


3.  近年来关于优化方法设计上的进步，及在此领域的公开问题？


通过机器学习中的两个例子尝试解答第一个问题，文本分类问题和深度神经网络感知问题；通过介绍随机梯度法的基本定理以及实际应用来说明第二个问题；第三步主要介绍noise reduction method 和 the second-order method 以及关于正则模型的方法来说明第三个问题

**2. machine learning case studies**

2.1 文本分类


确定文本类别是自然语言处理的一个基本问题，比如我正在写的这篇文章应该归到哪个类别。显然人工分类对于小样本文本非常有效，但是数量激增的话，远非人力所能处理，基于自然语言处理方面的知识，我们可以用向量来表示一篇文档，那么问题就转化为基本的机器学习分类问题。


比如n个样本{(x1,y1),(x2,y2),...,(xn,yn)}，xi 表示第i个文档向量，yi表示第i个文档所属类别。通过样本数据我们学习一个预测函数h，来对未分类样本进行预测，那么我们用经验风险误差Rn(h)来评价预测学习的好坏

**![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629124336062-1872602544.png)**


通常我们可以将预测函数表示成 ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629124520156-256760395.png)  w表示h需要学习的参数，t表示一个偏置项，同时我们需要一个合适的损失函数，比如![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629125046593-170801296.png)来替换(2.1)中的**1**，这样问题就变成一个比较纯粹的优化问题
![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629125211515-544128356.png)，其中右边那项为正则项，防止模型太复杂过拟合。


2.2 深度神经网络


深度神经网络受启发于生物学上的神经学，希望通过计算机来模拟人脑的学习过程。




 2.3 总的来说


问题描述：一个训练集 ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629131649781-1997025515.png),   损失函数 ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629131750749-1834943562.png),
 (可以为逻辑损失，合页损失等等) ，预测函数h(w;x)

要解决 ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629132055562-107968468.png)，记  ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629132414202-648452317.png)，
  实际应用一般为经验风险![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629132624359-1908138153.png)，理论分析使用期望风险![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629132705656-1853487797.png)，通常经验风险容易过拟合，需要加入正则项来泛化模型，即常用的结构风险最小化。即(2.3)所示。
**3. overview of optimization methods**

3.1 stochastic gradient method(SG)


首先表示经验风险，![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629134040577-517354351.png)，f是l与h的复合函数，那么w的更新公式为![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629134351171-167856667.png)

可以看出SG方法每次迭代只需计算一个梯度值，随机过程依赖于i_k的选取，不是梯度下降法，在期望上是下降的。


3.2 batch optimization methods


w的更新公式为                ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629134712890-1671483247.png)


每次迭代需计算的数量与n成比例，每次迭代代价比SG要高，可以考虑并行化，并且有很多以此衍生的算法。


 3.3  motivation for stochastic methods


直观上看：  batch的数量增加，SG的还是一样，对于n非常大，SG比较有利。


实践：

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629135407593-1200301449.png) SG初始收敛速度非常快


例子

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629135837249-546651980.png)  当SG迭代到w*附近时，就不太确定继续迭代方向，这时速度就会变得非常慢。


3.4 Theoretical motivation


batch 方法在满足一定假设下可以达到线性收敛（几何收敛），每次迭代代价正比于n

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629140343687-700811378.png)


SG可以同时实现R和Rn 次线性收敛 ， 每次迭代与n无关

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629140626031-1179782428.png) ，![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629140654765-2088286248.png)

                                        两者总的复杂度                      ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160629140858999-1622558437.png)


    当 R(w)-R(w*)<e 

**4. Analyses of Stochastic Gradient Methods**


不失一般性，将期望风险R(w)和经验风险Rn(w)的目标函数表示如下

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627154008359-1251584643.png)

本节主要讨论SG算法的收敛性及迭代上界

**![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627153530124-1865535576.png)**




以上算法就称之为SG， g主要为三种形式

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627155649031-460749474.png)


4.1 两个基本引理


通常SG的收敛性证明，需要目标函数F的光滑性、假设F的梯度利普希茨连续，

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627154914374-1613963831.png)


根据这个猜想得出一个重要的不等式

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627155039077-1463440856.png)


由算法4.1知道 w(k+1)依赖于 ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630103255062-395711184.png)  ，{![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630103403874-1937864876.png)}可以看成随机种子，![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630103935312-985172598.png)可以看成依赖于w(k)的分布，每次根据随机种子来挑选g，
![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627160108859-1519060607.png)


对不等式取期望就得到(4.4) ，不等式左边第一项可下降，而第二项相当于噪音，干扰收敛。

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627162350859-399237769.png)


猜想2

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627162439921-970614118.png)



![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627162522390-948651591.png)


引理2

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627162805484-1175447299.png)


SG方法的收敛依赖于不等式右边两项的协调。




4.2  SG for Strong Convex Objectives


强凸目标函数在一些优化文献里常常被讨论，不仅由于函此能得到较强的结论，实际实践也会遇到(如机器学习中的正则项)


强凸函数定义

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627165709593-1066409455.png)


如意得到

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627172418046-1624268620.png)


其中c<=L


定步长的收敛定理

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627172937327-1880247292.png)


当M=0时对于batch梯度法有线性收敛速度。M>0时收敛，存在噪声阻止收敛到最优解。

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630110128765-1859894473.png)






减小步长收敛定理4.7

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627174355562-19831634.png)


考虑mini-batch与  single example 的比较

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630110646562-787959820.png)


虽然batch每次迭代代价比single 大，但是可以使用较大步长，收敛较快。


4.3 SG for General Objectives

非凸函数可能有多个局部最优解，要达到全局最优，相对较难。


定步长

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160627182418999-1082883289.png)


步长减小

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630111448702-939596448.png)


4.4 work complexity for large-scale learning


数据越大，往往能更好拟合数据，且不会过拟合，然而需要更多的训练时间。


定性分析，  ![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630113750531-144498683.png)是经验风险最小值点的逼近，花H是预测函数族，app--approximation error,  est--estimation error,  opt--optimization error

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630113207390-923454099.png)




 那么逼近误差， Tmax 为时间预算

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630114144156-303141739.png)


经过分析  T（n,e) 表示迭代次数， e*表示所能达到的最小逼近误差

![](https://images2015.cnblogs.com/blog/794637/201606/794637-20160630114314515-2000157133.png)


4.5 commentary


1 SG在渐近收敛是速度非常慢，对步长要求严格


2  

**5. noise reduction methods**

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701120004406-457715746.png)从SG方法出发可以衍生出多种方法，从不同方面来提升SG的稳定性及效率。


根据上面提到的噪声，本节主要介绍减小噪声的方法，来提高收敛速度。


主要3种方法 dynamic sampling，gradient aggregation，iterate averaging .


1. dynamic sampling


首先是前面的假设：

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701195918187-721781700.png)


有定理

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701200429343-1180624923.png)


要使得![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701200558468-900780845.png)成立，  令

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701200409265-35275222.png)


就有

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701200729593-1995660133.png)


可以看到每次迭代的样本大小是增加的，总的复杂度

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701201253124-1451148500.png)  与SG的复杂度是一样的，为 O(1/e),  特别的取 ![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701201555859-697088838.png)

实际上该方法不常采用。


2. gradient  aggregation


 主要是对梯度计算的处理来纠正梯度方向。 简介两种方法


SVRG  (stochastic variance reduced gradient) 在内循环里随机抽取梯度，纠正偏差。 g(~)是  Rn梯度的无偏估计。

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701203141687-1993724312.png)


SAGA (stochastic average gradient) 

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701205458015-1042095338.png)


同样是线性收敛，g是Rn梯度的无偏估计。


3. iterative averaging

![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701212002609-346847129.png)


步长一般取下降速度为![](https://images2015.cnblogs.com/blog/794637/201607/794637-20160701212130765-981622215.png)， 次线性收敛速度。











