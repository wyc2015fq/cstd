# 机器学习实战系列（二）：决策树算法 - Machine Learning with Peppa - CSDN博客





2018年07月05日 17:05:10[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：116








这个月开始练习《机器学习实战》，原书比较久远了，且代码和练习都是基于Python2，我个人是升级到了Python3，因此使用最新的版本来写这些习题。具体2和3其实在基础语法上并没有太多差别，一些高级特性比如装饰器工厂，协程，IO等Python3的新用法，一般机器学习也用不上，因为追求性能的话都会用C/C++等语言去实现，Python只是小规模的测试用。

课程数据和代码也放在我的Github：[Machine learning in Action](https://github.com/Y1ran/Machine-Learning-in-Action-Python3)，目前刚开始做，有不对的欢迎指正，也欢迎大家star。除了 版本差异，代码里的部分函数以及代码范式也和原书不一样（因为作者的代码实在让人看的别扭，我改过后看起来舒服多了）。在这个系列之后，我还会写一个scikit-learn机器学习系列，因为在实现了源码之后，带大家看看SKT框架如何使用也是非常重要的。    

## 决策树算法的工作原理：

    上一章我们讲的KNN算法，虽然可以完成很多分类任务，但它最大的缺点是无法给出数据的内在含义，而决策树的主要优势就在于数据形式非常容易理解。决策树算法能够读取数据集合，决策树的一个重要任务是为了数据所蕴含的知识信息，因此，决策树可以使用不熟悉的数据集合，并从中提取一系列规则，在这些机器根据数据集创建规则是，就是机器学习的过程。   

    在构造决策树时，第一个需要解决的问题就是，如何确定出哪个特征在划分数据分类是起决定性作用，或者说使用哪个特征分类能实现最好的分类效果。这样，为了找到决定性的特征，划分川最好的结果，我们就需要评估每个特征。当找到最优特征后，依此特征，数据集就被划分为几个数据子集，这些数据自己会分布在该决策点的所有分支中。此时，如果某个分支下的数据属于同一类型，则该分支下的数据分类已经完成，无需进行下一步的数据集分类；如果分支下的数据子集内数据不属于同一类型，那么就要重复划分该数据集的过程，按照划分原始数据集相同的原则，确定出该数据子集中的最优特征，继续对数据子集进行分类，直到所有的特征已经遍历完成，或者所有叶结点分支下的数据具有相同的分类。

     创建分支的伪代码函数createBranch（）如下：

```python
if so return 类标签;
else
      寻找划分数据集的最好特征
      划分数据集
      创建分支结点
            for 每个分支结点
                 调用函数createBranch并增加返回结点到分支结点中//递归调用createBranch（）
      return 分支结点
```

检测数据集中的每一个子项是否属于同一分类。了解了如何划分数据集后，我们可以总结出决策树的一般流程：

（1）收集数据

（2）准备数据：构造树算法只适用于标称型数据，因此数值型数据必须离散化

（3）分析数据

（4）训练数据：上述的构造树过程构造决策树的数据结构

（5）测试算法：使用经验树计算错误率



（6）使用算法：在实际中更好地理解数据内在含义 





## 信息增益

     划分数据集的大原则是：将无序的数据变得更加有序。在划分数据集前后信息发生的变化称为信息增益，如果我们知道如何计算信息增益，就可以计算每个特征值划分数据集获得的信息增益，而获取信息增益最高的特征就是最好的特征。接下来，我们讲学习如何计算信息增益，而提到信息增益我们又不得不提到一个概念"香农熵"，或者简称熵。

熵定义为信息的期望值。 熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事务可能划分在多个分类之中，则符号xi的信息定义为

![](https://img-blog.csdn.net/20170721163243559?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中p(xi)是选择该分类的概率。有人可能会问，信息为啥这样定义啊？答曰：前辈得出的结论。这就跟1+1等于2一样，记住并且会用即可。上述式中的对数以2为底，也可以e为底(自然对数)。通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到：

![](https://img-blog.csdn.net/20170721163315976?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    期中n是分类的数目。熵越大，随机变量的不确定性就越大。当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的解释就是，这概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck，k = 1,2,3,···,K，|Ck|为属于类Ck的样本个数，这经验熵公式可以写为：

![](https://img-blog.csdn.net/20170721163342519?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)




## 计算熵

```python
from math import log
from numpy import *

def cal_entropy(data):
    '''计算样本实例的熵'''
    entries_num = len(data)
    label_count = {} #字典存储每个类别出现的次数
 
    for vec in data:
        cur_label = vec[-1] 
    # 将样本标签提取出来，并计数
        label_count[cur_label] = label_count.get(cur_label,0) + 1
    Entropy = 0.0
    # 对每一个类别，计算样本中取到该类的概率
    # 最后将概率带入，求出熵
    for key in label_count:
        prob = float(label_count[key]) / entries_num
        Entropy += prob * math.log(prob, 2) #此处使用numpy.math
    return (0-Entropy)
```

## 使用熵选择最优特征并进行分割

```python
from Cal_Entropy import *

def Split_Data(dataset, axis, value):
    '''
    使用传入的axis以及value划分数据集
    axis代表在每个列表中的第X位，value为用来划分的特征值
    '''
    new_subset = []
    # 利用循环将不符合value的特征值划分入另一集合
    # 相当于将value单独提取出来（或作为叶节点）
    for vec in dataset:
        if vec[axis] == value:
            feature_split = vec[:axis]
            feature_split.extend(vec[axis + 1:])
            new_subset.append(feature_split)
    # extend将VEC中的元素一一纳入feature_split
    # append则将feature_split作为列表结合进目标集合
            
    return new_subset

def Split_by_entropy(dataset):
    '''
    使用熵原则进行数据集划分
    @信息增益:info_gain = old -new
    @最优特征：best_feature
    @类别集合：uniVal
    '''
    feature_num = len(dataset[0]) - 1
    ent_old = cal_entropy(dataset)
    best_gain = 0.0
    best_feature = -1
    # ENT_OLD代表划分前集合的熵，ENT_NEW代表划分后的熵
    # best_gain将在迭代每一次特征的时候更新，最终选出最优特征
    for i in range(feature_num):
        feature_list = [x[i] for x in dataset]
        uniVal = set(feature_list)
        ent_new = 0.0
        # 使用set剔除重复项，保留该特征对应的不同取值
        for value in uniVal:
            sub_set = Split_Data(dataset, i, value)
            prob = len(sub_set) / float(len(dataset))
            # 使用熵计算函数求出划分后的熵值
            ent_new += prob * (0 - cal_entropy(sub_set))
        
        # 由ent_old - ent_new选出划分对应的最优特征
        Info_gain = ent_old - ent_new
        if(Info_gain > best_gain):
            best_gain = Info_gain
            best_feature = i
            
    return best_feature
```



## 多数表决法以及树的生成

```python
import operator
from Split_by_entropy import *

def Majority_vote(classList):
    '''
    使用多数表决法：若集合中属于第K类的节点最多，则此分支集合
            划分为第K类
    '''
    classcount = {}
    for vote in classList:
        classcount[vote] = classcount.get(vote,0) + 1
    sorted_count = sorted(classcount.items(), key = operator.itemgetter(1),\
                          reverse = True)
    # 获取每一类出现的节点数（没出现默认为0）并进行排序
    # 返回最大项的KEY所对应的类别
    return sorted_count[0][0]

def Create_Tree(dataset,labels):

    classList = [x[-1] for x in dataset]
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    #
    if len(dataset[0]) == 1:
        return Majority_vote(classList)
    
    best_feature = Split_by_entropy(dataset)
    best_labels = labels[best_feature]
    
    myTree = {best_labels:{}}
    # 此位置书上写的有误，书上为del(labels[bestFeat])
    # 相当于操作原始列表内容，导致原始列表内容发生改变
    # 按此运行程序，报错'no surfacing'is not in list
    # 以下代码已改正
    
    # 复制当前特征标签列表，防止改变原始列表的内容
    subLabels=labels[:]
    # 删除属性列表中当前分类数据集特征
    del(subLabels[best_feature])

    # 使用列表推导式生成该特征对应的列
    f_val = [x[best_feature] for x in dataset]
    uni_val = set(f_val)
    for value in uni_val:
        # 递归创建子树并返回
        myTree[best_labels][value] = Create_Tree(Split_Data(dataset\
              ,best_feature,value), subLabels)
    
    return myTree
```

## 树的可视化

```python
import matplotlib.pyplot as plt

# 定义文本框和箭头格式
decisionNode = dict(boxstyle="sawtooth", fc="0.8")
leafNode = dict(boxstyle="round4", fc="0.8")
arrow_args = dict(arrowstyle="<-")
 
# 绘制带箭头的注释
def plotNode(nodeTxt, centerPt, parentPt, nodeType):
     createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',
              xytext=centerPt, textcoords='axes fraction',
              va="center", ha="center", bbox=nodeType, arrowprops=arrow_args )
 
     
def Num_of_leaf(myTree):
    '''计算此树的叶子节点数目'''
    num_leaf = 0
    first_node = myTree.keys()
    first_node = list(first_node)[0]
    second_dict = myTree[first_node]
    # Python3中使用LIST转换firstnode，原书使用[0]直接索引只能用于Python2
    # 对于树，每次判断value是否为字典，若为字典则进行递归，否则累加器+1
    for key in second_dict.keys():
        if type(second_dict[key]).__name__ =='dict':
            num_leaf += Num_of_leaf(second_dict[key])
        else: num_leaf += 1
    return num_leaf

def Depth_of_tree(myTree):
    '''计算此树的总深度'''
    depth = 0
    first_node = myTree.keys()
    first_node = list(first_node)[0]
    second_dict = myTree[first_node]
    
    for key in second_dict.keys():
        if type(second_dict[key]).__name__ =='dict':
            pri_depth = 1 + Depth_of_tree(second_dict[key])
        else: pri_depth = 1
        # 对于树，每次判断value是否为字典，若为字典则进行递归，否则计数器+1
        if pri_depth > depth: depth = pri_depth
    return depth

def retrieveTree(i):
    '''
   保存了树的测试数据
     '''
    listOfTrees =[{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', \
                                1: 'yes'}}}},{'no surfacing': {0: 'no', \
    1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}
                  ]
    return listOfTrees[i]

def plotmidtext(cntrpt,parentpt,txtstring):
    '''作用是计算tree的中间位置    
    cntrpt起始位置,parentpt终止位置,txtstring：文本标签信息
    '''
    xmid=(parentpt[0]-cntrpt[0])/2.0+cntrpt[0]
    # cntrPt 起点坐标 子节点坐标   
    # parentPt 结束坐标 父节点坐标
    ymid=(parentpt[1]-cntrpt[1])/2.0+cntrpt[1] # 找到x和y的中间位置
    createPlot.ax1.text(xmid,ymid,txtstring)
    
    
def plottree(mytree,parentpt,nodetxt):
    numleafs=Num_of_leaf(mytree)
    depth=Depth_of_tree(mytree)
    firststr=list(mytree.keys())[0]
    cntrpt=(plottree.xoff+(1.0+float(numleafs))/2.0/plottree.totalw,plottree.yoff)
    # 计算子节点的坐标 
    plotmidtext(cntrpt,parentpt,nodetxt) #绘制线上的文字  
    plotNode(firststr,cntrpt,parentpt,decisionNode)#绘制节点  
    seconddict=mytree[firststr]
    plottree.yoff=plottree.yoff-1.0/plottree.totald
    # 每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的
    for key in seconddict.keys():
        if type(seconddict[key]).__name__=='dict':
            plottree(seconddict[key],cntrpt,str(key))
        else:
            plottree.xoff=plottree.xoff+1.0/plottree.totalw
            plotNode(seconddict[key],(plottree.xoff,plottree.yoff),cntrpt,leafNode)
            plotmidtext((plottree.xoff,plottree.yoff),cntrpt,str(key))
    plottree.yoff=plottree.yoff+1.0/plottree.totald
 
    
def createPlot(intree):
    # 类似于Matlab的figure，定义一个画布(暂且这么称呼吧)，背景为白色 
    fig=plt.figure(1,facecolor='white')
    fig.clf()    # 把画布清空 
    axprops=dict(xticks=[],yticks=[])   
    # createPlot.ax1为全局变量，绘制图像的句柄，subplot为定义了一个绘图，
    # 111表示figure中的图有1行1列，即1个，最后的1代表第一个图 
    # frameon表示是否绘制坐标轴矩形 
    createPlot.ax1=plt.subplot(111,frameon=False,**axprops) 
    
    plottree.totalw=float(Num_of_leaf(intree))
    plottree.totald=float(Depth_of_tree(intree))
    plottree.xoff=-0.6/plottree.totalw;plottree.yoff=1.2;
    plottree(intree,(0.5,1.0),'')
    plt.show()
```

## 决策树分类以及序列化存储

```python
def classify(inp_tree, labels, test_vec):
    first_node = list(inp_tree.keys())[0]
    second_dict = inp_tree[first_node]
    index = labels.index(first_node)
    
    for key in second_dict.keys():
        if test_vec[index] == key:
            if type(second_dict[key]).__name__ == 'dict':
                class_label = classify(second_dict[key], labels, test_vec)
            else:   class_label = second_dict[key]
    return class_label

def store_tree(inp_tree, filename):
    import pickle
    with open(filename,'w') as fp:
        pickle.dump(inp_tree, fp)
    
def grab_tree(filename):
    import pickle
    fr = open(filename)
    return pickle.load(fr)
```

## 测试

```python
def create_data():
    dataSet = [[1,1,'yes'],
               [1,1,'yes'],
               [1,0,'no'],
               [0,1,'no'],
               [0,1,'no']]
    labels = ['no surfacing', 'flippers']
    return dataSet, labels

if __name__ == '__main__':
    myData, labels = create_data()
    print(myData)
    print(cal_entropy(myData))
    
    print(Split_Data(myData,0,1))
    print(Split_by_entropy(myData))
    
    mytree = Create_Tree(myData, labels)
    print(mytree)
    
    myTree = retrieveTree(0)
    print(Num_of_leaf(myTree), Depth_of_tree(myTree))
    myTree['no surfacing'][3] = 'maybe'
    createPlot(myTree)

    with open('lenses.txt') as fp:
        lenses = [line.strip().split('\t') for line in fp.readlines()]
        lensesLabels=['age','prescript','astigmatic','tearRate']
    
    lense_Tree = Create_Tree(lenses, lensesLabels)
    #createPlot(lense_Tree)
    #print(classify(lense_Tree, lensesLabels, ['young','hyper','yes','reducedno']))
```

## 输出数据

```python
[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
0.9709505944546686
[[1, 'yes'], [1, 'yes'], [0, 'no']]
1
{'flippers': {0: 'no', 1: {'no surfacing': {0: 'no', 1: 'yes'}}}}
3 2
```

![](https://img-blog.csdn.net/20180705171118188?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NTIxNTU0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



