# 一个有趣的说法：多层神经网络的致命问题与过拟合 - Machine Learning with Peppa - CSDN博客





2018年08月14日 19:17:02[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：122








Bengio在[Learning Deep Architectures for AI](http://svn.ucc.asn.au:8080/oxinabox/Uni%20Notes/honours/refTesting/Citavi/DeepBeliefSystems/CitaviFiles/RecycleBin/Theano%20Development%20Team%202013%20-%20DeepLearning%2001%20documentation.pdf) 一书中举了一个有趣的例子。他说：最近有人表示，他们用传统的深度神经网络把训练error降到了0，也没有用你的那个什么破Pre-Training嘛！

然后Bengio自己试了一下，发现确实可以，但是是建立在把接近输出层的顶隐层神经元个数设的很大的情况下。于是他把顶隐层神经元个数限到了20，然后这个模型立马露出马脚了。无论是训练误差、还是测试误差，都比相同配置下的Pre-Training方法差许多。

也就是说，顶层神经元在对输入数据直接点对点记忆，而不是提取出有效特征后再记忆。这就是神经网络的最后一个致命问题：过拟合，庞大的结构和参数使得，尽管训练error降的很低，但是test error却高的离谱。



### 过拟合还可以和Gradient Vanish、局部最小值混合三打，具体玩法是这样的：
- 
由于Gradient Vanish，导致深度结构的较低层几乎无法训练，而较高层却非常容易训练。

- 
较低层由于无法训练，很容易把原始输入信息，没有经过任何非线性变换，或者错误变换推到高层去，使得高层解离特征压力太大。

- 
如果特征无法解离，强制性的误差监督训练就会使得模型对输入数据直接做拟合。


其结果就是，A Good Optimation But a Poor Generalization，这也是SVM、决策树等浅层结构的毛病。

Bengio指出，这些利用局部数据做优化的浅层结构基于先验知识（Prior）: *Smoothness*。即，给定样本(xi,yi)(xi,yi)，尽可能从数值上做优化，使得训练出来的模型，对于近似的x，输出近似的y。

然而一旦输入值做了泛型迁移，比如两种不同的鸟，鸟的颜色有别，且在图像中的比例不一，那么SVM、决策树几乎毫无用处。

因为，对输入数据简单地做数值化学习，而不是解离出特征，对于高维数据（如图像、声音、文本），是毫无意义的。然后就是最后的事了，由于低层学不动，高层在乱学，所以很快就掉进了吸引盆中，完成神经网络三杀。



