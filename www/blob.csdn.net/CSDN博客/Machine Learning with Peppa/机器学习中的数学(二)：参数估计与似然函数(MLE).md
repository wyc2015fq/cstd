# 机器学习中的数学(二)：参数估计与似然函数(MLE) - Machine Learning with Peppa - CSDN博客





2018年02月18日 22:20:52[刺客五六柒](https://me.csdn.net/qq_39521554)阅读数：342
所属专栏：[机器学习与数据挖掘](https://blog.csdn.net/column/details/18961.html)









在机器学习中，我们经常使用一个模型来描述生成观察数据的过程。例如，我们可以使用一个随机森林模型来分类客户是否会取消订阅服务（称为流失建模），或者我们可以用线性模型根据公司的广告支出来预测公司的收入（这是一个线性回归的例子）。每个模型都包含自己的一组参数，这些参数最终定义了模型本身。

我们可以把线性模型写成 y = mx + c 的形式。在广告预测收入的例子中，x 可以表示广告支出，y 是产生的收入。m 和 c 则是这个模型的参数。这些参数的不同值将在坐标平面上给出不同的直线（见下图）。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/eeb78ffc99a14b0b8f0833061fe9b274.jpeg)

参数值不同的三个线性模型。

因此，参数为模型定义了一个蓝图。只有将参数选定为特定值时，才会给出一个描述给定现象的模型实例。

最大似然估计的直观解释

最大似然估计是一种确定模型参数值的方法。确定参数值的过程，是找到能最大化模型产生真实观察数据可能性的那一组参数。

上述的定义可能听起来还是有点模糊，那么让我们通过一个例子来帮助理解。

假设我们从某个过程中观察了 10 个数据点。例如，每个数据点可以代表一个学生回答特定考试问题的时间长度（以秒为单位）。这 10 个数据点如下图所示：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/58ab734022a541528aeea309ed490bc5.png)

我们观察到的 10 个（假设的）数据点。

我们首先要决定哪个模型最适合描述生成数据的过程，这一步至关重要。至少，我们应该对使用哪种模型有一个不错的想法。这个判断通常来自于一些领域内专家，但我们不在这里讨论这个问题。

对于这些数据，我们假设数据生成过程可以用高斯分布（正态分布）进行充分描述。对以上数值目测一番就可以得知，高斯分布是合理的，因为这 10 个点的大部分都集中在中间，而左边和右边的点都很少。（因为我们只使用了 10 个数据点，做出这样的草率决定是不明智的，但考虑到我是用某个确定的分布函数生成这些数据点，我们就凑合着用吧）。

回想一下高斯分布有两个参数：均值μ和标准差σ。这些参数的不同值会对应不同的曲线（就像上面的直线一样）。我们想知道「哪条曲线最可能产生我们观察到的数据点」？（见下图）。用最大似然估计法，我们会找到与数据拟合得最好的 μ、σ 的值。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/4b1283ce7add4876b3380da3e47fcfdd.jpeg)

10 个数据点和可能得出这些数据的高斯分布。f_1 是均值为 10、方差为 2.25（方差等于标准偏差的平方）的正态分布，也可以表示为 f_1∼N(10, 2.25)。其它曲线为 f_2∼N(10, 9)、f_3∼N(10, 0.25)、f_4∼N(8,2.25)。最大似然的目标是找到最有可能生成已知观察数据分布的参数值。

我生成这 10 个数据的真实分布是 f_1~N(10, 2.25)，也就是上图中的蓝色曲线。

计算最大似然估计

现在我们对最大似然估计有了直观的理解，我们可以继续学习如何计算参数值了。我们找到的参数值被称为最大似然估计（maximum likelihood estimates，MLE）。

我们同样将用一个例子来演示这个过程。假设这次有三个数据点，我们假设它们是从一个被高斯分布充分描述的过程生成的。这些点是 9、9.5 和 11。那么如何用最大似然估计逼近这个高斯分布的参数 μ 和 σ 呢?

我们要计算的是同时观察到所有这些数据的概率，也就是所有观测数据点的联合概率分布。因此，我们需要计算一些可能很难算出来的条件概率。我们将在这里做出第一个假设，假设每个数据点都是独立于其他数据点生成的。这个假设能让计算更容易些。如果事件（即生成数据的过程）是独立的，那么观察所有数据的总概率就是单独观察到每个数据点的概率的乘积（即边缘概率的乘积）。

从高斯分布中生成的单个数据点 x 的（边缘）概率是：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/507e5ff22f9444068ca7d42424462fb6.png)

在表达式 P(x; μ, σ) 中的分号是为了强调在分号后的符号都是概率分布的参数。所以千万不要把这个与条件概率相混淆。条件概率一般会用竖线来表达，比如说 P(A| B)。

在我们的例子中，同时观察到这三个数据点的总（联合）概率是：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/11973e71b3de434094e8fb9dde499a6b.jpeg)

我们只要找出能够让上述表达式最大化的μ、σ值就可以了。

如果你在数学课上学过微积分，那么你可能会意识到有一种技巧可以帮助我们找到函数的最大值（和最小值）。我们所要做的就是求出函数的导数，把导函数设为零然后重新变换方程，使其参数成为方程的未知数。然后就这样，我们将得到参数的 MLE 值。我将串讲一下这些步骤，但我假设读者知道如何对常用函数进行微分。

对数似然函数

上述的总概率表达式实际上是很难微分，所以它几乎总是通过对表达式取自然对数进行简化。这完全没问题，因为自然对数是一个单调递增的函数。这意味着，如果 x 轴上的值增加，y 轴上的值也会增加（见下图）。这一点很重要，因为它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，我们可以用更简单的对数概率来代替原来的概率。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/064c1c53f0994fe0af3b112cfbe43d69.jpeg)

原函数的单调性，左边是 y = x，右边是（自然）对数函数 y = ln(x)。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/7992108cb2e148adaff1045f984ec35c.jpeg)

这是一个非单调函数的例子，因为从左至右 f(x) 会上升，然后下降，然后又上升。

取初始表达式的对数能得到：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/ca8f78744bf64c32b9f858c912db42f8.png)

我们可以用对数的运算法则再一次简化这个表达式，得到：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/4fcbc2d9d6b14b91bb693f31084bba4b.png)

这个表达式可以通过求导得到最大值。在这个例子中，我们要找到平均值 μ。为此我们对函数求 μ 的偏导数，得到：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/f6699c4eee114ee3b48f223a28b6fc6e.png)

最后，设置等式的左边为零，然后以μ为未知数整理式子，可以得到：

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/d599aaf9863f41c4b77a387fe2d88572.png)

这样我们就得到了 μ 的最大似然估计。我们可以用同样的方法得到 σ 的最大似然估计，这留给有兴趣的读者自己练习。

最大似然估计小结

最大似然估计总是能精确地得到解吗？

简单来说，不能。更有可能的是，在真实的场景中，对数似然函数的导数仍然是难以解析的（也就是说，很难甚至不可能人工对函数求微分）。因此，一般采用期望最大化（EM）算法等迭代方法为参数估计找到数值解，但总体思路还是一样的。

为什么叫「最大似然（最大可能性）」，而不是「最大概率」呢？

好吧，这只是统计学家们卖弄学问（但也是有充分的理由）。大多数人倾向于混用「概率」和「似然度」这两个名词，但统计学家和概率理论家都会区分这两个概念。通过观察这个等式，我们可以更好地明确这种混淆的原因。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/a8952d1f29664acd9ae36130a189ed9e.png)

这两个表达式是相等的！所以这是什么意思？我们先来定义 P(data; μ, σ) 它的意思是「在模型参数μ、σ条件下，观察到数据 data 的概率」。值得注意的是，我们可以将其推广到任意数量的参数和任何分布。

另一方面，L(μ, σ; data) 的意思是「我们在观察到一组数据 data 之后，参数 μ、σ 取特定的值的似然度。」

上面的公式表示，给定参数后数据的概率等于给定数据后参数的似然度。但是，尽管这两个值是相等的，但是似然度和概率从根本上是提出了两个不同的问题——一个是关于数据的，另一个是关于参数值的。这就是为什么这种方法被称为最大似然法（极大可能性），而不是最大概率。

什么时候最小二乘参数估计和最大似然估计结果相同？

最小二乘法是另一种常用的机器学习模型参数估计方法。结果表明，当模型向上述例子中一样被假设为高斯分布时，MLE 的估计等价于最小二乘法。

直觉上，我们可以通过理解两种方法的目的来解释这两种方法之间的联系。对于最小二乘参数估计，我们想要找到最小化数据点和回归线之间距离平方之和的直线（见下图）。在最大似然估计中，我们想要最大化数据同时出现的总概率。当待求分布被假设为高斯分布时，最大概率会在数据点接近平均值时找到。由于高斯分布是对称的，这等价于最小化数据点与平均值之间的距离。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/fd26b312887649d187be55db68e8094c.jpeg)



