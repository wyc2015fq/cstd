# Stanford ex1: Linear Regression - lijunweiyhn的博客 - CSDN博客





2016年04月10日 11:30:58[_Junwei](https://me.csdn.net/lijunweiyhn)阅读数：238








**代码资源地址：**http://download.csdn.net/detail/lijunweiyhn/9491877

        第一次实验主要做的是一个线性回归实验，对二维线性数据进行拟合。而后对于新数据，可以用拟合好的θ对新数据进行预测。




先说说一维特征的样本怎么处理。




%%==============================一维特征样本处理=====================================%%

**Part 1: Basic Function**

这一部分仅仅只是让我们学会简单的函数编写和调用，看看就行了。输出的是一个5*5的单位矩阵。




**       Part 2: Plotting**

这一部分让我们把数据绘制出来。读取的是ex1data1.txt里面的数据。ex1data1.txt里面有97个样本。第一列是X值，第二列是对应的y值。具体举例的应用环境看给出的ex1.pdf。

**       绘制完毕后，可以查看整个数据的分布。**



**       Part 3: Gradient descent**

这一部分计算的梯度下降。给X多加一列你懂的，X with subscript 0，并且让X0=1(对应的是θ0)。因为这里的每个样本就只是一维的特征样本，所以，初始化θ vector为两行一列。并设置迭代次数和学习速度α。计算loss函数用个最简单的误差平方公式即可。θ就2个参数，连最优化函数都没用到，直接人工设置迭代次数+一个循环就可以求得收敛结果。完了得到训练好的θ vector，然后直接画出这条直线：plot(X(:,2),
 X*theta, '-')即可。至于预测新数据，你懂的，用训练好的θ与新来样本做乘积运算即是预测值。

       后面绘制J(θ)函数就不做介绍了。大概就是：

1造一个网格：

           theta0_vals = linspace(-10, 10, 100);

           theta1_vals = linspace(-1, 4, 100);




       2初始化网格大小的J矩阵，用作保存它的每种θ向量取值的结果:

          J_vals = zeros(length(theta0_vals), length(theta1_vals));




       3对这网格里面的J进行计算：

         for i = 1:length(theta0_vals)

             for j = 1:length(theta1_vals)
        t = [theta0_vals(i); theta1_vals(j)];    
        J_vals(i,j) = computeCost(X, y, t);

             end

         end



一维的线性回归只是让我们对其有个了解。下面看看多维特征的线性回归。

**%%==============================多维特征样本处理=====================================%%**


那要是特征有多个，即多维特征，那怎么办呢?ex1data2.txt给出了多维特征样本的数据。第一列可能代表房子大小，第二列的意思是有多少个卧室，最后列是房子的出售价格（也就是一个label）。



**       Part 1: Feature Normalization**

一共是47个样本，把第一列和第二列，也就是特征值赋给X。把房屋价格给y,由于整个数据代表三维空间，不方便画出，所以程序以打印形势给出了数据。把X求平均值，赋给参数mu；再把X求标准差赋给参数sigma。然后对每一个样本做特征Normalization处理，及减去mu,除以sigma。

**Part 2: Gradient Descent **

同样的，设置讲学习速度和迭代次数，更新迭代至收敛即可，然后得到拟合好的θ，记得预测数据时候要在前面加一列X0,并且新进入的数据记得做平均化处理。







     因为线性回归不是特别让人困惑，所以介绍的不是特别详细。

     如有问题可以邮件联系：ljw.hfut@foxmail.com

















