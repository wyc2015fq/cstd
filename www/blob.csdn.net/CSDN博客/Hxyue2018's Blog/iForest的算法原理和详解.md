# iForest的算法原理和详解 - Hxyue2018's Blog - CSDN博客





2018年12月10日 19:11:33[Hxyue2018](https://me.csdn.net/Super_Json)阅读数：430








> 
*"An outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism."*

异常检测 (anomaly detection)，或者又被称为“离群点检测” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。


通常我们定义“异常”的两个标准或者说假设：
- 异常数据跟样本中大多数数据不太一样。
- 异常数据在整体数据样本中占比比较小。

> 
对于异常检测而言，最直接的做法是利用各种统计的、距离的、密度的量化指标去描述数据样本跟其他样本的疏离程度（详见异常检测概论）。而 Isolation Forest (Liu et al. 2011) 的想法要巧妙一些，它尝试直接去刻画数据的“疏离”(isolation)程度，而不借助其他量化指标。Isolation Forest 因为简单、高效，在学术界和工业界都有着不错的名声。


# 算法介绍

我们先用一个简单的例子来说明 Isolation Forest 的基本想法。假设现有一组数据（如下图所示），我们要对这组数据进行随机切分，希望可以把点“星”区分出来。具体的，确定一个维度的特征，并在最大值和最小值之间随机选择一个值 x ，然后按照小于 x 和 大于等于x 可以把数据分成左右两组。然后再随机的按某个特征维度的取值把数据进行细分，重复上述步骤，直到无法细分，直到数据不可再分。直观上，异常数据由于跟其他数据点较为疏离，可能需要较少几次切分就可以将它们单独划分出来，而正常数据恰恰相反。这正是 Isolation Forest（IForest）的核心概念。

![Isolation an outlier](https://quantdare.com/wp-content/uploads/2018/03/outlier2.gif)

# 模型训练

iForest （Isolation Forest）孤立森林 是一个基于Ensemble的快速异常检测方法，具有线性时间复杂度和高精准度。IF采用二叉树去对数据进行切分，数据点在二叉树中所处的深度反应了该条数据的“疏离”程度。整个算法大致可以分为两步：iForest属于Non-parametric和unsupervised的方法，即不用定义数学模型也不需要有标记的训练。怎么来切这个数据空间是iForest的设计核心思想，本文仅介绍最基本的方法。由于切割是随机的，所以需要用ensemble的方法来得到一个收敛值（蒙特卡洛方法），即反复从头开始切，然后平均每次切的结果。iForest 由t个iTree（Isolation Tree）孤立树 组成，每个iTree是一个二叉树结构，其实现步骤如下：

**训练**：构建一棵 iTree 时，先从全量数据中抽取一批样本，然后随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支。然后，在左右两个分支数据中，重复上述步骤，直到满足如下条件：
- 数据不可再分，即：只包含一条数据，或者全部数据相同。
- 二叉树达到限定的最大深度。

![](https://upload-images.jianshu.io/upload_images/4517099-a933215a5dee170f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/499/format/webp)

# **模型预测**

计算数据 x 的异常分值时，先要估算它在每棵 iTree 中的路径长度（也可以叫深度）。具体的，先沿着一棵 iTree，从根节点开始按不同特征的取值从上往下，直到到达某叶子节点。假设 iTree 的训练样本中同样落在 x 所在叶子节点的样本数为 T.size ，则数据 x 在这棵 iTree 上的路径长度 h(x) ，可以用下面这个公式计算：

![](https://img-blog.csdnimg.cn/20181210195123584.png)

公式中，e 表示数据 x 从 iTree 的根节点到叶节点过程中经过的边的数目，C(T.size) 可以认为是一个修正值，它表示在一棵用 T.size 条样本数据构建的二叉树的平均路径长度。一般的，C(n) 的计算公式如下：

![](https://img-blog.csdnimg.cn/20181210195230493.png)

其中，H(n-1) 可用 ln(n-1)+0.5772156649 估算，这里的常数是欧拉常数。数据 x 最终的异常分值 Score(x) 综合了多棵 iTree 的结果：

![](https://img-blog.csdnimg.cn/2018121019552184.png)

公式中，E(h(x)) 表示数据 x 在多棵 iTree 的路径长度的均值，$\psi$ 表示单棵 iTree 的训练样本的样本数，$C(\psi)$ 表示用 $\psi$ 条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。

从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。

# **个人见解**

1. iForest具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。

2. iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。对这类数据，建议使用子空间异常检测（Subspace Anomaly Detection）技术。此外，切割平面默认是axis-parallel的，也可以随机生成各种角度的切割平面，详见“[On Detecting Clustered Anomalies Using SCiForest](https://link.jianshu.com?t=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-15883-4_18)”。

3. iForest仅对Global Anomaly 敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点 （Local Anomaly）。目前已有改进方法发表于PAKDD，详见“[Improving iForest with Relative Mass](https://link.jianshu.com?t=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-06605-9_42)”。

4. iForest推动了重心估计（Mass Estimation）理论发展，目前在分类聚类和异常检测中都取得显著效果，发表于各大顶级数据挖掘会议和期刊（如SIGKDD，ICDM，ECML）。

Isolation Forest 是无监督的异常检测算法，在实际应用时，并不需要黑白标签。需要注意的是：（1）如果训练样本中异常样本的比例比较高，违背了先前提到的异常检测的基本假设，可能最终的效果会受影响；（2）异常检测跟具体的应用场景紧密相关，算法检测出的“异常”不一定是我们实际想要的。比如，在识别虚假交易时，异常的交易未必就是虚假的交易。所以，在特征选择时，可能需要过滤不太相关的特征，以免识别出一些不太相关的“异常”。

# 相关代码






