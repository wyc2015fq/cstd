# 卷积层感受野和坐标映射 - aiaiai010101的博客 - CSDN博客

2017年03月12日 21:05:20[aiaiai010101](https://me.csdn.net/aiaiai010101)阅读数：668



转载自http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/

如有版权问题，请联系博主删除本博客

# Receptive Field (感受野)

这是一个非常重要的概念，receptive field往往是描述两个feature maps A/B上神经元的关系，假设从A经过若干个操作得到B，这时候B上的一个区域只会跟a上的一个区域相关，这时候成为的感受野。用图片来表示：
![receptive_field.jpg](http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/receptive_field.jpg)
receptive_field.jpg

在上图里面，map 3里1x1的区域对应map 2的receptive field是那个红色的7x7的区域，而map 2里7x7的区域对应于map 1的receptive field是蓝色的11x11的区域，所以map 3里1x1的区域对应map 1的receptive field是蓝色的11x11的区域。

那么很容易得出来，receptive field的计算公式如下：
- 对于Convolution/Pooling layer:

**（此处存疑，我认为应该把右侧的下标i统一成i+1）**其中表示第层layer的输入的某个区域，表示第层layer的步长，表示kernel
 size，注意，不需要考虑padding size。
- 对于Neuron layer(ReLU/Sigmoid/…)

# Coordinate Mapping

通常，我们需要知道网络里面任意两个feature map之间的坐标映射关系，如下图，我们想得到map 3上的点映射回map
 2所在的位置。
![coordinate_map.jpg](http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/coordinate_map.jpg)
coordinate_map.jpg

计算公式如下：
- 对于Convolution/Pooling layer:

其中表示第层layer的输入的某个点，表示第层layer的步长，表示kernel
 size，
- 对于Neuron layer(ReLU/Sigmoid/…)

上面是计算任意一个layer输入输出的坐标映射关系，如果是计算任意feature map之间的关系，只需要用简单的组合就可以得到，下图是一个简单的例子：
![coordinate_map_example.jpg](http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/coordinate_map_example.jpg)
coordinate_map_example.jpg

# Convolutionalize (卷积化)

最近掀起了[FCN(全卷积网络)](http://arxiv.org/abs/1411.4038)风，这种网络里面不包括全连接层(fully connected layer)。

## 卷积层跟全连接层的区别

**卷积层**的操作跟传统的**滑窗**(sliding windows)很相似，把kernel作用于输入的不同的区域然后产生对应的特征图，由于这样的性质，给定一个卷积层，它并不要求输入是固定大小的，它可能根据输入大小的不同而产生大小不一样的特征图。
![sliding-windows.png](http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/sliding-windows.png)
sliding-windows.png

**全连接层**的操作是把输入拉成一个一维的向量，然后对这一维的向量进行点乘，这就要求输入大小是固定的。

那么如果使用一个包含fc层的模型(如AlexNet)就必须使用固定大小的输入，其实有时候这是非常不方便以及不合理的，比如下图，如果我要把红框的塔输入网络，就必须得对它进行变成，假设是放到AlexNet里面，因为输入是224x224，那么就会对图片产生变形。
![warp-image.png](http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/warp-image.png)
warp-image.png

那么有没有办法使得网络可以接受任意的输入？实际上是可以的，只需要把全连接层变成卷积层，这就是所谓的卷积化。这里需要证明卷积化的等价性。直观上理解，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把卷积作用的区域扩大为整个输入，那就变成全连接了，我就不给出形式化定义了。所以我们只需要把卷积核变成跟输入的一个map的大小一样就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。举个例子，比如AlexNet，fc6的输入是256x6x6，那么这时候只需要把fc6变成是卷积核为6x6的卷积层就好了。

**个人阅读理解：**

**本文的坐标映射公式，本人持保留态度。**

**不过坐标映射在SPP-net和FCN中均有重要作用。**


