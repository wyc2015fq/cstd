# GAN之父在NIPS 2016上做的报告：两个竞争网络的对抗（含译文下载） - 结构之法 算法之道 - CSDN博客





2017年03月06日 23:32:40[v_JULY_v](https://me.csdn.net/v_JULY_v)阅读数：20362
所属专栏：[机器学习十大算法系列](https://blog.csdn.net/column/details/datamining.html)









> 
> 
## GAN之父在NIPS 2016上做的报告：两个竞争网络的对抗






作者：Ian Goodfellow
翻译：七月在线DL翻译组
译者：范诗剑 汪识瀚 李亚楠
审校：管博士 寒小阳 加号
责编：翟惠良 July
声明：本译文仅供学习交流，有任何翻译不当之处，敬请留言指正。转载请注明出处。
下载：[https://ask.julyedu.com/question/7664](https://ask.julyedu.com/question/7664)




## 前言

    今年春节前，萌生一个想法，深度学习越发火热，但一些开创性的论文多半来自国外，如果组织一些朋友把这些英文论文翻译成中文，是不是可以让信息流通的更快、更顺畅？

    说干就干。春节前两周组建好七月在线DL翻译组，然后翻译组的小伙伴们即开始翻译，有一组更是在春节期间翻译了GAN之父在NIPS 2016上做的长达60页的报告，当时着实震惊了一把。而且，这篇报告中的GAN也不过是2016年刚火起来，如此，本报告兼具经典和最新，值得好好学习一下。

![](https://img-blog.csdn.net/20170306234749396)


    下面，我们就来看看GAN之父到底在这篇长达60页的论文当中说了些啥。
    事情回到2016年的NIPS上，Ian Goodfellow做了主题为《生成对抗网络（Generative Adversarial Networks）》的报告，当时他的报告包括以下主题：
- 为什么生成式模型是一个值得研究的课题
- 生成式模型的工作原理，以及与其他生成模型的对比
- 生成式对抗网络的原理细节
- GAN相关的研究前沿
- 目前结合GAN与其他方法的主流图像模型
关于原英文精辟演示文稿请点击——
PDF版：www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf
KeyNote版：www.iangoodfellow.com/slides/2016-12-04-NIPS.key



## 一句话描述GAN——
    GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于**真实训练样本**。
    更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。

![](https://img-blog.csdn.net/20170306234730052)

如下图中的左右两个场景：

![](https://img-blog.csdn.net/20170306233816874)

## 为什么要研究GAN
    你或许会这么以为：对于计算机视觉领域该模型虽然能提供更多的图像，但这恰恰是真实世界并不缺少的



## GAN的基本原理
生成对抗网络是一种生成模型（Generative Model），其背后最基本的思想就是从训练库里获取很多的训练样本（Training Examples），从而学习这些训练案例生成的概率分布。
- 生成模型为高维分布的表示与处理提供了一个绝佳的测试机会——此类高维分布往往是工程应用中的重要研究对象；

![](https://img-blog.csdn.net/20170306233834937)

- 生成式模型能以多种方式嵌入至强化学习中；

![](https://img-blog.csdn.net/20170306233852025)

- 生成模型可以接受缺失训练数据，或者可以被用来预测缺失数据。生成对抗模型，使得机器学习可以处理复合式问题。

![](https://img-blog.csdn.net/20170306233920245)



## 基于GAN的应用

### ——iGAN（交互式生成对抗网络）
    用户可以绘制一幅草稿，然后iGAN会使用GAN模型来生成最相似的合理图像。

![](https://img-blog.csdn.net/20170306233936312)

### ——IAN（自省对抗网络）

![](https://img-blog.csdn.net/20170306233951120)


### ——图对图变换
    将单幅卫星图像变为地图；将涂鸦转化为相片级别图像等；由于许多这样的转换都存在超过一种的正确输出，为保证模型训练的正确性，使用生成模型就有了必要性。

![](https://img-blog.csdn.net/20170306234003199)

## GAN之最大似然估计的模型

![](https://img-blog.csdn.net/20170306234018844)

![](https://img-blog.csdn.net/20170306234031344)




## GAN的损失函数

![](https://img-blog.csdn.net/20170306234044360)

## DCGAN——深度的卷积GAN

![](https://img-blog.csdn.net/20170306234057969)![](https://img-blog.csdn.net/20170306234112376)

## GAN的tips和tricks——（下文简称t&t）
    很难具体的说哪些技巧更有效，实际情况是，它们可以在某些任务中提升效果，也可能在另一些任务中起相反作用。因此这些技巧可以拿来尝试，但不要把它们当成是某种最优方法。具体包括：使用标签参与训练；单边标签平滑；将batch normalization虚拟化；是否平衡G和D（小编理解：作者目前的观点是，GANs主要是估计两个概率密度分布的比值，而只有当鉴别器足够完美时才有可能正确估值。所以这里更应该强化D函数）。
关于怎样训练GAN模型，详见GitHub库：http://github.com/soumith/ganhacks

### t&t1.使用标签参与训练

![](https://img-blog.csdn.net/20170306234126220)

### t&t2.单边标签平滑
    GAN的工作方式是让discriminator估算两个概率密度分布的比值，但是深度神经网络倾向于生成过高置信度的结果，容易走极端，这对模型是不利的。尤其是基于对抗生成的网络，它的分类器倾向线性推断并产生出置信度极高的结果。

### t&t3.将batch normalization虚拟化

![](https://img-blog.csdn.net/20170306234141279)



## 后记
- 关于我们。七月在线DL翻译组是由一群热爱翻译、热爱DL、英语六级以上的研究生或博士组成，有七月在线的学员，也有非学员。本翻译组翻译的所有全部论文仅供学习交流，宗旨是：汇集顶级内容 帮助全球更多人。目前已经翻译数十篇顶级DL论文，详见：[https://ask.julyedu.com/question/7612](https://ask.julyedu.com/question/7612)
- 加入我们。如果你过了英语六级、是研究生或博士、且熟练DL、热爱翻译，欢迎加入我们翻译组，微博私信@研究者July
- GAN课程。为了帮助更多人更好的了解、学习、入门GAN，今年上半年，我们七月在线亦会开《生成对抗网络班》，从头到尾详解GAN的原理及其实战应用，敬请期待。
    七月在线July、二零一七年三月七日。


