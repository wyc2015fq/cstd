# 算法建模的一些总结 - 快来学习鸭～～～ - CSDN博客





2018年10月28日 17:02:15[DivinerShi](https://me.csdn.net/sxf1061926959)阅读数：110








## **问题建模**

解决一个机器学习问题，都先需要对问题进行分析，确定我们的目的是什么，明确了目的后，对问题进行建立模型，建立的模型

前需要确定好我们建立的模型面对的目标，这个目标需要尽可能逼近最开始我们想解决问题的目的，只有我们最初的目的和建模的目标尽量一致才能确保后续所有的工作都是有效的。比如一个电商平台想要利用推荐系统提高消费者的购买量，那么我们的目的就是尽可能让消费者买我们推荐的东西，提高GMV。把这个问题抽象化为模型就是要让我们模型推荐的商品让消费者买，那么模型的目标就是让模型预测的结果和消费者真实购买想法的结果尽量一致，说白了就是建模过程中损失函数的定义，是否接近最初的目的。具体在建模过程中，只需要按照最初的目标进行建模即可，比如，我们可以将预测消费者买或者不买的行为定义二分类问题，来最小化真实消费者意图和模型预测的结果。也可以将问题定义为点击流预测，如消费者购买行为数据流下，消费者下次购买行为预测。这个就看具体情况了。

## 建模过程

    建模过程就是将问题抽象化的过程。在这个过程中，首先需要一份干净的数据，所以我们需要对数据进行筛选和清洗。比如还是消费者购买行为，如果使用双十一的数据来对问题建模，那么十有八九模型并不会特别好，因为这天的数据也许大家只是觉得便宜所以就买了。有了一份干净的数据，就需要将数据进行整理了。特征工程上场了。数据分为结构化数据和无结构话数据。通俗的讲，结构化数据就是数据像表里的每一行，一行表示一个消费者的一个行为，他是谁，买了啥，都整理在每个字段中，整整齐齐。而非结构化数据就像图片，没有整齐的结构。大多数的机器学习模型都需要将数据处理成结构化数据。输入模型的数据需要有一定的格式。

## 评价指标

精确率：预测正确的正样本/预测为正确的样本

召回率：预测正确的正样本/真正正确的正样本

这两者相互矛盾，精确率越高，往往召回率越低，反之依然；

所以不同的问题，可能对精确率和正确率 需求不一样，需要对预测正样本进行设置不同的阈值。当然也有一些评价整体性能的评价指标。

如PR曲线，ROC曲线

PR曲线 ，横轴为召回率，纵轴为精确率，所以PR曲线，越往右上角越好；

ROC曲线，横轴为预测错误的负样本占整体训练样本中负样本的比例；纵轴为预测正确的正样本占整体训练样本中正样本的比例；

因为ROC的横轴和纵轴的计算，都没有用到预测出的正样本集为分母，所以在面对正负样本不平衡的情况下，ROC并不敏感；而PR曲线反应出的结果就很敏感。绘制ROC曲线也较为简单，我们先把所有样本按预测概率值从小到大排序好，然后以每条样本的概率值为阈值计算横轴和纵轴的值，专业名词好像叫FPR和TPR，可以去查下混淆矩阵，我不爱记这些名词，懂含义就行了。然后把这些点连接起来就是ROC曲线。ROC曲线在正负样本不平衡的时候，可能会有波动，比如极端情况，正样本只有两个，当预测出一个的时候，会纵轴一直保持在50%，然后横轴一直往右走，当正样本由预测出一个后，就一下子跳到了100%。

### AUC值

谈到ROC曲线，就肯定需要了解auc值，auc的英文是area under roc curve 就是指的roc曲线下的面积。因为当ROC曲线越靠近左上角说明模型的整体效果越好，所以这和曲线下面积越大是等价的。当auc等于1的时候就是roc曲线是一条直线，模型是完美的。但是auc或者说roc曲线反应的只是是否正样本比负样本排在前面的一个值，反应的是模型的整体效果；但是没有比较其具体的概率值，如果要比较整体的概率值，就需要用到其他评价指标，如MAE，MSE等；所以对一些为了比较topN效果的任务跑的算法，单单比较auc值可能就不够了，因为也许auc值不错，但是topN中召回的结果大部分是负样本，这样对最终的效果也是不会起到任何作用；所以在实际应用中要确保线上的目标和线下的评价指标一致。这样才能有效提高算法效果。在实践中，对于正负样本不平衡的数据，其实auc值是会偏高的，所以还需要做好正负样本平衡，以及添加其他的评价指标来辅助。前面也说来，auc只是判断了整体是否比负样本排在前面的一个效果比较，所以针对不同任务，也许还需要加入topN等召回率，MSE等来一起判断算法优化是否有效。还有一些排序相关的指标，可以去了解下，MAP，NDCG等。

## 数据噪声

真实数据中，存在噪声是不可避免的。数据中或多或少肯定会有噪声，但是要想办法把噪声去掉一些，留有一点噪声也没关系，少量的噪声反而能提高模型的性能。前提是训练数据量还可以。

噪声：特征值缺失，超出特征值域范围，等，也可能标注不对。去除了一些特征的噪声模型效果反而变差，所以噪声也带有一些信息能用于构建模型，比如特征数据缺失，可以把缺失也理解为一种特征。

## 采样

用的比较多的：

简单采样：如负样本下采样，正样本上采样（上采样的时候，如果正样本确实很少，那么可以用smote等生成数据来上采样）

层级采样：如某个类目预测准确率低，如果该类目样本少，就用层级采样，把这部分数据上采样；

分层训练：将负样本划分成多份，正样本和一部分负样本训练，训练多个模型。

自助法（bootstrapping）：自主采样（Bootstrap Sampling）有放回的重复采样，来构造训练集和测试集；

这样一条样本没有被采到的概率是1-1/n 经过n次采样还没有被采集到的概率是(1-1/n)^n

取极限 lim（1-1/n）^n 等于1/e 约等于0.368

这意味着当数据量很大的时候，约有36.8%的数据不会被采集到训练集中，用这部分数据来作为测试集。 

但是自助法改变了最开始的数据集分布，所以单个模型的训练会引入偏差，所以可以用在随机森林这种很多模型和在一起的模型，来消除偏差。

## 特征工程

数据和特征决定了机器学习算法的上线，而模型和算法只是不断逼近这个上线而已。

特征工程相当于数据和模型之间的传输通道，如果通道太小，很大有用的信息根本传不过去。或者说把数据转化成

算法能理解的格式。不同的数据，如数值型，离散型，文本，图像，视频。都可以想办法转化为实向量，特征工程表示的越好，模型越能学习到数据中隐含的规律。

**探索性数据分析**

拿到一份数据后，先要对数据进行分析，画图，统计等等，至少要知道每个特征有哪些取值，取值范围等。目的是尽可能的理解数据本身的一些特性，方便提取特征，去掉异常值，构建初步的模型。

可视化：箱形图，直方图，多变量图，散点图，主成分分析

定量技术：样本均值，方差，分位数，峰度，偏度

数值特征主要考虑大小和分布，线性模型，线性回归，逻辑回归对大小敏感，因为模型很难协调好不同参数之间的变化。

树模型不需要归一化，如果模型对输入数据有一些隐式或者显式的假设，那么需要确保数据的分布和模型的假设是一致的。线性回归用的是平方误差，所以一般假设误差是服从高斯分布的。（这也是线性回归需要做归一化的一个原因，一个原因是保障梯度优化时一致，减少震荡；还有一个是对数据归一化，onehot来变换数据，使得数据服从高斯分布。有时候数据的取值差的很多，onehot后，就能近似高斯。）

### 特征处理常见方式：



1.截断

  数据精度的截断，有时候很多数据不需要太大的精度，比如金额，几分几分的不需要，精确到元就可以了。

2.离散化。分桶

连续性特征离散化，有时候精度很小，可能只是噪声。我们只需要保留必要的信息就可以了。离散化后可以把连续的特征当作类别特征。而且连续特征在不同数据段的分布不一致，离散化可以挖掘出这些信息。而一些长尾的信息，比如一些少量的很大的值，可以用对数转换。

3.二值化

数值特征的一种常用类型是计数特征，点击量等等；转变为用户是否访问过

4.缩放

数据归一化（多种归一化方式，l2，l1，标准化，对数缩放（对长尾数据很有效，能缩小长尾数据大端数据，扩大小端的数据））

5.缺失值处理

缺失数据可以补充，用均值，中值，差值，0值；对于特征分布是高斯分布的，用均值填充；长尾的用中值，连续特征用前后差值；不知道怎么填的连续特征用0值，或者单独把缺失数据的提取出一列，作为新的特征；还可以训练一个单独的模型来预测出缺失值，这个可以作为用户画像。或者不处理，现在有很多模型可以直接处理缺失数据，入xgboost，直接学习出分裂方向来处理缺失数据。

6.特征交叉

来发现一些，非线形的特征，对两个特征进行加，减，乘，除，可以通过特征选择方法；

用特征选择（统计检验或者模型的特征重要性）或者直觉来选择一些有用的交叉；现在很多模型可以自动学习特征交叉组合，如FM，FFM等。

7.非线形编码

如用核函数，比如高斯核，多项式核等来做非线形映射；像SVM一样；



**解决非线形问题：**

1.svm的核函数，

2.树模型

3.深度学习

4.特征交叉



### 文本特征

    现在nlp这块快被DL占领，但是一些机器学习算法还是很有效，文本特征产生的数据比较稀疏，如BOW。

文本预处理：字符大小写统一，分词，去除无用字符，提取词根，拼写纠错，词干提取，标点符号编码，文档特征，实体插入和提取，word2vec，文本相似性，去除停用词，去除稀有词，TF-IDF，LDA，LSA等。

文本统计特征：不考虑词顺序，计算文本长度，单词个数，数字个数，字母个数，大小写单词个数，大小写字母个数，标点符号个数，

特征字符个数，数字占比，字母占比，特殊字符占比等，以及名词个数，动词个数等；

余弦相似度，jaccard杰卡德相似度

编辑距离：两个字符串由一个转化成另外一个所需要的最少的编辑操作次数，word2vec

特征选择

目的：

1. 简化模型，让模型更加具有可解释性，高可解释性可以让模型稳定性更有把握，而且也为业务运营工作提供指引和决策；

2. 改善性能

3. 降低过拟合风险

特征选择方法：去掉方差小的特征，pearson相关系数，协方差，基于模型的特征排序，LASSO通过l1惩罚，让特定的回归系数变成0；

持续总结中。。。。。。



