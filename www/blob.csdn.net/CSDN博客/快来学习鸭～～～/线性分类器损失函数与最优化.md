# 线性分类器损失函数与最优化 - 快来学习鸭～～～ - CSDN博客





2017年03月05日 19:20:16[DivinerShi](https://me.csdn.net/sxf1061926959)阅读数：3426








## **author:DivinerShi**停止更新，看到一个很完整的笔记：[http://cs231n.github.io/](http://cs231n.github.io/)线性分类器损失函数与最优化假设有3类 cat car frog![这里写图片描述](https://img-blog.csdn.net/20170305190114426?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)第一列第二行的5.1表示真实类别为cat，然后分类器判断为car的的分数为5.1。**那这里的这个loss怎么去计算呢？**这里就要介绍下SVM的损失函数，叫**hinge loss**。如上图所示，我们去计算第一列的损失，计算方法如下：```因为真实的类为cat，所以我们让其他两类的分数去减去真实类的分数，相当于去计算真实类和其他类之间的误差。得到第一列的误差计算如下：```![这里写图片描述](https://img-blog.csdn.net/20170305190145867?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)**这里3.2为真实类的分数，所以为被减数**。![这里写图片描述](https://img-blog.csdn.net/20170305190209211?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)这个计算方法就是用的多类SVM损失。## SVM损失（hinge loss）给定一个样本![这里写图片描述](https://img-blog.csdn.net/20170305190339476?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，当是一张图片，yi是一个整型标签。成绩向量s=f(xi,W)，**SVM损失定义如下：**![这里写图片描述](https://img-blog.csdn.net/20170305190410711?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)这是一个二分类支持向量机的泛化，它计算了所有不正确的例子，将所有不正确的类别的评分，与正确类别的评分之差加1，将得到的数值与0作比较，取两者中的最大值，然后将所有的数值进行求和。我们不仅要使得正确类别的评分高于错误类别的评分，还使用了一个**安全系数**，系数的值为1，**为啥安全系数为1？**根据W的不同，得出的分数也不同，W按比例缩放，分数也在缩放，所以安全系数为1是随意给的。全部训练集的Loss是![这里写图片描述](https://img-blog.csdn.net/20170305190502415?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，就是上面那个多类SVM的mean。和![这里写图片描述](https://img-blog.csdn.net/20170305190531166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)看上去差不多，但是其实有差别，我们不是在不断的缩放或者平移这一损失，而是在非线性的改变支持向量机SVM，其实第二种就是square hinge loss，第一个是hinge loss。![这里写图片描述](https://img-blog.csdn.net/20170305190557510?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)问：那么利用hinge loss的最大损失和最小损失分别为多少？答：最小值：0，最大值：无穷大问：最开始的时候，W都很小，接近于零，那么在优化的时候分数很接近零，就是说左边计算出的Sj都接近零，那么损失值会怎么样？答：像左边只有三类的，平均损失会接近2。**正确性检查程序**一般为了**正确性检查程序**，当从很小的数W开始优化时，你的第一个损失函数，如果你的函数形式基本正确，只需要去看得到的数是否有意义，例如当在这个例子中损失为2，说明程序正确运行。矢量化实现：![这里写图片描述](https://img-blog.csdn.net/20170305190616870?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)```Def Li_vecterized(x,y,W):Score =W.dot(x)Margins = np.maximum(0,scores-scores[y]+1)Margins[y]=0Loss_i = np.sum(margins)Return Loss_i```**问**：如果我们找到了一个W，使得损失函数为零，那么这个W是否是独一无二的？**答**：当找到一个W后，用2W去计算Loss，发现损失还是为零，而且越大，分数越大，差本来就是小于-1的数，那更加小于-1## 所以引入了正则化 R(W)正则化![这里写图片描述](https://img-blog.csdn.net/20170305190725072?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)R(W)衡量了W的好坏，我们不仅仅想要数据拟合得更好，也希望能优化W,训练损失和你用于测试集的泛化损失，所以正则化是一系列通过损失来使目标相加的技术，和前面的项形成竞争，前面的想适应你的训练数据，而这个想要让W呈现一种特殊的方式，所以有时在目标中他们会相互竞争。事实证明，正则项有时会让训练错误变得更多，甚至造成错误的分类，但是我们注意到，他会使测试结果表现得更好。**常见正则化或者权值衰减：**![这里写图片描述](https://img-blog.csdn.net/20170305190746104?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)还有max norm /drop out等**例：**X=[1,1,1,]W1 = [1,0,0,0]W2 = [0.25,0.25,0.25,0.25]他们乘积都为1：![这里写图片描述](https://img-blog.csdn.net/20170305190807716?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)不加正则化，分数一样，loss一样，但L2正则化考虑了x中的大部分东西，L2会认为W2好些。## SoftmaxSoftmax分类：一般化的逻辑斯蒂回归![这里写图片描述](https://img-blog.csdn.net/20170305190826389?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)假设这些分数都是未标准化的对数概率这一列表示真实图片为猫，然后判断为这三类的分数分别为3.2、5.1、-1.7。求图片为某一类的概率：![这里写图片描述](https://img-blog.csdn.net/20170305190849202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)使正确分类的概率的对数最大，根据损失函数，我们要使负的正确分类概率的对数最小，所以概率对数是分数的扩展。![这里写图片描述](https://img-blog.csdn.net/20170305190907093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)![这里写图片描述](https://img-blog.csdn.net/20170305190924453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)问：上面这个Loss的最大值、最小值？答：看log函数就能看出来，为1的时候最小为0，为0的时候最大为无穷大。问：刚开始训练的时候，W很小，那么损失值是多少呢？答：是类别数分之一的对数负值，可用于检查代码## Hinge loss和cross-entropy loss比较下图分别展示了hinge loss和cross-entropy loss的计算过程。![这里写图片描述](https://img-blog.csdn.net/20170305190952371?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)**Hinge loss**的话只有当分类正确，即![这里写图片描述](https://img-blog.csdn.net/20170305191011501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)的时候，损失为0但是**cross entropy loss**对每个样例的每个分数都有用到。**当分数发生微小变化的时候，cross entropy loss会变，hinge loss不会变**。下一章 反向传播与神经网络初步



