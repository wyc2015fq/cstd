# 决策树算法推导分析 - 快来学习鸭～～～ - CSDN博客





2017年09月29日 13:30:59[DivinerShi](https://me.csdn.net/sxf1061926959)阅读数：487








**Author:**  DivinerShi

决策树模型就是利用树形结构来按特征对数据进行分类，本质上是从训练数据集中归纳出一组分类规则，以每个树的节点为分裂节点，分裂时按不同的分裂规则来划分数据。叶子节点代表对应样本所属类别。

由决策树的根节点到叶子节点的每一条路径构成一条**规则**：路径上内部节点的特征对应着规则的条件，而叶节点的类对应着规则的结论。

**优点：**输入数据可以不做归一化，数据清洗阶段可以相对少做许多工作；对缺失值不敏感，可以处理不相关特征数据；效率高，速度快。

**缺点：**连续性特征往往需要离散化；处理特征关联性比较强的数据表现得不是很好。

**可参考：**[http://blog.csdn.net/keepreder/article/details/47168383](http://blog.csdn.net/keepreder/article/details/47168383)

决策树路径的一个**重要性质**：互斥并且完备。就是说每个实例都被一条路径或者一条规则所覆盖，而且只被一条路径或者一条规则所覆盖。这里所谓覆盖指实例的特征与路径上的特征一致或者实例满足规则的条件。

## 熵

了解决策树前一定要先了解熵，熵在信息论中表示随机变量不确定性的度量，用于描述随机变量的混乱度，熵的值越大表示该随机变量很混乱，熵值低表示该随机变量不混乱，可以很清晰的对其进行区分。所以我们在划分决策树的时候，就是尽力去找到能使得熵值很低的特征来对数据进行划分，以此作为决策树的划分准则。

## 到底什么是熵？

假设现在有32只队伍进入世界杯，但是你错过了直播，你去问其他人，那只队伍是冠军？但是他不要让你猜，猜一次一块钱，猜对为止。那么最省钱的方式是：先问：冠军球队在1-16号之间吗？如果猜对了，则继续问：是在1-8号之间吗？这就是一个二分的问题，这样去猜是最快的，而我们这样只需要猜五次就肯定能猜到了那只队伍是冠军了。所以这个问题值五块钱。

计算机中数据都是用“比特”（bit）来表示的，所以香农用bit来表示一个信息的信息量。一个比特是一位二进制数，一个字节是8个比特。比如上面的球队冠军问题信息量就是5比特，如果64个球队找冠军，那么信息量就是6比特，可以看出来，具体的计算如下

（log32=5;log64=6）。

这里又有一个问题，其实各个球队夺冠的概率是不一样的，像西班牙、巴西、德国、意大利夺冠的概率就比日本、南非、韩国大很多。所以其实在真正计算的时候，是可以加入一些先验信息的。那么当我们在猜球队的时候，可以把一些概率大的少数球队猜一组，概率小的猜一组，它的准确信息量应该是
![这里写图片描述](https://img-blog.csdn.net/20170929130832510?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中，![这里写图片描述](https://img-blog.csdn.net/20170929130901439?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)分别是这32支球队夺冠的概率。香农把它称为“信息熵”，一般用符号H表示，单位是比特。当概率相同的时候就是5比特。对于任意一个随机变量X，它的熵定义：
![这里写图片描述](https://img-blog.csdn.net/20170929130932856?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)这里对数以2为底或者以e为底时熵的单位分别称为比特（bit）或纳特（nat）。
## 条件熵

H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵（conditional entropy）H(Y|X),定义为X给定条件下Y的条件概率分布的熵对X的数学期望
![这里写图片描述](https://img-blog.csdn.net/20170929131032944?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 信息增益

信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。

特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即![这里写图片描述](https://img-blog.csdn.net/20170929131306052?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息
## 设：![这里写图片描述](https://img-blog.csdn.net/20170929182336833?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)信息增益的计算**1.计算数据集D的熵H(D)**![这里写图片描述](https://img-blog.csdn.net/20170929182431930?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)**2.计算特征A对数据集D的条件熵H(D|A)**![这里写图片描述](https://img-blog.csdn.net/20170929182453026?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)**3.计算信息增益**![这里写图片描述](https://img-blog.csdn.net/20170929182515998?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)## 信息增益比特征A对训练数据集D的信息增益比![这里写图片描述](https://img-blog.csdn.net/20170929182615368?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵![这里写图片描述](https://img-blog.csdn.net/20170929182650156?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)之比，即![这里写图片描述](https://img-blog.csdn.net/20170929182744261?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，n是特征A取值的个数。## Gini指数分类问题中，假设有K个类，样本点属于第k个类的概率为![这里写图片描述](https://img-blog.csdn.net/20170929185035673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，则概率分布的基尼指数定义为![这里写图片描述](https://img-blog.csdn.net/20170929185104767?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)对于给定的样本集合D,其基尼指数为![这里写图片描述](https://img-blog.csdn.net/20170929185139322?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，![这里写图片描述](https://img-blog.csdn.net/20170929185202465?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)是D中属于第k类的样本子集，K是类的个数。特征A的条件下，根据A是否取某个值可以将集合D分划为两子集，其基尼指数定义为![这里写图片描述](https://img-blog.csdn.net/20170929185242720?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)## 信息增益、信息增益比、gini指数的联系因为信息增益的计算中，可以看上面的公式，条件熵的每个累乘因子中都有一个概率值![这里写图片描述](https://img-blog.csdn.net/20170929183404642?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，表达的是特征每个项所占总数据量的比。这样的话，如果某个特征的选择很多的话，极端情况下，该特征将每个样本划分成一个子集，那么条件熵约等于0，那么信息增益就会很高，但是其实这种情况混乱度是很高的。也就是说信息增益存在偏向于选择取值较多的特征的问题。所以信息增益比通过对信息增益除以数据集关于特征A的值的熵来消除该问题。而cart树因为本身就是二叉树，不存在偏向于有多个选择的特征的问题，但gini指数进一步对熵做了泰勒展开，基尼系数就是f(x)=-lnx在x=1的地方一阶泰勒展开得到f(x)=1-x。所以gini=sum[x(1-x)]=1-sum(x^2)## 决策树算法（ID3、C4.5）![这里写图片描述](https://img-blog.csdn.net/20170929182913708?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3hmMTA2MTkyNjk1OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)## CART树CART总体思想和上面写的一样，只是CART是二叉树，每次划分都是一个二分，所以在分割数据集的时候需要去寻找最优切分变量和最优切分点。回归树回归树的话，选择最优切分变量和切分点用最小二乘去计算，使得找到的最优切分变量和最优切分点所计算得到的值最小。以此变量和切点分割数据集。并以分割好的区域的均值作为预测值输出。分类树分类树的话，用基尼指数去做分类，计算特征不同值的基尼指数，选择最小的作为切分点。## 参考：1.统计学习方法-李航2.数学之美-吴军






