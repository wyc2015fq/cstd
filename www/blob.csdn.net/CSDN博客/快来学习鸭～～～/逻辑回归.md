# 逻辑回归 - 快来学习鸭～～～ - CSDN博客





2018年11月08日 23:17:08[DivinerShi](https://me.csdn.net/sxf1061926959)阅读数：77








# 逻辑回归

逻辑回归是一个简单的分类算法。

之前介绍过线性回归：[https://blog.csdn.net/sxf1061926959/article/details/66976356](https://blog.csdn.net/sxf1061926959/article/details/66976356)

线性回归是最基本的回归模型，建议了解逻辑回归前可以先了解下线性回归。

**其实如果只是想单纯的解决问题，线性回归也可以用于解决二分类问题，只需要对输出的值设定一个阈值即可实现**；

**但是线性回归在计算误差的时候，起数据域内的所有数，包括异常值，对模型的的影响是一样的，数据对模型的敏感度不会应数据的偏离程度发生变化。但是在分类问题中，其实最难区分的是边界附近的数据，而那些可以肯定是正类或者负类的数据并不是那么重要。**

而且为了让算法的目标和建模的目的一致，需要对模型输出的结果进行映射到不同类目上，即线性回归的实数域映射到[0,1]两个值上。有个跃阶函数是这样的： ![H(x)=\begin{Bmatrix} H(x)=0, x>0 & \\ H(x)=1, x<=0 & \end{Bmatrix}](https://private.codecogs.com/gif.latex?H%28x%29%3D%5Cbegin%7BBmatrix%7D%20H%28x%29%3D0%2C%20x%3E0%20%26%20%5C%5C%20H%28x%29%3D1%2C%20x%3C%3D0%20%26%20%5Cend%7BBmatrix%7D),从目的的角度来说可以解决问题了，但是这个并不是连续可导函数，并不能解决我们上面提出的异常值对模型敏感度一致的问题，我们需要一个可导函数，将这个函数直接应用到模型优化过程中去。这里就要引出sigmoid函数了，该函数满足了该要求。

## sigmoid推导

这里我们假设样本点为![x = \left \{ x_0,x_1,...,x_n \right \}](https://private.codecogs.com/gif.latex?x%20%3D%20%5Cleft%20%5C%7B%20x_0%2Cx_1%2C...%2Cx_n%20%5Cright%20%5C%7D) ，我们知道线性回归其实是将每个特征![x_i](https://private.codecogs.com/gif.latex?x_i)加上一个对应的权值![w_i](https://private.codecogs.com/gif.latex?w_i)来获得最后的输出结果。那么这个思想其实可以扩展到分类问题上。

定义一个最简单的假设，假设现在需要做的是二分类，类别分别为0和1，并且样本点x针对x属于类别0和1的权值为![w_o](https://private.codecogs.com/gif.latex?w_o)和![w_1](https://private.codecogs.com/gif.latex?w_1)，权值对应到每个特征为![w_0= \left \{ x_00,x_01,...,x_0n \right \}](https://private.codecogs.com/gif.latex?w_0%3D%20%5Cleft%20%5C%7B%20x_00%2Cx_01%2C...%2Cx_0n%20%5Cright%20%5C%7D)和![w_1= \left \{ x_10,x_11,...,x_1n \right \}](https://private.codecogs.com/gif.latex?w_1%3D%20%5Cleft%20%5C%7B%20x_10%2Cx_11%2C...%2Cx_1n%20%5Cright%20%5C%7D)。

线性回归模型的公式是这样的![y = w*x+b](https://private.codecogs.com/gif.latex?y%20%3D%20w*x&plus;b),我们把b作为![w_0](https://private.codecogs.com/gif.latex?w_0)就可以简化为![y=w*x](https://private.codecogs.com/gif.latex?y%3Dw*x),这样我们如果y只有0和1两个值的时候，其实就是一个分类任务。我们先对公式做一个恒等变换，加上一个指数函数，可以得到![exp(y) = exp(w*x)](https://private.codecogs.com/gif.latex?exp%28y%29%20%3D%20exp%28w*x%29),这样的话我们就开始计算样本点x属于，如下，我们可以分别计算出样本点属于0和1的输出值：

![y_0 = w_0 * x](https://private.codecogs.com/gif.latex?y_0%20%3D%20w_0%20*%20x)

![y_1 = w_1 * x](https://private.codecogs.com/gif.latex?y_1%20%3D%20w_1%20*%20x)

加上我们上面定义的恒等变换，就是

![exp(y_0) = exp(w_0 * x)](https://private.codecogs.com/gif.latex?exp%28y_0%29%20%3D%20exp%28w_0%20*%20x%29)

![exp(y_1) = exp(w_1 * x)](https://private.codecogs.com/gif.latex?exp%28y_1%29%20%3D%20exp%28w_1%20*%20x%29)

对于分类问题，我们期待的不是输出一个实数域的值，最好的是希望得到输出值属于某个类的概率，并且他们的概率和应该等于1.

所以我们可以得出如下的概率：

![P(y=0|x) = exp(w_0 *x)/(exp(w_1 *x)+exp(w_0 *x))](https://private.codecogs.com/gif.latex?P%28y%3D0%7Cx%29%20%3D%20exp%28w_0%20*x%29/%28exp%28w_1%20*x%29&plus;exp%28w_0%20*x%29%29)

![P(y=1|x) = exp(w_1 *x)/(exp(w_1 *x)+exp(w_0 *x))](https://private.codecogs.com/gif.latex?P%28y%3D1%7Cx%29%20%3D%20exp%28w_1%20*x%29/%28exp%28w_1%20*x%29&plus;exp%28w_0%20*x%29%29)

因为他们两的概率和为1，所以![P(y=0|x)](https://private.codecogs.com/gif.latex?P%28y%3D0%7Cx%29)可以简化为![P(y=0|x) = 1- P(y=1|x)](https://private.codecogs.com/gif.latex?P%28y%3D0%7Cx%29%20%3D%201-%20P%28y%3D1%7Cx%29)

而![P(y=1|x) = exp(w_1 *x)/(exp(w_1 *x)+exp(w_0 *x))](https://private.codecogs.com/gif.latex?P%28y%3D1%7Cx%29%20%3D%20exp%28w_1%20*x%29/%28exp%28w_1%20*x%29&plus;exp%28w_0%20*x%29%29)可以进一步化简，分子分母同除以![exp(w_1 *x)](https://private.codecogs.com/gif.latex?exp%28w_1%20*x%29)就可以得到：

![P(y=1|x) = 1/(1+exp((w_0 - w_1) *x))](https://private.codecogs.com/gif.latex?P%28y%3D1%7Cx%29%20%3D%201/%281&plus;exp%28%28w_0%20-%20w_1%29%20*x%29%29),我们再把![(w_0 - w_1)](https://private.codecogs.com/gif.latex?%28w_0%20-%20w_1%29)定义为w，就能得到

![P(y=1|x) = 1/(1+exp(w*x))](https://private.codecogs.com/gif.latex?P%28y%3D1%7Cx%29%20%3D%201/%281&plus;exp%28w*x%29%29)，**看见了吗？sigmoid函数出来了**

## 交叉熵

现在经过sigmoid函数，模型有了输出，然后就可以定义优化函数进行优化了，首先还是按之前线性回归的推导思路，需要定义一个最大似然函数

![](https://img-blog.csdnimg.cn/20181108230007472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N4ZjEwNjE5MjY5NTk=,size_16,color_FFFFFF,t_70)

公式比较难写，直接上网复制了一个。然后就是正常的似然函数解法，加log转换，再最大化转最小化：

![](https://img-blog.csdnimg.cn/20181108230406512.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N4ZjEwNjE5MjY5NTk=,size_16,color_FFFFFF,t_70)

**而这个函数其实就是交叉熵损失函数，神经网络的早期，在用梯度下降优化神经网络的时候，一直用的是求平方误差MSE，但是平方误差存在一个严重的问题，因为求导后的函数里存在sigmoid(x)(1-sigmoid(x))这一项，这一项是sigmoid以及其导数的乘积。正是因为这个，导致了严重了梯度消失问题，但是交叉熵并没有解决梯度消失问题，而是缓解。好了扯远了。**

在实际编程中交叉熵的定义有时候会出现一些小bug，我在这里做了简单的介绍[https://blog.csdn.net/sxf1061926959/article/details/83821733](https://blog.csdn.net/sxf1061926959/article/details/83821733)。

优化函数有了，就可以直接利用梯度下降算法进行优化了，我在[https://blog.csdn.net/sxf1061926959/article/details/72728244](https://blog.csdn.net/sxf1061926959/article/details/72728244)介绍了详细的梯度下降推导，可以参考学习。当然具体的优化方法不止梯度下降算法，还可以使用拟牛顿法来高效训练，如LBFGS。

本部分是个人对逻辑回归的理解。



