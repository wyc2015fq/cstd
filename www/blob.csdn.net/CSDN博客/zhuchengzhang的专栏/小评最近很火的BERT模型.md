# 小评最近很火的BERT模型 - zhuchengzhang的专栏 - CSDN博客





2019年04月04日 14:06:54[无声云泪](https://me.csdn.net/zhuchengzhang)阅读数：97








应*Pattern Recognition*审稿人要求，加入最近很火的BERT模型作为我们提出的文本表示模型的对比方法。在此和大家共同学习分析一下最近深度学习在自然语言处理领域的网红模型BERT，尝试品味BERT的精髓，探讨BERT可能的扩展方向。

# 模型小解

BERT模型全名是**B**idirectional **E**ncoder **R**epresentations from **T**ransformers. 翻译过来可能叫做*基于转换器的双向编码表示*。这篇文章是在2018年由Google AI的研究者Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova首先发表于[arXiv](https://arxiv.org/pdf/1810.04805.pdf)上。一时间在NLP领域乃至深度学习圈爆红，许多学者公认其为2018年与*ELMo*齐名的一大技术突破。我们且看，BERT模型是真的惊艳四方？还是学术圈的舆论炒作呢？

## BERT的用途

首先，我们要明白BERT模型究竟是做什么的。BERT模型是一个**预训练语言表示模型**（pre-trained language representations model）。所谓预训练语言表示模型，就是先用这个模型在**可与最终任务无关的大数据集**上训练处语言的表示，然后将学到的知识（表示）用到**任务相关**的语言表示上。这样做的原因是考虑到（1）若任务相关的数据集可能很小，小数据无法反映出语言间的复杂关系，同样也很容易让复杂的深度网络模型过拟合；（2）若任务相关的数据集很大，大数据上的训练时间很长，要在短时间内、特别是有限计算资源下利用深度网络学到相关的信息是困难的。提出BERT模型的论文将预训练语言表示模型根据实际使用时的策略也分为两类，一类是基于特征的方法（feature-based approach），另一类是基于微调的方法（fine-tuning approach）。基于特征的方法利用预训练好的模型提取文本特征，并将所提取的特征作为额外的特征加入到针对特定任务的表示模型之中。之前提到的ELMo就是基于特征的方法中的典型代表。基于微调的方法则是使用特定任务的数据集和标签来微调预训练好的模型（网络）参数，从而使得预训练的模型能够适应特定任务。今天咱们解读的BERT模型就属于基于微调方法的模型。

## BERT的优势

知道了BERT模型究竟是做什么的，下一个问题就是BERT模型相对于其他模型的优势到底在哪儿，为何它可以这么火。文章的作者们在引言中回答了我们的这个问题。原文中写道

> 
The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.


这句话的意思翻译过来就是：**当前语言模型的主要局限是在于它们都是单向的，这限制了预训练中可选的网络结构**。这儿说的单向，是指建模的时候只考虑了语言单个方向上的依赖关系。例如从左向右考虑一句话，用一个单词左侧的单词（作为上下文）来预测这个单词。[图1](#BERT_Framework)中间展示的OpenAI GPT的机构就是一种典型的单向模型。其实单向模型还包括了LSTM，GRU等典型的RNN网络。可能熟悉文本表示的朋友已经有了小疑问，很早之前就有了*双向递归神经网络*（Bidirectional RNN）？我刚看的时候也有这个困惑，但根据[图1](#BERT_Framework)中ELMo的结构和BERT结构对比，我想作者想表达的是即便双向递归神经网络考虑了两个方向的顺序，但是仅仅是独立考虑的。换而言之，ELMo中每一个独立的LSTM都是只考虑了一个方向的上下文，即一个词和左右两侧的上下文的关系在这种模型中总总只有一侧被模型捕获到，模型不能捕获这个词同时和左右两侧上下文的关系。由此可见，当前模型所考虑到的语言中的关系是不完备的。

![BERT模型图](https://img-blog.csdnimg.cn/20190404084952853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podWNoZW5nemhhbmc=,size_16,color_FFFFFF,t_70)

**图1**：不同预训练模型结构图。BERT使用双向转换器，OpenAI GPT使用从左至右的转换器，ELMo使用级联方法连接独立训练的从左至右和从右至左的LSTM。



## BERT的原理

BERT模型为了对文本进行双向的建模，采用的不是LSTM等RNN网络有序输入的模式，而是类似于全连接网络（fully-connected network）输入完整句子的模式。从这儿可以看出，BERT所说的*双向关系*（bi-directional）建模方式，精准的说应该是*无向关系*（non-directional）建模的关系。这儿，可能大家会有一个疑问：CNN和RNN等网络的核心思想就是捕获*序关系*，因此在图像、文本等有序数据的学习上展现出显著的优势，但这儿为何反其道而行，回到了最初的全连接呢？这不得不说到Google团队之前的一项成果[**转换器**（Transformer）](https://arxiv.org/pdf/1706.03762.pdf)。该成果**利用基于特别设计的注意力机制（Attention mecanism）的简单全连接网络取代了复杂的CNN和RNN网络**，不但大大减少了训练时间，同时有效的提升了网络性能。当然，虽然不去显式考虑序关系了，但是每一个词的位置和词与词之间的相对位置关系却是自然语言中最重要的关系之一，决定着一个语句的含义，在建模时绝对不能忽略。为了补充这种位置信息，[转换器](https://arxiv.org/pdf/1706.03762.pdf)不仅将每个输入的词进行编码，同样对每个词的位置进行编码，然后利用词编码和位置编码之间的运算，将位置信息补充到词编码之中（这个设计被BERT保留并扩展，见[图2](#BERT_Inputs)）。同时，利用注意力机制，动态的学习不同位置的词之间的相互关系。有兴趣同学可以继续深入研究一下这个[转换器](https://arxiv.org/pdf/1706.03762.pdf)的详细结构，在此我就不详细解释了。

![BERT_Inputs](https://img-blog.csdnimg.cn/20190404132139515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podWNoZW5nemhhbmc=,size_16,color_FFFFFF,t_70)
**图2**：BERT模型输入示意图。


说到这儿，我们可以看到，BERT模型其实也并不是第一个对本文进行双向建模的模型。但是，之前的转换器之类的模型却只是用到语句的翻译之中，属于一种有监督的训练模型。而BERT模型继承了它双向建模的思想，并继续发扬光大，将其用到了无监督的预训练语言表示学习之中。从有监督到无监督，看似一小步，实则一大步。能找到一种有效的无监督目标函数来确保模型能从数据中学习到想要捕获的关系，是相当具有挑战的。在我看来，BERT模型吸取了自编码机（auto-encoder）、词编码模型（word2vec）等无监督模型的设计思想，又结合所要捕获的*无序关系*和*句与句关系*等信息的特点，提出了对于转换器全新的无监督目标函数。从这个贡献来说，BERT模型被称为第一个捕获文本双向关系的预训练语言表示模型是当之无愧的。

BERT模型提出了一种新的预训练目标：**掩码语言模型**（masked language model）。掩码语言模型随机给输入的文字符号打上掩码，即遮盖掉该文字符号，然后用这个文字符号的上下文信息来预测这个文字符号原本的值。相对于按照从左到右顺序对文本建模的语言模型，掩码语言模型的训练目标使得所学习到的语言表示可以融合一个文字左右两侧的上下文信息。为了逼近真实的数据分布，BERT模型采用了3种不同的打码方式：80%的情况下替换为标识符，10%的情况下替换为随机文字，10%的情况下保持文字符号不变。具体请参考[原文](https://arxiv.org/pdf/1810.04805.pdf)，在此便不赘述。

除了使用上述掩码语言模型外，BERT还同时使用了一个**后句预测**（next sentence prediction）的任务为训练目标，与掩码语言模型一起来进行对语言表示的训练。这个设计用以捕获句子与句子之间的相互关系，有利于预训练的通用表示在文本匹配等任务的应用。这也是为何[图2](#BERT_Inputs)中有Segment embeddings了，其实是为了学出前句和后句的位置关系。

综上，BERT模型的方法原理可以概括为**用掩码语言模型和后据预测两个无监督目标函数来训练转换器模型**。

# 模型鉴赏

## BERT模型可借鉴的核心思想

了解过BERT模型的方法原理后，对于可以借鉴的思想大家可能各有想法，欢迎大家在留言中各抒己见。在此，我说说我的想法，抛砖引玉。
- [转换器](https://arxiv.org/pdf/1706.03762.pdf)模型的思想可以被用在多种有序数据特征提取的建模中，特别是对于短文本的建模。从本质上说，其打破了CNN和RNN只能捕获单一的近邻关系和序关系的能力边界，利用注意力机制的优势来学不同位置上、不同尺度下的关系。值得注意的是，这儿注意力机制可以学到不同输入下不同尺度的不同的关系(动态关系)，而CNN学到的关系相对于位置而言确实固定不变的（filter的值学完后即固定不变），RNN学到的关系是考虑不到复杂的上下文关系的（Memory中的值高度依赖上一状态）。
- BERT在设计mask方法的时以及设计后句预测目标函数时，充分考虑到了所学到的表示可能用于的后续工作的特点。这种设计思想对于一种通用表示的无监督目标函数设计来说非常重要。
- BERT的总体结构上非常简洁，无针对某一类情况特别的设计，但是确显示出非常强大的学习能力和性能。在解决实际问题中，面对的往往是非常大的复杂数据，简单模型往往表现出更好的性能。同时，模型的简洁也保证了其计算效率，是完成大规模数据处理的基础。

## BERT模型可扩展的工作

学习过BERT模型后，觉得各位大牛们可以在这几个方面继续推进：
- 对于长文本的表示。BERT模型以基于注意力机制的转换器作为基础，天生的一个缺陷是无法作用于长文本。对于长文本，单用当前的注意力机制是不足以捕获各种复杂的交互关系的。可能需要设计一个深度的注意力机制，层级化地捕获各种关系。
- 对于噪音的容忍能力。BERT模型适用于短文本，但是社交网络中或者日常通讯中适用的短文本往往包含大量的噪音（不规则表示、错别字等），如何让BERT模型进一步拥有噪音容忍的能力值得研究。
- 对于一词多义的表示。虽然通过其上下文可以在一定程度上缓解一词多义的影响，但是毕竟一词多义对于BERT模型的原始输入中词编码影响极大，需要加入一定的机制来解决一词多意的表示问题。
- BERT模型利用第一位[CLS]的表示输出作为整句话的表示，进而进行文本分类任务。但是在预训练的时候，仅仅用**后句预测**作为目标函数来训练。可以看出，如果分类任务不是建立在语句间相互关系的基础上时，例如对*欺诈文本的分类任务*，由于预训练的目标和实际任务相差太大，可能会有不太理想的效果。因此，如何更有效的在特定任务上进行fine-tuning是值得研究的。

欢迎各位继续在留言中探讨可扩展方向，有好的扩展方向或研究点我也会持续更新~~



