# R语言入门视频笔记--9--随机与数据描述分析 - 默槑 - 博客园







# [R语言入门视频笔记--9--随机与数据描述分析](https://www.cnblogs.com/modaidai/p/7028584.html)





古典概型的样本总量是一定的，且每种可能的可能性是相同的，



1、中位数：median(x)

2、百分位数：quantile(x)或者quantile(x,probe=seq(0,1,0.2))　　#后面这个是设置参数，零到一的范围，每隔0.2算一次

　　不知道叫啥的很方便的函数：fivenum(x,na.rm=TRUE)　　#输出五个数最大值、最小值、下四分位数、上四分位数、中位数



3、协方差：用于看两组数据之间的关系，看看是不是有一定的关联性

　　他有一个相关系数r，r越接近1，则相关性越高，反之，越接近零就越低

　　cov(x[2:4])　　#这就是求协方差矩阵

　　cor(x[2:4])　　#这是求协方差r矩阵

4、研究变量之间的关系：

　　1、函数关系：有精确地函数表达式，关系非常明确，比如银行利率和收益的关系

　　2、相关关系：非确定关系，有关系蛋是没那么明确，比如身高和体重的关系，可分为下面的两种

　　　　　　一、平行关系：相关分析（一元、多元），比如一个人的物理成绩与数学成绩好坏的关系，表面看起来是有很强的相关的，其实是背后有很多因素起作用

　　　　　　　　　　　　　　cor.test(x1,x2)（相关系数显著性的假设检验）　　　　#输出的数据时：95%可信度下，相关系数所在的范围

　　　　　　二、依存关系：回归分析（一元、多元），一个变量决定另一个变量，起决定作用的是自变量，另一个是因变量,x是自变量，y是因变量

　　　　　　　　　　　　　　　　lm(y~1+x)　　#一元线性回归分析，会输出它的公式，截距和斜率。其调用形式是：fitted.model <- lm(formula,data=data.frame)

　　　　　　　　　　　　　　　　　　　　 #formula是公式   data.frame为数据框  结果放在fitted.model中

　　　　　　　　　　　　　　　　　　　　 #例如 fm2<-lm(y~x1+x2,data=production)　　　适应于y关于x1和x2的多元回归模型

　　　　　　　　　　　　　　　　　　　　  #y~1+x或者y~x均表示y=a+bx有截距形式的线性模型。而通过原点的线性模型可以表达为：y~x-1或y~x+0或y~0+x

　　　　　　　　　　　　　　　　 summary：使用summary(里面放回归分析的结果)，来更详细的参数与结果，coefficients这一栏是描述计算的结果，后面的Pr(>|t|)是p_value，

　　　　　　　　　　　　　　　　　　　 算出的值对于这组数据合不合理，越小越好最后面星号越多越好，说明结果算的很好，模型建得很好，有时可能是两颗星，一颗星，

　　　　　　　　　　　　　　　　　　　 甚至一个点，甚至连一个点都没有，说明算出来的东西越不合理。

　　　　　　　　　　　　　　　　　　　 做这个线性回归是有前提的：1、这个样本变量是正态分布的2、自变量和因变量是线性的关系，如果这两个前提有一个不满足，

　　　　　　　　　　　　　　　　　　　 则这个模型的合理性和准确性不能保证。有一个检验模型合不合理的指标，就是Multiple R-squared，也就是R的平方

　　　　　　　　　　　　　　　　　　　　，相关系数平方，越接近一越合理。函数最下面的结果是F检验和它的p值

　　　　　　　　　　　　　　　　   anova：方差分析函数，anova()，也可以得到summary函数结果的一部分，也只有一部分

　　　　　　　　　　　　　　　　　predict：z=data.frame(x=185) ; predict(a,z)　　#这是一个预测函数，a是已经建好的线性模型，z是输入的数值，可以得到结果，很方便



　　　　　　　　　　　　　　多元回归：不止两组数据进行线性回归的计算，可能有三个四个五个数据进行，就要用到一些分析方法，比如逐步回归：其有两种方式，

　　　　　　　　　　　　　　　　　　　向前引用法（从零开始，加一组，再加一组，再加一组数据看看），向后剔除法（先把所有数据都算进去，减一组，再减一组，

　　　　　　　　　　　　　　　　　　　再减一组数据看看），逐步筛选法（一边加一边减）

　　　　　　　　　　　　　　　　　　　sl <- step(s,direction="forward")    这是多元线性回归的公式，direction="forward"   方向等于forward是向前引用法，

　　　　　　　　　　　　　　　　　　　　　　backward是向后剔除法，both是两者都有，这个函数是通过判断AIC的值来进行数据的删除和添加，知道找到最优组合

　　　　　　　　　　　　　　　　　　　　　　但有时这个全自动化的函数也会出错，这时候就需要手工去进行筛选，add1()增加 和 drop1()删除      这两个函数。

　　　　　　　　　　　　　　　　　　　　　　比如drop1(s)：s是已经用step函数处理过的模型，或者是新lm模型，然后会告诉你删除其中一个数据，

　　　　　　　　　　　　　　　　　　　　　　AIC的变化，然后你可以根据它的结果重新建立模型，再去看这个模型怎么样

　　　　　　　　　　　　　　　　　　　会用到进行lm的更新，公式：new_lm<-updata(原来的lm, .~. , +新的数据)，这是加入新的数据，减去原来的是一组数据，可以把

　　　　　　　　　　　　　　　　　　　加号变为减号，减号后面是你要删除的数据

　　　　　　　　　　　　　　　　　　　得出一个好的结果靠这些指标衡量：RSS（Residual standard error：残差平方和）越小越好        

　　　　　　　　　　　　　　　　　　　　　　　　　　　　　R的平方(Multiple R-squared：相关性系数)  越大越接近1越好

　　　　　　　　　　　　　　　　　　　　　　　　　　　　　AIC（Akaike information criterion : 赤池信息准则  越小越好（大多数时候是非常好用的，但有时候是不好用的） 



5、回归诊断　　　　　　　　　　　　　　　　　　　

 通常回归诊断需要诊断都有下面这些东西：

　　1、样本是否符合正态分布假设？　　　　　　　　　　　　

　　　　有些数据是符合的，有些是不符合的，所以需要我们了解样本是不是正态分布



　　2、是否存在离群值导致模型产生较大误差？　　　　　　

　　　　数据是抽提出来的，误差不能避免，有时正、负误差会抵消掉，对结果就没有影响。但有时候误差会偏离正常值很远的数据，这种数据我们叫它离群值，离群值对模型

　　的影响非常大，可能使得这个模型跟它原本的模样完全的偏离

　　　　如何发现这些离群值并把它剔除出去，使得这个模型回归正轨，这也是回归诊断中需要讨论的东西



　　3、线性模型是否合理？

　　　　我们假设这个关系是线性的，但是自然界中有很多关系未必是线性的，可能是二次多项式，也可能是一个指数的关系，还可能是更复杂的关系，

　　甚至有的时候我们没办法写出它的表达式。我们需要知道我们做这个线性的假设是否合理，我们怎么样去判断



　　4、误差是否满足独立性、等方差、正态分布等假设条件

　　　　误差一般也是满足独立性、等方差、正态分布的，所谓独立性是指误差与y，也就是因变量是没有关系的，误差不会随着y的值变化。

　　我们也要确定误差是否满足上述条件

　　　　还有自变量中真正独立的有哪些？这里的所谓独立是指：这个自变量不会跟随其他自变量变化

　　5、是否存在多重共线性？

　　　　 多重共线性：所有的自变量中，有不是真正独立的自变量，这种情况可能会产生一个情况，就是计算过程中出现的一些矩阵是不可逆的，表现出来的就是这个矩阵的

　　行列式或者是这个矩阵的最小特征值非常接近零。由于在求回归模型的过程中需要求这矩阵的逆，如果这个矩阵是在这种非常接近不可逆的情况下，那很可能这个误差

　　非常非常的大，使得这个模型基本上失去了意义



一、正态分布检验

函数：shopiro.test()　　#如果算出来是不符合的就可以认为样本不符合正态分布，如果符合就认为它符合正态分布

　　shopiro.test(x$x1)　　#如果这个的结果中的p-value很小，就说明这个统计学意义很明显，需要拒绝假设，也就是说这个样本是不符合正态分布的。如果这个p-value很接近一

　　　　　　　　　　　　　说明这个样本不能被否定为不是正态分布，也就可以认识它是正态分布

 二、多重共线性检验

这里的检验原理是，把样本的数据组成一个矩阵，再乘以它的转置得到一个新的矩阵，然后再求这个矩阵的特征根，用特征根的最大值除以最小值，得到一个比值，这个值就叫kappa值，一般若kappa值小于100，就认为多重共线性的程度很小，若在100~1000之间，就认为存在中等程度或较强的多重共线性。若kappa>1000，则认为存在严重的多重共线性

函数/步骤：1、先data.frame()把数据组合好

　　　　　 2、在使用cor()求相关系数矩阵

　　    　　 3、kappa()　　　#得到kappa值

　　   　　  4、eigen(x)　　　#这个函数是用来求矩阵的特征根，在这里主要是看哪个样本中有很严重的多重共线性



1、广义回归线性模型

有时这个模型不是标准的线性的，我们也想给他做出来，这时候可以用广义线性模型

函数/步骤：1、先用data.frame()把数据组合好

　　　　　 2、glm(模型,family=binormal,data=xx)  #family=binormal    的意思就是我们的因变量是二元的，1或者0。比如牛张不张嘴，张就是1，不张就是0

　　　　　 3、最后得到P=exp(y的系数+x的系数*x) / (1+exp(y的系数+x的系数*x))

2、非线性回归

有时这个线就是曲线，这时候就不能用线性回归，就要用有曲线的。

比如　　x <- lm(y~log(x))　　y <-lm(log(y)~x)　　z <- lm(log(y)~log(x))这些东西

拟合的问题：拟合不足（把幂函数拟合为直线），过度拟合（把误差都算进去，导致输入正确的数会偏离非常大）



R软件中有一个函数：nls()--非线性模型的参数估计　　　　视频里面没有仔细讲  就算了吧^-^












