# 神经网络总结（bp） - 村雨sup - 博客园








[博客园](https://www.cnblogs.com/)[首页](https://www.cnblogs.com/cunyusup/)[新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)[联系](https://msg.cnblogs.com/send/%E6%9D%91%E9%9B%A8sup)[管理](https://i.cnblogs.com/)[订阅](https://www.cnblogs.com/cunyusup/rss)![订阅](https://www.cnblogs.com/images/xml.gif)





一、从生物到计算机

神经细胞利用电-化学过程交换信号。输入信号来自另一些神经细胞。这些神经细胞的轴突末梢（也就是终端）和本神经细胞的树突相遇形成突触（synapse），信号就从树突上的突触进入本细胞。信号在大脑中实际怎样传输是一个相当复杂的过程，但就我们而言，重要的是把它看成和现代的计算机一样，利用一系列的0和1来进行操作。就是说，大脑的神经细胞也只有两种状态：兴奋（fire）和不兴奋（即抑制）。发射信号的强度不变，变化的仅仅是频率。神经细胞利用一种我们还不知道的方法,把所有从树突突触上进来的信号进行相加，如果全部信号的总和超过某个阀值，就会激发神经细胞进入兴奋（fire）状态，这时就会有一个电信号通过轴突发送出去给其他神经细胞。如果信号总和没有达到阀值，神经细胞就不会兴奋起来。这样的解释有点过分简单化，但已能满足我们的目的。





二、神经网络基本原理

图中，左边几个灰底圆中所标字母w代表浮点数，称为权重（weight，或权值，权数）。进入人工神经细胞的每一个input(输入)都与一个权重w相联系，正是这些权重将决定神经网络的整体活跃性。你现在暂时可以设想所有这些权重都被设置到了-１和１之间的一个随机小数。因为权重可正可负，故能对与它关联的输入施加不同的影响，如果权重为正，就会有激发（excitory）作用，权重为负，则会有抑制（inhibitory）作用。当输入信号进入神经细胞时，它们的值将与它们对应的权重相乘，作为图中大圆的输入。大圆的‘核’是一个函数，叫激励函数(activation function)，它把所有这些新的、经过权重调整后的输入全部加起来，形成单个的激励值(activation value)。激励值也是一浮点数，且同样可正可负。然后，再根据激励值来产生函数的输出也即神经细胞的输出：如果激励值超过某个阀值（作为例子我们假设阀值为1.0），就会产生一个值为1的信号输出；如果激励值小于阀值1.0，则输出一个0。这是人工神经细胞激励函数的一种最简单的类型。在这里，从激励值产生输出值是一个阶跃函数。

**阶跃函数**



** 一个人工神经细胞(从现在开始，我将把“人工神经细胞”简称它为“神经细胞”) 可以有任意n个输入，n代表总数。可以用下面的数学表达式来代表所有n个输入：**

**x****1****, x****2****, x****3****, x****4****, x****5****, ..., x****n**

**同样 n 个权重可表达为:**

**  w****1****, w****2****, w****3****, w****4****, w****5**** ..., w****n**

**请记住，激励值就是所有输入与它们对应权重的之乘积之总和，因此，现在就可以写为:**

**      a = w****1****x****1**** + w****2****x****2**** + w****3****x****3**** + w****4****x****4**** + w****5****x****5**** +...+ w****n****x****n**

**以这种方式写下的求和式，用希腊字母Σ来简化：**





**图4以图形的方式表示了此方程。请别忘记，如果激励值超过了阀值，神经细胞就输出1; 如果激活小于阀值，则神经细胞的输出为0。这和一个生物神经细胞的兴奋和抑制是等价的。我们假设一个神经细胞有5个输入，他们的权重w都初始化成正负1之间的随机值(-1 < w < 1) 。 表2说明了激励值的求和计算过程。**

** 表2  神经细胞激励值的计算**
|**输 入**|**权 重**|**输入与权重的乘积**|**运行后总和**|
|----|----|----|----|
|**1**|** 0.5**|** 0.5**|** 0.5 **|
|**0**|**-0.2**|** 0**|** 0.5**|
|**1 **|**-0.3 **|**-0.3 **|**0.2 **|
|**1 **|**0.9**|** 0.9 **|**1.1 **|
|**0**|**0.1**|** 0**|**1.1 **|



大脑里的生物神经细胞和其他的神经细胞是相互连接在一起的。为了创建一个人工神经网络，人工神经细胞也要以同样方式相互连接在一起。为此可以有许多不同的连接方式，其中最容易理解并且也是最广泛地使用的，就是如图5所示那样，把神经细胞一层一层地连结在一起。这一种类型的神经网络就叫前馈网络（feedforword network）。这一名称的由来，就是因为网络的每一层神经细胞的输出都向前馈送（feed）到了它们的下一层（在图中是画在它的上面的那一层)，直到获得整个网络的输出为止。



由图可知，网络共有三层（译注：输入层不是神经细胞，神经细胞只有两层）。输入层中的每个输入都馈送到了隐藏层，作为该层每一个神经细胞的输入；然后，从隐藏层的每个神经细胞的输出都连到了它下一层（即输出层）的每一个神经细胞。图中仅仅画了一个隐藏层，作为前馈网络，一般地可以有任意多个隐藏层。但在对付你将处理的大多数问题时一层通常是足够的。事实上，有一些问题甚至根本不需要任何隐藏单元，你只要把那些输入直接连结到输出神经细胞就行了。另外，我为图5选择的神经细胞的个数也是完全任意的。每一层实际都可以有任何数目的神经细胞，这完全取决于要解决的问题的复杂性。但神经细胞数目愈多，网络的工作速度也就愈低，而且容易出现**过拟合**的现象，网络的规模总是要求保持尽可能的小。



一个简单的例子：

下面我们来看它是怎么完成的。我们以字符识别作为例子。设想有一个由8x8个格子组成的一块面板。每一个格子里放了一个小灯，每个小灯都可独立地被打开（格子变亮）或关闭（格子变黑），这样面板就可以用来显示十个数字符号。图6显示了数字“4”。









要解决这一问题，我们必需设计一个神经网络，它接收面板的状态作为输入，然后输出一个1或0；输出1代表ANN确认已显示了数字“4”，而输出0表示没有显示“4”。因此，神经网络需要有64个输入(每一个输入代表面板的一个具体格点) 和由许多神经细胞组成的一个隐藏层，还有仅有一个神经细胞的输出层，隐藏层的所有输出都馈送到它。

 　　一旦神经网络体系创建成功后，它必须接受训练来认出数字“4”。为此可用这样一种方法来完成：先把神经网的所有权重初始化为任意值。然后给它一系列的输入，在本例中，就是代表面板不同配置的输入。对每一种输入配置，我们检查它的输出是什么，并调整相应的权重。如果我们送给网络的输入模式不是“4”， 则我们知道网络应该输出一个0。因此每个非“4”字符时的网络权重应进行调节，使得它的输出趋向于0。当代表“4”的模式输送给网络时，则应把权重调整到使输出趋向于1。

 如果你考虑一下这个网络，你就会知道要把输出增加到10是很容易的。然后通过训练，就可以使网络能识别0到9 的所有数字。但为什么我们到此停止呢？我们还可以进一步增加输出，使网络能识别字母表中的全部字符。这本质上就是**手写体识别的工作原理**。对每个字符，网络都需要接受许多训练，使它认识此文字的各种不同的版本。到最后，网络不单能认识已经训练的笔迹，还显示了它有显著的归纳和推广能力。也就是说，如果所写文字换了一种笔迹，它和训练集中所有字迹都略有不同，网络仍然有很大几率来认出它。正是这种归纳推广能力，使得神经网络已经成为能够用于无数应用的一种无价的工具，从人脸识别、医学诊断，直到跑马赛的预测，另外还有电脑游戏中的bot（作为游戏角色的机器人）的导航，或者硬件的robot（真正的机器人）的导航。

 　这种类型的训练称作有监督的学习（supervised learnig），用来训练的数据称为训练集（training set）。调整权重可以采用许多不同的方法。对本类问题最常用的方法就是反向传播（backpropagation，简称backprop或BP）方法。



利用梯度下降法优化Bp神经网络

## 反向传播（backpropagation）：训练神经网络反向传播是使用数据来训练神经网络的算法，它是神经网络的梯度下降算法。 假设我们有一个训练集，其中含有输入向量和相应的目标输出向量。同时，假定我们的网络已经拥有一组权量（相当于我们知道每个神经元的激活函数），那么接下来，我们就需要使用以下算法来调整这些权量。 

## 1、利用初始权量，在输入向量上运行前向传播，从而得到所有网络所有神经元的输出。

## 2、这样，每个输出层神经元都会得到一个误差，即输出值与实际值之差。 

## 3、计算作为神经元权量的函数的误差的梯度，然后根据误差降低最快的方向调整权量。 

## 4、将这些输出误差反向传播给隐藏层以便计算相应误差。 

## 5、计算这些误差的梯度，并利用同样的方式调整隐藏层的权量。 不断迭代，直到网络收敛。



这里你就是用了反馈机制，即将实际结果和预期结果相比较，找出两者的不同之处，并借此改善下一次的行为,预期结果和实际结果之间的差距越小，你下一次需要调整的幅度就越小。神经网络正是通过这种方式学习，利用一种叫做“反向传播”的反馈机制，它由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，神经网络的学习在权重值修改过程中完成。误差达到所期望值时，神经网络学习结束。





首先介绍代价函数：

二次代价函数



式子代表预测值与样本值的差得平方和



由于使用的是梯度下降法，我们对变量w，b分别求偏导：



这种函数对于处理线性的关系比较好，但是如果遇到s型函数（如下图所示），效率不高。



从图中我们看出：当我们想要趋近于1时，B点接近于1，变化趋势变小（很正确），A点与1距离较远，变化趋势较大（很正确），C点（假设在x = -3处）远离1，变化趋势很小（发生错误），因此，二次代价函数中单凭梯度的大小决定变化的快慢是不对的。

由此我们引出了第二个代价函数——交叉熵代价函数

2，交叉熵代价函数



右边是balabalabalabalab的推导过程，最终得到表达式：



结论如上↑

在网上，发现可以通过神经网络工具箱这个GUI界面来创建神经网络,其一般的操作步骤如下：

**1：在输入命令里面输入nntool命令，或者在应用程序这个选项下找到Netrual Net Fitting 这个应用程序，点击打开，就能看见如下界面**











**2：输入数据和输出数据的导入（在本文中选取了matlab自带的案例数据）**



**3：随机选择三种类型的数据所占的样本量的比例，一般选取默认即可**





**4：隐层神经元的确定**


**5：训练算法的选取，一般是选择默认即可，选择完成后点击<train>按钮即可运行程序**

**6：根据得到的结果，一般是MSE的值越小，R值越接近1，其训练的效果比较，并第二张图给出了神经网络的各参数的设置以及其最终的结果，其拟合图R越接近1，模型拟合的更好**






最终的结果图

**7：如果所得到的模型不能满足你的需求，则需重复上述的步骤直至能够得到你想要的精确度****8：将最终的得到的各种数据以及其拟合值进行保存，然后查看，就可以得到所要的拟合值**



**最后参考了网上和MATLAB的帮助，给出了一些与神经网络相关的函数**
 图形用户界面功能。 
    nnstart - 神经网络启动GUI 
    nctool - 神经网络分类工具 
    nftool - 神经网络的拟合工具 
    nntraintool - 神经网络的训练工具 
    nprtool - 神经网络模式识别工具 
    ntstool - NFTool神经网络时间序列的工具 
    nntool - 神经网络工具箱的图形用户界面。 
    查看 - 查看一个神经网络。 

  网络的建立功能。 
    cascadeforwardnet - 串级，前馈神经网络。 
    competlayer - 竞争神经层。 
    distdelaynet - 分布时滞的神经网络。 
    elmannet - Elman神经网络。 
    feedforwardnet - 前馈神经网络。 
    fitnet - 函数拟合神经网络。 
    layrecnet - 分层递归神经网络。 
    linearlayer - 线性神经层。 
    lvqnet - 学习矢量量化（LVQ）神经网络。 
    narnet - 非线性自结合的时间序列网络。 
    narxnet - 非线性自结合的时间序列与外部输入网络。 
    newgrnn - 设计一个广义回归神经网络。 
    newhop - 建立经常性的Hopfield网络。 
    newlind - 设计一个线性层。 
    newpnn - 设计概率神经网络。 
    newrb - 径向基网络设计。 
    newrbe - 设计一个确切的径向基网络。 
    patternnet - 神经网络模式识别。 
    感知 - 感知。 
    selforgmap - 自组织特征映射。 
    timedelaynet - 时滞神经网络。 

  利用网络。 
    网络 - 创建一个自定义神经网络。 
    SIM卡 - 模拟一个神经网络。 
    初始化 - 初始化一个神经网络。 
    适应 - 允许一个神经网络来适应。 
    火车 - 火车的神经网络。 
    DISP键 - 显示一个神经网络的属性。 
    显示 - 显示的名称和神经网络属性 
    adddelay - 添加延迟神经网络的反应。 
    closeloop - 神经网络的开放反馈转换到关闭反馈回路。 
    formwb - 表格偏见和成单个向量的权重。 
    getwb - 将它作为一个单一向量中的所有网络权值和偏差。 
    noloop - 删除神经网络的开放和关闭反馈回路。 
    开环 - 转换神经网络反馈，打开封闭的反馈循环。 
    removedelay - 删除延迟神经网络的反应。 
    separatewb - 独立的偏见和重量/偏置向量的权重。 
    setwb - 将所有与单个矢量网络权值和偏差。 

  Simulink的支持。 
    gensim - 生成Simulink模块来模拟神经网络。 
    setsiminit - 集神经网络的Simulink模块的初始条件 
    getsiminit - 获取神经网络Simulink模块的初始条件 
    神经元 - 神经网络Simulink的模块库。 

  培训职能。 
    trainb - 批具有重量与偏见学习规则的培训。 
    trainbfg - 的BFGS拟牛顿倒传递。 
    trainbr - 贝叶斯规则的BP算法。 
    trainbu - 与重量与偏见一批无监督学习规则的培训。 
    trainbuwb - 与体重无监督学习规则与偏见一批培训。 
    trainc - 循环顺序重量/偏见的培训。 
    traincgb - 共轭鲍威尔比尔重新启动梯度反向传播。 
    traincgf - 共轭弗莱彻-里夫斯更新梯度反向传播。 
    traincgp - 共轭波拉克- Ribiere更新梯度反向传播。 
    traingd - 梯度下降反向传播。 
    traingda - 具有自适应LR的反向传播梯度下降。 
    traingdm - 与动量梯度下降。 
    traingdx - 梯度下降瓦特/惯性与自适应LR的反向传播。 
    trainlm - 采用Levenberg -马奎德倒传递。 
    trainoss - 一步割线倒传递。 
    trainr - 随机重量/偏见的培训。 
    trainrp - RPROP反向传播。 
    trainru - 无监督随机重量/偏见的培训。 
    火车 - 顺序重量/偏见的培训。 
    trainscg - 规模化共轭梯度BP算法。 

  绘图功能。 
    plotconfusion - 图分类混淆矩阵。 
    ploterrcorr - 误差自相关时间序列图。 
    ploterrhist - 绘制误差直方图。 
    plotfit - 绘图功能适合。 
    plotinerrcorr - 图输入错误的时间序列的互相关。 
    plotperform - 小区网络性能。 
    plotregression - 线性回归情节。 
    plotresponse - 动态网络图的时间序列响应。 
    plotroc - 绘制受试者工作特征。 
    plotsomhits - 小区自组织图来样打。 
    plotsomnc - 小区自组织映射邻居的连接。 
    plotsomnd - 小区自组织映射邻居的距离。 
    plotsomplanes - 小区自组织映射重量的飞机。 
    plotsompos - 小区自组织映射重量立场











