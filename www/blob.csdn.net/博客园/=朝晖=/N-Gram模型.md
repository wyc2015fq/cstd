# N-Gram模型 - =朝晖= - 博客园
# [N-Gram模型](https://www.cnblogs.com/dhcn/p/7493036.html)
http://www.cnblogs.com/chaosimple/p/3376438.html
N-Gram模型时大词汇连续语音识别中常用的一种语言模型，对中文而言，我们称之为汉语语言模型（CLM, Chinese Language Model）。汉语语言模型利用上下文中相邻词间的搭配信息，在需要把连续无空格的拼音、笔画，或代表字母或笔画的数字，转换成汉字串（即句子）时，可以计算出最大概率的句子，从而实现从到汉字的自动转换，无需用户手动选择，避开了许多汉字对应一个相同的拼音（或笔画串、数字串）的重码问题。
该模型基于这样一种假设，第n个词的出现只与前面n-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现的概率的乘积。这些概率可以通过直接从语料中统计n个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。
在介绍N-Gram模型之前，我们先来做个香农游戏（Shannon Game）。我们给定一个词，然后猜测下一个词是什么。当我说“艳照门”这个词时，你想到的下一个词时什么？我想大家很有可能会想到“陈冠希”，基本上不会有人想到“陈志杰”吧。N-Gram模型的主要思想就是这样的。
对于一个句子T，我们怎么计算它出现的概率呢？假设T是由词序列W1, W2, …, Wn组成的，那么P(T)=P(W1W2…Wn)=P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1)。
但是这种方法存在两个致命的缺陷：
    一个缺陷是参数空间过大，不可能实用化；另外一个缺陷是数据稀疏严重。
为了解决这个问题，我们引入马尔科夫假设：
    一个词的出现仅仅依赖于它前面出现的一个或者有限的几个词。
如果一个词的出现仅仅依赖于它前面出现的一个词，我们就称之为Bi-Gram。即：
P(T)=P(W1W2…Wn)=P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1)
   ≈P(W1)P(W2|W1)P(W3|W2)…P(Wn|Wn-1)
如果一个词的出现仅依赖于它前面出现的两个词，那么我们称之为Tri-Gram。在实践中用的最多的就是Bi-Gram和Tri-Gram，而且效果不错，高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精确度却提高的不多。
那么我们怎么样才能够得到P(Wn|W1W2…Wn-1)呢？一种简单的估计方法就是最大似然估计。即：
P(Wn|W1W2…Wn-1)=C(W1W2…Wn)/C(W1W2…Wn-1)
剩下的工作就是在训练语料库中数数儿了，即统计序列W1W2…Wn出现的次数和W1W2…Wn-1出现的次数。
下面我们用Bi-Gram举个例子，假设语料库总词数为13748，
![image](https://images0.cnblogs.com/blog/407700/201310/18171638-c87b895734e748ff9a188265bccbe6bb.png)
![image](https://images0.cnblogs.com/blog/407700/201310/18171638-c325ffe1717e4763838913964e3971fc.png)
这里还有一个问题要说：那就是数据稀疏问题！假设词表中有20000个词，如果采用Bi-Gram模型，则可能的N-Gram就有400000000个，如果是Tri-Gram，那么可能的N-Gram就有8000000000000个！那么对于其中的很多词对的组合，在语料库中都没有出现，根据最大似然估计得到的概率将会是0，这会造成很大的麻烦，在算句子的概率时一旦其中的某项为0，那么整个句子的概率就会为0，最后的结果时，我们的模型只能算可怜兮兮的几个句子，而大部分的句子算得的概率时0。因此，我们要进行数据平滑（Data Smoothing），数据平滑的目的有两个：一个是使所有的N-Gram概率之和为1，使所有的N-Gram概率都不为0。

