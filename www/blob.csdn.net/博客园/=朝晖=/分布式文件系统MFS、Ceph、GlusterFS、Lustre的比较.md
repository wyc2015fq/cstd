# 分布式文件系统MFS、Ceph、GlusterFS、Lustre的比较 - =朝晖= - 博客园
# [分布式文件系统MFS、Ceph、GlusterFS、Lustre的比较](https://www.cnblogs.com/dhcn/p/7120955.html)
原文：http://blog.csdn.net/metaxen/article/details/7108958
||**MooseFS(MFS)**|**Ceph**|**GlusterFS**|**Lustre**|
|----|----|----|----|----|
|**Metadata server**|单个MDS。存在单点故障和瓶颈。|多个MDS，不存在单点故障和瓶颈。MDS可以扩展，不存在瓶颈。|无，不存在单点故障。靠运行在各个节点上的动态[算法](http://lib.csdn.net/base/datastructure)来代替MDS,不需同步元数据,无硬盘I/O瓶颈。|双MDS(互相备份)。MDS不可以扩展，存在瓶颈。|
|**FUSE**|支持|支持|支持|支持|
|**访问接口**|POSIX|POSIX|POSIX|POSIX/MPI|
|**文件分布/数据分布**|文件被分片，数据块保存在不同的存储服务器上。|文件被分片，每个数据块是一个对象。对象保存在不同的存储服务器上。|Cluster Translators(GlusterFS集群存储的核心)包括AFR、DHT（和Stripe三种类型。AFR相当于RAID1，每个文件都被复制到多个存储节点上。Stripe相当于RAID0，文件被分片，数据被条带化到各个存储节点上。Translators可以组合，即AFR和stripe可以组成RAID10，实现高性能和高可用。|可以把大文件分片并以类似RAID0的方式分散存储在多个存储节点上。|
|**冗余保护/副本**|多副本|多副本|镜像|无|
|**数据可靠性**|由数据的多副本提供可靠性。|由数据的多副本提供可靠性。|由镜像提供可靠性。|由存储节点上的RAID1或RAID5/6提供可靠性。假如存储节点失效，则数据不可用。|
|**备份**||||提供备份工具。支持远程备份。|
|**故障恢复**|手动恢复|当节点失效时，自动迁移数据、重新复制副本。|当节点、硬件、磁盘、网络发生故障时，系统会自动处理这些故障，管理员不需介入。|无|
|**扩展性**|增加存储服务器，可以提高容量和文件操作性能。但是由于不能增加MDS，因此元数据操作性能不能提高，是整个系统的瓶颈。|可以增加元数据服务器和存储节点。容量可扩展。文件操作性能可扩展。元数据操作性能可扩展。|容量可扩展。|可增加存储节点，提高容量可文件操作性能，但是由于不能增加MDS，因此元数据操作性能不能提高，是整个系统的瓶颈。|
|**安装/部署**|简单|简单|简单|复杂。而且Lustre严重依赖内核，需要重新编译内核。|
|**开发语言**|C|C++|C|C|
|**适合场景**|大量小文件读写|小文件|适合大文件。对于小文件，无元数据服务设计解决了元数据的问题。但GlusterFS并没有在I/O方面作优化，在存储服务器底层文件系统上仍然是大量小文件，本地文件系统元数据访问是瓶颈，数据分布和并行性也无法充分发挥作用。因此，GlusterFS的小文件性能还存在很大优化空间。|大文件读写|
|**产品级别**|小型|中型|中型|重型|
|**应用**|国内较多|无|较多用户使用|HPC领域。|
|**优缺点**|实施简单，但是存在单点故障。|不稳定，目前还在实验阶段，不适合于生产环境。|无元数据服务器，堆栈式[架构](http://lib.csdn.net/base/architecture)(基本功能模块可以进行堆栈式组合，实现强大功能)。具有线性横向扩展能力。由于没有元数据服务器，因此增加了客户端的负载，占用相当的CPU和内存。但遍历文件目录时，则实现较为复杂和低效，需要搜索所有的存储节点。因此不建议使用较深的路径。|很成熟、很庞大。|

