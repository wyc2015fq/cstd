# 视频人员行为识别 - =朝晖= - 博客园
# [视频人员行为识别](https://www.cnblogs.com/dhcn/p/9530564.html)
https://blog.csdn.net/linolzhang/article/details/78034823
一. 提出背景
       目标：给定一段视频，通过分析，得到里面人员的动作行为。
       问题：可以定义为一个分类问题，通过对预定的样本进行分类训练，解决一个输入视频的多分类问题。
       这里提出的问题是简单的图片（视频）分类问题，该问题的前提条件是：场景目标为单人，并且占据图片比较大的比例，如下图所示：
![](https://img-blog.csdn.net/20170919213902115)
       还有一类问题是基于行人检测，去估计行人的姿态和动作，暂时不在本篇讨论范围内。
二. 行为识别的发展
       和其他领域一样，我们还是先从未被深度学习攻占的传统方法讲起，我们标记的里程碑算法是 iDT。
       论文：[Action Recognition with Improved Trajectories](https://hal.inria.fr/hal-00873267v2/document)
       iDT 方法是基于 DT（Dense Trajectories）方法，第一印象可以理解为 稠密光流 的轨迹。
![](https://img-blog.csdn.net/20170920194358281)
       如图所示，我们将算法描述为以下步骤：
> 
1）在原始图像多尺度上进行密集特征点采样，采样间隔为W（上图左）；
2）进行有效的特征点筛选（只保留有用的），这里选用的方式是基于自相关矩阵的特征值；
      和直接通过surf去选择特征点的思路基本上一样。
      该 Step 形成空域信息。
3）跟踪特征点，在时间轴形成特征点的轨迹序列（上图中）；
      该 Step 形成时域信息。
4）对应每个时间片上的每个特征点，在该点影响范围内 分别进行特征采样（HOG、HOF、MBH）（上图右），
      对序列进行编码（Fisher Vector），得到 Total 特征；
5）采用分类器（SVM）进行分类；
      具体方法不再展开，这里可能存在的问题是： 运动的背景可能会对光流有很大的影响。
      基于这个假设，iDT 的改进方法通过估计相机的运动模型来消除背景影响，即通过 SurF 特征匹配来估算相邻帧的投影变换矩阵。另外，论文设置了一个 Human Detector 来消除 人员变换 对运动模型估算的影响（框内不参与估算）。
三. 深度学习方法
       深度学习方法攻占该领域的时间是2014年，开山之作也是具有代表性的 Two Stream 方法。
       论文下载：[Two-Stream Convolutional Networks for Action Recognition in Videos](https://arxiv.org/abs/1406.2199)
       来看其框架图：
![](https://img-blog.csdn.net/20170920215108414)
       描述非常清晰，通过CNN网络对 single frame 提取图像的特征信息，下面通过多帧间的密集光流（与iDT类似）提出时域信息，后面通过 fusion＋分类来输出结果。
       针对 Two Stream 的改进比较多，主要思路包括 网络的改进、Fusion 方法、结合RNN（LSTM）、选择Key Frame等， 这里没有太多创新的东西，可以自己refer一下。
       这里重点提一下 C3D Network，也就是3D卷积。
       论文下载：[Learning Spatiotemporal Features with 3D Convolutional Networks](https://arxiv.org/abs/1412.0767)
       来看示意图：
![](https://img-blog.csdn.net/20170920221009257)
       与传统卷积的区别就在于将平面特征的提取扩展到3维，将空域特征和时域特征同时提取，该方法相比传统的2D方法，效率有明显的提高，基于VGG-like网络帧率达到了300FPS＋。
       虽然精度并不高，但是C3D是该方向上的一个创新，同样的基于视频的Task也将C3D看作是一个比较好的方法：
       Code：[http://vlg.cs.dartmouth.edu/c3d/](http://vlg.cs.dartmouth.edu/c3d/)
四. 参考数据集
       Action Recognition 相关数据库比较多，这里仅列出几个常用的供参考：
> 
UCF101: [http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php)
HMDB51: [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)
Sports-1M: [http://cs.stanford.edu/people/karpathy/deepvideo](http://cs.stanford.edu/people/karpathy/deepvideo/)
YouTube-8M: [https://research.google.com/youtube8m/download.html](https://research.google.com/youtube8m/download.html)
ActivityNet: [http://activity-net.org/download.html](http://activity-net.org/download.html)
