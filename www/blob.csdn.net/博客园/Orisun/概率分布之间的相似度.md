# 概率分布之间的相似度 - Orisun - 博客园







# [概率分布之间的相似度](https://www.cnblogs.com/zhangchaoyang/articles/7103888.html)





我曾经讲过如何度量两个随机变量之间的[独立性](http://www.cnblogs.com/zhangchaoyang/articles/2642032.html)和[相关性](http://www.cnblogs.com/zhangchaoyang/articles/2631907.html)，今天来讲一下如何度量两个[概率分布](http://www.cnblogs.com/zhangchaoyang/articles/3405093.html)之间的相似度。

在概率论中，f散度用来度量两个概率分布$P$和$Q$之间的距离，距离函数具有如下形式：

\begin{equation}D_f(P||Q) \equiv \int_{\Omega}f\left(\frac{dP}{dQ}\right)dQ\label{f}\end{equation}

f散度是一类算法的统称，并不是一个具体的算法，因为(\ref{f})式中并没有具体指定f是什么函数。当指定了f函数后，就得到具体的f散度算法，今天我们就讲其中的两种：Kullback–Leibler divergence和Hellinger distance。

## Kullback–Leibler divergence

概率分布p的熵为

$$H(p)=-\sum_xp(x)logp(x)$$

随机变量x服从的概率分布p(x)往往是不知道的，我们用q(x)来近似逼迫p(x)，q到p的交叉熵定义为

$$H(p,q)=E_p[-logq]=-\sum_xp(x)logq(x)$$

KL散度(Kullback–Leibler divergence)是熵与交叉熵之差

$$D_{KL}(p||q)=H(p)-H(p,q)=\sum_xp(x)logq(x)-\sum_xp(x)logp(x)$$

当p和q这两个分布完全吻合时，KL散度（或者叫KL距离）为0。当p和q没有交集时，其KL散度是一个常数，这个时候KL散度就反映不了距离了。

交叉熵不具有对称性，所以KL散度也不具有对称性，即$D_{KL}(p||q) \ne D_{KL}(q||p)$，KL散度不满足[三角不等式](https://en.wikipedia.org/wiki/Triangle_inequality)。

为了找到一种具有对称性的距离度量方式，人们发明了**Jensen–Shannon divergence**：

$$D_{JS}(p||q)=\frac{1}{2}D_{KL}(p||\frac{p+q}{2})+\frac{1}{2}D_{KL}(q||\frac{p+q}{2})$$

## Hellinger distance

对于连续分布

$$D_H(p||q)=\frac{1}{\sqrt{2}}\sqrt{\int\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^2dx}$$

对于离散分布

$$D_H(p||q)=\frac{1}{\sqrt{2}}\sqrt{\sum_x\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^2}$$

上式可以被看作两个离散概率分布平方根向量的欧式距离

$$D_H(p||q)=\frac{1}{\sqrt{2}}\parallel\sqrt{p(x)}-\sqrt{q(x)}\parallel_2$$

只有在如下情况时Hellinger距离才会取得最大值1：

$$p(x_i)*q(x_i)=0且p(x_i) \ne q(x_i)$$














