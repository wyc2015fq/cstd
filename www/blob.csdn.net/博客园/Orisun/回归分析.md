# 回归分析 - Orisun - 博客园







# [回归分析](https://www.cnblogs.com/zhangchaoyang/articles/2640700.html)





回归分析本质上就是一个函数估计的问题（函数估计包括参数估计和非参数估计），就是找出因变量（DV,Dependent Variable）和自变量（IV,Independent Variable）之间的因果关系。本文讲两种回归分析的方法：一般线性回归（ordinary linear regression）和逻辑斯谛回归（logistic regression）。更确切地讲线性回归和Logistic回归都属于参数估计，线性回归假设X和Y满足线性关系Y=θTX，Logistic回归假设![](http://www.forkosh.com/mathtex.cgi?y=g(\theta^Tx))，g是单极性Sigmoid函数。

### 线性回归

假设有一个房屋销售的数据如下：
|面积(m^2)|销售价钱（万元）|
|----|----|
|123|250|
|150|320|
|87|160|
|102|220|
|...|...|

用X表示房屋面积，用Y表示售价，画在坐标轴上发现它们近似满足线性关系，于是用一个线性函数来模拟。Y=θTX。θ是模型参数，怎么求呢？在有导师数据的情况下求模型参数θ最常用的方法就是最大似然函数法（ML,*maximum likelihood*）。何谓likelihood？likelihood就是一个条件概率：P(Y|X)。

假设根据特征的预测结果与实际结果有误差ε(i)，那么预测结果θTx(i)和真实结果y(i)满足下式：

![](http://www.forkosh.com/mathtex.cgi?y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)})

一般来讲误差满足均值为0的高斯分布。那么x和y的条件概率就是：

![](http://www.forkosh.com/mathtex.cgi?p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}))

要实现Maximum Likelihood，目标函数自然是：

![](http://www.forkosh.com/mathtex.cgi?max\%20\%20\prod_{i=1}p(y^{(i)}|x^{(i)};\theta))

取自然对数，把连乘变为连加，你就会发现目标函数变成了：

![](http://www.forkosh.com/mathtex.cgi?min\%20\%20\frac{1}{2}\sum_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2)

即最小化误差平方和。这是一个无约束的最优化问题，通常的解决思路是：梯度下降法和牛顿迭代法，具体方法可参考[无约束最优化方法](http://www.cnblogs.com/zhangchaoyang/articles/2600491.html)。另外对于“最小化误差平方和”这种问题，还有一种直接的解法--最小二乘法，即

![](http://www.forkosh.com/mathtex.cgi?\theta=(X^TX)^{-1}X^T%20\overrightarrow{y})

其中，![](http://www.forkosh.com/mathtex.cgi?(X^TX)^{-1}X^T)是X的广义逆（又叫伪逆）。

由此我们也看到最小二乘法可以用来求解线性回归问题的理论依据来源于极大似然估计。似然估计和矩估计是参数估计中两个最基本的方法。

**在使用线性回归时有几个限制：**
- 不应该使用这种方法来预测和建立模型时所使用的数据值相关差甚远的值。
- 避免模型中自变量之间有较高的相关性。
- 对噪声数据敏感。
- 一般假设误差变量（ error variances 或residuals）服从均值为0的正态分布。

### 逻辑斯谛回归

**在二值分类中线性回归遇到的问题**

 线性回归分析一般用于预测，而不用于分类。在二值分类问题中，因变量只有两种取值:0和1。用p(x)来表示y=1的概率，那么p(x)为线性函数的假设会遇到以下3个挑战：
- 线性函数的值可能跑到[0,1]之外。
- 从样本数据上看p(x)明显就不是线性函数。
- 线性回归假设误差变量是服从正态分布的，但是当y只可能取0和1时，很难满足这一点。

**逻辑斯谛回归的来由**

现有一组身高和性别的数据，我们接下来要根据身高来判定为男性的概率。

![](http://luna.cas.usf.edu/~mbrannic/files/regression/gifs/lo1.gif)

纵轴为P（判定为男性的概率），横轴为X（身高），显然这一个S型的曲线，可设：


|![](http://luna.cas.usf.edu/~mbrannic/files/regression/gifs/lo3.gif)|………………（1）|
|----|----|



或

![](http://luna.cas.usf.edu/~mbrannic/files/regression/gifs/lo4.gif)

现在我们定义一个变量，叫做odds ration（成败优胜比）。

![](http://luna.cas.usf.edu/~mbrannic/files/regression/gifs/lo5.gif)

当P=0.9时，odds=9；当P=0.1时，odds=0.11。这个不具有对称性，很讨厌。我们来变换一下，搞一个具有对称性的函数出来。
|![](http://luna.cas.usf.edu/~mbrannic/files/regression/gifs/lo7.gif)|………………（2）|
|----|----|

由（1）式和（2）式得：

![](http://www.forkosh.com/mathtex.cgi?\dpi{150}log(odds)=ln(\frac{P}{1-P})=a+bX)

**线性回归和逻辑斯谛回归的关系**

逻辑斯谛回归又叫对数回归，其本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射。即一般线性回归中认为：![](http://www.forkosh.com/mathtex.cgi?y=\theta^Tx)，而在逻辑斯谛回归中我们认为![](http://www.forkosh.com/mathtex.cgi?y=g(\theta^Tx))。

g(z)是个S型函数，它把连续值映射到[0,1]上。

![](http://www.forkosh.com/mathtex.cgi?g(z)=\frac{e^Z}{1+e^{z}})

![](http://www.forkosh.com/mathtex.cgi?y=\frac{e^{\theta^Tx}}{1+e^{\theta^Tx}})

**关于计算**

自变量有l个特征，每个特征的权值为w，记z为各特征的线性组合：

![](http://www.forkosh.com/mathtex.cgi?z=w_0+w_1x_1+w_2x_2+\cdots+w_lx_l)

y的取值记为+1和-1。则条件概率：

![](http://www.forkosh.com/mathtex.cgi?p(y=+1|x)=g(z)=g(\sum_{k=0}^l{w_kx_k})\qquad(1))

![](http://www.forkosh.com/mathtex.cgi?g(z)=\frac{1}{1+e^{-z}})，则![](http://www.forkosh.com/mathtex.cgi?1-g(z)=g(-z))

![](http://www.forkosh.com/mathtex.cgi?p(y=-1|x)=1-p(y=+1|x)=1-g(z)=g(-z)=g(yz))

有n组观测值，似然函数为：

![](http://www.forkosh.com/mathtex.cgi?L(\overrightarrow{w})=\sum_{i=1}^n{lng(y_iz_i)}\qquad(2))

其中![](http://www.forkosh.com/mathtex.cgi?z_i=\sum_{k=0}^l{w_kx_{ik}})

梯度下降法求似然函数的极值点，先来算一上梯度的方向是什么。

注意到![](http://www.forkosh.com/mathtex.cgi?g'(z)=g(z)g(-z))，可得

![](http://www.forkosh.com/mathtex.cgi?\frac{\partial{L}}{\partial{w_k}}=\sum_{i=1}^n{y_ix_{ik}g(-y_iz_i)}\qquad(3))

令学习率为ε，则w的迭代公式为：

![](http://www.forkosh.com/mathtex.cgi?w_k^{(t+1)}=w_k^{(t)}+\epsilon\sum_{i=1}^n{y_ix_{ik}g(-y_iz_i)}\qquad(4))

当w前后两次变化量很小时，停止迭代。

上面是采用梯度下降法，你同样也可以采用牛顿迭代法来解，我就不详细介绍了。

 调整后的似然函数

![](http://www.forkosh.com/mathtex.cgi?max\%20\%20L(\overrightarrow{w})=\sum_{i=1}^n{lng(y_iz_i)}-\frac{C}{2}\sum_{k=1}^l{w_k^2}\qquad(5))

注意：上式中k是从1开始的，不是从0开始的。 另外使用(5)式时必须对输入进行标准化，因为如果输入X同比缩放的话w的解也会跟着变化。

这个式子的目的是使各个wk尽可能地平均，这样在预测未来的数据时概括性一般会更好一些。此时

![](http://www.forkosh.com/mathtex.cgi?\frac{\partial{L}}{\partial{w_k}}=\sum_{i=1}^n{y_ix_{ik}g(-y_iz_i)}-Cw_k,\%20k\ne0\qquad(6))

做过一次Logistic Regrssion的组内分享，可以看下当时的[PPT](http://files.cnblogs.com/zhangchaoyang/Logistic%E5%9B%9E%E5%BD%92.pdf)。

### 结束语

在回归分析中，通常还会涉及其他一些问题：变量的[独立性检验](http://www.cnblogs.com/zhangchaoyang/articles/2642032.html)和相合性检验，预测结果的置信区间和置信度等等，有时间可以多了解一下。












