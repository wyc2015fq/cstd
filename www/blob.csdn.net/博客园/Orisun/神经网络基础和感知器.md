# 神经网络基础和感知器 - Orisun - 博客园







# [神经网络基础和感知器](https://www.cnblogs.com/zhangchaoyang/articles/2588728.html)





### 神经元的变换函数

从净输入到输出的变换函数称为神经元的变换函数，即![](http://www.forkosh.com/mathtex.cgi?\dpi{150}o_j=f(net_j)=f(W_j^TX))
- 阈值型变换函数
比如符号函数

![](https://pic002.cnblogs.com/images/2012/103496/2012071220124970.png)

- 非线性变换函数
比如单极性Sigmoid函数

![](https://pic002.cnblogs.com/images/2012/103496/2012071220141455.png)
又比如双极性S型（又曲正切）函数

![](https://pic002.cnblogs.com/images/2012/103496/2012071220155486.png)

- 分段性变换函数
比如

![](https://pic002.cnblogs.com/images/2012/103496/2012071220275274.png)

- 概率型变换函数
这时输入与输出之间的关系是不确定的，需要用一个随机函数来描述输出状态为1或为0的概率。设输出为1的概率为

![](https://pic002.cnblogs.com/images/2012/103496/2012071220302173.png)

T为温度参数，这种神经元模型也称为热力学模型。


### 学习规则

改变权值的规则称为学习规则或学习算法。
|学习规则|权值调整|权值初始化|学习方式|变换函数| |
|----|----|----|----|----|----|
|向量式|元素式| | | | |
|Hebbian|![](https://pic002.cnblogs.com/images/2012/103496/2012071220513997.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071220525041.png)|0附近的小随机数|无导师|任意|
|离散Percrptron|![](https://pic002.cnblogs.com/images/2012/103496/2012071221043469.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071221051698.png)|任意|有导师|二进制|
|连续感知器δ规则|![](https://pic002.cnblogs.com/images/2012/103496/2012071221155246.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071221164611.png)|任意|有导师|连续|
|最小均方LMS（Widrow-Hoff规则）|![](https://pic002.cnblogs.com/images/2012/103496/2012071221332823.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071221340281.png)|任意|有导师|任意|
|相关Correlation|![](https://pic002.cnblogs.com/images/2012/103496/2012071221422152.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071221450027.png)|0|有导师|任意|
|胜者为王Winner-take-all|![](https://pic002.cnblogs.com/images/2012/103496/2012071221574390.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071221590170.png)|随机，归一化|无导师|连续|
|外星Outstar|![](https://pic002.cnblogs.com/images/2012/103496/2012071222100270.png)|![](https://pic002.cnblogs.com/images/2012/103496/2012071222091876.png)|0|有导师|连续|

Hebb学习规则指出：当神经元的突触前膜电位与突触后膜电位同时为正时，突触传导增强；电位相反时，突触传导减弱。应预先设置权值饱和值，防止输入和输出正负始终一致时出现权值无约束增长。

η是学习率。

在离散感知器学习规则中，期望输出dj和实际输出sgn(WjTX)取值都是-1和1。这种感知器仅适合于二进制神经元。

连续感知器δ规则要求变换函数是可导的，因此只能用于有导师学习中定义的连续变换函数，如Sigmoid函数。实际上δ规则是由输出与期望的最小平方误差推导出来的。

![](https://pic002.cnblogs.com/images/2012/103496/2012071221233564.png)

![](https://pic002.cnblogs.com/images/2012/103496/2012071221253946.png)

![](https://pic002.cnblogs.com/images/2012/103496/2012071221271966.png)

最小均方学习规则实际上是δ规则的特例--在δ规则中令![](https://pic002.cnblogs.com/images/2012/103496/2012071221371228.png)。最小均方学习规则与变换函数无关，不需要对变换函数求导，不仅学习速度快，而且具有较高的精度。它能使实际输出与期望输出之间的平均方差最小（什么意思？why?）。

胜者为王规则中有一个竞争层，对于特定的输入，竞争层的每个神经元均有输出响应，其中响应最大的神经元j*成为获胜神经元，只有获胜神经元才有权调整其权值向量。学习率应该随着学习的进展而减小。

外星学习规则使权向量向期望输出靠拢。

### 单层感知器

 单层感知器只有输入层和输出层，它仅对线性可分问题具有分类能力，在实际中很少使用。

### 多层感知器

 隐藏层的加入使感知器能够解决非线性的分类问题，并且双隐藏层感知器足以解决任何复杂的分类问题。

当变换函数从线性函数变为非线性函数时，分类边界的基本元素从直线变为曲线，这样整个分类边界线变成连续光滑的曲线，从而提高感知器的分类能力。

对于各隐藏层节点来说，不存在期望输出，因而学习规则对隐藏层权值不适用。

### 自适应线性单元（Adaptive Linear Neuron）

使用最小均方学习规则LMS（Least Mean Square）,即最小二乘法。

![](https://pic002.cnblogs.com/images/2012/103496/2012071222372587.png)












