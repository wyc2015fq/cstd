# 学习搜索引擎心得（10.25-11.25) - LeftNotEasy - 博客园







# [学习搜索引擎心得（10.25-11.25)](https://www.cnblogs.com/LeftNotEasy/archive/2009/11/27/1612144.html)





一晃时间就过去了一个月的时间了，从找到工作到现在也有一个月的时间了。

回顾这一个月的时间，感觉学习了一些东西，但是没有到理想的效率。


从10月25日差不多正式的开始了解搜索引擎算起，到11月25日，列举一下完成事情的内容，以后这些内容的记录要更频繁一些，也需要分享些经验。东西堆在一起之后就不太想记录下来了。




**10月25日 - 11月6日**，看了一下Managing Gigabytes（以下简称MG）这本书，感觉还是很不错的一本书，翻译版叫做《深入搜索引擎》，作者是新西兰的一位教授，属于比较严肃的类型，但是对知识点讲的还是非常的细，特别是文本压缩，索引，查询，索引构造讲得不错，都是比较传统的方法，最先进的一些方法没有怎么涉及，怪不得是作为Stanford的教科书和参考书。



**11月6日 - 11月12日**，lw告诉我，需要下载百度知道，和天涯问答的内容，这样可以构建自己的语料库，程序语言就用perl吧。

    于是花了两天来看了下perl，一个非常好用的工具，字符串处理超方便，就是效率不怎么样，可能是我太菜了的原因吧，呵呵。


    下载网页的内容无外乎就是做一个简单的爬虫，MG书中没有讲，又去网上查了一些资料，简单的爬虫设计其实挺简单，不停的分析web页面中的html含有的链接，把需要的页面写到文件里面去，这块使用perl天生的正则表达式非常的容易，代码也很短。

    百度的网页最好下载，甚至连正则表达式都不需要，因为百度知道的链接是如 http://zhidao.baidu.com/question/**126947921**.html ，把.html之前的9维数字进行枚举就可以了。


    天涯问答的稍微复杂一点，主要需要分析链接的类型，如果是如同


    http://wenda.tianya.cn/wenda/thread?tid=40734b71c6b7a07e这样的形式，也就是中间含有thread的，则为问题

    如果是

    http://wenda.tianya.cn/wenda/label?lid=68aa0de477338ddc&clk=cts_ls，也就是中间含有label的，则为一类问题的集合， 把集合想象为树的节点，问题想象为树的叶子，进行宽度优先搜索或者深度优先搜索就可以了。


    如果需要perl源程序的可以跟我联系。



**11月12日 - 11月22日**，看了一些Information Retrieve这本书，看了其中的一些内容，好些不太懂，看英文还是有点费力-_-，这本书写得我感觉比MG要轻松易懂一点，主要看了看Clustering（聚类）和Classification（分类）的方法，主要有kNN与向量法。

    学会了用perl处理下载下来的网页，将无用的信息删除，比如一篇内容就剩下分类和主要的文本内容，html标签和一些无用的东西都删除了。

    学会了基本的分词算法，最简单的分词算法是正向，反向，最大匹配法，在这基础上有一个叫做双向最大匹配法，其实就是在正向和反向分词做完之后选择一个结果更好的，可以认为留下的单字越少越好，也可以认为分出的词数越少越好，分词后的结果可能是下面的



(不然)\引用\起来\必然\要\出现\牵强\附会\(的)\
现象\
造成\引用\(不得)\体\
今天\(来)\老师\(把)\(这)\一组\诗句\奉献\(给)\(你)\
希望\(你)\(能)\收录\(在)\(自己)\(的)\文件\夹\中\
梅须逊雪三分白\
雪却输梅一段香\
卢\梅\坡\
雪\似\梅花\
梅花\似\雪\
似和不似都奇艳\
    括号括出来的是停用词，看起来效果还是不错把，呵呵。 另外对诗句的处理就是用了一些小小的trick，可能造成的错误很多，仅仅是做个实验。




**11月24日 - 11月28日** 这几天不幸感冒，今天才算缓过气来，准备一下计划吧。




另外公布一个我注册的google code地址作为我的第一篇日志的总结，如果有需要的朋友可以任意使用里面的内容，包括了我的代码和一些我找到的参考资料，可以无责任的使用他们，另外如果有朋友想要扩充这个知识库，可以联系一下我


http://code.google.com/p/mynlp/ 


















