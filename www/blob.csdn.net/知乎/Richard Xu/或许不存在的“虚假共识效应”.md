# 或许不存在的“虚假共识效应” - 知乎
# 

**这是本专栏的第 *28* 篇日记**

（又是快一个月没写专栏了……）

（本学期在蹭决策心理学的课，和行为经济学有一定的交叉）

1977年前后，三位心理学家Ross, Greene和House进行了这样一项实验：他们随机地找来40名斯坦福大学的学生，然后询问他们，是否愿意挂着一块写有“Repent!（忏悔！）”的纸板在校园里行走30分钟；在获得这些学生的答复之后，又进一步询问这些学生，在全校范围内，他们认为有多少比例的人会愿意这么做，有多少比例的人则不会愿意这么做。

在这40名学生中，有20人表示愿意接受这项疯狂的挑战，另外20人则表示拒绝，各占一半。然而，愿意接受这项挑战的学生估计，全校有63.5%的学生愿意这么做，而拒绝的学生估计的比例则只有23.3%。换言之，这些学生都表现出一种相同的倾向：认为校园中和自己做出相似选择的人占比更大。

像这样的实验，这三位心理学家还进行了另外三项，这些实验的结果最终被总结在1977年发表的论文“The ‘False Consensus Effect’: An Egocentric Bias
in Social Perception and Attribution Processes”当中，他们将上面提到的这种倾向首次命名为“False Consensus Effect”（虚假共识效应）。

在现实日常中，“虚假共识效应”并不罕见。

Granberg & Brent(1983)就发现，投票者在估计自己所支持的候选人所获得的支持率时，总是倾向于高估。以去年的美国大选为例，川粉普遍会认为有（比实际）更多的人是川粉，而希粉则会认为有（比实际）更多的人是希粉。

Mullen(1983)则提供了另一个例子，一名参加智力竞赛的选手，当他相信自己能够对某一道题给出正确回答时，他会高估其他人答出这道题的概率，反之亦然。

再比如，我们有时候会觉得别人没有“常识”，然而事实上，我们认为别人所缺乏的“常识”，未必真的是常识，而是因为我们自己对某些方面的知识更加熟稔，而错误地高估了别人同样掌握这些知识的概率。

反之，如果我们被委派了某项任务，但是任务难度超出了我们的能力范围，我们会不自觉地认为这项任务对其他人也同样困难，进而低估其他人完成任务的可能性。

可以看出，“虚假共识效应”其实就是一种“锚定效应（Anchoring）”，在评估一项我们并不确定的事件时，我们会以自己为标准进行“锚定”，再在这个基础上结合其他信息进行“调整（Adjustment）”，这种决策过程的结果就是过分强化了自己所支持或者所认为的事情，认为周围的人都与自己想的相似了。

随着“虚假共识效应”的文献与日俱增，并不是所有心理学家都对这套理论买账。的确，我们在决策中确实有“锚定”的做法，但是这种做法就一定导致更差的评估吗？或者说，这样的做法就一定是不理性的吗？

Dawes在1989年发表了论文"Statistical Criteria for Establishing a Truly False
Consensus Effect"，大张旗鼓地对文献中的“虚假共识效应”进行反击。尽管他使用的例子非常简单，但是其威力不可小觑：事实上，以自己进行“锚定”有时能够比不进行“锚定”得到更好的评估结果，而且完全有可能是一项理性的选择。在这篇文章中，他所使用的主要武器就是贝叶斯更新（Bayesian updating）（事实上他还使用了回归分析的方法，但是这两种做法得到的结果是相似的，个人认为贝叶斯更新的做法更具有Behavioral Model的意味）。

他所使用的第一个例子（见论文Table 1）是这样的：假如现在有三个人和两项政策，前两个人支持政策A而反对政策B，第三个人反对政策A而支持政策B。每个人除了知道自己的倾向之外，还知道在两项政策中，有一项有2人支持而另一项只有1人支持（但是他们不知道分别是哪一项），进而可以计算出总人次的1/2选择了支持。

如果他们都了解“虚假共识效应”，因而决定不以自己的选择进行“锚定”的话，他们应当认为两项政策都有1/2的人支持，这一预测的均方误差（Mean Squared Error）是1/36；但是，如果他们以自己的选择进行“锚定”，认为自己选择的政策有5/9的人支持，而自己反对的政策只有4/9的人支持，那么均方误差只有2/81；事实上，可以证明这种“锚定”方法是使得均方误差最小的做法。

那么这个5/9和4/9是怎么来的呢？假定我现在是第一个人，我知道，我支持一项政策的无条件概率(Unconditional Probability)是1/2，如果政策A有2/3的支持率，那么我支持它的条件概率是2/3，如果政策A有1/3的支持率，那么我支持它的条件概率是1/3，而政策A的支持率到底是1/3还是2/3，各有1/2的概率。于是我们可以用贝叶斯公式：

P(A有2/3支持率|我支持A) 

= P(我支持A|A有2/3支持率)*P(A有2/3支持率)/P(我支持A)

= (2/3)*(1/2)/(1/2)

= 2/3

P(A有1/3支持率|我支持A) 

= P(我支持A|A有1/3支持率)*P(A有1/3支持率)/P(我支持A)

= (1/3)*(1/2)/(1/2)

= 1/3

于是给定我支持A，政策A，或者说所支持的政策的支持率的条件期望就是2/3*2/3+1/3*1/3=5/9，同理可以算出，政策B，或者说我反对的政策的支持率的条件期望就是2/3*1/3+1/3*2/3=4/9。（注意：A的支持率和B的支持率的和等于1只是一个巧合，实际上这里是因为无条件支持率为1/2的缘故，如果无条件支持率改为2/3，那么A的支持率和B的支持率的和就应该是4/3）

于是我们就可以得出结论，以自己为样本进行一次贝叶斯更新所得到的预测结果是均方误差最小的。

这里面最Intuitive但是也是最Tricky的地方，就是我们**拿自己做为整体的一个样本进行了贝叶斯更新**。Intuitive在于，这里面的逻辑看起来很简单：如果更多的人支持A，那么我就更有可能支持A，如果没多少人支持A，那么我也不太可能支持A，既然我支持了A，那么应该就有更多的人支持A，这个“更多”到底有多少用贝叶斯更新公式算一下就知道了。Tricky在于，实际上我自己并不是一个合格的“样本”，因为我明确地知道自己的倾向，所以像"P(我支持A|A有2/3支持率)=2/3"这样的说法有一种微妙的违和感，**就好像我支持A完全是临时按照整个人群(population)的支持率分布抽取的结果，而并不是我自己的意志。**

Dawes的另一个例子则直接指向了本文一开始提到的实验。假如我是参与这个实验的一个学生，我并不清楚校园中有多少人愿意接受这项挑战，那么不妨就认为这个比例p的先验分布是[0,1]上的均匀分布。接着，我自己选择接受或者拒绝这项挑战，相当于一个单次Bernoulli实验，实验成功的概率就是全校学生愿意接受挑战的比例p。接下来，我就可以据此更新在给定我愿意/拒绝挑战的情况下，p的后验分布是什么样的了。

我们知道，给定比例p，进行n次Bernoulli实验，其中有k次成功的条件概率是![\binom{n}{k} p^k(1-p)^{n-k}](https://www.zhihu.com/equation?tex=%5Cbinom%7Bn%7D%7Bk%7D+p%5Ek%281-p%29%5E%7Bn-k%7D)。那么反过来，如果我们进行n次Bernoulli实验，其中有k次成功，比例为p的条件概率就是

![\frac{\binom{n}{k}p^k(1-p)^{n-k}}{\int_0^1 \binom{n}{k}q^k(1-q)^{n-k} dq}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cbinom%7Bn%7D%7Bk%7Dp%5Ek%281-p%29%5E%7Bn-k%7D%7D%7B%5Cint_0%5E1+%5Cbinom%7Bn%7D%7Bk%7Dq%5Ek%281-q%29%5E%7Bn-k%7D+dq%7D)。

我们感兴趣的是p的条件期望，也就是![\int_0^1  p\frac{\binom{n}{k}p^k(1-p)^{n-k}}{\int_0^1 \binom{n}{k}q^k(1-q)^{n-k} dq} dp = \frac{\int_0^1 \binom{n}{k}p^{k+1}(1-p)^{n-k} dp}{\int_0^1 \binom{n}{k}q^k(1-q)^{n-k} dq}](https://www.zhihu.com/equation?tex=%5Cint_0%5E1++p%5Cfrac%7B%5Cbinom%7Bn%7D%7Bk%7Dp%5Ek%281-p%29%5E%7Bn-k%7D%7D%7B%5Cint_0%5E1+%5Cbinom%7Bn%7D%7Bk%7Dq%5Ek%281-q%29%5E%7Bn-k%7D+dq%7D+dp+%3D+%5Cfrac%7B%5Cint_0%5E1+%5Cbinom%7Bn%7D%7Bk%7Dp%5E%7Bk%2B1%7D%281-p%29%5E%7Bn-k%7D+dp%7D%7B%5Cint_0%5E1+%5Cbinom%7Bn%7D%7Bk%7Dq%5Ek%281-q%29%5E%7Bn-k%7D+dq%7D)，参考我之前回答过的一个问题[x^3（1-x）^2016的不定积分怎么求？](https://www.zhihu.com/question/50420430)，把积分写成Beta函数的形式，或者用我回答中推导的公式![\int_0^1 x^m(1-x)^ndx =    \frac{1}{(n+m+1)\binom{n+m}{m}}](https://www.zhihu.com/equation?tex=%5Cint_0%5E1+x%5Em%281-x%29%5Endx+%3D++++%5Cfrac%7B1%7D%7B%28n%2Bm%2B1%29%5Cbinom%7Bn%2Bm%7D%7Bm%7D%7D)，都可以得出![ \frac{\int_0^1 \binom{n}{k}p^{k+1}(1-p)^{n-k} dp}{\int_0^1 \binom{n}{k}q^k(1-q)^{n-k} dq} = \frac{k+1}{n+1} \frac{\frac{1}{n+2}}{\frac{1}{n+1}} = \frac{k+1}{n+2}](https://www.zhihu.com/equation?tex=+%5Cfrac%7B%5Cint_0%5E1+%5Cbinom%7Bn%7D%7Bk%7Dp%5E%7Bk%2B1%7D%281-p%29%5E%7Bn-k%7D+dp%7D%7B%5Cint_0%5E1+%5Cbinom%7Bn%7D%7Bk%7Dq%5Ek%281-q%29%5E%7Bn-k%7D+dq%7D+%3D+%5Cfrac%7Bk%2B1%7D%7Bn%2B1%7D+%5Cfrac%7B%5Cfrac%7B1%7D%7Bn%2B2%7D%7D%7B%5Cfrac%7B1%7D%7Bn%2B1%7D%7D+%3D+%5Cfrac%7Bk%2B1%7D%7Bn%2B2%7D)。

当我选择自己是否愿意接受挑战时，我相当于进行了一个单次Bernoulli实验(n=1)。如果我接受，那么k=1，我应该预测p=2/3；如果我拒绝，那么k=0，我应该预测p=1/3。

这时候再回过来看一下数据，参与实验的学生中，接受挑战的学生估计接受者的比例是63.5%，拒绝挑战的学生估计接受者的比例是23.3%，实际上离我们给出的2/3和1/3这两个标准相差都不大（毕竟是只有40个学生的小样本）；和这个实验类似的Study 3和Study 4的其他实验的结果也大多在2/3和1/3附近；换言之，这些学生虽然看起来表现出了“虚假共识效应”，但是他们对于共识的高估都是在合理范围内的，甚至可以说是接近理性的。

（事实上，Dawes建议的做法是比较这两个比例的差值（这里是40.2%）是否接近我们计算出的比例的差值（这里是1/3），因为这个“比例的差值”和先验分布的期望无关，只和先验无条件概率分布的方差有关。）

不过话说回来，Dawes也在论文中说明了，他并不是不承认“虚假共识效应”，而是试图指出，**要想论证某一行为符合“虚假共识效应”，必须给什么样的共识是虚假的，而什么样的共识又不是虚假的划出明确而且正确的界限。**比如说，之所以在Ross, Greene and House(1977)中的诸多实验中，本文一开始的这个实验最常出场，我猜测原因之一就是，恰好各有一半人选择了接受和拒绝，随后却给出了大相径庭的估计，这能够给人以一种“正确的估计明明应该是50%，因而这些人表现出虚假共识效应”的强烈感受。然而，Dawes通过统计手段说明，一个合理的估计完全可能和实际上有多少人选择了接受或拒绝无关，而且这个合理的估计其实远高于50%，这个实验其实落在了并非“虚假共识”的界限之内。不划分清楚这个界限，心理学家们讨论“虚假共识效应”就都是凭感觉，所得出的结论可信度自然大打折扣。

（一不小心又码了3000字……每次都写太长自己都不想看……）

*(Photo credit: "[Character is shown by the things that we do](https://link.zhihu.com/?target=https%3A//visualhunt.com/f/photo/32328128270/e1b07f131a/)" by [Loco Steve](https://link.zhihu.com/?target=https%3A//www.flickr.com/photos/locosteve/32328128270/) via [Visual Hunt](https://link.zhihu.com/?target=https%3A//visualhunt.com/) / [CC BY-SA](https://link.zhihu.com/?target=http%3A//creativecommons.org/licenses/by-sa/2.0/))*

