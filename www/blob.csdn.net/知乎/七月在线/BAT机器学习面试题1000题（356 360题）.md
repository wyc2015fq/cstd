# BAT机器学习面试题1000题（356~360题） - 知乎
# 





**356、xgboost如何寻找最优特征？是有放回还是无放回的呢？**

解析：

xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 -- 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.

xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost 还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合。




**357、谈谈判别式模型和生成式模型？**

解析：

判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。

生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。

由生成模型可以得到判别模型，但由判别模型得不到生成模型。

常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场

常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机




**358、线性分类器与非线性分类器的区别以及优劣**

解析：

线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。

线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。

非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。

常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归

常见的非线性分类器：决策树、RF、GBDT、多层感知机

SVM两种都有（看线性核还是高斯核）

引用自@伟祺







**359、L1和L2的区别**

解析：

L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 

比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.

简单总结一下就是： 

L1范数: 为x向量各个元素绝对值之和。 

L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数 

Lp范数: 为x向量各个元素绝对值p次方和的1/p次方.

在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征，即L1范数可以使权值稀疏，方便特征提取。 

L2范数可以防止过拟合，提升模型的泛化能力。

L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？看导数一个是1一个是w便知, 在靠进零附近, L1以匀速下降到零, 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说, 重要性不在一个数量级上)尽快剔除, L2则是把特征贡献尽量压缩最小但不至于为零. 两者一起作用, 就是把重要性在一个数量级(重要性最高的)的那些特征一起平等共事(简言之, 不养闲人也不要超人)。







**360、L1和L2正则先验分别服从什么分布**

解析：

面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。

引用自：@齐同学

先验就是优化的起跑线, 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。

对参数引入高斯正态先验分布相当于L2正则化, 这个大家都熟悉：



![](https://pic4.zhimg.com/v2-86fa9ed4f0cf8b37097e37382193ea9b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='338' height='70'></svg>)









![](https://pic4.zhimg.com/v2-9972b21ba8b04f3dd1ff70774b0e8db3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='547'></svg>)







对参数引入拉普拉斯先验等价于 L1正则化, 如下图：



![](https://pic3.zhimg.com/v2-9d0eb1f74f8bc820924c7f4cc6e79d76_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='393' height='85'></svg>)









![](https://pic3.zhimg.com/v2-f379d9364c8cd4213051e06fe30e4b5a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='510'></svg>)







从上面两图可以看出, L2先验趋向零周围, L1先验趋向零本身。

引用自：@AntZ




想要获取更多机器学习和深度学习方面知识，点击下方
[机器学习集训营 第六期「线上线下结合，线下在北上深广杭沈济郑成武西十一城」- 七月在线​www.julyedu.com![图标](https://pic2.zhimg.com/v2-0d5b82db757beb2dcc9eb1c23938bf99_180x120.jpg)](https://link.zhihu.com/?target=http%3A//www.julyedu.com/weekend/train6)[深度学习​www.julyedu.com](https://link.zhihu.com/?target=http%3A//www.julyedu.com/course/getDetail/112)



