# BAT机器学习面试1000题系列（306-310） - 知乎
# 



**306. LSTM神经网络输入输出究竟是怎样的？**
@YJango，本题解析来源：[https://www.zhihu.com/question/41949741](https://www.zhihu.com/question/41949741)
Recurrent Layers——介绍（[https://zhuanlan.zhihu.com/p/24720659?refer=YJango](https://zhuanlan.zhihu.com/p/24720659?refer=YJango)）
- 第一要明确的是神经网络所处理的单位全部都是：向量

下面就解释为什么你会看到训练数据会是矩阵和张量
- 常规feedforward 输入和输出：矩阵

      输入矩阵形状：(n_samples, dim_input)
       输出矩阵形状：(n_samples, dim_output)
注：真正测试/训练的时候，网络的输入和输出就是向量而已。加入n_samples这个维度是为了可以实现一次训练多个样本，求出平均梯度来更新权重，这个叫做Mini-batch gradient descent。 如果n_samples等于1，那么这种更新方式叫做Stochastic Gradient Descent (SGD)。
**Feedforward 的输入输出的本质都是单个向量。**
- 常规Recurrent (RNN/LSTM/GRU) 输入和输出：张量

      输入张量形状：(time_steps, n_samples,  dim_input)
       输出张量形状：(time_steps, n_samples,  dim_output)
注：同样是保留了Mini-batch gradient descent的训练方式，但不同之处在于多了time step这个维度。 
**Recurrent 的任意时刻的输入的本质还是单个向量，只不过是将不同时刻的向量按顺序输入网络。所以你可能更愿意理解为一串向量 a sequence of vectors，或者是矩阵。**

python代码表示预测的话：


```
import numpy as np 
#当前所累积的hidden_state,若是最初的vector，则hidden_state全为0
hidden_state=np.zeros((n_samples, dim_input))
#print(inputs.shape)：（time_steps, n_samples, dim_input)
outputs = np.zeros((time_steps, n_samples, dim_output))

for i in range(time_steps): 
    #输出当前时刻的output，同时更新当前已累积的hidden_state outputs[i],
    hidden_state = RNN.predict(inputs[i],hidden_state)
#print(outputs.shape)：(time_steps, n_samples, dim_output)
```


但需要注意的是，Recurrent nets的输出也可以是矩阵，而非三维张量，取决于你如何设计。
- 若想用一串序列去预测另一串序列，那么输入输出都是张量 (例如语音识别或机器翻译 一个中文句子翻译成英文句子（一个单词算作一个向量），机器翻译还是个特例，因为两个序列的长短可能不同，要用到seq2seq；
- 若想用一串序列去预测一个值，那么输入是张量，输出是矩阵 （例如，情感分析就是用一串单词组成的句子去预测说话人的心情）

**Feedforward 能做的是向量对向量的one-to-one mapping，Recurrent 将其扩展到了序列对序列 sequence-to-sequence mapping.**
但单个向量也可以视为长度为1的序列。所以有下图几种类型：
![](https://pic3.zhimg.com/v2-d4c592a9a6f8f7f977462a85baee112a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='720' height='225'></svg>)
除了最左侧的one to one是feedforward 能做的，右侧都是Recurrent所扩展的

若还想知道更多
- 可以将Recurrent的横向操作视为累积已发生的事情，并且LSTM的memory cell机制会选择记忆或者忘记所累积的信息来预测某个时刻的输出。
- 以概率的视角理解的话：就是不断的conditioning on已发生的事情，以此不断缩小sample space
- RNN的思想是: current output不仅仅取决于current input，还取决于previous state；可以理解成current output是由current input和previous hidden state两个输入计算而出的。并且每次计算后都会有信息残留于previous hidden state中供下一次计算




**307.以下关于PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)描述错误的是？**
A.PDF描述的是连续型随机变量在特定取值区间的概率
B.CDF是PDF在特定区间上的积分
C.PMF描述的是离散型随机变量在特定取值点的概率
D.有一个分布的CDF函数H(x),则H(a)等于P(X<=a)

正确答案：A
解析：
概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。
概率密度函数（p robability density function，PDF ）是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。
累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对于所有实数x 与pdf相对。




**308.线性回归的基本假设有哪些？(ABDE)** A.随机误差项是一个期望值为0的随机变量；
B.对于解释变量的所有观测值，随机误差项有相同的方差；
C.随机误差项彼此相关；
D.解释变量是确定性变量不是随机变量，与随机误差项之间相互独立；
E.随机误差项服从正态分布




**309.处理类别型特征时，事先不知道分类变量在测试集中的分布。要将 one-hot encoding（独热码）应用到类别型特征中。那么在训练集中将独热码应用到分类变量可能要面临的困难是什么？**
A. 分类变量所有的类别没有全部出现在测试集中
B. 类别的频率分布在训练集和测试集是不同的
C. 训练集和测试集通常会有一样的分布
答案为：A、B ，如果类别在测试集中出现，但没有在训练集中出现，独热码将不能进行类别编码，这是主要困难。如果训练集和测试集的频率分布不相同，我们需要多加小心。




**310.假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？**
A. ReLU
B. tanh
C. SIGMOID
D. 以上都不是
答案为：B，该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。


