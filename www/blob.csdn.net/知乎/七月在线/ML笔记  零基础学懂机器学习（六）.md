# ML笔记 | 零基础学懂机器学习（六） - 知乎
# 



## **随机森林（random Forest）**

决策树有许多优点：训练时间复杂度低、预测过程比较快速、模型展示容易（容易将得到的决策树做成图片展示出来）等。但同时，单决策树又有些不好的地方：容易过拟合。常用的减轻过拟合的方法有剪枝，但还是不够。

**a) **模型组合（例boosting，bagging等）与决策树相关的算法比较多，这些算法最终生成N(可能会有几百棵以上）棵树，这样可以大大的减少单决策树带来的毛病，有点类似于三个臭皮匠等于一个诸葛亮的做法，虽然这几百棵决策树中的每一棵都很简单（相对于C4.5这种单决策树来说），但是他们组合起来确是很强大。




Boostrap提供了一种组合方法的思想，就是将基分类器的训练结果进行综合分析，而其它的名称如Bagging。Boosting是对组合方法的具体演绎。

组合方法总体上可以分为两种。 

第一种，通过处理训练数据集。这种方法根据某种抽样分布，通过对原始数据集进行再抽样来得到多个数据集。抽样分布决定了一个样本被选作训练的可能性大小，然后使用特定的学习算法为每个训练集建立一个分类器。Bagging袋装和Boosting提升都是这样的思想。Adaboost是Boosting当中比较出众的一个算法。

第二种，通过处理输入特征。在这种方法中，通过选择输入特征的子集来形成每个训练集。随机森林就是通过处理输入特征的组合方法，并且它的基分类器限制成了决策树。

具体方法参见[http://blog.csdn.net/zjsghww/article/details/51591009](https://link.zhihu.com/?target=http%3A//blog.csdn.net/zjsghww/article/details/51591009)




**b) **在最近几年的paper上，如iccv这种重量级的会议，iccv 09年（[http://www.cvpapers.com/iccv2009.html](https://link.zhihu.com/?target=http%3A//www.cvpapers.com/iccv2009.html)）的里面有不少的文章都是与Boosting与随机森林相关的。模型组合+决策树相关的算法有两种比较基本的形式 - 随机森林与GBDT((Gradient Boost Decision Tree)，其他的比较新的模型组合+决策树的算法都是来自这两种算法的延伸。




**c)** 随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。

 我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。该动物到底是老鼠还是松鼠，要依据投票情况来确定，获得票数最多的类别就是森林的分类结果。森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想（关于bagging的一个有必要提及的问题：bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性）




**有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？**

　　每棵树的按照如下规则生成：

　　1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；

　　从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本（理解这点很重要）。

　　为什么要随机抽样训练集？

　　如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；

　　为什么要有放回地抽样？

　　如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

　　2）如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；

　　3）每棵树都尽最大程度的生长，并且没有剪枝过程。

　　一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

　　随机森林分类效果（错误率）与两个因素有关：
- 森林中任意两棵树的相关性：相关性越大，错误率越大；
- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

　　减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。




## **GBDT(gradient boost decision tree)**

　　GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。

　　GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。

　　更多可以参考[https://toutiao.io/posts/u52t61/preview](https://link.zhihu.com/?target=https%3A//toutiao.io/posts/u52t61/preview)及[http://www.cnblogs.com/pinard/p/6140514.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6140514.html)




