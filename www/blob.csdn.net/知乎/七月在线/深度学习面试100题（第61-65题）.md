# 深度学习面试100题（第61-65题） - 知乎
# 



61题

**深度学习中有什么加快收敛/降低训练难度的方法？**

解析：

瓶颈结构

残差

学习率、步长、动量

优化方法

预训练

62题

**请简单说下计算流图的前向和反向传播**

解析：



![](https://pic1.zhimg.com/v2-f5e54286e5ee1e295237746dffad8934_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='232'></svg>)



63题

**请写出链式法则并证明**

解析：

链式法则或链锁定则（英语：chain rule），是求复合函数导数的一个法则。设f和g为两个关于x的可导函数，则复合函数



![](https://pic4.zhimg.com/v2-cf213e2fccab74979a0ced2eb64f3247_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='76' height='27'></svg>)



的导数



![](https://pic3.zhimg.com/v2-13e43368ad8f6b7f4c63f39b69f7e6d6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='83' height='25'></svg>)



为



![](https://pic1.zhimg.com/v2-f3f74388f089807f67cca40b62110ba8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='214' height='26'></svg>)



以下是一个简单的例子



![](https://pic2.zhimg.com/v2-51f1237a4a48e6749f3342834f05a0bd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='870' height='266'></svg>)



以下的简单的一个证明



![](https://pic4.zhimg.com/v2-7721c854400979fd408d3300766b8c4b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='294'></svg>)



64题

**请写出Batch Normalization的计算方法及其应用**

解析：

**机器学习流程简介**

1）一次性设置（One time setup）

- 激活函数（Activation functions）

- 数据预处理（Data Preprocessing）

- 权重初始化（Weight Initialization）

- 正则化（Regularization：避免过拟合的一种技术）

- 梯度检查（Gradient checking）

2）动态训练（Training dynamics）

- 跟踪学习过程 （Babysitting the learning process）

- 参数更新 （Parameter updates)

- 超级参数优化（Hyperparameter optimization）

- 批量归一化（Batch Normalization简称BN，其中，Normalization是数据标准化或归一化、规范化，Batch可以理解为批量，加起来就是批量标准化。解决在训练过程中中间层数据分布发生改变的问题，以防止梯度消失或爆炸、加快训练速度）

3）评估（Evaluation）

- 模型组合（Model ensembles）

(训练多个独立的模型，测试时，取这些模型结果的平均值)

为什么输入数据需要归一化（Normalized Data），或者说，归一化后有什么好处呢？

原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低，所以需要使用输入数据归一化方法，使训练数据与测试数据的分布相同。

另外一方面，加之神经网络训练时一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

为了让训练深度网络简单高效，研究者提出了随机梯度下降法（SGD），但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。

举个例子，比如某个神经元 x = 1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx 0.1 *1 = 0.1；

如果 x = 20, 这样 Wx = 0.1 * 20 = 2。现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了。

如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大, tanh 激励函数输出值也还是 接近1。



![](https://pic1.zhimg.com/v2-8ee08b12cf3e651b10a019166cd3f798_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='523' height='354'></svg>)



换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了. 这样很糟糕, 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生。



![](https://pic3.zhimg.com/v2-eab4fd82bd38e68c7263b85facf48f62_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='600' height='326'></svg>)



既然 x 换到了隐藏层当中, 我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢? 答案是可以的, 因为大牛们发明了一种技术, 叫做 batch normalization, 正是处理这种情况。

Batch Normalization由Google提出在这篇论文中《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出。

与激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层。

**BN的本质原理**：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理（归一化至：均值0、方差为1），然后再进入网络的下一层。不过归一化层可不像我们想象的那么简单，它是一个可学习、有参数（γ、β）的网络层。

归一化公式：



![](https://pic3.zhimg.com/v2-db6a2d61ac84ff4945735e192d7fac72_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='308' height='121'></svg>)



以下是Normalization过程（引用Google论文中的解释）：
![](https://pic3.zhimg.com/v2-dcc939fc636e829963794179d76fa492_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='499' height='409'></svg>)



输入：输入数据x1..xm（这些数据是准备进入激活函数的数据）

计算过程中可以看到,

1.求数据均值；

2.求数据方差；

3.数据进行标准化（个人认为称作正态化也可以）

4.训练参数γ，β

5.输出y通过γ与β的线性变换得到新的值

How to BN？

怎样学BN的参数就是经典的chain rule。

在正向传播的时候，通过可学习的γ与β参数求出新的分布值

在反向传播的时候，通过链式求导方式，修正γ与β以及相关权值



![](https://pic3.zhimg.com/v2-06959f2e022a6f23f601f575da59c6fa_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='485' height='254'></svg>)



Why is BN？

因为BN保证每一层的输入分布稳定，这一点本身可以使得训练加速，而且另一方面它也可以帮助减少梯度消失和梯度爆炸的现象。

梯度消失

关于梯度消失，以sigmoid函数为例子，sigmoid函数使得输出在[0,1]之间。



![](https://pic1.zhimg.com/v2-b8bbaf14c651dc20b11337fceacae130_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='336' height='130'></svg>)



事实上x到了一定大小，经过sigmoid函数的输出范围就很小了，参考下图



![](https://pic4.zhimg.com/v2-c33518db878a62b1d04b39cb983ae65f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='251' height='204'></svg>)



如果输入很大，其对应的斜率就很小，我们知道，其斜率（梯度）在反向传播中是权值学习速率。所以就会出现如下的问题



![](https://pic3.zhimg.com/v2-020ead2fbd87f7648dc9370193c08e5a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='527' height='378'></svg>)



在深度网络中，如果网络的激活输出很大，其梯度就很小，学习速率就很慢。假设每层学习梯度都小于最大值0.25，网络有n层，因为链式求导的原因，第一层的梯度小于0.25的n次方，所以学习速率就慢，对于最后一层只需对自身求导1次，梯度就大，学习速率就快。

这会造成的影响是在一个很大的深度网络中，浅层基本不学习，权值变化小，后面几层一直在学习，结果就是，后面几层基本可以表示整个网络，失去了深度的意义。

**梯度爆炸**

关于梯度爆炸，根据链式求导法，第一层偏移量的梯度=激活层斜率1x权值1x激活层斜率2x…激活层斜率(n-1)x权值(n-1)x激活层斜率n。假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加。

65题

**神经网络中会用到批量梯度下降（BGD）吗？为什么用随机梯度下降（SGD）?**

解析：

1）一般不用BGD

2）a. BGD每次需要用到全量数据，计算量太大

b. 引入随机因素，即便陷入局部极小，梯度也可能不为0，这样就有机会跳出局部极小继续搜索（可以作为跳出局部极小的一种方式，但也可能跳出全局最小。还有解决局部极小的方式：多组参数初始化、使用模拟退火技术）


