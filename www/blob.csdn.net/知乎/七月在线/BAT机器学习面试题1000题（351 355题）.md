# BAT机器学习面试题1000题（351~355题） - 知乎
# 



**351题**

**LR与线性回归的区别与联系**




解析：

LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。

引用自：@AntZ

个人感觉逻辑回归和线性回归首先都是广义的线性回归，

其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，

另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

引用自：@nishizhen

逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

引用自：@乖乖癞皮狗




**352题**

**简单说下有监督学习和无监督学习的区别**

解析：

有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）

无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)




**353题**

**请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？**

解析：

集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权).

决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost决策树使用二阶泰勒展开系数计算最优分裂.

下面所提到的学习器都是决策树:

Bagging方法: 

    学习器间不存在强依赖关系, 学习器可并行训练生成, 集成方式一般为投票;

    Random Forest属于Bagging的代表, 放回抽样, 每个学习器随机选择部分特征去优化;

Boosting方法: 

   学习器之间存在强依赖关系、必须串行生成, 集成方式为加权和;

    Adaboost属于Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数;

    GBDT属于Boosting的优秀代表, 对函数残差近似值进行梯度下降, 用CART回归树做学习器, 集成为回归模型;

    xgboost属于Boosting的集大成者, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息, 集成模型可分类也可回归. 由于它可在特征粒度上并行计算, 结构风险和工程实现都做了很多优化, 泛化, 性能和扩展性都比GBDT要好。

关于决策树，这里有篇《决策树算法》（链接：[http://blog.csdn.net/v_july_v/article/details/7577684](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/7577684)）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文"Adaptive Boosting"（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。

引用自：@AntZ

xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：

1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数

2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性

3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的

引用自：@Xijun LI




**354题**

**为什么xgboost要用泰勒展开，优势在哪里？**

解析：

xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。

引用自：@AntZ




**355题**

**协方差和相关性有什么区别？**

解析：

相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。



![](https://pic1.zhimg.com/v2-3ec9b2fbd78a5e9308e45fa9e85e7040_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='713' height='58'></svg>)







为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。



![](https://pic2.zhimg.com/v2-8b172605307bbc5efce1c19ca2be1141_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='225' height='95'></svg>)



想要获取更多机器学习和深度学习方面知识，欢迎点击下方
[机器学习集训营 第六期「线上线下结合，线下在北上深广杭沈济郑成武西十一城」- 七月在线​www.julyedu.com![图标](https://pic2.zhimg.com/v2-0d5b82db757beb2dcc9eb1c23938bf99_180x120.jpg)](https://link.zhihu.com/?target=http%3A//www.julyedu.com/weekend/train6)


[深度学习 第四期 [加送数学视频，且提供CPU和GPU双云平台，另3人拼团立减100]​www.julyedu.com![图标](https://pic4.zhimg.com/v2-beae58bf3f58f26863e792069479323f_180x120.jpg)](https://link.zhihu.com/?target=http%3A//www.julyedu.com/course/getDetail/112)





