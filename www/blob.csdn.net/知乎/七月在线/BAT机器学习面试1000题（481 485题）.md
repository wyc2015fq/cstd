# BAT机器学习面试1000题（481~485题） - 知乎
# 



**481、Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）**

A、各类别的先验概率P(C)是相等的

B、以0为均值，sqr(2)/2为标准差的正态分布

C、特征变量X的各个维度是类别条件独立随机变量

D、P(X|C)是高斯分布

正确答案是：C

解析：

朴素贝叶斯的条件就是每个变量相互独立。

来源@刘炫320，链接：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)







**482、关于支持向量机SVM,下列说法错误的是（）**

A、L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力

B、Hinge 损失函数，作用是最小化经验分类错误

C、分类间隔为1/||w||，||w||代表向量的模

D、当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习

正确答案是：C

解析：

A正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。

B正确。

C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。

D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大

来源：@刘炫320，链接：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)







**483、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()  **

A、EM算法

B、维特比算法

C、前向后向算法

D、极大似然估计

正确答案是：D

解析：

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

前向后向算法：用来算概率

极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。

来源：@刘炫320，链接：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)







**484、在Logistic Regression 中,如果同时加入L1和L2范数,不会产生什么效果（）**

A、以做特征选择,并在一定程度上防止过拟合

B、能解决维度灾难问题

C、能加快计算速度

D、可以获得更准确的结果

正确答案是：D

解析：

Ｌ１范数具有系数解的特性，但是要注意的是，Ｌ１没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难. 

在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。

对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅《范数规则化》（链接：[http://blog.csdn.net/zouxy09/article/details/24971995/](https://link.zhihu.com/?target=http%3A//blog.csdn.net/zouxy09/article/details/24971995/)）。

来源：@刘炫320，链接：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)







**485、机器学习中L1正则化和L2正则化的区别是？**

A、使用L1可以得到稀疏的权值

B、使用L1可以得到平滑的权值

C、使用L2可以得到稀疏的权值

正确答案是：A

解析：

L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.

L2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。

L1正则化/Lasso 

L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

L2正则化/Ridge regression 

L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。

对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。

可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。

因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)




**今日分享**：

今晚的【推荐系统面试求职】直播课来喽

这次直播课，我们的张同学会分享：

1、推荐算法工程师的职责和要求

2、怎样准备推荐算法工程师的面试

3、面试会问哪些问题

今晚8点半，我们不见不散~

直播链接**→**：**[推荐系统面试求职分享](https://link.zhihu.com/?target=https%3A//www.julyedu.com/live/m_room/D91E8C546A7A96E09C33DC5901307461)**




**今日学习推荐：**

为了让小伙伴们更好的学习，我们为你准备了 **机器学习，深度学习，计算机视觉**

**推荐系统实战** 相关方面知识，详情点击下方学习~
[机器学习集训营 第六期「线上线下结合，线下在北上深广杭沈济郑成武西十一城」- 七月在线​www.julyedu.com![图标](https://pic2.zhimg.com/v2-0d5b82db757beb2dcc9eb1c23938bf99_180x120.jpg)](https://link.zhihu.com/?target=http%3A//www.julyedu.com/weekend/train6)[深度学习集训营 第二期「线上线下结合，线下在北京和上海」- 七月在线​www.julyedu.com![图标](https://pic4.zhimg.com/v2-620bf2896a8efee54df506fbff928993_180x120.jpg)](https://link.zhihu.com/?target=http%3A//www.julyedu.com/weekend/dl2)[计算机视觉 第二期 [催生数千亿市值公司的核心技术，3人拼团立减100]​www.julyedu.com![图标](https://pic1.zhimg.com/v2-ad678df023d3186019376baff35f88f4_180x120.jpg)](https://link.zhihu.com/?target=https%3A//www.julyedu.com/course/getDetail/123)[推荐系统实战 [从十大层面从零构建一个个推荐系统]​www.julyedu.com![图标](https://pic2.zhimg.com/v2-bdd7b4ffb850ec6fafd9ac889d5bcd59_180x120.jpg)](https://link.zhihu.com/?target=https%3A//www.julyedu.com/course/getDetail/124)

