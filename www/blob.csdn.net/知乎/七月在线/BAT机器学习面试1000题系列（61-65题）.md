# BAT机器学习面试1000题系列（61-65题） - 知乎
# 



**61.说说梯度下降法**

@LeftNotEasy，本题解析来源：[http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html)

下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。






![](https://pic4.zhimg.com/v2-8200b0f7cdd3d8f3cd86cff4f384cbb3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='312' height='176'></svg>)



我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等，我们可以做出一个估计函数：



![](https://pic1.zhimg.com/v2-5b62534aff980863261b75104f10a174_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='240' height='29'></svg>)



θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：



![](https://pic2.zhimg.com/v2-c61859dd4016dd73a584b105c8d4e4b5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='124' height='30'></svg>)



我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个进行评估的函数称为损失函数（loss function），描述h函数不好的程度，在下面，我们称这个函数为J函数

在这儿我们可以做出下面的一个损失函数：



![](https://pic4.zhimg.com/v2-39d65a283d14f7272757f8fb98120513_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='240' height='89'></svg>)



换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。

如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，另外一种就是梯度下降法。

梯度下降法的算法流程如下：

1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。

2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。

为了描述的更清楚，给出下面的图：



![](https://pic2.zhimg.com/v2-ccea920471296d22ae4ac5844b283305_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='350' height='258'></svg>)
这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分。θ0，θ1表示θ向量的两个维度。

在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。

然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。



![](https://pic4.zhimg.com/v2-75a6dcfbe2fc04d5e93bf2566bbe27a3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='303' height='222'></svg>)
当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示：



![](https://pic1.zhimg.com/v2-1516b9516b95e779e67427e01850bd40_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='315' height='234'></svg>)



上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。

下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：



![](https://pic2.zhimg.com/v2-b85b48fcdab8325df0f0f46b509e5985_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='431' height='53'></svg>)



下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。



![](https://pic1.zhimg.com/v2-b6c7acb857d391958bfb66622765205c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='355' height='54'></svg>)
一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。

用更简单的数学语言进行描述步骤2）是这样的：



![](https://pic3.zhimg.com/v2-68a078e8ae127f21b65a8834ad45366a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='171' height='240'></svg>)






**62.梯度下降法找到的一定是下降最快的方向么？**  梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。by林小溪（[https://www.zhihu.com/question/30672734/answer/139689869](https://www.zhihu.com/question/30672734/answer/139689869)）。
 一般解释梯度下降，会用下山来举例。假设你现在在山顶处，必须抵达山脚下（也就是山谷最低处）的湖泊。但让人头疼的是，你的双眼被蒙上了无法辨别前进方向。换句话说，你不再能够一眼看出哪条路径是最快的下山路径，如下图（图片来源：[http://blog.csdn.net/wemedia/details.html?id=45460](https://link.zhihu.com/?target=http%3A//blog.csdn.net/wemedia/details.html%3Fid%3D45460)）：



![](https://pic4.zhimg.com/v2-c18b81505268338d705c81c21440d083_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='318'></svg>)



最好的办法就是走一步算一步，先用脚向四周各个方向都迈出一步，试探一下周围的地势，用脚感觉下哪个方向是下降最大的方向。换言之，每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向（当前最陡峭的位置向下）走一步。就这样，每要走一步都根据上一步所在的位置选择当前最陡峭最快下山的方向走下一步，一步步走下去，一直走到我们感觉已经到了山脚。
 当然这样走下去，我们走到的可能并不一定是真正的山脚，而只是走到了某一个局部的山峰低处。换句话说，梯度下降不一定能够找到全局的最优解，也有可能只是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。



![](https://pic3.zhimg.com/v2-c4767a233b387c0635100638a411c282_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='317'></svg>)



@zbxzc（[http://blog.csdn.net/u014568921/article/details/44856915](https://link.zhihu.com/?target=http%3A//blog.csdn.net/u014568921/article/details/44856915)）：更进一步，我们来定义输出误差，即对于任意一组权值向量，那它得到的输出和我们预想的输出之间的误差值。定义误差的方法很多，不同的误差计算方法可以得到不同的权值更新法则，这里我们先用这样的定义：

![](https://pic1.zhimg.com/v2-672f877d05ff4daea816b4ebbcc38360_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='302' height='90'></svg>)
上面公式中D代表了所有的输入实例，或者说是样本，d代表了一个样本实例，od表示感知器的输出，td代表我们预想的输出。
 这样，我们的目标就明确了，就是想找到一组权值让这个误差的值最小，显然我们用误差对权值求导将是一个很好的选择，导数的意义是提供了一个方向，沿着这个方向改变权值，将会让总的误差变大，更形象的叫它为梯度。

![](https://pic3.zhimg.com/v2-c73b3960646a72ae4b028ffc1ff94182_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='92'></svg>)
既然梯度确定了E最陡峭的上升的方向，那么梯度下降的训练法则是：

![](https://pic1.zhimg.com/v2-9113a2f869645cc01d0726506129319c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='461' height='82'></svg>)



梯度上升和梯度下降其实是一个思想，上式中权值更新的+号改为-号也就是梯度上升了。梯度上升用来求函数的最大值，梯度下降求最小值。

这样每次移动的方向确定了，但每次移动的距离却不知道。这个可以由步长（也称学习率）来确定，记为α。这样权值调整可表示为：



![](https://pic1.zhimg.com/v2-c3fc45c84526bf468a44e026c1344eb8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='215' height='61'></svg>)



总之，梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是“最速下降法”。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：






![](https://pic4.zhimg.com/v2-69844f30debabee64a6fb0d809e97d57_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='350' height='384'></svg>)



正因为梯度度下降法在接近最优解的区域收敛速度明显变慢，所以利用梯度下降法求解需要很多次的迭代。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。by@wtq1993，[http://blog.csdn.net/wtq1993/article/details/51607040](https://link.zhihu.com/?target=http%3A//blog.csdn.net/wtq1993/article/details/51607040)

随机梯度下降

普通的梯度下降算法在更新回归系数时要遍历整个数据集，是一种批处理方法，这样训练数据特别忙庞大时，可能出现如下问题：

1）收敛过程可能非常慢；

2）如果误差曲面上有多个局极小值，那么不能保证这个过程会找到全局最小值。

为了解决上面的问题，实际中我们应用的是梯度下降的一种变体被称为随机梯度下降。

上面公式中的误差是针对于所有训练样本而得到的，而随机梯度下降的思想是根据每个单独的训练样本来更新权值，这样我们上面的梯度公式就变成了：



![](https://pic4.zhimg.com/v2-8957ec23e38ad0f5750715aefdcd8523_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='459' height='91'></svg>)



经过推导后，我们就可以得到最终的权值更新的公式：



![](https://pic1.zhimg.com/v2-d42adec70fc2060ee35e5e3fa89959b8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='293' height='123'></svg>)



有了上面权重的更新公式后，我们就可以通过输入大量的实例样本，来根据我们预期的结果不断地调整权值，从而最终得到一组权值使得我们的算法能够对一个新的样本输入得到正确的或无限接近的结果。 

这里做一个对比

设代价函数为



![](https://pic4.zhimg.com/v2-15bdbf902560b174cffe20a2ba9bdab3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='565' height='144'></svg>)



## 批量梯度下降






![](https://pic1.zhimg.com/v2-d5ce761f4a9652c8666b42f3b0fd2750_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='631' height='152'></svg>)



参数更新为： 



![](https://pic4.zhimg.com/v2-cad218f7486ba224337f86edb3f453b3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='151'></svg>)



i是样本编号下标，j是样本维数下标，m为样例数目，n为特征数目。所以更新一个θj需要遍历整个样本集



![](https://pic1.zhimg.com/v2-9f5f08c336e29f7a9459521371d30934_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='468' height='99'></svg>)



## 随机梯度下降

参数更新为：



![](https://pic4.zhimg.com/v2-0d26430d69c21d2872d5a0da81fdeefb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='557' height='100'></svg>)



i是样本编号下标，j是样本维数下标，m为样例数目，n为特征数目。所以更新一个θj只需要一个样本就可以。



![](https://pic3.zhimg.com/v2-64da6a0c02ef2a900fd3576912634372_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='541' height='181'></svg>)



下面两幅图可以很形象的对比各种优化方法（图来源：[http://sebastianruder.com/optimizing-gradient-descent/](https://link.zhihu.com/?target=http%3A//sebastianruder.com/optimizing-gradient-descent/)）：



![](https://pic1.zhimg.com/v2-5d5166a3d3712e7c03af74b1ccacbeac_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='620' height='480'></svg>)



SGD各优化方法在损失曲面上的表现

从上图可以看出， Adagrad、Adadelta与RMSprop在损失曲面上能够立即转移到正确的移动方向上达到快速的收敛。而Momentum 与NAG会导致偏离(off-track)。同时NAG能够在偏离之后快速修正其路线，因为其根据梯度修正来提高响应性。



![](https://pic1.zhimg.com/v2-4a3b4a39ab8e5c556359147b882b4788_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='620' height='480'></svg>)



SGD各优化方法在损失曲面鞍点处上的表现




**63.牛顿法和梯度下降法有什么不同**

@wtq1993，[http://blog.csdn.net/wtq1993/article/details/51607040](https://link.zhihu.com/?target=http%3A//blog.csdn.net/wtq1993/article/details/51607040)
 1）牛顿法（Newton's method）

牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

## 具体步骤：

首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f  ' (x0)（这里f ' 表示函数 f  的导数）。然后我们计算穿过点(x0,  f  (x0)) 并且斜率为f '(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：



![](https://pic1.zhimg.com/v2-67d17c4d18d49350164d3ab365511604_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='334' height='55'></svg>)



我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f  (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：



![](https://pic2.zhimg.com/v2-4da11e827b300343a0126023b01e043d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='223' height='64'></svg>)



已经证明，如果f  ' 是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f  ' (x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：



![](https://pic2.zhimg.com/v2-b5ca9af78b7a58090462c400e067887d_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='673' height='480'></svg>)



关于牛顿法和梯度下降法的效率对比：

a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。

b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。



![](https://pic4.zhimg.com/v2-eca4862f39d84ea99284cbc7d489eb13_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='266' height='273'></svg>)



注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

牛顿法的优缺点总结：

优点：二阶收敛，收敛速度快；

缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。




**64.什么是拟牛顿法（Quasi-Newton Methods）**

@wtq1993，[http://blog.csdn.net/wtq1993/article/details/51607040](https://link.zhihu.com/?target=http%3A//blog.csdn.net/wtq1993/article/details/51607040)
 拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

具体步骤：

拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：



![](https://pic3.zhimg.com/v2-c6d39afdf5e6b6e6f8f85c8f6ccfdd82_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='305' height='87'></svg>)



这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：



![](https://pic1.zhimg.com/v2-7959081dcadd2e6dc9e1e35fedacf8a0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='180' height='52'></svg>)



　　其中我们要求步长ak满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵Bk代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：



![](https://pic4.zhimg.com/v2-f590dd81af3174c304b1e98cb1c5baef_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='371' height='59'></svg>)



我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求



![](https://pic3.zhimg.com/v2-6a4ecfd3e183bfd248acf2c21de760a6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='271' height='54'></svg>)



从而得到



![](https://pic4.zhimg.com/v2-97f272ee638bfbd92ff37cf550fcb91b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='304' height='51'></svg>)



这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。




**65.请说说随机梯度下降法的问题和挑战**
![](https://pic4.zhimg.com/v2-6b6ed73b93efd292d403994224ce8027_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='225'></svg>)![](https://pic4.zhimg.com/v2-5cea3448dbea85f127477ddfa1b02837_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='300'></svg>)![](https://pic4.zhimg.com/v2-23605b55339f454d52deb178493e75a3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='480'></svg>)![](https://pic3.zhimg.com/v2-78d3a9199f02639e12d1a41598e09a5e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='480'></svg>)
那到底如何优化随机梯度法呢？详情请点击：[https://ask.julyedu.com/question/7913](https://link.zhihu.com/?target=https%3A//ask.julyedu.com/question/7913)


