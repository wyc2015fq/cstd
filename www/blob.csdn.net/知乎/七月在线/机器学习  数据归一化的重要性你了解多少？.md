# 机器学习 | 数据归一化的重要性你了解多少？ - 知乎
# 



在机器学习中，为何要经常对数据做归一化，很多伙伴或许并不知道这样做的作用，今天小七给大家简单介绍下~




机器学习模型被互联网行业广泛应用，

如排序（参见：排序学习实践[http://www.cnblogs.com/LBSer/p/4439542.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/LBSer/p/4439542.html)）、推荐、反作弊、定位（参见：基于朴素贝叶斯的定位算法[http://www.cnblogs.com/LBSer/p/4020370.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/LBSer/p/4020370.html)）等。

一般做机器学习应用的时候大部分时间是花费在特征处理上，其中很关键的一步就是对特征数据进行归一化。





为什么要归一化呢？很多同学并未搞清楚




维基百科给出的解释：1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度。




下面再简单扩展解释下这两点。




01

归一化后加快了梯度下降求最优解的速度




1 归一化为什么能提高梯度下降法求解最优解的速度？

斯坦福机器学习视频做了很好的解释：[https://class.coursera.org/ml-003/lecture/21](https://link.zhihu.com/?target=https%3A//class.coursera.org/ml-003/lecture/21)




如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；




而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。




因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。



![](https://pic1.zhimg.com/v2-3623fb9ca7a0154b9356a5b7350b86d4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='809' height='417'></svg>)






02

归一化有可能提高精度




2 归一化有可能提高精度

一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。




03

归一化的类型

 归一化的类型





1）线性归一化
![](https://pic2.zhimg.com/v2-74c8767c8268eb8633b53e395cf1f83d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='190' height='48'></svg>)
这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。





2）标准差标准化

经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：
![](https://pic1.zhimg.com/v2-23b574f2cb6bfc48a2ad4bd411072908_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='79' height='30'></svg>)
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。





3）非线性归一化

经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。


