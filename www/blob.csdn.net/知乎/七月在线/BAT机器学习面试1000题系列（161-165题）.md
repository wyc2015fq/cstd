# BAT机器学习面试1000题系列（161-165题） - 知乎
# 



**161.当在卷积神经网络中加入池化层(pooling layer)时，变换的不变性会被保留，是吗？**

A 不知道

B 看情况

C 是

D 否

　　答案：（C）

使用池化时会导致出现不变性。




**162.**当数据过大以至于无法在RAM中同时处理时，哪种梯度下降方法更加有效？

A 随机梯度下降法(Stochastic Gradient Descent)

B 不知道

C 整批梯度下降法(Full Batch Gradient Descent)

D 都不是

　　答案：（A）




**163.下图是一个利用sigmoid函数作为激活函数的含四个隐藏层的神经网络训练的梯度下降图。这个神经网络遇到了梯度消失的问题。下面哪个叙述是正确的？**



**![](https://pic1.zhimg.com/v2-584948662e91cd078a988c3c0603978c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='348'></svg>)**



　　A 第一隐藏层对应D，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应A
　　B 第一隐藏层对应A，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应D
　　C 第一隐藏层对应A，第二隐藏层对应B，第三隐藏层对应C，第四隐藏层对应D
　　D 第一隐藏层对应B，第二隐藏层对应D，第三隐藏层对应C，第四隐藏层对应A 

　　答案：（A）

由于反向传播算法进入起始层，学习能力降低，这就是梯度消失。 




**164.对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？**

A 其他选项都不对

B 没啥问题，神经网络会正常开始训练

C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西

D 神经网络不会开始训练，因为没有梯度改变

答案：（C）




**165.下图显示，当开始训练时，误差一直很高，这是因为神经网络在往全局最小值前进之前一直被卡在局部最小值里。为了避免这种情况，我们可以采取下面哪种策略？**



**![](https://pic1.zhimg.com/v2-f3a656c63b3b9ebd94987e1781a3cff0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='261'></svg>)**
A  改变学习速率，比如一开始的几个训练周期不断更改学习速率
 B 一开始将学习速率减小10倍，然后用动量项(momentum)
 C 增加参数数目，这样神经网络就不会卡在局部最优处
 D 其他都不对 

　　答案：（A）

　　选项A可以将陷于局部最小值的神经网络提取出来。


