# BAT机器学习面试1000题系列（116-120题） - 知乎
# 



**116.如何进行特征选择？**  特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解
 常见的特征选择方式：
 1. 去除方差较小的特征
 2. 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
 3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
 4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。




**117.数据预处理**   1. 缺失值，填充缺失值fillna：
 i. 离散：None,
 ii. 连续：均值。
 iii. 缺失值太多，则直接去除该列
 2. 连续值：离散化。有的模型（如决策树）需要离散值
 3. 对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作
 4. 皮尔逊相关系数，去除高度相关的列




**118.你知道有哪些数据处理和特征工程的处理？**
![](https://pic3.zhimg.com/v2-337ddb5c72dcc38b1b396b3712bf289e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='707'></svg>)



  更多请查看此课程《机器学习工程师 第八期 [六大阶段、层层深入]》第7次课 特征工程（[https://www.julyedu.com/course/getDetail/65](https://link.zhihu.com/?target=https%3A//www.julyedu.com/course/getDetail/65)）




**119.简单说说特征工程**
**![](https://pic3.zhimg.com/v2-7529eadf3ddfa31c832fb4c03d82ac92_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='620' height='460'></svg>)**



上图来源：[http://www.julyedu.com/video/play/18](https://link.zhihu.com/?target=http%3A//www.julyedu.com/video/play/18)




**120.请对比下Sigmoid、Tanh、ReLu这三个激活函数**
![](https://pic4.zhimg.com/v2-caa57823d492debf38f79c34561660c7_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='401'></svg>)



logistic函数，应用在Logistic回归中。<span style="color: rgb(51, 51, 51); font-family:;" new="" times="" 14px;"="">logistic回归的目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

假设函数
![](https://pic2.zhimg.com/v2-6bf0897a6deb29ba6813ec9143cb2c19_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='235' height='53'></svg>)
其中x是n维特征向量，函数g就是logistic函数。

而
![](https://pic1.zhimg.com/v2-328e73a24f9d2880434d8d4523eab4f4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='124' height='48'></svg>)
的图像是



![](https://pic3.zhimg.com/v2-791f908561ca2edafa63b8d16f62b46a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='332' height='274'></svg>)



可以看到，将无穷映射到了(0,1)。

而假设函数就是特征属于y=1的概率。



![](https://pic1.zhimg.com/v2-f9c18735113e1407c23f74164bcb6d5c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='245' height='52'></svg>)



从而，当我们要判别一个新来的特征属于哪个类时，只需求
![](https://pic2.zhimg.com/v2-b51f7e387e0dbf3ad7b9d19d11eb28a5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)
即可，若
![](https://pic2.zhimg.com/v2-b51f7e387e0dbf3ad7b9d19d11eb28a5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)
大于0.5就是y=1的类，反之属于y=0类。



![](https://pic4.zhimg.com/v2-a3176c20eae1c09897a8f6db414a6e9f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='354'></svg>)![](https://pic1.zhimg.com/v2-5f633e259c75e2bef309e70b04faf148_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='401'></svg>)
更多详见：[https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ)




