# BAT机器学习面试题1000题（361~365题） - 知乎
# 



**361、简单介绍下logistics回归？**

解析：

Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

    假设函数



![](https://pic2.zhimg.com/v2-e777c240f4312e03dc9ed4e73cca5dc5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='235' height='53'></svg>)







    其中x是n维特征向量，函数g就是logistic函数。

    而



![](https://pic3.zhimg.com/v2-33eeee452f0633c7ea309cc80838419e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='124' height='48'></svg>)




的图像是



![](https://pic3.zhimg.com/v2-a5dcfc5e9117abcb54a8255b4a1a9666_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='332' height='274'></svg>)







    可以看到，将无穷映射到了(0,1)。

    而假设函数就是特征属于y=1的概率。



![](https://pic3.zhimg.com/v2-f48b73f8d5df4083c18e164e346339e6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='245' height='52'></svg>)







    从而，当我们要判别一个新来的特征属于哪个类时，只需求



![](https://pic1.zhimg.com/v2-10407afe808396b311be78950917e2fc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)



即可，若



![](https://pic1.zhimg.com/v2-10407afe808396b311be78950917e2fc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)



大于0.5就是y=1的类，反之属于y=0类。

    此外，



![](https://pic1.zhimg.com/v2-10407afe808396b311be78950917e2fc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)




只和



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)



有关，



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)



>0，那么



![](https://pic1.zhimg.com/v2-10407afe808396b311be78950917e2fc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)



>0.5，而g(z)只是用来映射，真实的类别决定权还是在于



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)



。再者，当



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)



>>0时，



![](https://pic1.zhimg.com/v2-10407afe808396b311be78950917e2fc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)



=1，反之



![](https://pic1.zhimg.com/v2-10407afe808396b311be78950917e2fc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='33' height='16'></svg>)



=0。如果我们只从



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)




出发，希望模型达到的目标就是让训练数据中y=1的特征



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)



>>0，而是y=0的特征



![](https://pic1.zhimg.com/v2-afe8e79fb7cd3372af5f0ddb7da66c2c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='23' height='17'></svg>)



<<0。Logistic回归就是要学习得到θ，使得正例的特征远大于0，负例的特征远小于0，而且要在全部训练实例上达到这个目标。

    接下来，尝试把logistic回归做个变形。首先，将使用的结果标签y = 0和y = 1替换为y = -1,y = 1，然后将



![](https://pic2.zhimg.com/v2-4325b0ba5a332c42f4fefe048f8050f1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='218' height='17'></svg>)



（



![](https://pic3.zhimg.com/v2-a5a068a5b99e2deae17bad934ce9a19e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='39' height='16'></svg>)



)中的



![](https://pic3.zhimg.com/v2-2ef709217214c2f4627d80448d280852_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='14' height='16'></svg>)



替换为b，最后将后面的



![](https://pic1.zhimg.com/v2-f085d47a698448530c9dd178ff758360_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='145' height='16'></svg>)



替换为



![](https://pic3.zhimg.com/v2-d5043fa2d17993faf2ce93148dcd3692_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='26' height='17'></svg>)




。如此，则有了



![](https://pic4.zhimg.com/v2-ce6a41f7ceadd8e6c912a43901967d87_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='92' height='17'></svg>)



。也就是说除了y由y=0变为y=-1外，线性分类函数跟logistic回归的形式化表示



![](https://pic2.zhimg.com/v2-9d011c3e39ceeca578504392fda10039_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='186' height='17'></svg>)




没区别。

    进一步，可以将假设函数



![](https://pic3.zhimg.com/v2-a4944917098bd059099af3181999c75a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='136' height='18'></svg>)




中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：



![](https://pic4.zhimg.com/v2-a6df9eb9ba9d9a78f7d4341d75497a4b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='131' height='30'></svg>)







**362、说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。**

解析：

给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，其中实例



![](https://pic1.zhimg.com/v2-192c94c1f3233a31747f2efb80f6507c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='53' height='15'></svg>)




，而实例空间



![](https://pic2.zhimg.com/v2-9c852f563229912549eff3c642a65d39_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='66' height='16'></svg>)



，yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

    Adaboost的算法流程如下：

步骤1. 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。



![](https://pic3.zhimg.com/v2-b8f824b2ac627e03a04c70667476a8aa_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='562' height='69'></svg>)






步骤2. 进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮

	a. 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）：



![](https://pic2.zhimg.com/v2-3916946e3bc54e92adfdd8ed9da30091_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='212' height='36'></svg>)






b. 计算Gm(x)在训练数据集上的分类误差率



![](https://pic1.zhimg.com/v2-5d59ab4599b33f9616827b9a5e925d14_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='399' height='64'></svg>)



由上述式子可知，Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。

	c. 计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）：



![](https://pic1.zhimg.com/v2-7557706de2209fa0219dfc86fdaf2604_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='201' height='77'></svg>)







由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

	d. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代



![](https://pic2.zhimg.com/v2-15ed845210d2cb703fad8781576363a1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='328' height='86'></svg>)







使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

    其中，Zm是规范化因子，使得Dm+1成为一个概率分布：



![](https://pic4.zhimg.com/v2-f5613e61ce06fa4cfa7e84ceda4f6a3f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='290' height='64'></svg>)






步骤3. 组合各个弱分类器



![](https://pic1.zhimg.com/v2-19277953d478704ce5ecec07f15217c0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='228' height='78'></svg>)







从而得到最终分类器，如下：



![](https://pic4.zhimg.com/v2-adb01a0883ae48d3a14a75a6de1dc24b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='407' height='78'></svg>)







更多请查看此文：《Adaboost 算法的原理与推导》（链接：[http://blog.csdn.net/v_july_v/article/details/40718799](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/40718799)）。




**363 、经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：**



![](https://pic1.zhimg.com/v2-75565dbdad9b075e2327dfa8c8bd0754_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='311'></svg>)




这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。

解析：

用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么"拼写检查"要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求



![](https://pic2.zhimg.com/v2-26b556f3ddc75cb1c62e0568d61df021_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='53' height='19'></svg>)



的最大值。

    而根据贝叶斯定理，有：



![](https://pic2.zhimg.com/v2-2ee7f079979bf924a793e2519256b029_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='234' height='19'></svg>)






由于对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化



![](https://pic1.zhimg.com/v2-c4153f35077eb586f82b4abdff74bc64_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='106' height='19'></svg>)







即可。其中：

P(c)表示某个正确的词的出现"概率"，它可以用"频率"代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。

P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见博客中的这篇文章。

    所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见这里。




**364、为什么朴素贝叶斯如此“朴素”？**

解析：

因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是"很简单很天真"地假设样本特征彼此独立. 这个假设现实中基本上不存在, 但特征相关性很小的实际情况还是很多的, 所以这个模型仍然能够工作得很好。

引用自：@AntZ




**365、	请大致对比下plsa和LDA的区别**




解析：

pLSA中，主题分布和词分布确定后，以一定的概率



![](https://pic4.zhimg.com/v2-1014b3af723634f608bb6cb21b2c4c3f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='56' height='17'></svg>)



、



![](https://pic1.zhimg.com/v2-9e43bf38dfd22de9e90220a36d677b70_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='60' height='19'></svg>)



分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：



![](https://pic1.zhimg.com/v2-8fec6ee9ebc5ec13a3e1482fca5209f8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='25' height='19'></svg>)




（由



![](https://pic1.zhimg.com/v2-9e43bf38dfd22de9e90220a36d677b70_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='60' height='19'></svg>)



转换而来）和



![](https://pic2.zhimg.com/v2-f0bcb4e30607bfd701e015e0750c246d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='22' height='16'></svg>)



（由



![](https://pic4.zhimg.com/v2-1014b3af723634f608bb6cb21b2c4c3f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='56' height='17'></svg>)



转换而来）。

文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值。

举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示（图截取自沈博PPT上）：



![](https://pic1.zhimg.com/v2-7ca9f619e4c8ce973c51fd49902ec040_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='490' height='348'></svg>)







但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。

文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。

还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：



![](https://pic4.zhimg.com/v2-df32e2a8856f06360044ef0c3971acbb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='490' height='348'></svg>)






换言之，LDA在pLSA的基础上给这两参数（



![](https://pic4.zhimg.com/v2-1014b3af723634f608bb6cb21b2c4c3f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='56' height='17'></svg>)




、



![](https://pic1.zhimg.com/v2-9e43bf38dfd22de9e90220a36d677b70_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='60' height='19'></svg>)




）加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布



![](https://pic4.zhimg.com/v2-684f85069944690f030df49e8da27e43_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='12' height='9'></svg>)



，和一个词语分布的先验分布Dirichlet分布



![](https://pic4.zhimg.com/v2-e2b3d40bbb2052e5ca23194ba4466277_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='12' height='18'></svg>)



。综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

更多请参见：《通俗理解LDA主题模型》（链接：[http://blog.csdn.net/v_july_v/article/details/41209515](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/41209515)）。




想要获取更多机器学习和深度学习相关知识，点击下方
[机器学习集训营 第六期「线上线下结合，线下在北上深广杭沈济郑成武西十一城」- 七月在线​www.julyedu.com![图标](https://pic2.zhimg.com/v2-0d5b82db757beb2dcc9eb1c23938bf99_180x120.jpg)](https://link.zhihu.com/?target=https%3A//www.julyedu.com/weekend/train6)[深度学习​www.julyedu.com](https://link.zhihu.com/?target=http%3A//www.julyedu.com/course/getDetail/112)



