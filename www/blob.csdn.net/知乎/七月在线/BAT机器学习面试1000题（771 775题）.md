# BAT机器学习面试1000题（771~775题） - 知乎
# 



**771、"过拟合是有监督学习的挑战，而不是无监督学习"以上说法是否正确：**

A、对

B、错

正确答案是： B

解析：

答案：B

我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数。




**772、下列表述中，在k-fold交叉验证中关于选择K说法正确的是：**

A、较大的K并不总是好的，选择较大的K可能需要较长的时间来评估你的结果

B、相对于期望误差来说，选择较大的K会导致低偏差（因为训练folds会变得与整个数据集相似）

C、在交叉验证中通过最小化方差法来选择K值

D、以上都正确

正确答案是：D

解析：

答案：D

较大的K意味着更小的偏差（因为训练folds的大小接近整个dataset）和更多的运行时间（极限情况是：留一交叉验证）。当选取K值的时候，我们需要考虑到k-folds 准确度的方差。




**773、一个回归模型存在多重共线问题。在不损失过多信息的情况下，下列哪个操作不可取**

A、移除共线的两个变量

B、移除共线的两个变量其中一个

C、我们可以计算方差膨胀因子（variance inflation factor)来检查存在的多重共线性并采取相应的措施

D、移除相关变量可能会导致信息的丢失，为了保留这些变量，我们可以使用岭回归(ridge)或lasso等回归方法对模型进行惩罚

正确答案是：A

解析：

答案：A

为了检查多重共线性，我们可以创建相关系数矩阵来辨别和移除相关系数大于75%的变量(阈值根据情况设定),除此之外，我们可以使用VIF方法来检查当前存在的共线变量。VIF<=4表明没有多种共线，VIF>=10表明有着严重的多重共线性。当然，我们也可以使用公差(tolerance)作为评估指标。

但是,移除相关变量可能导致信息的丢失，为了保留这些变量，我们可以使用带惩罚的回归方法。我们也可以在相关变量之间随机加入噪音，使得变量之间存在差异。但增加噪音可能影响准确度，因此这种方法应该小心使用。




**774、评估模型之后，得出模型存在偏差，下列哪种方法可能解决这一问题：**

A、减少模型中特征的数量

B、向模型中增加更多的特征

C、增加更多的数据

D、B 和 C

E、以上全是

正确答案是： B

解析：

答案 ：B

高偏差意味这模型不够复杂(欠拟合)，为了模型更加的强大，我们需要向特征空间中增加特征。增加样本能够降低方差




**775、在决策树中，用作分裂节点的information gain说法不正确的是**

A、较小不纯度的节点需要更多的信息来区分总体

B、信息增益可以使用熵得到

C、信息增益更加倾向于选择有较多取值的属性

正确答案是：A

解析：

使用信息增益作为决策树节点属性选择的标准，由于信息增益在类别值多的属性上计算结果大于类别值少的属性上计算结果，这将导致决策树算法偏向选择具有较多分枝的属性。




**七月在线，年终大回馈 | 100本纸质西瓜书免费包邮送**
**领取戳：**[免费送书](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Q4d_Sse1i2ZF3vLU9kORug)

邀好友免费畅学经典好课
VIP年会员震撼升级，1折开抢
更多AI课程等你1元秒走
**传送门**＞＞[双旦会场](https://link.zhihu.com/?target=https%3A//www.julyedu.com/christyear/index)


**今日学习推荐：**
为了让小伙伴们更好的学习，我们为你准备了 **机器学习** 、**无人驾驶实战、spark大数据实战班Linux从零入门实战** 相关方面知识，详情点击下方学习~



[机器学习集训营 第七期「线上线下结合，线下在北上深广杭沈济郑成武西长十二城」- 七月在线​www.julyedu.com](https://link.zhihu.com/?target=http%3A//www.julyedu.com/weekend/train7)[https://www.julyedu.com/course/getDetail/136​www.julyedu.com](https://link.zhihu.com/?target=https%3A//www.julyedu.com/course/getDetail/136)[https://www.julyedu.com/course/getDetail/133​www.julyedu.com](https://link.zhihu.com/?target=https%3A//www.julyedu.com/course/getDetail/133)[https://www.julyedu.com/course/getDetail/137​www.julyedu.com](https://link.zhihu.com/?target=https%3A//www.julyedu.com/course/getDetail/137)





