# 调教神经网络来生成老婆(划掉)动漫头像的一些尝试（二） - 知乎
# 

第一部分请猛戳[这里](https://zhuanlan.zhihu.com/p/57553117)。

有了一个训练好的GAN以后，我们可以用它来干一些什么事情呢？最简单的事情就是采样生成图片啦。除了上一部分演示的直接采样以外，我们也可以在latent space中游走/插值，生成平滑的过渡动画。比如下面是几个不同画风表情的炮姐之间穿插的动画。
![](https://pic2.zhimg.com/v2-5badcbe2a56792ec2da35a777860e0dd_b.gif)
炮姐的画风变化可能不太剧烈。。来一个七头身和二头身的小埋对比。。
![](https://pic2.zhimg.com/v2-2bcb6336112c9d121a167168a4b19009_b.gif)
但这样玩有一个缺陷是，我们其实对latent variable没有任何控制。完全只是随机产生一个latent variable，然后扔给GAN去生成一张图像。有没有更刺激一点的玩法呢？如果我们手头有一张图片，就想让GAN生成这张图片，然后再对latent variable做一些调整，有没有可能呢？ 

如果把这个问题抽象成一个数学问题，其实就是一个优化问题。在训练的时候我们做的事情是，通过调整网络的参数，使得网络生成的图片（的分布）和给定的图片尽可能的相似。而在这里我们想干的事情是，固定网络的参数，通过调整网络的输入，使得生成的图片本身和给定的图片尽可能的相似。所以本质上我们拿到latent variable的过程和网络的训练过程是一样的，唯一的不同是loss和优化的变量不同。这里的loss因为不是针对分布所以不能用discriminator，也不能用像素的欧氏距离，而应该用perceptual loss。这里我随便选了个VGG16的feature embedding上的L2 distance，定好变量，扔给SGD就万事大吉。稍微调调learning rate，随便优化个几百个迭代就好了。

那么结果如何呢？下面这个图上面一行是我们给这个优化算法的输入，而下面一行是我们找到的latent variable生成的图像。可以看到前景基本上是靠谱的，只是背景很难学出来。 芙兰||达 是个比较意外的反例。在优化的时候它的perceptual loss其实降到了很低，但出来的结果人眼看上去还是相当不靠谱。所以这个loss还是有改进的空间。也许用triplet loss训练一个metric network出来然后再去优化latent variable会比较好。另外，可能因为炮姐占了训练数据的大头，大多数非炮姐的角色出来的都很差。不过这也有可能是SGD的问题，没办法跳出局部最小。 
![](https://pic3.zhimg.com/v2-96dd47e9856f4a8cf2062253735ddd72_b.jpg)对latent variable的优化结果
有了latent variable以后，就可以探索周围的方向了。这个有正规军和野路子两种思路。正规军的思路是，我们在整个latent space里面采样比如1000张图像，然后放到网上去标注属性，比如年龄，性别，发色，瞳色等等。然后用这个做训练数据做一把LDA (Linear Discriminate Analysis，不是那个topic model)。就可以找到年龄对应的方向，性别对应的方向等等了。那如果我没钱标数据怎么办？钢铁侠和蜘蛛侠教导我们，富人靠科技，穷人靠变异。对我们穷人来说，就给一个latent variable加个随机扰动，不停变异手工看结果不就好了。具体的说，比如对第一列的这两个炮姐，我们可以给她的latent variable加一些随机的向量，然后把新的向量扔给GAN去生成图片，得到下面这些图像。
![](https://pic4.zhimg.com/v2-60eb019dc37b4d342c3d5050fd7410b3_b.jpg)对latent方向的随机采样
那么如果运气够好（或者采样足够多）的话，从中间我们就可以发现一些比较有希望的方向。比如第一行的最后一列炮姐的眼睛睁开了。就是长相怎么在往结标淡希的方向发展。。我们在这个方向上做个动画看看。
![](https://pic1.zhimg.com/v2-dc72806378b321a12ca54a5983598610_b.gif)
我擦咧这眼睛怎么就突然长出来了！太惊悚了吧！还有如果往这个方向走太远的话，整个画风都变得诡异了。。大眼睛小嘴巴尖下巴。。气氛突然网红了起来。。 

不过如果适当选择方向和控制动画的起止点的话，还是可以有不错的效果的。比如我们可以让炮姐长出发饰和衣服，或者张开嘴巴。
![](https://pic1.zhimg.com/v2-8ec12d50c753c02d2a1894774ca965b0_b.gif)![](https://pic3.zhimg.com/v2-342d1fcc36f31aad563ae3e3681d363a_b.gif)
当然因为我们变异是有误差的，没办法精准地只变化一个因素。如果想要更精准的方向，就只能在周围进一步精细采样然后挑选了。不过总的来看，如果只对一个因素感兴趣的话，这比自己标数据还是要快的。下面是另外两个小埋的例子。可能因为训练数据中很少出现眼睛半睁的图，所以GAN生成的动画中间眼睛都是突然睁开的。。
![](https://pic2.zhimg.com/v2-90ca121b0b53e8fbf70518f5a9eec2b9_b.gif)![](https://pic1.zhimg.com/v2-b93c74df4b4e3b34b0be2cd1745651f0_b.gif)
这个StyleGAN的探索系列到这里就告一段落了。如果你有什么其他新奇好玩的想法，欢迎在评论里交流鸭。

