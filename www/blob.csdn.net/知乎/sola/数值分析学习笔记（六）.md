# 数值分析学习笔记（六） - 知乎
# 

在这一章中，我们主要关注线性方程组的求解问题。

我们都知道：线性方程组的最经典与基础的求解方式就是Gauss消元法。基于Gauss消元法，我们可以给出主元优化的Gauss方法、同时可以求矩阵的逆、对矩阵进行LU分解等一系列的操作。在此，我们均默认读者对Gauss消元及其延伸方法有着最低限度的了解。

那么，我们要介绍的线性方程组求解方法与Gauss方法有何不同呢？

我们要介绍的是一些**求解线性方程组的迭代方法**。

方法的迭代属性使得这些方法**收敛得更快**，但代价是方法的**通用性变低**了。（不像Gauss方法适用于任意线性方程组求解）

然而，在追求高计算速度的今天，这些迭代方法仍旧有着极其重要的地位与广泛的应用。


为了后文误差分析便利，我们先引出向量与矩阵的范数的概念，用来度量任意两个同维向量或矩阵之间的距离。

以下若无特殊说明，**假定出现的向量均为 ![n](https://www.zhihu.com/equation?tex=n) 维列向量，矩阵均为 ![n\times n](https://www.zhihu.com/equation?tex=n%5Ctimes+n) 方阵**。

**定义：**

**称 ![||\ \ ||](https://www.zhihu.com/equation?tex=%7C%7C%5C+%5C+%7C%7C) 为向量范数（ ![\mathbb{R}^n\rightarrow \mathbb{R}](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5En%5Crightarrow+%5Cmathbb%7BR%7D) 的映射），若其满足以下性质：**

**正定性： ![||\vec{x}||\geq 0](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C%5Cgeq+0) ，等号当且仅当 ![\vec{x}=\vec{0}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3D%5Cvec%7B0%7D) 成立。**

**齐次性： ![\forall a\in \mathbb{R},||a\vec{x}||=|a|\ ||\vec{x}||](https://www.zhihu.com/equation?tex=%5Cforall+a%5Cin+%5Cmathbb%7BR%7D%2C%7C%7Ca%5Cvec%7Bx%7D%7C%7C%3D%7Ca%7C%5C+%7C%7C%5Cvec%7Bx%7D%7C%7C)**

**三角不等式： ![||\vec{x}+\vec{y}||\leq ||\vec{x}||+||\vec{y}||](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%2B%5Cvec%7By%7D%7C%7C%5Cleq+%7C%7C%5Cvec%7Bx%7D%7C%7C%2B%7C%7C%5Cvec%7By%7D%7C%7C)**

接下来，我们给出几个最常用的向量范数的定义：

**若 ![\vec{x}=\begin{bmatrix} x_1\\  x_2\\  ...\\  x_n \end{bmatrix}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3D%5Cbegin%7Bbmatrix%7D+x_1%5C%5C++x_2%5C%5C++...%5C%5C++x_n+%5Cend%7Bbmatrix%7D)**

**则 ![||\vec{x}||_1=\sum_{j=1}^n|x_j|](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C_1%3D%5Csum_%7Bj%3D1%7D%5En%7Cx_j%7C) 称为向量的 ![l_1](https://www.zhihu.com/equation?tex=l_1) 范数。**

**![||\vec{x}||_2=\sqrt{\sum_{j=1}^nx_j^2}](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C_2%3D%5Csqrt%7B%5Csum_%7Bj%3D1%7D%5Enx_j%5E2%7D) 称为向量的 ![l_2](https://www.zhihu.com/equation?tex=l_2) 范数。**

**![||\vec{x}||_\infty=max_{1\leq j\leq n}|x_j|](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C_%5Cinfty%3Dmax_%7B1%5Cleq+j%5Cleq+n%7D%7Cx_j%7C) 称为向量的 ![l_\infty](https://www.zhihu.com/equation?tex=l_%5Cinfty) 范数。**

读者容易自行验证这三类范数都满足向量范数定义中的三个性质。

（关于 ![l_2](https://www.zhihu.com/equation?tex=l_2) 范数的三角不等式性质证明，我们利用Cauchy不等式即可）

即： ![\vec{x}^T\vec{y}\leq ||\vec{x}||\ ||\vec{y}||](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5ET%5Cvec%7By%7D%5Cleq+%7C%7C%5Cvec%7Bx%7D%7C%7C%5C+%7C%7C%5Cvec%7By%7D%7C%7C) ，这也是线性代数中非常常用的不等式。

**向量范数**从本质上来说，给我们一种方式来**度量一个向量的长度**。

有了长度的概念，我们很自然地，模仿数学分析的过程，可以定义**向量之间的距离 **![d(\vec{x},\vec{y})=||\vec{x}-\vec{y}||](https://www.zhihu.com/equation?tex=d%28%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%29%3D%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C) 。

则我们可以得到：

**若 ![\vec{x}=\begin{bmatrix} x_1\\  x_2\\  ...\\  x_n \end{bmatrix},\vec{y}=\begin{bmatrix} y_1\\  y_2\\  ...\\  y_n \end{bmatrix}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3D%5Cbegin%7Bbmatrix%7D+x_1%5C%5C++x_2%5C%5C++...%5C%5C++x_n+%5Cend%7Bbmatrix%7D%2C%5Cvec%7By%7D%3D%5Cbegin%7Bbmatrix%7D+y_1%5C%5C++y_2%5C%5C++...%5C%5C++y_n+%5Cend%7Bbmatrix%7D)**

**向量之间的 ![l_1](https://www.zhihu.com/equation?tex=l_1) 距离： ![d_1(\vec{x},\vec{y})=\sum_{j=1}^n|x_j-y_j|](https://www.zhihu.com/equation?tex=d_1%28%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%29%3D%5Csum_%7Bj%3D1%7D%5En%7Cx_j-y_j%7C)**

**向量之间的 ![l_2](https://www.zhihu.com/equation?tex=l_2) 距离： ![d_2(\vec{x},\vec{y})=\sqrt{\sum_{j=1}^n(x_j-y_j)^2}](https://www.zhihu.com/equation?tex=d_2%28%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%29%3D%5Csqrt%7B%5Csum_%7Bj%3D1%7D%5En%28x_j-y_j%29%5E2%7D)**

**向量之间的 ![l_\infty](https://www.zhihu.com/equation?tex=l_%5Cinfty) 距离： ![d(\vec{x},\vec{y})=max_{1\leq j\leq n}|x_j-y_j|](https://www.zhihu.com/equation?tex=d%28%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%29%3Dmax_%7B1%5Cleq+j%5Cleq+n%7D%7Cx_j-y_j%7C)**


回忆数学分析中，我们在定义了距离后，便能够定义数列的极限。

在此同样地，我们在定义了向量间的距离后，可以定义**向量列的极限**。

我们模仿分析中的做法进行推广：

**定义：**

**若 ![\forall \varepsilon>0,\ \exists N,\ s.t.\ k>N](https://www.zhihu.com/equation?tex=%5Cforall+%5Cvarepsilon%3E0%2C%5C+%5Cexists+N%2C%5C+s.t.%5C+k%3EN) 时，都有 ![||\vec{x}^{(k)}-\vec{x}||<\varepsilon](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%5E%7B%28k%29%7D-%5Cvec%7Bx%7D%7C%7C%3C%5Cvarepsilon) 成立，则称向量列 ![\vec{x}^{(k)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D) 在 ![||\ \ ||](https://www.zhihu.com/equation?tex=%7C%7C%5C+%5C+%7C%7C) 定义的向量范数意义下收敛到 ![\vec{x}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D) 。**

读者可以自证： ![l_1,l_2,l_\infty](https://www.zhihu.com/equation?tex=l_1%2Cl_2%2Cl_%5Cinfty) 这三种向量范数意义下的收敛都是等价的。

事实上，我们有更强的结论：**所有向量范数意义下的收敛都是等价的。**

关于不同意义的向量范数之间的关系，此处不多赘述了。仅仅提及以下性质：

![||\vec{x}||_\infty\leq ||\vec{x}||_2\leq \sqrt{n}||\vec{x}||_\infty](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C_%5Cinfty%5Cleq+%7C%7C%5Cvec%7Bx%7D%7C%7C_2%5Cleq+%5Csqrt%7Bn%7D%7C%7C%5Cvec%7Bx%7D%7C%7C_%5Cinfty) 。证明也十分简单，不再给出。

关于向量的长度、向量间距离、向量列收敛性的定义，我们讨论至此已经可以建立一个完备的 ![\mathbb{R}^n](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5En) 上的向量空间。


下面，我们将向量的有关定义与结论尝试着推广到矩阵上。

类似向量范数的定义，我们定义矩阵范数。

**定义：**

**称 **![||\ \ ||](https://www.zhihu.com/equation?tex=%7C%7C%5C+%5C+%7C%7C)** 为矩阵范数（ **![\mathbb{R}^{n\times n}\rightarrow \mathbb{R}](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes+n%7D%5Crightarrow+%5Cmathbb%7BR%7D)** 的映射），若其满足以下性质：**

**正定性： **![||A||\geq 0](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C%5Cgeq+0)** ，等号当且仅当 **![A=0](https://www.zhihu.com/equation?tex=A%3D0)** 成立。**

**齐次性： **![\forall a\in \mathbb{R},||aA||=|a|\ ||A||](https://www.zhihu.com/equation?tex=%5Cforall+a%5Cin+%5Cmathbb%7BR%7D%2C%7C%7CaA%7C%7C%3D%7Ca%7C%5C+%7C%7CA%7C%7C)

**三角不等式： **![||A+B||\leq ||A||+||B||](https://www.zhihu.com/equation?tex=%7C%7CA%2BB%7C%7C%5Cleq+%7C%7CA%7C%7C%2B%7C%7CB%7C%7C)

**相容性： ![||AB||\leq ||A||\ ||B||](https://www.zhihu.com/equation?tex=%7C%7CAB%7C%7C%5Cleq+%7C%7CA%7C%7C%5C+%7C%7CB%7C%7C)**

我们在理解矩阵范数时，不要再像理解向量范数一般将其想象成一个数表。而是要关注到矩阵的本质是一个线性变换。此处要注意的是矩阵范数定义中的相容性是与向量范数定义不同的。

**矩阵范数**可以理解为是**对于矩阵作为线性变换的作用效果的度量**。

下面，我们进一步将向量范数与矩阵范数联系在一起：

**定理（向量范数诱导出的矩阵自然范数）**

** 若 ![||\ \ ||](https://www.zhihu.com/equation?tex=%7C%7C%5C+%5C+%7C%7C) 为一个向量范数，则由其诱导出的 ![||A||=max_{\vec{x}\neq \vec{0}}\frac{||A\vec{x}||}{||\vec{x}||}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C%3Dmax_%7B%5Cvec%7Bx%7D%5Cneq+%5Cvec%7B0%7D%7D%5Cfrac%7B%7C%7CA%5Cvec%7Bx%7D%7C%7C%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C%7D) 必定为矩阵范数，且称为矩阵 ![A](https://www.zhihu.com/equation?tex=A) 在向量范数 ![||\ \ ||](https://www.zhihu.com/equation?tex=%7C%7C%5C+%5C+%7C%7C) 诱导下的自然范数。**

这个定理告诉我们：矩阵范数中有很大一部分可以直接由我们前面讨论过的向量范数直接诱导而来。且其赋予了矩阵自然范数以明确的几何意义：将 ![A](https://www.zhihu.com/equation?tex=A) 看作一个线性变换，则其能使一个向量伸长的最大倍数(对应向量范数意义下)即为自然范数。

同时我们得到了一个常用的推论：对于任何矩阵的自然范数 ![||\ \ ||](https://www.zhihu.com/equation?tex=%7C%7C%5C+%5C+%7C%7C) ， ![||A\vec{x}||\leq ||A||\ ||\vec{x}||](https://www.zhihu.com/equation?tex=%7C%7CA%5Cvec%7Bx%7D%7C%7C%5Cleq+%7C%7CA%7C%7C%5C+%7C%7C%5Cvec%7Bx%7D%7C%7C) 。

当然，**并非所有矩阵范数都是自然范数**。

一个很显然的反例是：**矩阵的Frobenius范数： ![||A||_F=\sum_{i=1}^n\sum_{j=1}^na_{ij}^2](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_F%3D%5Csum_%7Bi%3D1%7D%5En%5Csum_%7Bj%3D1%7D%5Ena_%7Bij%7D%5E2) （所有元素平方和）**

反证：

假设其为自然范数，即 ![||A||_F=max\frac{||A\vec{x}||_V}{||\vec{x}||_V}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_F%3Dmax%5Cfrac%7B%7C%7CA%5Cvec%7Bx%7D%7C%7C_V%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C_V%7D)

考虑 ![A=I_n](https://www.zhihu.com/equation?tex=A%3DI_n) 的情况，此时 ![||I_n||_F=\sqrt{n}](https://www.zhihu.com/equation?tex=%7C%7CI_n%7C%7C_F%3D%5Csqrt%7Bn%7D) 。

但是， ![||I_n||_F=max\frac{||\vec{x}||_V}{||\vec{x}||_V}=1](https://www.zhihu.com/equation?tex=%7C%7CI_n%7C%7C_F%3Dmax%5Cfrac%7B%7C%7C%5Cvec%7Bx%7D%7C%7C_V%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C_V%7D%3D1) 。

矛盾！

注意到将 ![A](https://www.zhihu.com/equation?tex=A) 看作一个线性变换，则其能使一个向量伸长的最大倍数即为矩阵的自然范数。我们发现这与矩阵特征值的定义是吻合的。故引入谱半径的定义。

**定义：**

**谱半径 ![\rho(A)=max|\lambda_i|](https://www.zhihu.com/equation?tex=%5Crho%28A%29%3Dmax%7C%5Clambda_i%7C) ，其中 ![\lambda](https://www.zhihu.com/equation?tex=%5Clambda) 为矩阵 ![A](https://www.zhihu.com/equation?tex=A) 的复特征值**。

接下来，我们来着重介绍常用矩阵范数的计算方法。

![||A||_\infty](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty) 是由向量的无穷范数诱导出的矩阵无穷范数，即 ![||A||_\infty=max\frac{||A\vec{x}||_\infty}{||\vec{x}||_\infty}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty%3Dmax%5Cfrac%7B%7C%7CA%5Cvec%7Bx%7D%7C%7C_%5Cinfty%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C_%5Cinfty%7D) 。

类似地，我们能够得到 ![||A||_2,||A||_1](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_2%2C%7C%7CA%7C%7C_1) 作为向量范数诱导出的矩阵自然范数。

我们的问题是： ![||A||_1,||A||_2,||A||_\infty](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_1%2C%7C%7CA%7C%7C_2%2C%7C%7CA%7C%7C_%5Cinfty) 要如何简便地计算呢？

**定理（ ![||A||_2](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_2) 的计算）**

**![||A||_2=\sqrt{\rho(A^HA)}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_2%3D%5Csqrt%7B%5Crho%28A%5EHA%29%7D) ，其中 ![A^H](https://www.zhihu.com/equation?tex=A%5EH) 表示复共轭转置。**

证明：

由高代的知识，我们容易知道 ![A^HA](https://www.zhihu.com/equation?tex=A%5EHA) 为Hermite矩阵，故其特征值 ![\lambda_1,...,\lambda_n](https://www.zhihu.com/equation?tex=%5Clambda_1%2C...%2C%5Clambda_n) 均为实数，且每个特征值代数重数等于几何重数。

则我们取其特征向量并且进行Schimidt正交化得到 ![\vec{a_1},...,\vec{a_n}](https://www.zhihu.com/equation?tex=%5Cvec%7Ba_1%7D%2C...%2C%5Cvec%7Ba_n%7D) （一组基）。

则任意向量 ![\vec{x}=k_1\vec{a_1}+...+k_n\vec{a_n}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3Dk_1%5Cvec%7Ba_1%7D%2B...%2Bk_n%5Cvec%7Ba_n%7D) 可被线性表出。

则 ![\frac{||A\vec{x}||^2_2}{||\vec{x}||^2_2}=\frac{\vec{x}^H(A^HA)\vec{x}}{\vec{x}^H\vec{x}}=\frac{\sum_{i=1}^n\lambda_i|k_i|^2}{\sum_{i=1}^n|k_i|^2}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%7C%7CA%5Cvec%7Bx%7D%7C%7C%5E2_2%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C%5E2_2%7D%3D%5Cfrac%7B%5Cvec%7Bx%7D%5EH%28A%5EHA%29%5Cvec%7Bx%7D%7D%7B%5Cvec%7Bx%7D%5EH%5Cvec%7Bx%7D%7D%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%5Clambda_i%7Ck_i%7C%5E2%7D%7B%5Csum_%7Bi%3D1%7D%5En%7Ck_i%7C%5E2%7D)

故有 ![||A||_2=max\frac{||A\vec{x}||_2}{||\vec{x}||_2}=\sqrt{max \frac{\sum_{i=1}^n\lambda_i|k_i|^2}{\sum_{i=1}^n|k_i|^2}}=\sqrt{\rho(A^HA)}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_2%3Dmax%5Cfrac%7B%7C%7CA%5Cvec%7Bx%7D%7C%7C_2%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C_2%7D%3D%5Csqrt%7Bmax+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%5Clambda_i%7Ck_i%7C%5E2%7D%7B%5Csum_%7Bi%3D1%7D%5En%7Ck_i%7C%5E2%7D%7D%3D%5Csqrt%7B%5Crho%28A%5EHA%29%7D)

**定理（ ![||A||_\infty](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty) 的计算）**

**![||A||_\infty=max_i\ \sum_{j=1}^n|a_{ij}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty%3Dmax_i%5C+%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bij%7D%7C) ，为最大行和。**

证明：

不妨取 ![||\vec{x}||_\infty=1](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C_%5Cinfty%3D1) ，则 ![||A||_\infty=max||A\vec{x}||_\infty\leq max\sum_{j=1}^n|a_{ij}|\ max\ |x_j|=max\sum_{j=1}^n|a_{ij}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty%3Dmax%7C%7CA%5Cvec%7Bx%7D%7C%7C_%5Cinfty%5Cleq+max%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bij%7D%7C%5C+max%5C+%7Cx_j%7C%3Dmax%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bij%7D%7C)

接下来假设 ![\exists k,\ s.t.](https://www.zhihu.com/equation?tex=%5Cexists+k%2C%5C+s.t.)![max\sum_{j=1}^n|a_{ij}|=\sum_{j=1}^n|a_{kj}|](https://www.zhihu.com/equation?tex=max%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bij%7D%7C%3D%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bkj%7D%7C) ，则发现取![||\vec{x}||_\infty=1](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%7C%7C_%5Cinfty%3D1)，使得 ![x_j=sgn(a_{kj})](https://www.zhihu.com/equation?tex=x_j%3Dsgn%28a_%7Bkj%7D%29) ，则有 ![||A\vec{x}||_\infty=\sum_{j=1}^n|a_{kj}|](https://www.zhihu.com/equation?tex=%7C%7CA%5Cvec%7Bx%7D%7C%7C_%5Cinfty%3D%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bkj%7D%7C) 。

这说明 ![||A||_\infty=max||A\vec{x}||_\infty\geq\sum_{j=1}^n|a_{kj}|=max\sum_{j=1}^n|a_{ij}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty%3Dmax%7C%7CA%5Cvec%7Bx%7D%7C%7C_%5Cinfty%5Cgeq%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bkj%7D%7C%3Dmax%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bij%7D%7C)

故我们得到了 ![||A||_\infty=max_i\ \sum_{j=1}^n|a_{ij}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty%3Dmax_i%5C+%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bij%7D%7C) 。

**定理（ ![||A||_1](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_1) 的计算）**

**![||A||_1=max_j\ \sum_{i=1}^n|a_{ij}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_1%3Dmax_j%5C+%5Csum_%7Bi%3D1%7D%5En%7Ca_%7Bij%7D%7C) ，为最大列和。**

证明可以模仿无穷范数的证明进行。

最后，我们给出收敛矩阵的定义，为后续要介绍的线性方程求解方法进行铺垫。

**定义：若 ![\lim_{k\rightarrow\infty}A^k=0](https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Crightarrow%5Cinfty%7DA%5Ek%3D0) ，则 ![A](https://www.zhihu.com/equation?tex=A) 为收敛矩阵。**

收敛矩阵的几何意义很明确。**若一个矩阵作用在任意一个向量上足够多次能使得其收缩至 ![0](https://www.zhihu.com/equation?tex=0) 向量**，则矩阵为收敛矩阵。

下面给出矩阵收敛的一些等价条件，读者容易证明。

**定理：（矩阵收敛的充要条件）**

**（1）存在自然范数，使得 ![\lim_{n\rightarrow\infty}||A^n||=0](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%7C%7CA%5En%7C%7C%3D0)**

**（2）对任意自然范数， ![\lim_{n\rightarrow\infty}||A^n||=0](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%7C%7CA%5En%7C%7C%3D0)**

**（3） ![\rho(A)<1](https://www.zhihu.com/equation?tex=%5Crho%28A%29%3C1)**

**（4） ![\forall \vec{x},\ \lim_{n\rightarrow\infty}A^n\vec{x}=\vec{0}](https://www.zhihu.com/equation?tex=%5Cforall+%5Cvec%7Bx%7D%2C%5C+%5Clim_%7Bn%5Crightarrow%5Cinfty%7DA%5En%5Cvec%7Bx%7D%3D%5Cvec%7B0%7D)**

最后，简要给出一些矩阵范数以及谱半径的性质，读者可自证。

![||A||_F\leq ||A||_F||\vec{x}||_2](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_F%5Cleq+%7C%7CA%7C%7C_F%7C%7C%5Cvec%7Bx%7D%7C%7C_2)

![||A||_2\leq ||A||_F\leq \sqrt{n}||A||_2](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_2%5Cleq+%7C%7CA%7C%7C_F%5Cleq+%5Csqrt%7Bn%7D%7C%7CA%7C%7C_2)

对任意自然范数 ![|| \ \ ||](https://www.zhihu.com/equation?tex=%7C%7C+%5C+%5C+%7C%7C) ， ![\rho(A)\leq ||A||](https://www.zhihu.com/equation?tex=%5Crho%28A%29%5Cleq+%7C%7CA%7C%7C) （事实上，谱半径不仅仅是下界，也是下确界）

Gelfand公式： ![\rho(A)=\lim_{k\rightarrow\infty}||A^k||^{\frac{1}{k}}](https://www.zhihu.com/equation?tex=%5Crho%28A%29%3D%5Clim_%7Bk%5Crightarrow%5Cinfty%7D%7C%7CA%5Ek%7C%7C%5E%7B%5Cfrac%7B1%7D%7Bk%7D%7D) ，及其推论： ![\rho(A)\leq ||A^k||^{\frac{1}{k}}](https://www.zhihu.com/equation?tex=%5Crho%28A%29%5Cleq+%7C%7CA%5Ek%7C%7C%5E%7B%5Cfrac%7B1%7D%7Bk%7D%7D)


至此为止，我们对于范数的介绍告一段落。我们即将进入正题：介绍各种迭代求解线性方程组的方法。**我们需要清楚的是：向量范数是对于向量长度的一种度量，矩阵范数是对于矩阵作为线性变换作用在向量上的效果的度量。由向量与矩阵范数，我们可以定义向量间的距离，并将微积分（极限概念）扩展到向量空间与矩阵空间上。**


在本章的剩下内容中，我们都将集中精力在方程组 ![A\vec{x}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Bb%7D) 的数值求解方法上。

注意到这样一个方程组中的第 ![i](https://www.zhihu.com/equation?tex=i) 个方程为 ![a_{i1}x_1+...+a_{in}x_n=b_i](https://www.zhihu.com/equation?tex=a_%7Bi1%7Dx_1%2B...%2Ba_%7Bin%7Dx_n%3Db_i) 。

我们都很熟悉Gauss方法，因为其消元的操作非常易于我们理解。

但是现在，我们要跳出这样的思维局限。

容易发现求解方程从本质上来说与求解零点是相同的。只不过现在我们要求解的是一个更复杂的矩阵函数 ![A\vec{x}-\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D-%5Cvec%7Bb%7D) 的零点。

回忆我们之前介绍过的零点求解方法，我们想从那些方法中得到一些启发。

对于零点求解，主要的方法有二分法与不动点迭代法两大类。

二分法显然不适合用来处理复杂的线性方程组求解问题。我们想是否可以**类比不动点迭代**方法创造出一些全新的线性方程组求解方法。

在不动点迭代中，我们将 ![f(x)=0](https://www.zhihu.com/equation?tex=f%28x%29%3D0) 的问题化为 ![g(x)=x](https://www.zhihu.com/equation?tex=g%28x%29%3Dx) 求解，并且保证其等解性。

而在矩阵处理中，我们可以类似地，将**对角线上的元素进行变量分离，来起到不动点迭代中 ![x](https://www.zhihu.com/equation?tex=x) 的效果。（后文会继续解释这一点）**

由这样的一个基本想法，我们可以得到最基础的迭代求解方法：Jacobi方法。


**Jacobi方法**

我们对第 ![i](https://www.zhihu.com/equation?tex=i) 个方程分离变量得到

![a_{ii}x_i=b_i-(a_{i1}x_1+...+a_{i,i-1}x_{i-1}+a_{i,i+1}x_{i+1}+a_{in}x_n)](https://www.zhihu.com/equation?tex=a_%7Bii%7Dx_i%3Db_i-%28a_%7Bi1%7Dx_1%2B...%2Ba_%7Bi%2Ci-1%7Dx_%7Bi-1%7D%2Ba_%7Bi%2Ci%2B1%7Dx_%7Bi%2B1%7D%2Ba_%7Bin%7Dx_n%29)

进一步地， ![x_i=\frac{1}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x_j-\sum_{j=i+1}^{n}a_{ij}x_j)](https://www.zhihu.com/equation?tex=x_i%3D%5Cfrac%7B1%7D%7Ba_%7Bii%7D%7D%28b_i-%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7Da_%7Bij%7Dx_j-%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7Da_%7Bij%7Dx_j%29)

在变形完成之后，我们继续模仿求解零点的不动点方法。

在求解零点时，我们先选取初值 ![p_0](https://www.zhihu.com/equation?tex=p_0) ，并且生成 ![p_1=g(p_0),p_2=g(p_1),...](https://www.zhihu.com/equation?tex=p_1%3Dg%28p_0%29%2Cp_2%3Dg%28p_1%29%2C...)

在求解线性方程组中，我们类似地，**利用上一次的估计值来生成下一次的估计**。

我们**总是用 ![\vec{x}^{(k)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D) 表示第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中生成的对方程组解的估计**。

则我们得到如下**Jacobi方法的估计式：**

**给定迭代初值 ![\vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D) ， **![x_i^{(k)}=\frac{1}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x^{(k-1)}_j-\sum_{j=i+1}^{n}a_{ij}x^{(k-1)}_j)\ (k=1,2,...)](https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%29%7D%3D%5Cfrac%7B1%7D%7Ba_%7Bii%7D%7D%28b_i-%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j-%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j%29%5C+%28k%3D1%2C2%2C...%29)

一个问题是：我们要怎样衡量一个估计的好坏呢？

回忆之前提到的向量范数，我们在这里不妨取向量的无穷范数。

则我们可以定义出**第 ![k-1](https://www.zhihu.com/equation?tex=k-1) 次估计相对于第 ![k](https://www.zhihu.com/equation?tex=k) 次估计求解线性方程组的绝对误差： ![||\vec{x}^{(k)}-\vec{x}^{(k-1)}||_\infty](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D%5E%7B%28k%29%7D-%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%7C%7C_%5Cinfty) ，以及相对误差： **![\frac{||\vec{x}^{(k)}-\vec{x}^{(k-1)}||_\infty}{||\vec{x}^{(k)}||_\infty}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%7C%7C%5Cvec%7Bx%7D%5E%7B%28k%29%7D-%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%7C%7C_%5Cinfty%7D%7B%7C%7C%5Cvec%7Bx%7D%5E%7B%28k%29%7D%7C%7C_%5Cinfty%7D)

在实现中，我们常常利用这两个误差作为标准来判断估计是否已经足够得好。

读者可能仍旧会疑惑：Jacobi方法与不动点迭代在形式上完全不同，为何却说它是不动点迭代在矩阵代数中的推广呢？

我们下面尝试着将Jacobi方法的估计式写成矩阵形式即可立马清晰地看出其本质。

我们在后文都**假设对于矩阵 ![A](https://www.zhihu.com/equation?tex=A) ， ![D](https://www.zhihu.com/equation?tex=D) 为其对角元素组成的对角矩阵， ![L](https://www.zhihu.com/equation?tex=L) 为其下三角部分（不含主对角线）元素的相反数组成的下三角矩阵， ![U](https://www.zhihu.com/equation?tex=U) 为其上三角部分（不含主对角线）元素的相反数组成的上三角矩阵。**

**即进行一个简单的分解使得 ![A=D-L-U](https://www.zhihu.com/equation?tex=A%3DD-L-U)**

那么我们的估计式 ![x_i^{(k)}=\frac{1}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x^{(k-1)}_j-\sum_{j=i+1}^{n}a_{ij}x^{(k-1)}_j)](https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%29%7D%3D%5Cfrac%7B1%7D%7Ba_%7Bii%7D%7D%28b_i-%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j-%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j%29)

可以写成 ![D\vec{x}^{(k)}=(L+U)\vec{x}^{(k-1)}+\vec{b}](https://www.zhihu.com/equation?tex=D%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%28L%2BU%29%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bb%7D)

**当 ![D](https://www.zhihu.com/equation?tex=D) 可逆时，我们有 ![\vec{x}^{(k)}=D^{-1}(L+U)\vec{x}^{(k-1)}+D^{-1}\vec{b}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DD%5E%7B-1%7D%28L%2BU%29%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2BD%5E%7B-1%7D%5Cvec%7Bb%7D)** ，这就是**Jacobi方法的矩阵形式**。

我们容易发现我们将 ![A\vec{x}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Bb%7D) 转化为了等解的 ![\vec{x}=D^{-1}(L+U)\vec{x}+D^{-1}\vec{b}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3DD%5E%7B-1%7D%28L%2BU%29%5Cvec%7Bx%7D%2BD%5E%7B-1%7D%5Cvec%7Bb%7D) 形式。

进一步抽象的话，Jacobi方法给出了**线性方程组求解的 ![\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bc%7D) 迭代形式**。

其中 ![T=D^{-1}(L+U),\vec{c}=D^{-1}\vec{b}](https://www.zhihu.com/equation?tex=T%3DD%5E%7B-1%7D%28L%2BU%29%2C%5Cvec%7Bc%7D%3DD%5E%7B-1%7D%5Cvec%7Bb%7D) 。

而我们容易发现![\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bc%7D)与不动点迭代的形式是一致的。

故**Jacobi方法为不动点迭代的推广**。


我们试举一例。在之后的不同方法中，我们都将利用这个例子来进行方法之间的比较。

例：

![A=\begin{bmatrix} 0.2 &0.1  &1  &1  &0 \\ 0.1  &4  & -1 &1  &-1 \\   1& -1 & 60 &0  &-2 \\  1 & 1 &0  &8  &4 \\  0 & -1 & -2 & 4 & 700 \end{bmatrix} , \vec{b}=\begin{bmatrix} 1\\  2\\  3\\  4\\  5 \end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+0.2+%260.1++%261++%261++%260+%5C%5C+0.1++%264++%26+-1+%261++%26-1+%5C%5C+++1%26+-1+%26+60+%260++%26-2+%5C%5C++1+%26+1+%260++%268++%264+%5C%5C++0+%26+-1+%26+-2+%26+4+%26+700+%5Cend%7Bbmatrix%7D+%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+1%5C%5C++2%5C%5C++3%5C%5C++4%5C%5C++5+%5Cend%7Bbmatrix%7D)

我们给定初值 ![\vec{x}^{(0)}=\vec{0}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D%3D%5Cvec%7B0%7D) ，容许的误差为 ![0.01](https://www.zhihu.com/equation?tex=0.01) （无穷范数定义的相对误差标准）
![](https://pic3.zhimg.com/v2-626f3c8627cf009efdf581698d166ed2_b.jpg)

我们可以看到：进行了 ![34](https://www.zhihu.com/equation?tex=34) 次迭代后，估计最终停止。

这说明**Jacobi方法收敛的速度是不够快**的。


那么，问题是：要如何改进Jacobi方法呢？

我们再观察一下Jacobi方法的估计式 

![x_i^{(k)}=\frac{1}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x^{(k-1)}_j-\sum_{j=i+1}^{n}a_{ij}x^{(k-1)}_j)](https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%29%7D%3D%5Cfrac%7B1%7D%7Ba_%7Bii%7D%7D%28b_i-%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j-%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j%29)

我们发现：这样的一个估计式仍旧有改进的余地：

注意到**在第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中估计第 ![i](https://www.zhihu.com/equation?tex=i) 个分量时，我们已经完成了第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中对于 ![x_1,x_2,...,x_{i-1}](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C...%2Cx_%7Bi-1%7D) 的估计**。故我们可以用第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中的值而不是第 ![k-1](https://www.zhihu.com/equation?tex=k-1) 次迭代中的值。这样会显著地提高收敛速度。经过如此改进后的方法就是Gauss-Seidel方法。


**Gauss-Seidel方法**

**估计式：**![x_i^{(k)}=\frac{1}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x^{(k)}_j-\sum_{j=i+1}^{n}a_{ij}x^{(k-1)}_j)](https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%29%7D%3D%5Cfrac%7B1%7D%7Ba_%7Bii%7D%7D%28b_i-%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7Da_%7Bij%7Dx%5E%7B%28k%29%7D_j-%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j%29)

对于Gauss-Seidel方法，我们先如同前面一样给出其矩阵形式。

我们可以将估计式写成 ![D\vec{x}^{(k)}=L\vec{x}^{(k)}+U\vec{x}^{(k-1)}+\vec{b}](https://www.zhihu.com/equation?tex=D%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DL%5Cvec%7Bx%7D%5E%7B%28k%29%7D%2BU%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bb%7D)

得到 ![(D-L)\vec{x}^{(k)}=U\vec{x}^{(k-1)}+\vec{b}](https://www.zhihu.com/equation?tex=%28D-L%29%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DU%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bb%7D)

**当 ![D-L](https://www.zhihu.com/equation?tex=D-L) 可逆时， ![\vec{x}^{(k)}=(D-L)^{-1}U\vec{x}^{(k-1)}+(D-L)^{-1}\vec{b}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%28D-L%29%5E%7B-1%7DU%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%28D-L%29%5E%7B-1%7D%5Cvec%7Bb%7D)**

这就是**Gauss-Seidel方法的矩阵形式**。

同样地，我们容易看出其**也是不动点迭代方法的推广，而区别仅仅在于其进行了与Jacobi方法不同的等解变换，利用了不同的不动点迭代函数**。

读者可能会疑惑：这样一个小小的改动真的能带来很大的提升吗？

我们仍旧用前面相同的例子来说明。

例：

![A=\begin{bmatrix} 0.2 &0.1  &1  &1  &0 \\ 0.1  &4  & -1 &1  &-1 \\   1& -1 & 60 &0  &-2 \\  1 & 1 &0  &8  &4 \\  0 & -1 & -2 & 4 & 700 \end{bmatrix} , \vec{b}=\begin{bmatrix} 1\\  2\\  3\\  4\\  5 \end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+0.2+%260.1++%261++%261++%260+%5C%5C+0.1++%264++%26+-1+%261++%26-1+%5C%5C+++1%26+-1+%26+60+%260++%26-2+%5C%5C++1+%26+1+%260++%268++%264+%5C%5C++0+%26+-1+%26+-2+%26+4+%26+700+%5Cend%7Bbmatrix%7D+%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+1%5C%5C++2%5C%5C++3%5C%5C++4%5C%5C++5+%5Cend%7Bbmatrix%7D)

我们给定初值 ![\vec{x}^{(0)}=\vec{0}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D%3D%5Cvec%7B0%7D) ，容许的误差为 ![0.01](https://www.zhihu.com/equation?tex=0.01) （无穷范数定义的相对误差标准）
![](https://pic3.zhimg.com/v2-66225c2029c38290e1fe0164342cb6d6_b.jpg)
我们发现Gauss-Seidel方法竟然只需要 ![11](https://www.zhihu.com/equation?tex=11) 次迭代就能达成与Jacobi方法相同的效果。迭代次数降为了原先的三分之一。这是一个不小的突破。

事实上，大量实践都证明了**Gauss-Seidel方法在大多数情况下收敛速度快于Jacobi方法（但这并不意味着更优，后文将会提及）**。


**收敛条件**

我们已经得到了求解线性方程组的两种迭代方法，但是我们之前一直没有考虑一个十分重要的问题：这两种方法对任意矩阵都**收敛**吗？如若不然，**收敛条件**是什么呢？

下面我们对于这类利用不动点迭代进行推广得到的迭代方法进行收敛条件的分析。

我们如同之前一样，将**一个迭代方法抽象为 **![\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bc%7D) ，而Jacobi与Gauss-Seidel方法均只是 ![T,\vec{c}](https://www.zhihu.com/equation?tex=T%2C%5Cvec%7Bc%7D) 取一定值条件下的特例。

则我们有如下收敛条件：

**定理（线性方程组的迭代方法的收敛条件）**

**迭代方法 ![\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bc%7D) 收敛到 ![\vec{x}=T\vec{x}+\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3DT%5Cvec%7Bx%7D%2B%5Cvec%7Bc%7D) 的解当且仅当 ![\rho(T)<1](https://www.zhihu.com/equation?tex=%5Crho%28T%29%3C1) 。**

证明：

若 ![\rho(T)<1](https://www.zhihu.com/equation?tex=%5Crho%28T%29%3C1) ， ![\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}=T^2\vec{x}^{(k-2)}+(T+I)\vec{c}=...=T^k\vec{x}^{(0)}+(I+T+...+T^{k-1})\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2B%5Cvec%7Bc%7D%3DT%5E2%5Cvec%7Bx%7D%5E%7B%28k-2%29%7D%2B%28T%2BI%29%5Cvec%7Bc%7D%3D...%3DT%5Ek%5Cvec%7Bx%7D%5E%7B%280%29%7D%2B%28I%2BT%2B...%2BT%5E%7Bk-1%7D%29%5Cvec%7Bc%7D)

由于![\rho(T)<1](https://www.zhihu.com/equation?tex=%5Crho%28T%29%3C1) ，容易知道 ![I-T](https://www.zhihu.com/equation?tex=I-T) 可逆，且 ![\sum_{j=0}^\infty T^j=(I-T)^{-1}](https://www.zhihu.com/equation?tex=%5Csum_%7Bj%3D0%7D%5E%5Cinfty+T%5Ej%3D%28I-T%29%5E%7B-1%7D)

故 ![\lim_{k\rightarrow \infty}\vec{x}^{(k)}=\vec{x}^{(0)}\lim_{k\rightarrow \infty}T^k+(I-T)^{-1}\vec{c}](https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Crightarrow+%5Cinfty%7D%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%5Cvec%7Bx%7D%5E%7B%280%29%7D%5Clim_%7Bk%5Crightarrow+%5Cinfty%7DT%5Ek%2B%28I-T%29%5E%7B-1%7D%5Cvec%7Bc%7D)

回忆我们之前提及的收敛矩阵的性质。一个矩阵收敛当且仅当其谱半径小于 ![1](https://www.zhihu.com/equation?tex=1) 。

故 ![\lim_{k\rightarrow \infty}T^k=0](https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Crightarrow+%5Cinfty%7DT%5Ek%3D0) ， ![\lim_{k\rightarrow \infty}\vec{x}^{(k)}=(I-T)^{-1}\vec{c}](https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Crightarrow+%5Cinfty%7D%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%28I-T%29%5E%7B-1%7D%5Cvec%7Bc%7D)

即为 ![\vec{x}=T\vec{x}+\vec{c}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3DT%5Cvec%7Bx%7D%2B%5Cvec%7Bc%7D) 的解。

要证明逆命题，我们同样利用收敛矩阵的性质。要证明 ![\rho(T)<1](https://www.zhihu.com/equation?tex=%5Crho%28T%29%3C1) ，即等价于证明 ![\forall \vec{z},\ \lim_{k\rightarrow \infty}T^k\vec{z}=0](https://www.zhihu.com/equation?tex=%5Cforall+%5Cvec%7Bz%7D%2C%5C+%5Clim_%7Bk%5Crightarrow+%5Cinfty%7DT%5Ek%5Cvec%7Bz%7D%3D0) 。

类比于一元情况中对于不动点迭代收敛性的证明，我们同样利用Lagrange中值定理的形式进行估计。

我们发现 ![\vec{x}-\vec{x}^{(k)}=T(\vec{x}-\vec{x}^{(k-1)})=...=T^k(\vec{x}-\vec{x}^{(0)})](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D-%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%28%5Cvec%7Bx%7D-%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%29%3D...%3DT%5Ek%28%5Cvec%7Bx%7D-%5Cvec%7Bx%7D%5E%7B%280%29%7D%29)

则我们很自然地，取 ![\vec{x}^{(0)}=\vec{x}-\vec{z}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D%3D%5Cvec%7Bx%7D-%5Cvec%7Bz%7D) ，则 ![\vec{x}-\vec{x}^{(k)}=T^k\vec{z}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D-%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3DT%5Ek%5Cvec%7Bz%7D) 。

由于方法收敛， ![\lim_{k\rightarrow \infty}\vec{x}-\vec{x}^{(k)}=\vec{0}](https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Crightarrow+%5Cinfty%7D%5Cvec%7Bx%7D-%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%5Cvec%7B0%7D) ，故 ![\lim_{k\rightarrow \infty}T^k\vec{z}=\vec{0}](https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Crightarrow+%5Cinfty%7DT%5Ek%5Cvec%7Bz%7D%3D%5Cvec%7B0%7D) ，得证。

事实上，线性方程组的迭代方法作为不动点迭代的推广，在性质上与不动点迭代非常类似。

我们同样可以得到不动点迭代形式的误差估计。要注意的是， **![\rho(T)](https://www.zhihu.com/equation?tex=%5Crho%28T%29) 在某种程度上代替了导数在不动点迭代之中的作用**。 **![\rho(T)](https://www.zhihu.com/equation?tex=%5Crho%28T%29) 越小，方法收敛速度越快。**

由此，我们容易证明当 ![T](https://www.zhihu.com/equation?tex=T) 为**严格对角占优矩阵时，Jacobi方法与Gauss-Seidel方法均收敛**。（反证并且利用严格对角占优矩阵满秩的性质）

对于Jacobi与Gauss-Seidel方法，另外一点值得一提的是：它们之间**并无确定的优劣之分**。虽然我们一直在说Gauss-Seidel方法是Jacobi方法的改进，且大多数情况下能使得求解更快收敛，这却并不是绝对的。

对于 ![A=\begin{bmatrix} 2 &-1  &1 \\ 2  &2 & 2 \\   -1& -1 & 2   \end{bmatrix} , \vec{b}=\begin{bmatrix} -1\\  4\\  -5\end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+2+%26-1++%261+%5C%5C+2++%262+%26+2+%5C%5C+++-1%26+-1+%26+2+++%5Cend%7Bbmatrix%7D+%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+-1%5C%5C++4%5C%5C++-5%5Cend%7Bbmatrix%7D) ，读者容易验证Jacobi方法中 ![\rho(T_j)=\frac{\sqrt{5}}{2}](https://www.zhihu.com/equation?tex=%5Crho%28T_j%29%3D%5Cfrac%7B%5Csqrt%7B5%7D%7D%7B2%7D) ，而Gauss-Seidel方法中 ![\rho(T_g)=\frac{1}{2}](https://www.zhihu.com/equation?tex=%5Crho%28T_g%29%3D%5Cfrac%7B1%7D%7B2%7D) 。这意味着**Jacobi方法不收敛，G-S方法收敛**。

而对于 ![A=\begin{bmatrix} 1 &2  &-2 \\ 1  &1& 1 \\   2& 2 & 1   \end{bmatrix} , \vec{b}=\begin{bmatrix} 7\\  2\\  5\end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+1+%262++%26-2+%5C%5C+1++%261%26+1+%5C%5C+++2%26+2+%26+1+++%5Cend%7Bbmatrix%7D+%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+7%5C%5C++2%5C%5C++5%5Cend%7Bbmatrix%7D) ，读者容易验证Jacobi方法中 ![\rho(T_j)=0](https://www.zhihu.com/equation?tex=%5Crho%28T_j%29%3D0) ，而G-S方法中 ![\rho(T_g)=2](https://www.zhihu.com/equation?tex=%5Crho%28T_g%29%3D2) 。这意味着**Jacobi方法收敛，G-S方法不收敛**。


接下来我们要介绍的超松弛方法（SOR）本质上是对于G-S方法的改进，其引入了估计的权重以达到加速收敛与减小残差的目的。

**超松弛方法（SOR）**

**估计式：**![x_i^{(k)}=(1-w)\vec{x}_i^{(k-1)}+\frac{w}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x^{(k)}_j-\sum_{j=i+1}^{n}a_{ij}x^{(k-1)}_j)](https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%29%7D%3D%281-w%29%5Cvec%7Bx%7D_i%5E%7B%28k-1%29%7D%2B%5Cfrac%7Bw%7D%7Ba_%7Bii%7D%7D%28b_i-%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7Da_%7Bij%7Dx%5E%7B%28k%29%7D_j-%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7Da_%7Bij%7Dx%5E%7B%28k-1%29%7D_j%29)

可以看到，具体的做法就是为G-S方法中的估计式赋权 ![w](https://www.zhihu.com/equation?tex=w) ，而对 ![\vec{x}_i^{(k-1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D_i%5E%7B%28k-1%29%7D) 赋权 ![1-w](https://www.zhihu.com/equation?tex=1-w) 来生成第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中对于 ![x_i](https://www.zhihu.com/equation?tex=x_i) 的估计。

对于SOR的原理，我们不过多展开，而是侧重于方法的分析上。

类比之前两个方法分析中的做法，我们容易写出SOR估计式的矩阵形式。

**矩阵形式：当 **![D-wL](https://www.zhihu.com/equation?tex=D-wL)** 可逆时， **![\vec{x}^{(k)}=(D-wL)^{-1}[(1-w)D+wU]\vec{x}^{(k-1)}+w(D-wL)^{-1}\vec{b}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%28D-wL%29%5E%7B-1%7D%5B%281-w%29D%2BwU%5D%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2Bw%28D-wL%29%5E%7B-1%7D%5Cvec%7Bb%7D)

容易看到，SOR本质上也是不动点迭代方法的推广。

例：

![A=\begin{bmatrix} 0.2 &0.1  &1  &1  &0 \\ 0.1  &4  & -1 &1  &-1 \\   1& -1 & 60 &0  &-2 \\  1 & 1 &0  &8  &4 \\  0 & -1 & -2 & 4 & 700 \end{bmatrix} , \vec{b}=\begin{bmatrix} 1\\  2\\  3\\  4\\  5 \end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+0.2+%260.1++%261++%261++%260+%5C%5C+0.1++%264++%26+-1+%261++%26-1+%5C%5C+++1%26+-1+%26+60+%260++%26-2+%5C%5C++1+%26+1+%260++%268++%264+%5C%5C++0+%26+-1+%26+-2+%26+4+%26+700+%5Cend%7Bbmatrix%7D+%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+1%5C%5C++2%5C%5C++3%5C%5C++4%5C%5C++5+%5Cend%7Bbmatrix%7D)

我们给定初值 ![\vec{x}^{(0)}=\vec{0}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D%3D%5Cvec%7B0%7D) ，容许的误差为 ![0.01](https://www.zhihu.com/equation?tex=0.01) （无穷范数定义的相对误差标准）
![](https://pic3.zhimg.com/v2-cc43c977df49e7af1ab0c8b91d8cb9ee_b.jpg)

可以看到：取超松弛参数 ![w=1.25](https://www.zhihu.com/equation?tex=w%3D1.25) ，仅仅进行了6次迭代，就得到了我们想要的结果。

关于超松弛参数 ![w](https://www.zhihu.com/equation?tex=w) 的选取，事实上并不是任意的。

**定理（Kahan）**

**![A](https://www.zhihu.com/equation?tex=A) 对角元均非零， ![T_w=(D-wL)^{-1}[(1-w)D+wU]](https://www.zhihu.com/equation?tex=T_w%3D%28D-wL%29%5E%7B-1%7D%5B%281-w%29D%2BwU%5D) ，则 ![\rho(T_w)\geq |w-1|](https://www.zhihu.com/equation?tex=%5Crho%28T_w%29%5Cgeq+%7Cw-1%7C) 。**

证明：

由于 ![D](https://www.zhihu.com/equation?tex=D) 为对角矩阵，我们有 ![|T_w|=|(D-wL)^{-1}||(1-w)D+wU|=|D^{-1}|(1-w)^n|D|=(1-w)^n](https://www.zhihu.com/equation?tex=%7CT_w%7C%3D%7C%28D-wL%29%5E%7B-1%7D%7C%7C%281-w%29D%2BwU%7C%3D%7CD%5E%7B-1%7D%7C%281-w%29%5En%7CD%7C%3D%281-w%29%5En)

由特征值与行列式的关系，我们有 ![\lambda_1...\lambda_n=(1-w)^n](https://www.zhihu.com/equation?tex=%5Clambda_1...%5Clambda_n%3D%281-w%29%5En)

故易知 ![\rho(T_w)=max_i|\lambda_i|\geq |w-1|](https://www.zhihu.com/equation?tex=%5Crho%28T_w%29%3Dmax_i%7C%5Clambda_i%7C%5Cgeq+%7Cw-1%7C) 。

这个定理的一个重要推论是：**SOR方法收敛的必要条件是 ![|w-1|<1](https://www.zhihu.com/equation?tex=%7Cw-1%7C%3C1) ，即 ![0<w<2](https://www.zhihu.com/equation?tex=0%3Cw%3C2) 。**

事实上，对于**正定**矩阵而言， ![0<w<2](https://www.zhihu.com/equation?tex=0%3Cw%3C2)** 的SOR方法对任意初值必定收敛**。

对于**正定的三对角矩阵**而言，我们甚至能够找到理论上的最佳 ![w](https://www.zhihu.com/equation?tex=w) ，使得 ![\rho(T_w)=w-1](https://www.zhihu.com/equation?tex=%5Crho%28T_w%29%3Dw-1) 成立。（此处均不再赘述）



接下来，我们要讨论一类特殊的矩阵：**病态矩阵**。其为线性方程组的求解带来了不小的麻烦。

病态的线性方程组指的是：**即使 ![A,\vec{b}](https://www.zhihu.com/equation?tex=A%2C%5Cvec%7Bb%7D) 能够被精确表示，计算过程中的舍入误差会使得方程组的解的估计值与真实解相差甚远**。

也就是说，假设真实解为 ![\vec{x}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D) ，估计解为 ![\vec{y}](https://www.zhihu.com/equation?tex=%5Cvec%7By%7D) 。即使 ![||A\vec{x}-A\vec{y}||](https://www.zhihu.com/equation?tex=%7C%7CA%5Cvec%7Bx%7D-A%5Cvec%7By%7D%7C%7C) 足够小，![||\vec{x}-\vec{y}||](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C) 也可能较大。**较小的后向误差并不能保证较小的前向误差**。

例：

考虑 ![A=\begin{bmatrix} 1&2\\ 1.0001&2 \end{bmatrix}, \vec{b}=\begin{bmatrix} 3\\ 3.0001 \end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+1%262%5C%5C+1.0001%262+%5Cend%7Bbmatrix%7D%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+3%5C%5C+3.0001+%5Cend%7Bbmatrix%7D)

若我们得到的估计值为 ![\vec{y}=\begin{bmatrix} 3\\ -0.0001 \end{bmatrix}](https://www.zhihu.com/equation?tex=%5Cvec%7By%7D%3D%5Cbegin%7Bbmatrix%7D+3%5C%5C+-0.0001+%5Cend%7Bbmatrix%7D) ，则 ![A\vec{y}=\begin{bmatrix} 2.9998\\ 3.0001 \end{bmatrix}](https://www.zhihu.com/equation?tex=A%5Cvec%7By%7D%3D%5Cbegin%7Bbmatrix%7D+2.9998%5C%5C+3.0001+%5Cend%7Bbmatrix%7D) ，与 ![\vec{b}](https://www.zhihu.com/equation?tex=%5Cvec%7Bb%7D) 十分接近。

但是显然真实解为 ![\vec{x}=\begin{bmatrix} 1\\ 1 \end{bmatrix}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3D%5Cbegin%7Bbmatrix%7D+1%5C%5C+1+%5Cend%7Bbmatrix%7D) ，而 ![||\vec{x}-\vec{y}||](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C) 显然较大。

**病态的线性方程组的重要特征就是：系数矩阵的行或列的近线性相关现象。**即存在两行虽不线性相关，但是稍加一些微小的扰动后就会变得线性相关。

对此现象，我们还想要得到一个能用来衡量病态程度的标准。我们通过对于一般线性方程组的误差的估计自然地引出条件数的概念。

**定理（线性方程组的误差估计）**

**对于 ![A\vec{x}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Bb%7D) ， ![A](https://www.zhihu.com/equation?tex=A) 非奇异， ![\vec{y}](https://www.zhihu.com/equation?tex=%5Cvec%7By%7D) 为对于此方程组的估计值。 ![\vec{r}=\vec{b}-A\vec{y}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%3D%5Cvec%7Bb%7D-A%5Cvec%7By%7D) 定义为残差向量。则对任意矩阵的自然范数， ![||\vec{x}-\vec{y}||\leq ||\vec{r}||\ ||A^{-1}||](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C%5Cleq+%7C%7C%5Cvec%7Br%7D%7C%7C%5C+%7C%7CA%5E%7B-1%7D%7C%7C) ， ![\frac{||\vec{x}-\vec{y}||}{||\vec{x}||}\leq ||A||\ ||A^{-1}||\ \frac{||\vec{r}||}{||\vec{b}||}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C%7D%5Cleq+%7C%7CA%7C%7C%5C+%7C%7CA%5E%7B-1%7D%7C%7C%5C+%5Cfrac%7B%7C%7C%5Cvec%7Br%7D%7C%7C%7D%7B%7C%7C%5Cvec%7Bb%7D%7C%7C%7D) 。**

证明：

容易知道 ![||\vec{x}-\vec{y}||=||A^{-1}\vec{r}||\leq||A^{-1}||\ ||\vec{r}||](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C%3D%7C%7CA%5E%7B-1%7D%5Cvec%7Br%7D%7C%7C%5Cleq%7C%7CA%5E%7B-1%7D%7C%7C%5C+%7C%7C%5Cvec%7Br%7D%7C%7C)

而对于 ![A\vec{x}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Bb%7D) ，有 ![||\vec{b}||\leq||A||\ ||\vec{x}||](https://www.zhihu.com/equation?tex=%7C%7C%5Cvec%7Bb%7D%7C%7C%5Cleq%7C%7CA%7C%7C%5C+%7C%7C%5Cvec%7Bx%7D%7C%7C) 。

故 ![\frac{||\vec{x}-\vec{y}||}{||\vec{x}||}\leq||A||\ ||A^{-1}||\ \frac{||\vec{r}||}{||\vec{b}||}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%7C%7C%5Cvec%7Bx%7D-%5Cvec%7By%7D%7C%7C%7D%7B%7C%7C%5Cvec%7Bx%7D%7C%7C%7D%5Cleq%7C%7CA%7C%7C%5C+%7C%7CA%5E%7B-1%7D%7C%7C%5C+%5Cfrac%7B%7C%7C%5Cvec%7Br%7D%7C%7C%7D%7B%7C%7C%5Cvec%7Bb%7D%7C%7C%7D) 。

我们发现：由向量范数定义的相对误差的上界被 ![||A||\ ||A^{-1}||](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C%5C+%7C%7CA%5E%7B-1%7D%7C%7C) 控制着。

故我们很自然地如下定义条件数。

**条件数：**![K(A)=||A||\ ||A^{-1}||](https://www.zhihu.com/equation?tex=K%28A%29%3D%7C%7CA%7C%7C%5C+%7C%7CA%5E%7B-1%7D%7C%7C)

容易发现：**条件数越大，矩阵的病态程度越严重，舍入误差越有可能造成解的严重偏移**。

在求解线性方程组时，我们常常会选择避开病态的线性方程组。（比如上一章中提及的Hilbert矩阵，我们为了避开求解而引入正交多项式）

而当我们不得不求解病态的线性方程组时，我们常常利用迭代求精的方法。

**迭代求精方法**

迭代求精方法的思想很简单。我们先得到 ![A\vec{x}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Bb%7D) 的一个估计 ![\vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D) 。然而，由于病态性，这个估计显然是不合意的。

我们计算出估计的残差 ![\vec{r}^{(0)}=\vec{b}-A\vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%5E%7B%280%29%7D%3D%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%5E%7B%280%29%7D) ，并且**求解方程 ![A\vec{x}=\vec{r}^{(0)}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Br%7D%5E%7B%280%29%7D) ，得到估计 ![\vec{x}^{(1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%281%29%7D) 。**

**那么显然 ![\vec{x}^{(0)}+\vec{x}^{(1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D%2B%5Cvec%7Bx%7D%5E%7B%281%29%7D) 是一个比 ![\vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D) 要更好的估计。**

后面一直以此类推地进行上述过程。

迭代求精方法先产生出一个基估计，并在此基础上对每一次估计产生出的残差进一步迭代估计，将残差产生的影响累积到基估计上来起到调整的作用。

对于一个 **进行![t](https://www.zhihu.com/equation?tex=t) 位舍入**的计算过程而言，一般**当 ![K_\infty(A)<10^t](https://www.zhihu.com/equation?tex=K_%5Cinfty%28A%29%3C10%5Et) 时，迭代求精方法都能取得良好的效果**。（若条件数再大，则需要提高计算过程精度才能对准确度有本质提升）



最后，我们来介绍一种最复杂，但也是最有效并且应用最广的方法：**共轭梯度法**。

共轭梯度下降方法一般而言**假定 ![A](https://www.zhihu.com/equation?tex=A) 是一个正定矩阵**（从而必定是对称的）。

共轭梯度法的思路是：将线性方程组求解问题转化为求函数最小值的问题，然后利用函数梯度来对于函数的最小值进行估计。对于给定的初值，我们可以将共轭梯度法看成在一个函数曲面上的行走，而我们的目的是要走到函数的最小值处。在每一步估计过程中，我们关心的是：**1、我们这步应该朝哪个方向走 2、我们应该朝那个方向走多远的距离**

然而，方程的解与函数的最小值有什么关联呢？

下面的定理告诉我们如何将线性方程组求解转化为函数最小值求解问题。

**定理（共轭梯度方法的问题转化）**

**![\vec{r}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D) 为 ![Ax=\vec{b}](https://www.zhihu.com/equation?tex=Ax%3D%5Cvec%7Bb%7D) 的解当且仅当函数 ![g(\vec{x})=(\vec{x},A\vec{x})-2(\vec{x},\vec{b})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%29%3D%28%5Cvec%7Bx%7D%2CA%5Cvec%7Bx%7D%29-2%28%5Cvec%7Bx%7D%2C%5Cvec%7Bb%7D%29) 在 ![\vec{r}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D) 处取到最小值。**

证明：

我们假设在某一步估计时，我们从现在的估计点 ![\vec{x}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D) 处向着 ![\vec{v}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D) 方向走了 ![t||\vec{v}||](https://www.zhihu.com/equation?tex=t%7C%7C%5Cvec%7Bv%7D%7C%7C) 的长度。

即假设下一步估计为 ![\vec{x}+t\vec{v}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%2Bt%5Cvec%7Bv%7D) 。

我们考虑函数 ![g(\vec{x}+t\vec{v})=g(\vec{x})-2t(\vec{v},\vec{b}-A\vec{x})+t^2(\vec{v},A\vec{v})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%2Bt%5Cvec%7Bv%7D%29%3Dg%28%5Cvec%7Bx%7D%29-2t%28%5Cvec%7Bv%7D%2C%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%29%2Bt%5E2%28%5Cvec%7Bv%7D%2CA%5Cvec%7Bv%7D%29)

（这里注意利用 ![A](https://www.zhihu.com/equation?tex=A) 对称而带来的 ![(A\vec{x},\vec{y})=(\vec{x},A\vec{y})](https://www.zhihu.com/equation?tex=%28A%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%29%3D%28%5Cvec%7Bx%7D%2CA%5Cvec%7By%7D%29) 性质）

我们的目标是：使得 ![g(\vec{x}+t\vec{v})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%2Bt%5Cvec%7Bv%7D%29) 。然而， ![\vec{x}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D) 作为上一步的估计值我们无法改变。

我们真正能够选择的是：前进方向 ![\vec{v}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D) ，与前进距离 ![t](https://www.zhihu.com/equation?tex=t)

我们发现**最优的前进距离 ![t](https://www.zhihu.com/equation?tex=t) 是容易选取**的。

注意到 ![g(\vec{x}+t\vec{v})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%2Bt%5Cvec%7Bv%7D%29) 为关于 ![t](https://www.zhihu.com/equation?tex=t) 的二次函数，经过求导，我们很容易得到**当 ![t=\frac{(\vec{v},\vec{b}-A\vec{x})}{(\vec{v},A\vec{v})}](https://www.zhihu.com/equation?tex=t%3D%5Cfrac%7B%28%5Cvec%7Bv%7D%2C%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%29%7D%7B%28%5Cvec%7Bv%7D%2CA%5Cvec%7Bv%7D%29%7D) 时函数值最小**。

在 ![t](https://www.zhihu.com/equation?tex=t) 这样选取的情形下，，我们将其带回 ![g(\vec{x}+t\vec{v})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%2Bt%5Cvec%7Bv%7D%29) ，得到

![g(\vec{x}+t\vec{v})=g(\vec{x})-\frac{(\vec{v},\vec{b}-A\vec{x})^2}{(\vec{v},A\vec{v})}](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%2Bt%5Cvec%7Bv%7D%29%3Dg%28%5Cvec%7Bx%7D%29-%5Cfrac%7B%28%5Cvec%7Bv%7D%2C%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%29%5E2%7D%7B%28%5Cvec%7Bv%7D%2CA%5Cvec%7Bv%7D%29%7D)

容易发现，![\vec{r}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D) 为 ![Ax=\vec{b}](https://www.zhihu.com/equation?tex=Ax%3D%5Cvec%7Bb%7D) 的解当且仅当函数 ![g(\vec{x})=(\vec{x},A\vec{x})-2(\vec{x},\vec{b})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%29%3D%28%5Cvec%7Bx%7D%2CA%5Cvec%7Bx%7D%29-2%28%5Cvec%7Bx%7D%2C%5Cvec%7Bb%7D%29) 在 ![\vec{r}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D) 处取到最小值 ![g(\vec{r})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Br%7D%29) 。

由上述定理，我们可谓一举两得地得到了两个重要的结论。

**1、我们成功将线性方程组求解问题转化为了函数的最小值求解问题。**

**2、我们知道了当初始估计 ![\vec{x}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D) 以及前进方向 ![\vec{v}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D) 给定时，我们的最优前进距离 ![t=\frac{(\vec{v},\vec{b}-A\vec{x})}{(\vec{v},A\vec{v})}](https://www.zhihu.com/equation?tex=t%3D%5Cfrac%7B%28%5Cvec%7Bv%7D%2C%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%29%7D%7B%28%5Cvec%7Bv%7D%2CA%5Cvec%7Bv%7D%29%7D)**

事实上，我们不妨做如下理解：

考虑特殊情况，向量均为一维（均为数）

那么对于方程 ![ax=b(a>0)](https://www.zhihu.com/equation?tex=ax%3Db%28a%3E0%29) ，我们很自然地知道其零点即为 ![ax^2-2bx](https://www.zhihu.com/equation?tex=ax%5E2-2bx) 取最小值的点。而我们取定前进方向 ![v=1](https://www.zhihu.com/equation?tex=v%3D1) 时，给定初始估计 ![x](https://www.zhihu.com/equation?tex=x) ，我们的最佳前近距离 ![t=\frac{b-ax}{a}](https://www.zhihu.com/equation?tex=t%3D%5Cfrac%7Bb-ax%7D%7Ba%7D) 。

我们惊奇地发现：无论初值 ![x](https://www.zhihu.com/equation?tex=x) 取多少， ![x+tv=\frac{b}{a}](https://www.zhihu.com/equation?tex=x%2Btv%3D%5Cfrac%7Bb%7D%7Ba%7D) 为方程的真实解。事实上，这与我们后面阐述的共轭梯度方法的性质相吻合，**对于 ![n](https://www.zhihu.com/equation?tex=n) 维线性方程组，最多进行 ![n](https://www.zhihu.com/equation?tex=n) 次迭代估计后共轭梯度方法保证能够收敛到真实解（只要**![A](https://www.zhihu.com/equation?tex=A)**正定）**。

同时，我们已经能够理解为何需要系数矩阵的正定性了。正如 ![a>0](https://www.zhihu.com/equation?tex=a%3E0) 保证了 ![ax^2-2bx](https://www.zhihu.com/equation?tex=ax%5E2-2bx) 能够取到最小值一样，**![A](https://www.zhihu.com/equation?tex=A) 的正定性保证了 ![g(\vec{x})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%29) 的凸性**。其保证了 ![g(\vec{x})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%29) 的所有局部最小值均为全局最小值，为梯度下降做好了充足的准备。


接下来，让我们回到刚才讲到的地方。我们给出了已知前进方向的情况下最优的前近距离。接下来的一个自然的问题：前进方向要如何选取？

一个常见的思路是选择当前估计点处的负的梯度方向作为前进方向。由数学分析的知识，我们知道梯度方向是该点处函数值变化最剧烈的方向。

我们不妨求一下 ![g(\vec{x})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%29) 的梯度。

由于 ![\frac{\partial g}{\partial x_k}=2\sum_{i=1}^na_{ki}x_i-2b_k](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+g%7D%7B%5Cpartial+x_k%7D%3D2%5Csum_%7Bi%3D1%7D%5Ena_%7Bki%7Dx_i-2b_k)

**假设 ![\vec{r}=\vec{b}-A\vec{x}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%3D%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D) 表示估计的残差**。

容易得到 ![\nabla g(\vec{x})=-2\vec{r}](https://www.zhihu.com/equation?tex=%5Cnabla+g%28%5Cvec%7Bx%7D%29%3D-2%5Cvec%7Br%7D)

选取负梯度方向作为前进方向的迭代方法被称为**最速下降法**。它保证了每一步都沿着当前下降最快的方向前进。

然而，在这里，对于线性方程组而言，梯度方向并不是一个最好的前进方向。
![](https://pic1.zhimg.com/v2-cf79e5b32e9ad9cca8fc476637628824_b.jpg)
我们考虑方程组维数为 ![2](https://www.zhihu.com/equation?tex=2) 的情况。上图是三维曲面 ![g(\vec{x})](https://www.zhihu.com/equation?tex=g%28%5Cvec%7Bx%7D%29) 的一组等高线（等高线上函数值相等，越靠近中心等高线表示的高度越低）。

对于最速下降法，其对应的是绿色的下降过程。我们可以看到其每次都向最陡峭的方向下降。但是，**问题是：第一次下降与第三次下降在图中的前进路径是平行的**。我们虽然每次都在最快地下降，但是在不同点处很可能具有相同的负梯度方向，这使得我们做了很多“无用功”。

一个简单的改进想法就是：我要是能将上图中绿色下降过程中的第一次、第三次、第五次下降的过程**合并**为一次下降过程，将第二次、第四次下降过程**合并**为一次下降过程，如同红色下降过程一般，那么我们就能节省大量迭代次数。而这就是共轭梯度方法。如果我们能够做到**在每个不同前进方向上只前进一次**，那么我们至多迭代 ![n](https://www.zhihu.com/equation?tex=n) 次必定能获得 ![n](https://www.zhihu.com/equation?tex=n) 维线性方程组的精确解。

我们接下来要问：要如何找到那些不同的前进方向呢？

这个时候，又轮到**正交性**登场了。我们应该都已经很熟悉：正交性蕴含了线性无关性，且使得各个方向上的误差得以分离，简化我们的计算。

可是，另外一个问题是：这次我们要处理的函数不再像多项式一般具有很好的线性性质。对于高维空间的二次曲面我们要如何处理呢？

熟悉解析几何的读者一定知道在处理二次曲线中为了代替正交性，我们引入了**共轭**的概念。事实上，共轭性就是正交性的一个推广（这里不过多展开）。

在这里，我们类似地定义。**若 ![\forall i\neq j,\ (\vec{v}^{(i)},A\vec{v}^{(j)})=0](https://www.zhihu.com/equation?tex=%5Cforall+i%5Cneq+j%2C%5C+%28%5Cvec%7Bv%7D%5E%7B%28i%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28j%29%7D%29%3D0) ，我们就称所有前进方向 ![\vec{v}^{(i)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%28i%29%7D) 为一组 ![A-](https://www.zhihu.com/equation?tex=A-) 共轭的前进方向**。

我们下面利用定理来表明 ![A-](https://www.zhihu.com/equation?tex=A-) 共轭性带来的巨大好处。

**定理（![A-](https://www.zhihu.com/equation?tex=A-) 共轭前进方向的优越性）**

**![\vec{v}^{(1)},...,\vec{v}^{(n)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%281%29%7D%2C...%2C%5Cvec%7Bv%7D%5E%7B%28n%29%7D) 为一组![A-](https://www.zhihu.com/equation?tex=A-) 共轭前进方向。则对于 ![\forall \vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cforall+%5Cvec%7Bx%7D%5E%7B%280%29%7D)  ， ![t_k=\frac{(\vec{v}^{(k)},\vec{b}-A\vec{x}^{(k-1)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}](https://www.zhihu.com/equation?tex=t_k%3D%5Cfrac%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2C%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D) 为第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中的前进距离。则估计式为 ![\vec{x}^{(k)}=\vec{x}^{(k-1)}+t_k\vec{v}^{(k)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2Bt_k%5Cvec%7Bv%7D%5E%7B%28k%29%7D) 。则经过 ![n](https://www.zhihu.com/equation?tex=n) 次迭代后，必定得到精确解， ![A\vec{x}^{(n)}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%5E%7B%28n%29%7D%3D%5Cvec%7Bb%7D) 。**

证明：

利用估计式，我们重写 ![A\vec{x}^{(n)}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%5E%7B%28n%29%7D) ，得到 ![A\vec{x}^{(n)}=A\vec{x}^{(0)}+t_1A\vec{v}^{(1)}+...+t_nA\vec{v}^{(n)}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%5E%7B%28n%29%7D%3DA%5Cvec%7Bx%7D%5E%7B%280%29%7D%2Bt_1A%5Cvec%7Bv%7D%5E%7B%281%29%7D%2B...%2Bt_nA%5Cvec%7Bv%7D%5E%7B%28n%29%7D) 。

则我们考虑 ![(A\vec{x}^{(n)}-\vec{b},\vec{v}^{(k)})=(A\vec{x}^{(0)}-\vec{b},\vec{v})+t_1(\vec{v}^{(1)},A\vec{v}^{(k)})+...+t_n(\vec{v}^{(n)},A\vec{v}^{(k)})](https://www.zhihu.com/equation?tex=%28A%5Cvec%7Bx%7D%5E%7B%28n%29%7D-%5Cvec%7Bb%7D%2C%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%3D%28A%5Cvec%7Bx%7D%5E%7B%280%29%7D-%5Cvec%7Bb%7D%2C%5Cvec%7Bv%7D%29%2Bt_1%28%5Cvec%7Bv%7D%5E%7B%281%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%2B...%2Bt_n%28%5Cvec%7Bv%7D%5E%7B%28n%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29) 。

利用前进方向的共轭性，得到：

![(A\vec{x}^{(n)}-\vec{b},\vec{v}^{(k)})=(A\vec{x}^{(0)}-\vec{b},\vec{v}^{(k)})+t_k(\vec{v}^{(k)},A\vec{v}^{(k)})](https://www.zhihu.com/equation?tex=%28A%5Cvec%7Bx%7D%5E%7B%28n%29%7D-%5Cvec%7Bb%7D%2C%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%3D%28A%5Cvec%7Bx%7D%5E%7B%280%29%7D-%5Cvec%7Bb%7D%2C%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%2Bt_k%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29)

![t_k(\vec{v}^{(k)},A\vec{v}^{(k)})=(\vec{v}^{(k)},\vec{b}-A\vec{x}^{(0)})](https://www.zhihu.com/equation?tex=t_k%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%3D%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2C%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%5E%7B%280%29%7D%29)

故我们有： ![\forall k,\ (A\vec{x}^{(n)}-\vec{b},\vec{v}^{(k)})=0](https://www.zhihu.com/equation?tex=%5Cforall+k%2C%5C+%28A%5Cvec%7Bx%7D%5E%7B%28n%29%7D-%5Cvec%7Bb%7D%2C%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%3D0)

注意到：**关于 ![A](https://www.zhihu.com/equation?tex=A) 共轭的前进方向必定线性无关**（读者易证），故原命题得证。

这个定理肯定了我们刚才的猜想：![A](https://www.zhihu.com/equation?tex=A)** 共轭性是优越的，它使得我们在进行迭代求解时在每一步都把当前前进方向上的残差给清理干净，任意两次不同迭代中减小的残差都是相互“正交”的。共轭梯度方法将残差减小的有效性做到了极致。**

**故我们一般只取第一个前进方向**![\vec{v}^{(1)}=\vec{r}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%281%29%7D%3D%5Cvec%7Br%7D%5E%7B%280%29%7D)**为负梯度方向。**



接下来，我们离成功只有一步之遥。我们只需要想办法使得前进方向可以在方法的执行过程中被不断更新，且依旧维护其 ![A-](https://www.zhihu.com/equation?tex=A-) 共轭性就可以了。

由于第 ![k](https://www.zhihu.com/equation?tex=k) 次迭代中的负梯度方向为 ![\vec{r}^{(k-1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%5E%7B%28k-1%29%7D) ，我们想到能否通过对负梯度方向加上一个关于上一步前进方向 ![\vec{v}^{(k-1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D) 的偏移量，使得其方向被矫正，符合 ![A-](https://www.zhihu.com/equation?tex=A-) 共轭性。

即我们现在的目标转化为了：**假设我们已经有一组 ![A-](https://www.zhihu.com/equation?tex=A-) 共轭的前进方向 ![\vec{v}^{(1)},...,\vec{v}^{(k-1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%281%29%7D%2C...%2C%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D) （第一个前进方向取下降最快的负梯度方向**![\vec{r}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%5E%7B%280%29%7D)**），我们要找到 ![s_{k-1}](https://www.zhihu.com/equation?tex=s_%7Bk-1%7D) ，使得 ![\vec{v}^{(k)}=\vec{r}^{(k-1)}+s_{k-1}\vec{v}^{(k-1)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%28k%29%7D%3D%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2Bs_%7Bk-1%7D%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D) ，满足![A-](https://www.zhihu.com/equation?tex=A-) 共轭性**。

即满足 ![(A\vec{v}^{(k)},\vec{v}^{(k-1)})=0](https://www.zhihu.com/equation?tex=%28A%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%29%3D0) 。

![(A\vec{v}^{(k)},\vec{v}^{(k-1)})=(\vec{v}^{(k-1)},A\vec{r}^{(k-1)})+s_{k-1}(\vec{v}^{(k-1)},A\vec{v}^{(k-1)})](https://www.zhihu.com/equation?tex=%28A%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%29%3D%28%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%2CA%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%2Bs_%7Bk-1%7D%28%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%29)

我们**发现 ![s_{k-1}=-\frac{ (A\vec{r}^{(k-1)},\vec{v}^{(k-1)})}{ (A\vec{v}^{(k-1)},\vec{v}^{(k-1)})}](https://www.zhihu.com/equation?tex=s_%7Bk-1%7D%3D-%5Cfrac%7B+%28A%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%29%7D%7B+%28A%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%29%7D)。**

**由此，我们基本完成了整个共轭梯度法**。


接下来，我们利用一些性质来对共轭梯度法的表达式进行化简与总结。

首先是残差向量与前进方向的正交性。这是因为共轭梯度法**第 ![k-1](https://www.zhihu.com/equation?tex=k-1) 次迭代中的残差一定不再包含任何可以分解到第 ![k-1](https://www.zhihu.com/equation?tex=k-1) 次迭代中前进方向上的残差**。（共轭梯度法将残差减小的有效性做到极致）下面的定理说明了这一点。

**定理（残差与前进方向的正交性）**

**![\forall j=1,2,...k,\ (\vec{r}^{(k)},\vec{v}^{(j)})=0](https://www.zhihu.com/equation?tex=%5Cforall+j%3D1%2C2%2C...k%2C%5C+%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28j%29%7D%29%3D0) 。**

证明：

利用数学归纳法证明。此处仅仅进行简略的证明。

考虑 ![(\vec{r}^{(1)},\vec{v}^{(1)})=(\vec{r}^{(1)},\vec{r}^{(0)})=(\vec{r}^{(0)},\vec{r}^{(0)})-\frac{(\vec{r}^{(0)},\vec{r}^{(0)})}{(A\vec{v}^{(1)},\vec{v}^{(1)})}(A\vec{v}^{(1)},\vec{v}^{(1)})=0](https://www.zhihu.com/equation?tex=%28%5Cvec%7Br%7D%5E%7B%281%29%7D%2C%5Cvec%7Bv%7D%5E%7B%281%29%7D%29%3D%28%5Cvec%7Br%7D%5E%7B%281%29%7D%2C%5Cvec%7Br%7D%5E%7B%280%29%7D%29%3D%28%5Cvec%7Br%7D%5E%7B%280%29%7D%2C%5Cvec%7Br%7D%5E%7B%280%29%7D%29-%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%280%29%7D%2C%5Cvec%7Br%7D%5E%7B%280%29%7D%29%7D%7B%28A%5Cvec%7Bv%7D%5E%7B%281%29%7D%2C%5Cvec%7Bv%7D%5E%7B%281%29%7D%29%7D%28A%5Cvec%7Bv%7D%5E%7B%281%29%7D%2C%5Cvec%7Bv%7D%5E%7B%281%29%7D%29%3D0)

接着，假设 ![\forall j=1,2,...k,\ (\vec{r}^{(k)},\vec{v}^{(j)})=0](https://www.zhihu.com/equation?tex=%5Cforall+j%3D1%2C2%2C...k%2C%5C+%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28j%29%7D%29%3D0)

则 ![(\vec{r}^{(k+1)},\vec{v}^{(k+1)})=0](https://www.zhihu.com/equation?tex=%28%5Cvec%7Br%7D%5E%7B%28k%2B1%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28k%2B1%29%7D%29%3D0) （利用 ![t_{k+1}](https://www.zhihu.com/equation?tex=t_%7Bk%2B1%7D) 的表达式如上约去即可）

且 ![(\vec{r}^{(k+1)},\vec{v}^{(j)})=0](https://www.zhihu.com/equation?tex=%28%5Cvec%7Br%7D%5E%7B%28k%2B1%29%7D%2C%5Cvec%7Bv%7D%5E%7B%28j%29%7D%29%3D0) （利用前进方向的 ![A-](https://www.zhihu.com/equation?tex=A-) 共轭性）

即可得证。

事实上，由此我们又可以得到另一个推论：**残差向量之间相互正交**。即 ![i\neq j,\ (\vec{r}^{(i)},\vec{r}^{(j)})=0](https://www.zhihu.com/equation?tex=i%5Cneq+j%2C%5C+%28%5Cvec%7Br%7D%5E%7B%28i%29%7D%2C%5Cvec%7Br%7D%5E%7B%28j%29%7D%29%3D0) 。（读者模仿上述证明自证）这是容易理解的，因为共轭梯度法要求在每个前进方向上一次性地清理干净所有这个方向上的残差组成。那么不同次迭代的残差之间自然是正交的了。

下面我们开始最后一步：对于方法中用到参数的化简。

![t_k=\frac{(\vec{v}^{(k)},\vec{r}^{(k-1)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}=\frac{(\vec{r}^{(k-1)},\vec{r}^{(k-1)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}+s_{k-1}\frac{(\vec{v}^{(k-1)},\vec{r}^{(k-1)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}](https://www.zhihu.com/equation?tex=t_k%3D%5Cfrac%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D%3D%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D%2Bs_%7Bk-1%7D%5Cfrac%7B%28%5Cvec%7Bv%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D)

利用残差与前进方向的正交性，得到

![t_k=\frac{(\vec{r}^{(k-1)},\vec{r}^{(k-1)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}](https://www.zhihu.com/equation?tex=t_k%3D%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D)

对于 ![s_k=-\frac{(\vec{r}^{(k)},A\vec{v}^{(k)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}](https://www.zhihu.com/equation?tex=s_k%3D-%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D)

利用 ![(\vec{r}^{(k)},\vec{r}^{(k)})=-t_k(\vec{r}^{(k)},A\vec{v}^{(k)})](https://www.zhihu.com/equation?tex=%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k%29%7D%29%3D-t_k%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29)

以及 ![(\vec{r}^{(k-1)},\vec{r}^{(k-1)})=t_k(\vec{v}^{(k)},A\vec{v}^{(k)})](https://www.zhihu.com/equation?tex=%28%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%3Dt_k%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29)

我们得到 ![s_k=\frac{(\vec{r}^{(k)},\vec{r}^{(k)})}{(\vec{r}^{(k-1)},\vec{r}^{(k-1)})}](https://www.zhihu.com/equation?tex=s_k%3D%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k%29%7D%29%7D%7B%28%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D) 。

至此我们完成了全部的共轭梯度法工作。

**对共轭梯度法进行如下总结：**

**给定初值 ![\vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D) ，我们求出残差 ![\vec{r}^{(0)}=\vec{b}-A\vec{x}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%5E%7B%280%29%7D%3D%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%5E%7B%280%29%7D) ，使得 ![\vec{v}^{(1)}=\vec{r}^{(0)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%281%29%7D%3D%5Cvec%7Br%7D%5E%7B%280%29%7D) 作为第一次迭代的前进方向。**

**对于每一次迭代，我们要把握的是：**

**（这一步走多长） ![t_k=\frac{(\vec{r}^{(k-1)},\vec{r}^{(k-1)})}{(\vec{v}^{(k)},A\vec{v}^{(k)})}](https://www.zhihu.com/equation?tex=t_k%3D%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D%7B%28%5Cvec%7Bv%7D%5E%7B%28k%29%7D%2CA%5Cvec%7Bv%7D%5E%7B%28k%29%7D%29%7D)**

**（进行一次迭代，朝前进方向走一定长度） ![\vec{x}^{(k)}=\vec{x}^{(k-1)}+t_k\vec{v}^{(k)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%28k%29%7D%3D%5Cvec%7Bx%7D%5E%7B%28k-1%29%7D%2Bt_k%5Cvec%7Bv%7D%5E%7B%28k%29%7D)**

**（求出这一次迭代的残差，知道离终点还差多远） ![\vec{r}^{(k)}=\vec{b}-A\vec{x}^{(k)}](https://www.zhihu.com/equation?tex=%5Cvec%7Br%7D%5E%7B%28k%29%7D%3D%5Cvec%7Bb%7D-A%5Cvec%7Bx%7D%5E%7B%28k%29%7D)**

**（准备好对于前进方向的调整） ![s_k=\frac{(\vec{r}^{(k)},\vec{r}^{(k)})}{(\vec{r}^{(k-1)},\vec{r}^{(k-1)})}](https://www.zhihu.com/equation?tex=s_k%3D%5Cfrac%7B%28%5Cvec%7Br%7D%5E%7B%28k%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k%29%7D%29%7D%7B%28%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%2C%5Cvec%7Br%7D%5E%7B%28k-1%29%7D%29%7D)**

**（更新前进方向，决定下一次迭代要向哪里走） ![\vec{v}^{(k+1)}=\vec{r}^{(k)}+s_{k}\vec{v}^{(k)}](https://www.zhihu.com/equation?tex=%5Cvec%7Bv%7D%5E%7B%28k%2B1%29%7D%3D%5Cvec%7Br%7D%5E%7B%28k%29%7D%2Bs_%7Bk%7D%5Cvec%7Bv%7D%5E%7B%28k%29%7D)**


但是其实这还并不是共轭梯度方法的全部。

对于良态的线性方程组，上述方法已经足够取得好的效果，经常性地来说大约 ![\sqrt{n}](https://www.zhihu.com/equation?tex=%5Csqrt%7Bn%7D) 步就可以结束迭代。

然而上述方法始终没考虑**病态**方程组的情况。

然而，共轭梯度法是一个易于推广的方法。对于病态问题，我们只需要多一步**预条件**的处理即可。

考虑 ![A\vec{x}=\vec{b}](https://www.zhihu.com/equation?tex=A%5Cvec%7Bx%7D%3D%5Cvec%7Bb%7D) ， ![A](https://www.zhihu.com/equation?tex=A) 正定。现在若 ![A](https://www.zhihu.com/equation?tex=A) 出现问题，我们自然地想到：将 ![A](https://www.zhihu.com/equation?tex=A) 的条件经过**等解变换**来变好即可。

为了维持系数矩阵的正定性，我们利用矩阵的合同关系。

假设存在**满秩矩阵 ![C](https://www.zhihu.com/equation?tex=C) ， ![\hat{A}=C^{-1}A(C^{-1})^T](https://www.zhihu.com/equation?tex=%5Chat%7BA%7D%3DC%5E%7B-1%7DA%28C%5E%7B-1%7D%29%5ET) 与 ![A](https://www.zhihu.com/equation?tex=A) 合同，仍旧正定**。

那么原方程组与 ![\hat{A}C^T\vec{x}=C^{-1}\vec{b}](https://www.zhihu.com/equation?tex=%5Chat%7BA%7DC%5ET%5Cvec%7Bx%7D%3DC%5E%7B-1%7D%5Cvec%7Bb%7D) 等解。

**那么我们的做法很简单：**

**![\hat{A}=C^{-1}A(C^{-1})^T](https://www.zhihu.com/equation?tex=%5Chat%7BA%7D%3DC%5E%7B-1%7DA%28C%5E%7B-1%7D%29%5ET) ， ![\hat{\vec{x}}=C^T\vec{x},\ \hat{\vec{b}}=C^{-1}\vec{b}](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cvec%7Bx%7D%7D%3DC%5ET%5Cvec%7Bx%7D%2C%5C+%5Chat%7B%5Cvec%7Bb%7D%7D%3DC%5E%7B-1%7D%5Cvec%7Bb%7D)**

**我们只需要解方程组 ![\hat{A}\hat{\vec{x}}=\hat{\vec{b}}](https://www.zhihu.com/equation?tex=%5Chat%7BA%7D%5Chat%7B%5Cvec%7Bx%7D%7D%3D%5Chat%7B%5Cvec%7Bb%7D%7D) ，得到解为 ![\vec{y}](https://www.zhihu.com/equation?tex=%5Cvec%7By%7D) ，那么 ![(C^T)^{-1}\vec{y}](https://www.zhihu.com/equation?tex=%28C%5ET%29%5E%7B-1%7D%5Cvec%7By%7D) 即为原方程的解**。

有了上述做法后，我们只需要专注于如何选择满秩的 ![C](https://www.zhihu.com/equation?tex=C) ，使得 ![\hat{A}=C^{-1}A(C^{-1})^T](https://www.zhihu.com/equation?tex=%5Chat%7BA%7D%3DC%5E%7B-1%7DA%28C%5E%7B-1%7D%29%5ET) 的条件变好即可。

**一般而言，我们常常取**

**![C=D^{-\frac{1}{2}}=diag(\sqrt{\frac{1}{a_{11}}},\sqrt{\frac{1}{a_{22}}},...,\sqrt{\frac{1}{a_{nn}}})](https://www.zhihu.com/equation?tex=C%3DD%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D%3Ddiag%28%5Csqrt%7B%5Cfrac%7B1%7D%7Ba_%7B11%7D%7D%7D%2C%5Csqrt%7B%5Cfrac%7B1%7D%7Ba_%7B22%7D%7D%7D%2C...%2C%5Csqrt%7B%5Cfrac%7B1%7D%7Ba_%7Bnn%7D%7D%7D%29)**

**或者先对 ![A](https://www.zhihu.com/equation?tex=A) （正定）进行Cholesky分解 ![A=LL^T](https://www.zhihu.com/equation?tex=A%3DLL%5ET)**

**再取 ![C=L](https://www.zhihu.com/equation?tex=C%3DL) 。**

这些取法都能够有效地优化系数矩阵的条件，加快共轭梯度法的收敛。

对于先前的例子：

例：

![A=\begin{bmatrix} 0.2 &0.1  &1  &1  &0 \\ 0.1  &4  & -1 &1  &-1 \\   1& -1 & 60 &0  &-2 \\  1 & 1 &0  &8  &4 \\  0 & -1 & -2 & 4 & 700 \end{bmatrix} , \vec{b}=\begin{bmatrix} 1\\  2\\  3\\  4\\  5 \end{bmatrix}](https://www.zhihu.com/equation?tex=A%3D%5Cbegin%7Bbmatrix%7D+0.2+%260.1++%261++%261++%260+%5C%5C+0.1++%264++%26+-1+%261++%26-1+%5C%5C+++1%26+-1+%26+60+%260++%26-2+%5C%5C++1+%26+1+%260++%268++%264+%5C%5C++0+%26+-1+%26+-2+%26+4+%26+700+%5Cend%7Bbmatrix%7D+%2C+%5Cvec%7Bb%7D%3D%5Cbegin%7Bbmatrix%7D+1%5C%5C++2%5C%5C++3%5C%5C++4%5C%5C++5+%5Cend%7Bbmatrix%7D)

我们给定初值 ![\vec{x}^{(0)}=\vec{0}](https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5E%7B%280%29%7D%3D%5Cvec%7B0%7D) ，容许的误差为 ![0.01](https://www.zhihu.com/equation?tex=0.01) （无穷范数定义的相对误差标准）

利用共轭梯度法，我们容易知道只要矩阵条件不病态，理论上方法**必定在 ![5](https://www.zhihu.com/equation?tex=5) 步之内收敛**。对于正定的线性系统的求解，共轭梯度法比SOR要更加快。



对于线性方程组的迭代求法的介绍就到此为止了。

在本章后半部分中，我们先介绍了Jacobi方法与Gauss-Seidel方法，后介绍了其推广形式SOR方法。接着，我们引入了病态的概念并且提出了处理病态问题的精细迭代方法。最后，我们介绍了复杂却快速有效的共轭梯度方法以及预条件处理。

**总而言之，对于病态方程，我们通常利用精细迭代方法或者预条件处理后的共轭梯度方法来求解。一般而言，SOR方法的使用多于Jacobi以及Gauss-Seidel方法，但是这三种方法均不能保证对于任意满秩系数矩阵的收敛性。对于共轭梯度方法，虽然有着系数矩阵正定的前提条件，但是其是我们介绍的方法中最有效的，也基本能保证方法的收敛性。**

本章的内容就到这里，谢谢各位的阅读！

