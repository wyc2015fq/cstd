# 通俗易懂了解机器学习中的“回归”和“梯度下降方法”（二） - 知乎
# 

开始本文阅读前，建议您先花时间阅读：

[通俗易懂了解机器学习中的“回归”和“梯度下降方法”（一）](https://zhuanlan.zhihu.com/p/30759359)

## **三、寻找“公式”**

如何才能找到“小鸡仔重量”与“他们长大成为成年鸡后的重量”的公式呢？
![](https://pic4.zhimg.com/v2-fe352bc312621519e10df131157a8243_b.jpg)
回忆一下前两章的内容，我们曾经一度提到：生物学砖家告诉我们，小鸡仔的重量X和成年鸡的重量Y之间的关系是：

Y＝25X＋10（克）

但实际情况是，并没有这个公式，有的只是养鸡人手中记载的几十组小鸡仔的重量，以及他们长成成年鸡后的重量。

所以这里需要转变一下思路：

我们在学校里，一般会有砖家或者老师，告诉我们公式是Y＝25X＋10，然后给我X，我代入公式，求出Y即可。

但实际生活中，公式的细节我们是不知道的：
![](https://pic2.zhimg.com/v2-e69deddc0e88eccfe5123306ccd70f51_b.jpg)
我们手头有的是几十组X、Y，要通过这几十组X、Y确认出上面公式中的“25”和“10”到底应该是几？

这也就是寻找公式的过程了。而寻找到的公式，也叫做“回归方程”。

举个例子，如果就如之前故事的描述，鸡仔重量X与成年鸡重量Y是一个线性关系，即：

Y＝a+b*X

也就是要通过几十组已知的X、Y，求出上面方程中截距a和斜率b是多少（有些同学是不是已经回忆起没怎么认真上过的中学数学课）。
![](https://pic2.zhimg.com/v2-796d079258a6243bda527f77480469ed_b.jpg)
这个题目并不复杂，你可以使用“最小二乘法”计算出a，b。

而且现在很多比较好的计算器或是Excel都有计算截距，斜率的功能。

**但是问题来了：如果X，Y不是一个线性关系呢？**

比如： ![Y=a+b\times X^{2}+c\times X](https://www.zhihu.com/equation?tex=Y%3Da%2Bb%5Ctimes+X%5E%7B2%7D%2Bc%5Ctimes+X)

甚至： ![Y=a+b\times X^{5}－c\times X^{3}+d\times \sqrt[3]{X}……](https://www.zhihu.com/equation?tex=Y%3Da%2Bb%5Ctimes+X%5E%7B5%7D%EF%BC%8Dc%5Ctimes+X%5E%7B3%7D%2Bd%5Ctimes+%5Csqrt%5B3%5D%7BX%7D%E2%80%A6%E2%80%A6)

穷举是一种办法，写一个程序不是不可能做到。但是你会发现，随着方程的参数越来越多，a,b,c,d……穷举的数据量是惊人的。显然不是一种合理的做法。那怎么办呢？这就是我们接下来要讲的“梯度下降方法”要干的事情。

## **四、梯度下降方法**

这个章节，我们来详细讲解一下梯度下降方法的思路。

**1、前导知识：**

为了更好的理解梯度下降方法，我们先用最简单的线性方程，来回顾一下回归分析的大致过程，并且了解一些前导知识。

假设有一个线性方程：

Y＝b+w*X（这里，我们用b代表截距，w代表斜率）

我们的已知条件和问题是：目前，我们手头只有10个X、10个Y，b、w是我们要求出的未知数。

我们先将10个X、10个Y画在坐标图上：
![](https://pic3.zhimg.com/v2-e18a9f6ede478b11984d27fb2add01ea_b.jpg)
我们来想象一下，b,w的值，会决定一条直线的截距与斜率。因此，如果b,w确定了，那么它在图上就会形成一条直线：
![](https://pic4.zhimg.com/v2-c014d4c02996ca3e391affecc94cf5c3_b.jpg)
而很不幸，现在b,w是变量。那么每一组b,w就会对应一根直线。所以，在b,w是变量时，图上将有无数条直线：
![](https://pic2.zhimg.com/v2-bf73bde86fea5850c58f2b06a14d08a1_b.jpg)
而通过10个X、10个Y来求解b、w的过程，也就是要用图上的10个点，求解出一根直线，使得这根直线“尽可能的穿过”这10个点。

当然，很显然这10个点不可能都落在这条直线上，否则，就不是直线了：
![](https://pic1.zhimg.com/v2-be2f97420ab66396c4ba88fc89c29438_b.jpg)
所以“尽可能的穿过”的意思，是要让每个点距离这条直线的距离都尽可能的近。比如下面图片中，两条直线是很好取舍的。
![](https://pic1.zhimg.com/v2-24819b4ddbe3a638bdc962fcdb8033f0_b.jpg)
但只是凭感觉取舍是不够的，因此，我们需要将“尽可能的穿过”用更为量化的形式表达出来，也就是：让所有点到这条直线的距离最短。而每个点到直线的“距离”是什么呢？

我们假设这10个点的坐标为： 
![](https://pic1.zhimg.com/v2-4bb7b25ab411b81be0ddccdcd2eb9cdc_b.jpg)
*注：你可能会纳闷为什么y上面要加个“帽子”。这是因为要与 ![\hat{y}^1](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E1) 要与 ![y^1](https://www.zhihu.com/equation?tex=y%5E1) 区分开。继续往下看你就会明白。*

以第一个点为例 ：

第一个点坐标为:![(x^1, \hat{y}^1)](https://www.zhihu.com/equation?tex=%28x%5E1%2C+%5Chat%7By%7D%5E1%29)；

横坐标为![x^1](https://www.zhihu.com/equation?tex=x%5E1)时，直线 ![y=b+w\times x](https://www.zhihu.com/equation?tex=y%3Db%2Bw%5Ctimes+x) 上的点的坐标为： ![(x^1, y^1)](https://www.zhihu.com/equation?tex=%28x%5E1%2C+y%5E1%29) ，其中， ![y^1=b+w\times x^1](https://www.zhihu.com/equation?tex=y%5E1%3Db%2Bw%5Ctimes+x%5E1)

这时，第一个点距离直线的距离就是： ![\hat{y}^1-y^1](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E1-y%5E1) ，即， ![\hat{y}^1-(b+w\times x^1)](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E1-%28b%2Bw%5Ctimes+x%5E1%29) 。

我们用一个小动画更加形象的表示一下：
![](https://pic2.zhimg.com/v2-c941fb3801c2cf5590e8ace8830e0745_b.gif)
好了，下面我们要把所有的点到直线的距离加起来，即把 ![\hat{y}^1-y^1,\hat{y}^2-y^2,……,\hat{y}^{10}-y^{10}](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E1-y%5E1%2C%5Chat%7By%7D%5E2-y%5E2%2C%E2%80%A6%E2%80%A6%2C%5Chat%7By%7D%5E%7B10%7D-y%5E%7B10%7D) 加起来。但是你会发现有时候点在直线的上方，有时候点在直线的下方，所以 ![\hat{y}^n-y^n](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5En-y%5En) 有时是正的有时是负的。因此不能简单加总，而是需要平方求和： ![\sum_{n=1}^{10}({\hat{y}^n-y^n})^2](https://www.zhihu.com/equation?tex=%5Csum_%7Bn%3D1%7D%5E%7B10%7D%28%7B%5Chat%7By%7D%5En-y%5En%7D%29%5E2)

把 ![y^n=b+w\times x^n](https://www.zhihu.com/equation?tex=y%5En%3Db%2Bw%5Ctimes+x%5En) 带入，就得到：
![](https://pic3.zhimg.com/v2-a5ae75cf09ed4b9a51b97e85bb63f896_b.jpg)
这个公式就代表了我们需要找到的：所有点到直线的距离，而我们惊喜的发现：这个公式里面 ![\hat{y}^n，x^n](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5En%EF%BC%8Cx%5En) 是已知的，也就是说这里面只有两个未知数b和w。

在梯度下降方法中，我们给这个公式起个名字叫做损失函数，即Loss Function。为什么叫损失函数呢？你想想，这个函数的值越大越好还是越小越好？显然，所有点到直线的距离越小越好。跟“损失”一样，越小越好的一个函数。而这个函数中有两个未知数b,w，所以，我们用以下表示方法来表示“损失”函数：
![](https://pic3.zhimg.com/v2-394033ae42f3732746bb90295e5f19a2_b.jpg)
好了，现在我们的目标已经很明确了，就是要求解 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)  这个损失函数在b,w取何值时最小，然后用这个b,w形成的直线 ![y=b+w\times x](https://www.zhihu.com/equation?tex=y%3Db%2Bw%5Ctimes+x) ，也就是最合适的那条直线。换句话说，这条直线与那10个点的距离之和最近，最能体现出这10个点分布的特性。

有了上面的前导知识，下面再解释梯度下降方法也就更好理解了。

**2、梯度下降原理：**

回顾一下我们的目标：没有蛀牙！

Sorry，应该是：求解 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)  这个损失函数在b,w取何值时最小。

怎么求解呢？我们把问题简化，先假定有一个Loss Function只有一个变量w： ![L(w)](https://www.zhihu.com/equation?tex=L%28w%29)

然后我们把 ![L(w)](https://www.zhihu.com/equation?tex=L%28w%29) 的图形画出来，横轴是w，纵轴是L，假设是下面这样一个图形：
![](https://pic1.zhimg.com/v2-87a82d1c83502fb9b23206a2a23545a8_b.jpg)
如何寻找w为何值时 ![L(w)](https://www.zhihu.com/equation?tex=L%28w%29) 最小呢？对w进行穷举是一个办法，但是效率太低了。我们需要一种效率更高的方式。

这就需要用到函数导数的性质：我们都知道，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。而且我们发现，在最小值左边的斜率都是负的，在最小值右边的斜率都是正的，而最小值那一点的斜率为0。下面这个图会更直观一些：
![](https://pic1.zhimg.com/v2-dba0088b39127457ac0835da7a045d7c_b.gif)
利用函数导数的这一特性，我们寻找 ![L(w)](https://www.zhihu.com/equation?tex=L%28w%29) 最小值的方法是：
![](https://pic4.zhimg.com/v2-f6554ad72961bb494c8368881c98782f_b.jpg)
**第一步：**随机选取一个w值 ![w=w^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0) ；

**第二步：**在![w=w^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0) 这一点对LossFunction求导，如果导数值为负数，我们就调大 ![w](https://www.zhihu.com/equation?tex=w) 使 ![w=w^1 ](https://www.zhihu.com/equation?tex=w%3Dw%5E1+) ；反之亦然，即：如果导数值为正数，我们就减小 ![w](https://www.zhihu.com/equation?tex=w) 。

这里需要说明一下，我们在调大或者调小![w](https://www.zhihu.com/equation?tex=w)时，每次应该调整多少呢？从上图可以看出：

当我们在![w=w^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0) 这一点时，调整量为：
![](https://pic4.zhimg.com/v2-332f4f6099aa232e0767a9665cf799ab_b.jpg)
当我们在![w=w^1 ](https://www.zhihu.com/equation?tex=w%3Dw%5E1+) 这一点时，调整量为：
![](https://pic3.zhimg.com/v2-5c847e3063d4275b688115c060d557d2_b.jpg)
当我们在 ![w^3,w^4,w^5……](https://www.zhihu.com/equation?tex=w%5E3%2Cw%5E4%2Cw%5E5%E2%80%A6%E2%80%A6) 时以此类推，都是一个常数 ![\eta](https://www.zhihu.com/equation?tex=%5Ceta) 乘以这一点的导数值。

这是为什么呢？原因很简单：当这一点的导数值很大时，说明现在所处的位置距离LossFunction的最小值位置还很远，所以我们就要向着LossFunction最小值的方向迈一大步，而当这一点的导数值很小时，说明现在所处的位置已接近LossFunction最小值，所以就要小碎步调整。

此外，由于导数为负时w需要增加，导数为正时w需要减小，所以这个“调整量”前面要有一个负号。

那常数 ![\eta](https://www.zhihu.com/equation?tex=%5Ceta) 是什么呢？这个也是控制“调整量”大小的一个因子，这个值越大，我们每次迈的步子就越大，向极值位置靠拢的速度就越快。常数 ![\eta](https://www.zhihu.com/equation?tex=%5Ceta) 在梯度学习理论中有个名称，叫做“学习率”——learning rate。但它也不是越大越好，因为它很大时，当接近LossFunction最小值位置时，很容易一下子调整过了头，还得掉头往回跑～反而降低了调整的整体效率。

**最后一步：**就是不断的重复第二步，你会发现w的值最终会卡在一个区域，而这个区域是LossFunction的最小值附近。
![](https://pic1.zhimg.com/v2-8a36e9dfe54c85b25ef79fed5d33ef74_b.gif)
这样我们就把LossFunction的最小值时w的值求出了。

但你可能会发现这里会有一些小小的不完美，因为其实在右侧还有一个更小的最小值。这就与最初 ![w^0](https://www.zhihu.com/equation?tex=w%5E0) 的随机取值位置有关了，看了下图你就明白了：
![](https://pic4.zhimg.com/v2-9310527553f955c7ad00f99a2611dfa7_b.gif)
不过，总的来说，虽然有些小小的不完美，但相对穷举法来说，这种方法的效率提高了很多，而且，也是取到了一个使得LossFunction相对最小的w值。

讲到这里，梯度下降方法的基本原理就介绍完了。

下面我们更烧脑一些，回到我们的老朋友“ ![y=b+w\times x](https://www.zhihu.com/equation?tex=y%3Db%2Bw%5Ctimes+x) ”这个线性方程。

上文已经提到了，它的LossFunction里有b,w两个变量，所以我们用 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 来表示。聪明如你一定已经想到了，这个 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 的图形，就不是一个两维坐标轴上的曲线了，而是一个三维坐标轴上的曲面～
![](https://pic2.zhimg.com/v2-3a6283b0d617cb6189af1e7b845f1dad_b.jpg)
我们的做法与刚才还是一样的：随机选取LossFunction中w的值为 ![w^0](https://www.zhihu.com/equation?tex=w%5E0) ，b的值为 ![b^0](https://www.zhihu.com/equation?tex=b%5E0) ，然后进行求导。

但不一样的是：LossFunction图形现在不再是曲线，而是一个曲面。因此，要在这个曲面上  ![w=w^0,b=b^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0%2Cb%3Db%5E0) 确定的那一点上，求“偏导数”。

我们来看一下这个过程：

**第一步：**我们随机选取 ![w=w^0,b=b^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0%2Cb%3Db%5E0) ；

**第二步：**在 ![w=w^0,b=b^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0%2Cb%3Db%5E0) 时，求出![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 对 ![w](https://www.zhihu.com/equation?tex=w) 的偏导数
![](https://pic1.zhimg.com/v2-efeb7972b1fb194677296db0766b1100_b.jpg)
同时，求![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 对 ![b](https://www.zhihu.com/equation?tex=b) 的偏导数
![](https://pic2.zhimg.com/v2-41ec7a92e4231b2068c83268d315be71_b.jpg)
然后，分别调整 ![w^0,b^0](https://www.zhihu.com/equation?tex=w%5E0%2Cb%5E0) 至 ![w^1,b^1](https://www.zhihu.com/equation?tex=w%5E1%2Cb%5E1)
![](https://pic2.zhimg.com/v2-2bae49f4a5bf3c57d8903886f1e0698d_b.jpg)
**最后一步：**不断重复第二步，直到w，b的值最终卡在一个区域，而这个区域是LossFunction的一个最小值的附近。

这样w,b就求解出来了，同时，损失函数值最小的那条直线的回归方程![y=b+w\times x](https://www.zhihu.com/equation?tex=y%3Db%2Bw%5Ctimes+x) 也就找到了！

上面的解释还是比较抽象，咱们结合![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 的图形来看一看，在w,b两个变量的情况下，梯度下降方法到底在做什么。

首先，随机选取 ![w=w^0,b=b^0](https://www.zhihu.com/equation?tex=w%3Dw%5E0%2Cb%3Db%5E0) ，就是在![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 的曲面上确定了一个点。

然后，在这个点上求![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 对 ![w](https://www.zhihu.com/equation?tex=w) 的偏导数和![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 对 ![b](https://www.zhihu.com/equation?tex=b) 的偏导数，就是确定这个曲面在这个点上的“梯度”。这里简单解释一下“梯度”：“梯度”是一个方向，函数延该方向的变化是最快的。在咱们这个例子里，“梯度”的意思就是![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)下降最快的方向，也就是![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)的曲面上，向下最陡峭的方向。这也是为什么这个方法要叫做：“梯度下降”方法。

*注：如果还想更为深入了解“梯度”，可以复习一下高等数学中的“方向导数”、“梯度” 的概念～*

最后，调整 ![w^0,b^0](https://www.zhihu.com/equation?tex=w%5E0%2Cb%5E0) 到 ![w^1,b^1 ](https://www.zhihu.com/equation?tex=w%5E1%2Cb%5E1+) ，就是沿着“梯度”的方向（![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)下降最快的方向），尽快找到令![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)更小的w,b的值。 
![](https://pic3.zhimg.com/v2-4a87bf8bbf84f207247532d1a4861eda_b.gif)
不断的重复上面这一过程，最终会找到一个w和一个b，令 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 处于最小值附近！
![](https://pic1.zhimg.com/v2-68edf25de6173be69912b434465526f4_b.gif)
不过，前面提到的梯度下降方法的问题仍然是存在的，即：如果这个曲面上还有 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 值更低的位置，很可能由于出发点的不同，造成最后无法到达 ![L(w,b)](https://www.zhihu.com/equation?tex=L%28w%2Cb%29) 的最低点。

但是，如果你是进行线性回归，则没有以上担心，因为线性回归方程的LossFunction的曲面是具有凸性的（convex），也就是下面这个形状：
![](https://pic3.zhimg.com/v2-fc454ddd8f711e140357f353b18c88aa_b.jpg)
在这样的曲面上进行梯度下降，是一定能找到最小值的。线性回归方程的LossFunction具有凸性这一性质，是可以用数学方法证明的，在这里就不赘述了。

一个变量、两个变量的情况，上面都已经解释过了。举一反三，任意变量的LossFunction  ![L(a,b,c...)](https://www.zhihu.com/equation?tex=L%28a%2Cb%2Cc...%29) ，都可以通过逐一求偏导，再逐步延梯度方向调整每个变量取值，逐渐的逼近LossFunction的某个最小值，并最终求出这个最小值附近a,b,c……的值是多少。这样，类似下面这样复杂的方程：

![Y=a+b\times X^{5}－c\times X^{3}+d\times \sqrt[3]{X}……](https://www.zhihu.com/equation?tex=Y%3Da%2Bb%5Ctimes+X%5E%7B5%7D%EF%BC%8Dc%5Ctimes+X%5E%7B3%7D%2Bd%5Ctimes+%5Csqrt%5B3%5D%7BX%7D%E2%80%A6%E2%80%A6)

最终也可以确定出一个回归方程，这也是梯度下降方法最大的优势。

