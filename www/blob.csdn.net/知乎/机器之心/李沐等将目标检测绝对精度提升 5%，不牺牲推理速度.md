# 李沐等将目标检测绝对精度提升 5%，不牺牲推理速度 - 知乎
# 



> 天下没有免费的午餐？李沐等研究者在一份名为《Bag of Freebies for Training Object Detection Neural Networks》的论文中推翻了这一定理。他们在不牺牲推理速度的前提下将目标检测绝对精度提升了 5%。

目标检测无疑是计算机视觉领域最前沿的应用之一，吸引了各个领域诸多研究者的目光。最前沿的检测器，包括类似 RCNN 的单（SSD 或 YOLO）或多阶神经网络都是基于图像分类骨干网络，如 [VGG](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650757118%26idx%3D4%26sn%3Daf6ddb3274c8bb58a1043abf70da03eb%26chksm%3D871a9380b06d1a969b360c4dde13182675eee5011e9a6c4591540d5c0e09aa013840e793293a%26token%3D1948716613%26lang%3Dzh_CN)、[ResNet](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650757118%26idx%3D4%26sn%3Daf6ddb3274c8bb58a1043abf70da03eb%26chksm%3D871a9380b06d1a969b360c4dde13182675eee5011e9a6c4591540d5c0e09aa013840e793293a%26token%3D1948716613%26lang%3Dzh_CN)、Inception 或 MobileNet 系列。

然而，由于模型容量和训练复杂度相对较高，目标检测受到的关注相对较少，从最近的训练微调研究中获益也较少。更糟糕的是，不同的检测网络在没有明确的初始化、数据预处理及优化分析的情况下就开始挑选自己的训练流程，导致在采用最新的技术改进图像分类任务时出现了大量的混乱。

本文的研究者致力于探索能够在不造成额外计算成本的情况下有效提升流行的目标检测网络性能的方法。他们首先在目标检测上探索了一种混合技术。与《mixup: Beyond Empirical Risk Minimization》不同，本文的研究者认识到了多目标检测任务的特殊性质有利于实现空间不变的变换，因此提出了一种用于目标检测任务的视觉相干（visually coherent）图像混合方法。接下来，他们探讨了详细的训练流程，包括学习率调度、权重衰减和同步 BatchNorm。最后，他们探索了其训练微调的有效性，方法是逐渐将这些微调叠加，以训练单或多阶段目标检测网络。

本文的主要贡献如下：
- 首次系统地评估了不同目标检测流程中应用的多种训练启发式方法，为未来的研究提供了有价值的实践指导。
- 提出了一种为训练目标检测网络而设计的视觉相干图像混合方法，而且证明该方法可以有效提升模型的泛化能力。
- 在不修改网络架构和损失函数的情况下，在现有模型的基础上实现了 5% 的绝对精度性能提升。而且这些提升都是「免费的午餐」，无需额外的推理成本。
- 扩展了目标检测数据增强领域的研究深度，显著增强了模型的泛化能力，减少了过拟合问题。这些实验还揭示了可以在不同网络架构中一致提高目标检测性能的良好技术。

所有相关代码都是开源的，模型的预训练权重可以在 GluonCV Toolkit 中获取。

Gluon CV Toolkit 链接：[https://github.com/dmlc/gluon-cv](https://link.zhihu.com/?target=https%3A//github.com/dmlc/gluon-cv)
![](https://pic2.zhimg.com/v2-2b70633b09d44b87a69d934533f3f649_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='976' height='824'></svg>)图 1：Bag of Freebies 在不牺牲推理速度的前提下，显著提高了目标检测器的性能
**论文：Bag of Freebies for Training Object Detection Neural Networks**
![](https://pic2.zhimg.com/v2-4d5eee8851c2541cb7bac950f7a0afc1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='964' height='244'></svg>)
论文链接：[https://arxiv.org/pdf/1902.04103.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1902.04103.pdf)

**摘要：**与针对更好的图像分类模型所取得的巨大研究成就相比，训练目标检测器的努力在普及性和普遍性方面都存在不足。由于网络结构和优化目标要复杂得多，针对特定检测算法专门设计了不同的训练策略和流程。在本文中，我们探索了通用的微调，这些微调有助于在不牺牲推理速度的前提下将当前最佳的目标检测模型提高到一个新水平。我们的实验表明，这些「freebies」可以提高 5% 的绝对精度。

**3. 技术细节**

我们提出了一个用于目标检测的视觉相干图像混合方法，还介绍了为系统提升[目标检测](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650757118%26idx%3D4%26sn%3Daf6ddb3274c8bb58a1043abf70da03eb%26chksm%3D871a9380b06d1a969b360c4dde13182675eee5011e9a6c4591540d5c0e09aa013840e793293a%26token%3D1948716613%26lang%3Dzh_CN)模型性能而设计的数据处理和训练调度器。

3.1 用于目标检测的视觉相干图像混合

Zhang 等人在《mixup: Beyond Empirical Risk Minimization》中引入的混合概念被证明在分类网络中减少对抗干扰方面非常成功。他们提出的混合算法中混合比例的分布来自β分布（a = 0.2, b = 0.2）。大多数的混合几乎都是这种β分布的噪声。受到 Rosenfeld 等人启发式实验的激励，我们关注自然共现的目标呈现，这种呈现在目标检测中扮演重要角色。半对抗目标补丁移植方法不是传统的攻击方法。通过应用更复杂的空间变换，我们引入了遮挡，即在自然图像呈现中常见的空间信号干扰。

我们的实验中继续增加了 mixup 中使用的混合比例，由此产生的帧中的目标更有活力，也更符合自然表现，类似于低 FPS 电影中常见的过渡帧。图像分类和此类高比例 mixup 的视觉对比如图 2 和图 3 中所示。我们还使用了保留几何形状的对齐方式来进行图像混合，以避免在初始步骤中扭曲图像。我们还选择了视觉相干性更强的β分布，a >= 1 和 b >= 1，而不是按照图 4 所示的图像分类中相同的做法。

我们还通过实验利用 [YOLOv3](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650757118%26idx%3D4%26sn%3Daf6ddb3274c8bb58a1043abf70da03eb%26chksm%3D871a9380b06d1a969b360c4dde13182675eee5011e9a6c4591540d5c0e09aa013840e793293a%26token%3D1948716613%26lang%3Dzh_CN) 网络在 Pascal VOC 数据集上测试了经验混合比分布。表 1 显示了采用检测混合方法的实际改进情况。α和β都等于 1.5 的β分布略优于 1.0（相当于均匀分布），也优于固定均匀混合。
![](https://pic3.zhimg.com/v2-03b4dd1de9fc5b1a91a74640053dc1d6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='412' height='166'></svg>)表 1：在 Pascal VOC 2007 测试集上用 YOLOv3 验证多种混合方法的有效性。加权损失表示总体损失是多个比例为 0 比 1 的目标的损失之和，该比例是基于它们在原始训练图像中所属的图像混合比例算出的
**4. 实验**

为了对比所有微调方法对目标检测结果的改进，我们分别使用 YOLOv3 和 Faster-RCNN 作为单或多阶段 pipeline 的代表。为了适应大规模训练任务，我们使用 Pascal VOC 对精调技巧做评估，使用 COCO 数据集对总体性能提升和泛化能力做验证。
![](https://pic1.zhimg.com/v2-90ed216c8f3a11368346e9c5fa7925c8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='796' height='794'></svg>)图 7：在 COCO 2017 验证集上利用 BoF 得到的检测结果示例。![](https://pic1.zhimg.com/v2-77ae3f1776ed848c4a09ab116f633a64_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1075' height='295'></svg>)图 8：YOLOv3 在 COCO 数据集 80 个类别上的 AP 分析。红线表示使用 BoF 的性能提升，蓝线表示性能下降![](https://pic1.zhimg.com/v2-9d79ad8883263927fabb61404e64795c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1061' height='292'></svg>)图 9：Faster-RCNN resnet 50 在 C0C0 数据集 80 个类别上的 AP 分析。红线表示使用 BoF 的性能提升，蓝线表示性能下降![](https://pic2.zhimg.com/v2-c793c5fde2b71b410edf03ab645f8ab5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='433' height='249'></svg>)表 2：对 YOLOv3 的训练精调，在 Pascal VOC 2007 测试集 416×416 图像上评估![](https://pic2.zhimg.com/v2-c3f18ec93ca6e2fd46a3d758e72bc5fd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='405' height='172'></svg>)表 3:对 Faster-RCNN 的训练精调，在 Pascal VOC 2007 测试集 600 × 1000 图像上评估![](https://pic3.zhimg.com/v2-3367f2fc4a9e4ed97c2433827fa260fa_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='763' height='137'></svg>)表 4:利用 bag of freebies（BoF) 方法取得的进步，在 MS COCO 2017 验证集上评估![](https://pic2.zhimg.com/v2-648a06d2de52e2371496fb993e473961_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='549' height='120'></svg>)表 5：预训练图像分类和检测网络混合方法影响的组合分析![](https://pic4.zhimg.com/v2-84bafd2e391d4df4ba3e7cfe977edcdb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='515' height='114'></svg>)表 6：预训练图像分类和检测网络混合方法影响的组合分析。



