# 你的论文能否中顶会？这篇分析同行评审结果的论文可帮助你 - 知乎
# 



选自arXiv，作者：Dongyeop Kang等，机器之心编译。

> 在人工智能领域，会议论文是证明研究人员学术水平的重要一环。是否存在一些「技巧」可以提高论文被大会接收的几率？人工智能是否可以帮助我们？近日，来自卡耐基梅隆大学（CMU）研究者们收集了上万篇 AI 顶级会议的接收/被拒论文，并使用机器学习工具进行分析，获得了一些有趣的结果。最后，当然……这一研究的论文已经被即将在 6 月 1 日于美国举行的自然语言处理顶会 NAACL 2018 接收。

项目地址：[https://github.com/allenai/PeerRead](https://link.zhihu.com/?target=https%3A//github.com/allenai/PeerRead)

## **简介**

权威的科学会议利用同行评审来决定要将哪些论文列入其期刊或会议记录。虽然这一过程似乎对科学出版物至关重要，但其往往也饱受争议。意识到同行评审的重要影响，一些研究人员研究了这一过程中的各个方面，包括一致性、偏差、作者回应和一般评审质量 (Greaves 等，2006；Greaves 等，2011；De Silva and Vance, 2017)。例如，NIPS 2014 会议的组织者将 10% 的会议提交论文分配给两组不同的审查人员，以衡量同行评审过程的一致性，并观察到两个委员会对超过四分之一的论文接受/拒绝决定意见不一样 (Langford and Guzdial, 2015)。 

尽管已经有了这些努力，但是关于同行评审的定量研究还是有限的，很大程度上是由于只有很少的人能够接触到一个学术活动的同行评审（例如期刊编辑和程序主席）。本文的目的是通过首次引入一个用于研究目的的同行评审公共数据集: PeerRead，来降低科学界研究同行评审的障碍。 

在此论文中，我们使用三种策略来构建数据集: ( i ) 与会议主席、会议管理系统协作，允许作者和评审人分别选择其论文草稿和同行评审。( ii ) 抓取公开的同行评审，并用数字评分对文本评审进行标注，如「清晰度」和「影响」。(iii) 对与重要会议提交日期一致的 arXiv 提交论文进行编目，并在以后的会议记录中检查是否出现类似论文。总的来说，该数据集由 14700 篇论文草稿和相应的「接受/拒稿」决定组成，其中 3000 篇论文包含专家撰写的 10700 条文本评论。此外，我们将定期发布 PeerRead，为每年新的学术活动增加更多的内容。 

PeerRead 数据集可以以多种方式使用。对同行评审的定量分析可以提供见解，帮助更好地理解 (且可能改进) 评审过程的各种细微差别。例如，在该论文的第三部分中，我们分析了总体推荐分数与单个方面分数 (例如清晰度、影响和原创性) 之间的相关性，并量化了口头演示推荐评论与海报推荐评论有何不同。其他的样本可能包括匹配评论分数与作者，以揭示性别或国籍偏差。从教学角度来看，PeerRead 数据集还为经验不足的作者和首次审稿人提供了不同的同行评审实例。 
![](https://pic4.zhimg.com/v2-3678f126bbca084d0f7f71a896b4fd7b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='910' height='380'></svg>)
表 1： PeerRead 数据集。Asp. 代表评审是否具有特定方面的得分（例如清晰度）。注意，ICLR 包括由标注者给出的不同方面的得分（详见 2.4 部分）。Acc/Rej 一列表示接受/拒绝的论文比例。需要注意，NIPS 仅仅提供了被接受的论文的评审意见。 

作为一个自然语言处理资源，同行评审是个有趣的挑战，无论是从语义情感分析领域还是文本生成领域，前者前者预测被评审论文的不同属性，例如清晰度和原创性，后者则是在给定一篇论文的情况下，自动地生成它的评审意见。在被以足够高的质量解决时，这种类型的 NLP 任务也许会在评审过程中帮助审稿人、区域主席以及程序主席，例如，通过为某些论文投稿减少所需的审稿人数量。

在第四部分中，我们基于这个数据集引入了两个新的 NLP 任务：(i) 预测一篇论文是否会被某个学术会议接受，（ii）预测论文在某些方面的数字得分。我们的结果显示：在 PeerRead 的四个不同部分中，与大多数全部拒绝（reject-all) 基线相比，我们能够以误差减小 6--21%de 的结果预测「接受/拒稿」决策结果。由于我们使用的基线模型相当简单，因此有足够的空间来开发更强的模型以做出更好的预测。 

## **论文接受分类**

论文接受分类是一个二分类任务：给定一篇论文草稿，预测在一组预定的学术会议中它将会被接受还是拒稿。 

模型： 我们训练一个二值分类器来为一篇论文预测「接受/拒稿」的概率，也就是：P(accept=True | paper)。我们用不同类型的分类器做了实验：logistic 回归、使用线性核或者 RBF 核的 SVM、随机森林、最近邻、决策树、多层感知机、AdaBoost 以及朴素贝叶斯。我们使用了人工设计的特征，而不是神经网络模型，因为人工特征易于解释。 
![](https://pic2.zhimg.com/v2-529beaa0212257a73aeafa28fe74fd81_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='698' height='317'></svg>)
表 5： 接受分类的测试准确率。在所有的案例中，我们的最佳模型超越了大多数分类器。 

我们使用了 22 个粗略的特征，例如标题长度，专门的术语（例如「深度」和「神经」）是否出现在摘要中，以及稀疏和密集的词汇特征。 

实验设置：我们使用 PeerRead 数据集中的 ICLR 2017 和 arXiv 部分来做实验。我们为每一个 arXiv 类别训练了独立的模型：例如 cs.cl，cs.lg，以及 cs.ai。我们的所有模型都使用 python 的 sklearn 实现 (Pedregosa et al., 2011)。我们考虑了支持向量机和 logistic 回归的不同正则化参数（所有超参数的详细描述参见附录 A.1）。我们使用了标准的测试拆分，并且在训练集上使用了 5 重交叉验证。 

结果： 表 5 展示了我们在论文接受准确率上的测试准确率。在所有的例子中，我们的最佳模型都以大于 22% 的误差率优势超过大部分分类器。不过，由于我们的模型在评价给定论文所做工作的质量方面缺乏成熟性，所以这可能意味着我们定义的一些特征与某些强有力的论文，或者有偏差的审稿人的判断相关。 

我们对数据集中的 ICLR 和 arXiv 部分进行了控制变量研究。为了简化分析，我们为 arXiv 中的三种类别训练了一个模型。表 6 展示了当我们移除了其中的一个特征的时候，最佳模型的测试中准确率的绝对下降。该表显示，一些特征对分类决策有着很大的贡献：例如增加一个附录，大量的定理或公式，引文前的文本的平均长度，本文提交前五年内发表的论文数量，ICLR 的论文摘要中是否包含「最先进的技术（state of the art）」，或者 arXiv 的摘要中是否包含术语「神经（neural）」，以及标题的长度。 
![](https://pic4.zhimg.com/v2-0f880c22dc713a3583bdd63716b28923_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='655' height='300'></svg>)
表 6： 当我们从完整的模型中仅仅移除一个特征的时候，论文接收预测任务的绝对准确率的差别。 

图中具有较大负差别的特征更加显著，研究人员仅仅显示了每个部分最显著的 6 个特征。分别是：num_X：即 X 的数量（例如定理或者公式），avg_len_ref:引用前的文本平均长度，附录：文章是否包含附录，abstractX：摘要是否包含术语 X，num_uniq_words：唯一单词的数量，num_refmentions：提及的参考文献的数量，以及 #recent_refs：近五年内发表的参考文献的数量。 
![](https://pic1.zhimg.com/v2-ad005f9006a9b769cca2d498a723a268_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='266'></svg>)
图 1：PeerRead 测试集上预测任务的均方差（RMSE, 越小越好）：左侧-- ACL 2017，右侧：ICLR 2017。 

## **结论**

我们的实验表明，论文的某些属性与较高的接收率正相关，例如包含附录。我们希望其他研究人员能够发现新的我们还没有在这个数据集中的同行评审中探索到的机会。一个具体的例子就是，研究接受/拒绝的决定是否反映了对作者的人口偏见 (例如国籍) 将是有意义的。 

**论文：一个同行评审数据集（PeerRead）：集合、洞见以及自然语言处理应用**
![](https://pic3.zhimg.com/v2-6173e10c0287f2e6a3c98099e5e22f0e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1021' height='348'></svg>)
论文链接：[https://arxiv.org/abs/1804.09635](https://link.zhihu.com/?target=https%3A//github.com/allenai/PeerRead)

同行评审是科学文献出版过程中的重要组成部分。在本研究中，我们提出了第一个可用于研究目的的科学文献同行评审公共数据集 ( PeerRead v1 )，该数据集为研究这一重要的现象提供了机会。该数据集由 1 万 4 千 700 份论文草稿，以及包括 ACL、NIPS 和 ICLR 在内的顶级学术活动对应的接受/拒稿决定组成。数据集还包括专家为论文子集撰写的 1 万零 700 份文本同行评审。我们描述了数据收集过程，并提供了在同行评审中观察到的有趣现象。我们在此基础上提出了两个新颖的 NLP 任务，并给出了简单的基线模型。在第一个任务中，我们展示了简单的模型可以预测一篇论文是否被接受，与大多数基线模型相比，误差减少了 21 %。在第二个任务中，我们预测了评审方面的数值分数，结果表明，对于诸如「原创性」和「影响」的高方差方面，简单模型可以优于平均基线。


