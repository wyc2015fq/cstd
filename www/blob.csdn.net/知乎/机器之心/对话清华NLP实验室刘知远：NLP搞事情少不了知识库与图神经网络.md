# 对话清华NLP实验室刘知远：NLP搞事情少不了知识库与图神经网络 - 知乎
# 



> 在过去的 2018 年中，自然语言处理出现了很多令人激动的新想法与新工具。从概念观点到实战训练，它们为 NLP 注入了新鲜的活力。

在这一年中，清华大学副教授刘知远和他所在的清华自然语言处理实验室同样在这个领域做出了很多成果，他们关注如何结合深度神经网络与大型知识库，并期望构建更多鲁棒及可解释性的自然语言理解系统。在这一篇文章中，机器之心采访了刘知远教授，他向我们介绍了 NLP 在过去一年的重要发展历程，以及实验室的重要研究成果与方向。

## **从 18 年走进 19 年**

**机器之心：在过去一年，您认为人工智能或者机器学习领域方面有哪些比较重要的研究成果？**

**刘知远：**我认为去年最有里程碑意义的研究是 BERT，每个从事自然语言处理的研究者和开发者，甚至每个人工智能研究者都会比较关注。它的地位可能和 CV 中的 ResNet 相似，都是充分利用更多数据自动学习预训练模型，改进相关任务的性能

参考文章：[谷歌终于开源 BERT 代码：3 亿参数量，机器之心全面解读](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650751075%26idx%3D2%26sn%3D0a3ecd1af5f8549051760775e34db342%26chksm%3D871a841db06d0d0bcf3cc4e620bb384e050ba6e92224d338a8ddc1543add97a4a4e7919ebf15%26scene%3D21%23wechat_redirect)




**机器之心：Transformer 在 2018 年有了更广泛的应用，除了机器翻译以外，它在语言模型和问答系统也都有所应用。您认为这是不是体现了一种趋势？**

**刘知远：**我们可以把 Transformer 看做是以往神经网络的一个升级版，能够一定程度上解决以往 CNN 和 RNN 对长程依赖建模的问题。并且提出完全利用 Attention 机制来进行句子表示学习，结构也比较清晰，目前在机器翻译任务上取得了大幅提升。 

在文本表示方面，深度神经网络主要存在两种做法：一种是 RNN/GRU/LSTM 系列，将句子作为字符序列进行顺序编码处理；另一种如 CNN 就不再关注句子中词之间的序列关系，而是通过诸如 Convolution（卷积）的机制，从局部开始逐渐形成整个句子的表示。前一种做法更符合语言理解特点，而后一种做法则并行计算性能更好。

Transformer 现在有比较大的影响力，一个方面就是其每层都会利用 Attention（注意力）来捕捉全局的信息，能够提升长程依赖的学习能力，这是 CNN 所不具备的。同时 Transformer 能在 GPU 上得到非常好的加速，可以从更多训练数据学习更好的效果，这是 RNN 系列模型难以做到的。此外简单利用自注意力机制可以方便层次化建模，这也是之后深层 Transformer 以及 BERT 等相关模型效果显著的重要因素。

总的来说，Transformer 比较好地解决了 RNN 和 CNN 等传统神经网络模型的缺陷，但更侧重于模型的并行化，对于语言序列性质的考虑还比较简单。当然，我认为未来这两种做法还会有此消彼长的过程。可能再发展一两年，我们又会在 Transformer 中把句子的序列性质着重考虑进来，让性能进一步提高，这也是一个互相借鉴的过程。




**机器之心：您在 18 年都有哪些比较重要的研究成果，能介绍一下吗？**

**刘知远：**2018 年我们围绕知识与语言开展了很多工作。我们认识到不同类型知识对语言理解的重要意义，是纯数据驱动方法无法胜任的。所以，我们的总目标是构建知识指导的自然语言处理框架，近年来一直探索如何将世界知识、语言知识和行业知识用于自然语言理解，以及反过来如何利用深度学习技术从无结构文本中抽取各类知识。

2018 年，在世界知识方面，我们进一步探索了利用深度学习技术抽取实体关系的技术，推出了 OpenNRE 工具包，获得了国内外的广泛关注；我们也利用知识表示学习技术，将世界知识用于文本实体分类、信息检索的文本排序等任务，验证了世界知识对于文本理解的积极意义。在语言知识方面，我们进一步探索了如何将用义原标注的词汇知识库 HowNet 融入到深度学习语言模型中；我们也探索了跨语言进行义原知识标注的可行性，有望加速多语言义原标注的效率。在行业知识方面，我们探索了如何将自然语言处理技术与法律知识相结合，提高法律领域的智能化水平。

实际上，从无结构文本中抽取结构化知识形成大规模知识图谱后，这些知识反过来可以融入深度学习模型中，帮助我们更好地实现对自然语言的理解。我认为这是深度学习模型与知识图谱不断互相正反馈的过程：如果有越来越大、越来越精确的知识图谱，也会有越来越好、越来越鲁棒的自然语言理解模型，它们是共同发展的。

**机器之心：清华 NLP 实验室这边还会关注哪一些前沿研究成果或关注哪些方向？**

**刘知远：**我们实验室共有三位老师，孙茂松教授、刘洋副教授和我。我主要关注如何将知识图谱与文本理解相融合，以期更好地解决自然语言处理问题。这方面工作也得到了孙茂松教授的指导，孙老师作为首席科学家的 973 项目于 2018 年结题，我们提出的融合知识的语义表示学习框架和知识指导的自然语言处理框架就是 973 项目的重要成果。

刘洋老师则主要关注机器翻译这个自然语言处理的重要方向，最近一直探索如何更好地利用神经网络提升机器翻译性能，近年来发表了很多有影响力的工作，包括如何提升机器翻译模型的鲁棒性、可解释性、覆盖能力等等，曾经获得过自然语言处理顶级会议 ACL 的杰出论文奖。

孙茂松老师还对社会人文计算和艺术创作特别感兴趣，近年来围绕古诗自动创作开展研究，研制了大家熟知的九歌系统，在中央电视台机智过人等节目和场合出现过，深受大家喜爱。最近孙茂松老师还将研究扩展到了现代诗创作、音乐创作等。
- 九歌作诗系统：[https://jiuge.thunlp.cn//](https://link.zhihu.com/?target=https%3A//jiuge.thunlp.cn//)




**机器之心：在 2019 新的一年中，您认为整体上 NLP 会有哪些值得关注，或是有哪些可能会取得突破的地方？**

**刘知远：**我认为自然语言处理发展主要还是遵循两个大的方向。一个方向是如何更充分地从大规模数据中学习和挖掘有用信息。例如 BERT 这种预训练语言模型就是利用大规模无标注文本数据学习一般的语言知识，2013 年的 word2vec 也是类似工作的典范，无疑它们对整个自然语言处理领域都产生了非常大的影响。近年来机器翻译领域也在探索如何构建无监督机器翻译模型，也是更充分利用无监督数据的做法。这些都属于数据驱动的方法。

如何更充分挖掘数据的价值，无论是有标注数据、弱标注数据还是无标注数据，都是数据驱动模型的重要命题，仍然有很多开放问题等待解决。即使 2018 年有了 BERT，未来还会有更多的学习机制等待探索。 

另一个方向，也是我个人比较关注的，是如何更好地将结构化知识融入相关自然语言处理模型中，相当于把基于符号表示的各种先验知识和规则，引入到自然语言计算模型中。Google、DeepMind 等研究机构很重视这个方向，现在比较流行的图神经网络可以看做这方面的重要尝试。我们有望通过图神经网络将结构化知识融入深度学习，实现各种计算与推理任务。

我认为，未来这两个大方向都非常重要，它们会相互补充与促进：互联网上有海量的非结构化数据，需要 BERT 等深度学习模型发挥重要作用；我们也积累了海量结构化知识，如何利用这些知识也具有重要意义。

当然，很多热门学习模型在新的一年里还会有更多关注和进展，例如对抗学习、强化学习、图神经网络等；很多新颖的学习场景，如 zero/one/few shot learning 等，也开始有很多有益的探索。




**机器之心：最近清华也刚刚成立了知识智能研究中心，那么在新的一年中，知识中心主要也是关注如何结合知识与文本这方面的研究吗？**

**刘知远：**知识智能研究中心是清华大学人工智能研究院成立的首个研究中心。知识中心的使命是，在清华大学人工智能研究院的指导下，更好地整合清华校内与知识智能有关的研究力量，开展具有世界水平的基础理论研究，建设融通语言知识、常识知识、世界知识的大规模知识图谱及典型行业知识库，形成具有国际影响力的知识计算开放平台，同时积极开展高水平国际学术交流和产学研深度合作。我们希望通过知识中心，能够增进校内外以及国内外的学术交流，通过构建更大更好的知识图谱和计算平台，推动鲁棒可解释人工智能的实现。

在这次知识中心成立仪式上，我们发布了清华大学知识计算开放平台，内容涵盖语言知识（OpenHowNet）、世界知识（XLORE）和科技知识库（AMiner）。希望未来知识中心可以把知识计算开放平台做大做好，成为人工智能研究与应用的重要基础设施。

[清华人工智能研究院成立「知识智能研究中心」，发布四大知识平台](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650755893%26idx%3D2%26sn%3Dbdf0f888ae3bf86b2bb091497e958062%26chksm%3D871a974bb06d1e5d205f6c1fd9606600134beb7e0bed07cddc06f6688c448a6c9a8333560728%26scene%3D21%23wechat_redirect)

常识知识是鲁棒可解释人工智能必不可少的支撑，因此从 2019 年开始，我们计划利用若干年时间，逐步构建常识知识体系。常识知识具体以什么方式来构建，目前正在积极的讨论和规划中。

## **深度学习与 HowNet**

**机器之心：您认为深度学习与知识库结合的优势以及局限都在哪？**

**刘知远：**自然语言的最小使用单位是「词」。与图像的最小单位「像素」不同，语言中的「词」是典型的符号系统，背后关联着丰富的语义信息。因此，自然语言处理的难题之一是如何打破「词」这个屏障，更精确地捕捉词汇背后的语义信息。数据驱动的深度学习技术提供的解决方案是，将词的语义信息表示为词向量，通过语义组合表示句子语义，从而支持各种下游自然语言处理任务，并利用大规模数据提供的丰富上下文来学习词向量。这种方法最大的问题是可解释性差，我们只知道这个几百维的向量表示某个词，但为什么能表示这个词，这个词确切有什么含义，我们仍然不知道。

以 word2vec 为例，词嵌入向量能得到不同词之间的类比关系，例如「国王 - 男性= 王后 - 女性」。但是每次重新训练后，表示某个词的向量都是不一样的，它捕捉的只是词之间的关系。这也是为什么词嵌入向量只能描述词的分布而不能描述组成这个词的概念的原因。大多数深度学习模型都和词嵌入向量一样缺少可解释性，这也是深度学习被广泛诟病的地方。

然而对于自然语言处理而言，我们不仅希望模型能够理解或生成文本，更希望知道模型这样做的原因。对于人类而言，我们完成各种任务时同样会有一个「世界模型」，或者说一个「知识库」进行指导。借助「知识库」作为语境可以帮助我们对任务决策做出理性的解释。纯粹的数据驱动方法只能从大规模文本中学习与任务相关的某些「模式」，我们还需要另一种基于知识的方法才能灵活利用与理解这些模式。

所以，要想让计算机真正「理解」自然语言，还需要建立起文本背后的语义知识库。HowNet 正是这种词汇知识库，它秉承还原论思想，认为词汇语义可以用更小的语义单位来描述，这种语义单位被称为「义原」（Sememe），是最基本的、不易于再分割的意义的最小单位。HowNet 构建出了一套精细的义原体系（约 2000 个义原），并基于该义原体系累计标注了数十万词汇/词义的语义信息。
![](https://pic2.zhimg.com/v2-aa29c23a698462d7d3a8f836707d6dad_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='727'></svg>)HowNet 中由义原向上构建概念，由概念向上定义词。借助它们，词嵌入等模型能突破此层面的表征
2017 年以来，我们系统探索了 HowNet 在深度学习时代的应用价值，并在词汇语义表示、句子语义表示、词典扩展等任务上均得到了验证。研究发现，HowNet 通过统一的义原标注体系直接刻画语义信息，一方面能够突破词汇屏障，了解词汇背后丰富语义信息；另一方面每个义原含义明确固定，可被直接作为语义标签融入机器学习模型，使自然语言处理深度学习模型具有更好的鲁棒可解释性。

例如，我们将词汇义原知识融入到了神经语言模型 LSTM 中，能够有效提升了语言模型的可解释性。我觉得这是一个非常有意思的尝试，成功地把以义原为代表的语言知识与以数据为驱动的深度学习模型结合了起来。我们相信，只有充分融合知识与数据的优势，才能实现深层次的自然语言理解和计算。
- 论文：Language Modeling with Sparse Product of Sememe Experts 
- 论文地址：[http://aclweb.org/anthology/D18-1493](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/D18-1493)
![](https://pic2.zhimg.com/v2-222e80bd07c7a4fc72a1ad18dd85b775_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='971' height='693'></svg>)
如上所示为结合 HowNet 义原知识的语言模型架构，在给定循环神经网络的上下文向量后，模型会预测每个义原将在下个词中出现的概率，并借助上下文向量和义原信息进而预测词义（sense）的概率和词（word）的概率。

我们还认识到 HowNet 知识人工标注费时费力，我们还系统探索了如何利用深度学习模型对新词自动推荐义原，辅助知识的标注工作，这将为未来更快更好地扩充 HowNet 奠定技术基础。此外，我们还结合词向量学习技术与 HowNet 对领域词典进行自动扩展，这些都是充分利用 HowNet 知识和数据驱动深度学习技术的有益尝试。
- 论文：Lexical Sememe Prediction via Word Embeddings and Matrix Factorization
- 论文地址：[https://www.ijcai.org/proceedings/2017/0587.pdf](https://link.zhihu.com/?target=https%3A//www.ijcai.org/proceedings/2017/0587.pdf)



- 论文：Chinese LIWC Lexicon Expansion via Hierarchical Classification of Word Embeddings with Sememe Attention
- 论文地址：[https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16760](https://link.zhihu.com/?target=https%3A//aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16760)

所以总的而言，数据驱动的深度学习技术能够从海量文本中学习语义模式，却不可解释，因此还需要大规模知识库指导这些模式的原因和合理性。深度学习模型融合「世界模型」是非常有必要的。当然「世界模型」并不是一定就是 HowNet 这种形式。如果我们在计算过程中发现 HowNet 的缺点，自然也可以去改进它，或者寻找更好的语言知识表示方法。

## **图神经网络**

**机器之心：您可以介绍一下图神经网络和传统图模型，以及原来的深度学习之间的关系吗？**

**刘知远：**对于这个问题，我们需要看是从什么角度来考虑。从研究任务的角度来看，比如对于数据挖掘领域的社会网络分析问题，过去主要采用 graph embedding 技术，借鉴词嵌入的思想，为网络节点学习低维向量表示。

如果将图神经网络（GNN）跟 graph embedding 相比，最大的差别就是，像 DeepWalk、LINE 这些 graph embedding 算法，聚焦在如何对网络节点进行低维向量表示，相似的节点在空间中更加接近。相比之下，GNN 最大的优势是在于它不只可以对一个节点进行语义表示，具有更强大的建模本领。例如 GNN 可以表示子图的语义信息，对网络中一小部分节点构成的社区（community），GNN 可以把这个社区的语义表示出来，而这是以前的 graph embedding 不容易做到的。GNN 还可以在整个网络上进行信息传播、聚合等建模，也就是说它可以把网络当成一个整体进行建模。GNN 即使是对单个节点的表示也可以做得比过去方法更好，因为它可以更好地考虑这个节点周围的丰富信息。

由于我主要研究知识图谱，所以会更关注 GNN 与知识表示学习（knowledge embedding）之间的差别。TransE 等知识表示学习方法，一般是将知识图谱按照三元组来学习建模，学到每个实体和关系的低维向量表示，这样就可以去进行一些链接预测和实体预测任务。

这些知识表示学习方法同样没有办法整体地对知识图谱建模，因为它会将知识分割成三元组再进行建模。而 GNN 则有希望把整个知识图谱看成整体进行学习，并可以在知识图谱上开展注意力、卷积等操作，以学习图谱中实体与实体之间的复杂关系。即使实体间没有直接连接，GNN 也可以更好地对它们的关系建模。

过去，人们主要设计了随机游走、最短路径等图方法来利用知识图谱这种符号知识，但这些方法并没有办法很好地利用每个节点的语义信息。而传统的深度学习技术更擅长处理非结构文本、图像等数据。简言之，我们可以将 GNN 看做将深度学习技术应用到了符号表示的图数据上来，或者说是从非结构化数据扩展到了结构化数据。GNN 能够充分融合符号表示和低维向量表示，发挥两者优势。
- 清华大学图神经网络综述：模型与应用
- 论文：Graph Neural Networks: A Review of Methods and Applications
- 论文地址：[https://arxiv.org/abs/1812.08434](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.08434)



- DeepMind 等机构提出「图网络」：面向关系推理
- 论文：Relational inductive biases, deep learning, and graph networks
- 论文地址：[https://arxiv.org/abs/1806.01261](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1806.01261)




