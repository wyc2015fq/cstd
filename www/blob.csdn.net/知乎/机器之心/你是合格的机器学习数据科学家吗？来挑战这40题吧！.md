# 你是合格的机器学习数据科学家吗？来挑战这40题吧！ - 知乎
# 



> 
*目前机器学习是最抢手的技能之一。如果你是一名数据科学家，那就需要对机器学习很擅长，而不只是只有三脚猫的功夫。作为 DataFest 2017 的一部分，Analytics Vidhya 组织了不同的技能测试，从而数据科学家可以就这些关键技能进行自我评估。测试包括机器学习、深度学习、时序问题以及概率。这篇文章将给出机器学习测试问题的解答。你可以通过链接获得其他测试问题及解答。*


- 
深度学习：[40 Questions to test a data scientist on Deep Learning [Solution: SkillPower – Deep Learning, DataFest 2017]](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-deep-learning/)

- 
时序问题：[https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-time-series-solution-skillpower-time-series-datafest-2017/](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/04/40-questions-on-time-series-solution-skillpower-time-series-datafest-2017/)

- 
概率：[40 Questions on Probability for data science - [Solution: SkillPower - Probability, DataFest 2017]](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/)



在本文的机器学习测试中，超过 1350 人注册参与其中。该测试可以检验你对机器学习概念知识的掌握，并为你步入业界做准备。如果错过了实时测试，没有关系，你可以回顾本文以自我提升。机器之心对这些试题及解答进行了编译介绍。你能答对多少题呢？不妨与我们分享。


**整体分数分布**


下面给出了测试得分的分布，希望能帮助你了解一下自己的水平。成绩单也可以看这里：[Machine Learning : Leaderboard](https://link.zhihu.com/?target=https%3A//datahack.analyticsvidhya.com/contest/skillpower-machine-learning/lb)

![](https://pic3.zhimg.com/v2-9ca23b15bcdceacf610ac0fd93e0f14e_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='870' height='319'></svg>)
目前已有 210 人参与了这些试题的测试，最高分为 36。平均得分为 19.36，中位数为 21，最常出现的得分（Mode Score）为 27。


**测试题与解答**


假定特征 F1 可以取特定值：A、B、C、D、E 和 F，其代表着学生在大学所获得的评分。现在请答题：


**1. 在下面说法中哪一项是正确的？**


A. 特征 F1 是名义变量（nominal variable）的一个实例。

B. 特征 F1 是有序变量（ordinal variable）的一个实例。

C. 该特征并不属于以上的分类。

D. 以上说法都正确。


答案为（B）：有序变量是一种在类别上有某些顺序的变量。例如，等级 A 就要比等级 B 所代表的成绩好一些。


**2. 下面哪个选项中哪一项属于确定性算法？**


A.PCA

B.K-Means

C. 以上都不是


答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。


**3. 两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。**


A. 正确

B. 错误


答案为（A）：Y=X2，请注意他们不仅仅相关联，同时一个还是另一个的函数。尽管如此，他们的相关性系数还是为 0，因为这两个变量的关联是正交的，而相关性系数就是检测这种关联。详情查看：[Anscombe - Wikipedia](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Anscombe)'s_quartet


**4. 下面哪一项对梯度下降（GD）和随机梯度下降（SGD）的描述是正确的？**

- 
在 GD 和 SGD 中，每一次迭代中都是更新一组参数以最小化损失函数。

- 
在 SGD 中，每一次迭代都需要遍历训练集中的所有样本以更新一次参数。

- 
在 GD 中，每一次迭代需要使用整个训练集或子训练集的数据更新一个参数。



A. 只有 1

B. 只有 2

C. 只有 3

D.1 和 2

E.2 和 3

F. 都正确


答案为（A）：在随机梯度下降中，每一次迭代选择的批量是由数据集中的随机样本所组成，但在梯度下降，每一次迭代需要使用整个训练数据集。


**5. 下面哪个/些超参数的增加可能会造成随机森林数据过拟合？**

- 
树的数量

- 
树的深度

- 
学习速率



A. 只有 1

B. 只有 2

C. 只有 3

D.1 和 2

E.2 和 3

F. 都正确


答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率在随机森林中并不是超参数。增加树的数量可能会造成欠拟合。


**6. 假如你在「Analytics Vidhya」工作，并且想开发一个能预测文章评论次数的机器学习算法。你的分析的特征是基于如作者姓名、作者在 Analytics Vidhya 写过的总文章数量等等。那么在这样一个算法中，你会选择哪一个评价度量标准？**

- 
均方误差

- 
精确度

- 
F1 分数



A. 只有 1

B. 只有 2

C. 只有 3

D. 1 和 3

E. 2 和 3

F. 1 和 2


答案为（A）：你可以把文章评论数看作连续型的目标变量，因此该问题可以划分到回归问题。因此均方误差就可以作为损失函数的度量标准。


**7. 给定以下三个图表（从上往下依次为1，2，3）. 哪一个选项对以这三个图表的描述是正确的？**

![](https://pic3.zhimg.com/v2-9abdc2e559f9596f6f1d554a33284456_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='494' height='302'></svg>)![](https://pic4.zhimg.com/v2-67d61f734e1d84ad8179b04a0764fcc3_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='563' height='329'></svg>)![](https://pic2.zhimg.com/v2-2eaa0602dfc2712634c64a71f052340d_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='391' height='300'></svg>)

A. 1 是 tanh，2 是 ReLU，3 是 SIGMOID 激活函数

B. 1 是 SIGMOID，2 是 ReLU，3 是 tanh 激活函数

C. 1 是 ReLU，2 是 tanh，3 是 SIGMOID 激活函数

D. 1 是 tanh，2 是 SIGMOID，3 是 ReLU 激活函数


答案为（D）：因为 SIGMOID 函数的取值范围是 [0,1]，tanh 函数的取值范围是 [-1,1]，RELU 函数的取值范围是 [0,infinity]。


**8. 以下是目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是所少？**


A. -(5/8 log(5/8) + 3/8 log(3/8))

B. 5/8 log(5/8) + 3/8 log(3/8)

C. 3/8 log(5/8) + 5/8 log(3/8)

D. 5/8 log(3/8) – 3/8 log(5/8)


答案为（A）：信息熵的公式为：![](https://pic2.zhimg.com/v2-37950da5324eae87a4c16b8e281aeb45_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='172' height='44'></svg>)


**9. 假定你正在处理类属特征，并且没有查看分类变量在测试集中的分布。现在你想将 one hot encoding（OHE）应用到类属特征中。那么在训练集中将 OHE 应用到分类变量可能要面临的困难是什么？**


A. 分类变量所有的类别没有全部出现在测试集中

B. 类别的频率分布在训练集和测试集是不同的

C. 训练集和测试集通常会有一样的分布

D. A 和 B 都正确

E. 以上都不正确


答案为（D）：A、B 项都正确，如果类别在测试集中出现，但没有在训练集中出现，OHE 将会不能进行编码类别，这将是应用 OHE 的主要困难。选项 B 同样也是正确的，在应用 OHE 时，如果训练集和测试集的频率分布不相同，我们需要多加小心。


**10.Skip gram 模型是在 Word2vec 算法中为词嵌入而设计的最优模型。以下哪一项描绘了 Skip gram 模型？**

![](https://pic4.zhimg.com/v2-2a868208a7ae9034acabf665fcc3744b_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='562' height='364'></svg>)

A. A

B. B

C. A 和 B

D. 以上都不是


答案为（B）：这两个模型都是在 Word2vec 算法中所使用的。模型 A 代表着 CBOW，模型 B 代表着 Skip gram。


**11. 假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？**


A. ReLU

B. tanh

C. SIGMOID

D. 以上都不是


答案为（B）：该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。


**12. 对数损失度量函数可以取负值。**


A. 对

B. 错


答案为（B）：对数损失函数不可能取负值。


**13. 下面哪个/些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？**

- 
类型 1 通常称之为假正类，类型 2 通常称之为假负类。

- 
类型 2 通常称之为假正类，类型 1 通常称之为假负类。

- 
类型 1 错误通常在其是正确的情况下拒绝假设而出现。



A. 只有 1

B. 只有 2

C. 只有 3

D. 1 和 2

E. 1 和 3

F. 3 和 2


答案为（E）：在统计学假设测试中，I 类错误即错误地拒绝了正确的假设（即假正类错误），II 类错误通常指错误地接受了错误的假设（即假负类错误）。


**14. 下面在 NLP 项目中哪些是文本预处理的重要步骤？**

- 
词干提取（Stemming）

- 
移去停止词（Stop word removal）

- 
目标标准化（Object Standardization）



A. 1 和 2

B. 1 和 3

C. 2 和 3

D. 1、2 和 3


答案为（D）：词干提取是剥离后缀（「ing」，「ly」，「es」，「s」等）的基于规则的过程。停止词是与语境不相关的词（is/am/are）。目标标准化也是一种文本预处理的优良方法。


**15. 假定你想将高维数据映射到低维数据中，那么最出名的降维算法是 PAC 和 t-SNE。现在你将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？**


A. X_projected_PCA 在最近邻空间能得到解释

B. X_projected_tSNE 在最近邻空间能得到解释

C. 两个都在最近邻空间能得到解释

D. 两个都不能在最近邻空间得到解释


答案为（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。


**16-17 题的背景：给定下面两个特征的三个散点图（从左到右依次为图 1、2、3）。**

![](https://pic3.zhimg.com/v2-66d880ffbcab44ccadcc9070d69615ee_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='534' height='190'></svg>)
**16. 在上面的图像中，哪一个是多元共线（multi-collinear）特征？**


A. 图 1 中的特征

B. 图 2 中的特征

C. 图 3 中的特征

D. 图 1、2 中的特征

E. 图 2、3 中的特征

F. 图 1、3 中的特征


答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。


**17. 在先前问题中，假定你已经鉴别了多元共线特征。那么下一步你可能的操作是什么？**

- 
移除两个共线变量

- 
不移除两个变量，而是移除一个

- 
移除相关变量可能会导致信息损失。为了保留这些变量，我们可以使用带罚项的回归模型（如 ridge 或 lasso regression）。



A. 只有 1

B. 只有 2

C. 只有 3

D. 1 或 3

E. 1 或 2


答案为（E）：因为移除两个变量会损失一切信息，所以我们只能移除一个特征，或者也可以使用正则化算法（如 L1 和 L2）。


**18. 给线性回归模型添加一个不重要的特征可能会造成：**

- 
增加 R-square

- 
减少 R-square



A. 只有 1 是对的

B. 只有 2 是对的

C. 1 或 2 是对的

D. 都不对


答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。


**19. 假设给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？**


A. D1= C1, D2 < C2, D3 > C3

B. D1 = C1, D2 > C2, D3 > C3

C. D1 = C1, D2 > C2, D3 < C3

D. D1 = C1, D2 < C2, D3 < C3

E. D1 = C1, D2 = C2, D3 = C3

F. 无法确定


答案为（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。


**20. 假定你现在解决一个有着非常不平衡类别的分类问题，即主要类别占据了训练数据的 99%。现在你的模型在测试集上表现为 99% 的准确度。那么下面哪一项表述是正确的？**

- 
准确度并不适合于衡量不平衡类别问题

- 
准确度适合于衡量不平衡类别问题

- 
精确率和召回率适合于衡量不平衡类别问题

- 
精确率和召回率不适合于衡量不平衡类别问题



A. 1 and 3

B. 1 and 4

C. 2 and 3

D. 2 and 4


答案为（A）：参考问题 4 的解答。


**21. 在集成学习中，模型集成了弱学习者的预测，所以这些模型的集成将比使用单个模型预测效果更好。下面哪个/些选项对集成学习模型中的弱学习者描述正确？**

- 
他们经常不会过拟合

- 
他们通常带有高偏差，所以其并不能解决复杂学习问题

- 
他们通常会过拟合



A. 1 和 2

B. 1 和 3

C. 2 和 3

D. 只有 1

E. 只有 2

F. 以上都不对


答案为（A）：弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。


**22. 下面哪个/些选项对 K 折交叉验证的描述是正确的**

- 
增大 K 将导致交叉验证结果时需要更多的时间

- 
更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心

- 
如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量



A. 1 和 2

B. 2 和 3

C. 1 和 3

D. 1、2 和 3


答案为（D)：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。


23 题至 24 题的背景：交叉验证在机器学习超参数微调中是十分重要的步骤。假定你需要为 GBM 通过选择 10 个不同的深度值（该值大于 2）而调整超参数「max_depth」，该树型模型使用 5 折交叉验证。 4 折训练验证算法（模型 max_depth 为 2）的时间为 10 秒，在剩下的 1 折中预测为 2 秒。


**23. 哪一项描述拥有 10 个「max_depth」不同值的 5 折交叉验证整体执行时间是正确的？**


A. 少于 100 秒

B. 100-300 秒

C. 300-600 秒

D. 大于等于 600 秒

E. 无法估计


答案为（D）：因为深度为 2 的 5 折交叉验证每一次迭代需要训练 10 秒和测试 2 秒。因此五折验证需要 12*5 = 60 秒，又因为我们需要搜索 10 个深度值，所以算法需要 60*10 = 600。


**24. 在先前的答案中，如果你训练同样的算法调整 2 个超参数，假设是 max_depth 和 learning_rate。你想要选择正确的值对抗 max_depth（从给定的 10 个深度值中）和学习率（从 5 个不同的学习率做选择）。在此情况下，整体时间是下面的哪个？**


A.1000-1500 秒

B.1500-3000 秒

C. 多于或等于 3000 Second

D. 都不是


答案为（D）：和 23 题一样。


**25. 下表是机器学习算法 M1 的训练错误率 TE 和验证错误率 VE，基于 TE 和 VE 你想要选择一个超参数（H）。![](https://pic1.zhimg.com/v2-669c29b43100664c21c61e081fbf4534_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='831' height='259'></svg>)**

**基于上表，你会选择哪个 H 值？**


A.1

B.2

C.3

D.4

E.5


答案为（D）：看这个表，D 选项看起来是最好的。


**26. 为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？**


A. 将数据转换成零均值

B. 将数据转换成零中位数

C. 无法做到

D. 以上方法不行


答案为（A）：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。


问题 27-28 的背景：假设存在一个黑箱算法，其输入为有多个观察（t1, t2, t3,…….. tn）的训练数据和一个新的观察（q1）。该黑箱算法输出 q1 的最近邻 ti 及其对应的类别标签 ci。你可以将这个黑箱算法看作是一个 1-NN（1-最近邻）


**27. 能够仅基于该黑箱算法而构建一个 k-NN 分类算法？注：相对于 k 而言，n（训练观察的数量）非常大。**


A. 可以

B. 不行


答案为（A）：在第一步，你在这个黑箱算法中传递一个观察样本 q1，使该算法返回一个最近邻的观察样本及其类别，在第二步，你在训练数据中找出最近观察样本，然后再一次输入这个观察样本（q1）。该黑箱算法将再一次返回一个最近邻的观察样本及其类别。你需要将这个流程重复 k 次。


**28. 我们不使用 1-NN 黑箱，而是使用 j-NN(j>1) 算法作为黑箱。为了使用 j-NN 寻找 k-NN，下面哪个选项是正确的？**


A. j 必须是 k 的一个合适的因子

B. j>k

C. 不能办到


答案为（C）：原因和 27 题一样


**29. 有以下 7 副散点图（从左到右分别编号为 1-7），你需要比较每个散点图的变量之间的皮尔逊相关系数。下面正确的比较顺序是？**

![](https://pic4.zhimg.com/v2-58c60e7f36b47a77edaa2c9bb82faf27_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='664' height='92'></svg>)
- 
1<2<3<4


- 
1>2>3 > 4

- 
7<6<5<4

- 
7>6>5>4



A. 1 和 3

B. 2 和 3

C. 1 和 4

D. 2 和 4

答案为（B）：从图1到图4，相关性系数（绝对值）是递减的。但图4到7相关性系数绝对值是递增的（值为负数，如0、-0.3、-0.7、-0.99等）


**30. 你可以使用不同的标准评估二元分类问题的表现，例如准确率、log-loss、F-Score。让我们假设你使用 log-loss 函数作为评估标准。下面这些选项，哪个／些是对作为评估标准的 log-loss 的正确解释。**

- 
如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它。

- 
对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大。

- 
3.log-loss 越低，模型越好。



A.1 和 3

B.2 和 3

C.1 和 2

D.1、2、3


答案为（D）：答案无需解释。


问题 31-32 背景：下面是数据集给出的 5 个样本。

![](https://pic3.zhimg.com/v2-1c754baf3542a8a9928814856a64a41e_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='188' height='155'></svg>)

注意：图像中点之间的视觉距离代表实际距离。


**31. 下面哪个是 3-NN（3-最近邻）的留一法交叉验证准确率？**


A.0

B.0.4

C.0.8

D.1


答案为（C)：留一法交叉验证，我们将选择（n-1) 观察值作为训练，以及验证的 1 观察值。把每个点作为交叉验证点，然后找到 3 个最近邻点。所以，如果你在每个点上重复该步骤，你会为上图中给出的所有正类找到正确的分类，而错误分类负类。因此，得到 80% 的准确率。


**32. 下面哪个 K 值将会有最低的差一法（leave-one-out）交叉验证精确度？**


A. 1NN

B. 3NN

C. 4NN

D. 以上所有具有相同的差一法错误


答案（A）：在 1-NN 中，被错误分类的每一个点都意味着你将得到 0% 的精确度。


**33. 假设你被给到以下数据，你想要在给定的两个类别中使用 logistic 回归模型对它进行分类。你正在使用带有 L1 正则化的 logistic 回归，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？**


A. 第一个 w2 成了 0，接着 w1 也成了 0

B. 第一个 w1 成了 0，接着 w2 也成了 0

C. w1 和 w2 同时成了 0

D. 即使在 C 成为大值之后，w1 和 w2 都不能成 0


答案（B）：通过观察图像我们发现，即使只使用 x2，我们也能高效执行分类。因此一开始 w1 将成 0；当正则化参数不断增加时，w2 也会越来越接近 0。


**34. 假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。**

**注意：所有其他超参数是相同的，所有其他因子不受影响。**

- 
深度为 4 时将有高偏差和低方差

- 
深度为 4 时将有低偏差和低方差



A. 只有 1

B. 只有 2

C. 1 和 2

D. 没有一个


答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。


**35. 在 k-均值算法中，以下哪个选项可用于获得全局最小？**

- 
尝试为不同的质心（centroid）初始化运行算法

- 
调整迭代的次数

- 
找到集群的最佳数量



A. 2 和 3

B. 1 和 3

C. 1 和 2

D. 以上所有


答案（D）：所有都可以用来调试以找到全局最小。


**36. 假设你正在做一个项目，它是一个二元分类问题。你在数据集上训练一个模型，并在验证数据集上得到混淆矩阵。基于上述混淆矩阵，下面哪个选项会给你正确的预测。**

- 
精确度是~0.91

- 
错误分类率是~0.91

- 
假正率（False correct classification）是~0.95

- 
真正率（True positive rate）是~0.95



A. 1 和 3

B. 2 和 4

C. 1 和 4

D. 2 和 3


答案（C）：精确度（正确分类）是 (50+100)/165，约等于 0.91。真正率是你正确预测正分类的次数，因此真正率将是 100/105 = 0.95，也被称作敏感度或召回。


**37. 对于下面的超参数来说，更高的值对于决策树算法更好吗？**

- 
用于拆分的样本量

- 
树深

- 
树叶样本



A. 1 和 2

B. 2 和 3

C. 1 和 3

D. 1、2 和 3

E. 无法分辨


答案（E）：对于选项 A、B、C 来说，如果你增加参数的值，性能并不一定会提升。例如，如果我们有一个非常高的树深值，结果树可能会过拟合数据，并且也不会泛化。另一方面，如果我们有一个非常低的值，结果树也许与数据欠拟合。因此我们不能确定更高的值对于决策树算法就更好。


38-39 题背景 ：想象一下，你有一个 28x28 的图片，并使用输入深度为 3 和输出深度为 8 在上面运行一个 3x3 的卷积神经网络。注意，步幅是 1，你正在使用相同的填充（padding）。


**38. 当使用给定的参数时，输出特征图的尺寸是多少？**


A. 28 宽、28 高、8 深

B. 13 宽、13 高、8 深

C. 28 宽、13 高、8 深

D. 13 宽、28 高、8 深


答案（A）：计算输出尺寸的公式是：输出尺寸=(N – F)/S + 1。其中，N 是输入尺寸，F 是过滤器尺寸，S 是步幅。阅读这篇文章（链接：[Deep Learning for Computer Vision - Introduction to Convolution Neural Networks](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/)）获得更多了解。


**39. 当使用以下参数时，输出特征图的尺寸是多少？**


A. 28 宽、28 高、8 深

B. 13 宽、13 高、8 深

C. 28 宽、13 高、8 深

D. 13 宽、28 高、8 深


答案 (B)：同上


**40. 假设，我们正在 SVM 算法中为 C（惩罚参数）的不同值进行视觉化绘图。由于某些原因，我们忘记了使用视觉化标注 C 值。这个时候，下面的哪个选项在 rbf 内核的情况下最好地解释了下图（1、2、3 从左到右，图 1 的 C 值 是 C 1，图 2 的 C 值 是 C 2，图 3 的 C 值 是 C 3）中的 C 值。**


A. C1 = C2 = C3

B. C1 > C2 > C3

C. C1 < C2 < C3

D. 没有一个


答案 (C)：错误项的惩罚参数 C。它也控制平滑决策边界和训练点正确分类之间的权衡。对于 C 的大值，优化会选择一个较小边距的超平面。更多内容：[Understanding Support Vector Machine algorithm from examples (along with code)](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/)

选自[analyticsvidhya](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-machine-learning-solution-skillpower-machine-learning-datafest-2017/)**机器之心编译**


