# 前所未见：社会科学中的扯淡这么多，DARPA要用AI打假 - 知乎
# 



> 撰文 | 微胖
社会科学实验可复制率低的致命问题，不仅威胁着其是否有资格成为一门科学，同时也困扰着顶层决策者，因为他们需要这些理论来指导事关国家安全、公民利益的政策制定。预测市场的成功表明，科学家似乎非常善于预测哪些实验可能重复成功，而哪些实验可能失败。既然人类表现不俗，那么，DARPA 希望算法学会人类科学家的判断，甚至比他们预测的还准。他们希望这套系统自动给输入的理论打分，得分高低直接反应出该理论主张是否有潜力通过可重复实验，让大家将有限的时间和金钱成本放在真正科学研究上。

## **科幻还是科学？**

2012 年，汤姆·克鲁斯和凯蒂·赫尔姆斯宣布离婚。小报记者们激动表示，自己早就料到了这一天，因为偷拍照片中，两个人牵手姿势太别扭；

在一款爆红 TED 演讲中，哈佛美女教授 Amy Cuddy 告诉我们，可以从每个人的肢体语言中，观察出他是怎样一个人，而且心理强势的身体动作 (power pose) 可以助你成功，社交网站甚至一度掀起了」Fake it until you make it」的鸡汤潮；

你可能也听说过，如果一个人的嘴巴摆出微笑或因嘟嘴而皱眉，他的心情会随之改变，哪怕这个动作不是故意的；

一杯柠檬汁，加上真正的糖，可以帮助你恢复自制力；

......

心理学中最绝妙的发现，通常都是较为简化的结果，甚至还会为了迎合大众的口味进行重新包装，如今，无论是在媒体还是政策制定面前，这些结果的宣传都显得非常强劲。

然而, 可能很多人不知道，上述哈佛美女教授爆款 TED 演讲中的科学实验不仅没重复出来，原论文的另一个作者还直接在网上写了一封公开信，声称观点已变，现在认为，所谓的 power pose 效应不存在了。

一个不经意的微表情就能让心情随之改变的研究，出自德国心理学家弗里茨·斯特拉克（Fritz Strack），这个被视为教科书般的正统理论的重复性实验也以失败告终。

该理论所代表、二十世纪最让人着迷的心理学发现——启动效应，也正处在风雨飘摇之中。

尽管尼尔·卡尼曼曾在知名畅销书《思考：快与慢》中对其大加支持，但是，当研究人员一个接一个地试图重复那些众所周知效应背后的经典实验（包括 John Bargh 的开创性研究）时，结果都失败了。

坏消息并未就此止步。

另一个奠基性研究——自我损耗理论认为，我们的意志力是有限的，可以耗尽。然而，在接受实验可复制性的检验时，也惨遭滑铁卢。

2016 年，一个名为「开放科学合作」（The Open Science Collaboration，简称 OSC）的科学家团体在《科学》上发表了一个结果，引起心理学界乃至整个社会科学界不小的震动：

**重复 100 项刊登在心理学顶级期刊的研究，结果只有 36% 的实验结果得到重现。**

众所周知，研究具有可重复性，对了解科学发现的可信程度至关重要，然而，即便是心理学所属的整个社会科学领域（根据维基百科），结果也不妙。

2018 年，一群研究人员针对 2010 年—2015 年发表在最顶尖学术期刊《自然》和《科学》上的 21 项实验社会科学研究的重复结果，刊登在了《自然·人类行为》：

**它们只能重现 21 项研究中的 13 项结果，换算成百分比就是 62％。**

## **DARPA 的担忧**

目前心理分析和社会科学理论已广泛运用于美国的公共政策制定过程之中，并且产生了较为广泛的影响。

比如，国防部就经常利用社会和行为科学研究来制定计划，指导投资，评估结果，建立人类社会系统和行为模型的基础。这些科学研究都与国家安全所面临挑战有关。他们认为，社会科学研究可以帮助国防部深入了解国家安全方面的问题。

比如叛乱如何形成？人道主义救援如何分配？如何阻止敌人行动？

2008 年，国防部就曾推出「密涅瓦」计划，试图 弥补 军方在 某 些 政 策 相 关领 域 基 础 性科 研 的不足，合作 涵 盖 的学 科 领域非 常广 泛，包 括心理学。

而且，除了国家安全，其他公共政策制定领域，比如社会保障、环保、司法、公共健康等，也能看到心理学和社会科学的独特贡献。

既然社会科学理论在关键的可信度指标上存在致命问题，那么，有没有办法让大家直接看出那些通不过可重复性测试的理论呢？

比如，设计一套可以自动给输入理论打分的系统，得分高低直接反应出该理论主张是否有潜力通过可重复实验。

「它（这套系统）可能提供更好地做事的方法，」DARPA 的 SCORE（Systematizing Confidence in Open Research and Evidence）项目负责人 Adam Russell 说，帮助大家将有限的时间和金钱成本放在值得关注的研究上。

## **对症下药**

虽然知道水体被污染了，目前却没有很好的处理污水办法。

被宣传得最广、用来提高科学研究可重复性的方法，就是**预注册（pre-registration）**。科研人员在试验开始之前就向第三方说明他们的科学假设，以及数据分析计划等科研方案，以防日后进行 P 值操控。

还有一个事后诸葛的办法，就是做重复性研究。重复相同的实验，或者扩大实验看看效应是否泛化。近几年来，这类研究已经得到了基金资助机构的资助。

2017 年，非盈利组织开放科学中心（Center for Open Science，COS）搜集了一个包括三万个社会科学理论主张（claims）的数据集，试图检查这些主张的可复制性。其中，有三千个理论会被人工地进行复制，或交给预测市场处理。

剩余的交给算法。他们希望其他团队能设计各种自动化的算法，自动评估这些社会科学的研究，而人工进行的复制性工作会成为算法的 benchmark。

DARPA 给这一环节赞助了 760 万美元。官方希望他们**构建出一个自动化的评分系统，给输入的社会科学研究打出置信度的分数，供人类决策者参考。**

## **预测市场**

这是前所未有的尝试。算法有可能做到吗？

预测市场的成功开了一个好头。

在上述针对 21 个已发表在《自然》和《科学》上的社会科学实验进行复制实验之前，研究人员还设立了一个「预测市场」：

他们招募了 206 名志愿者（大多数是心理学家和经济学家），根据研究看起来是否可重复，将之视为「股票」进行出售或购买，正确投注最终被淘汰的研究，可以赚取更多。

开始时，每项研究每股 0.50 美元，价格飙升并下跌取决于交易者的活动。两周后，最终价格反映了交易者对每项研究成功复制的可能性的集体看法：

例如，0.87 美元的股票价格意味着一项研究有 87％的复制成功机会。

最终，交易者认为，研究复制成功率为 63％，而这一数字与人工复制的结果（实际 62％）的成功率非常接近。

看来，**科学家们似乎非常善于预测心理学和其他社会科学中的哪些研究能成功复制。**

那么，这些预测市场的「投资人」考量了那些变量呢？

一些人考虑了研究的**样本量**，小规模研究比大规模研究更有可能产生假阳性结果；

一些人研究了一种 **P 值的常用统计指标**：

如果结果的 P 值小于 0.05，称其为有统计学意义或阳性；如果一项研究包含大量的 P 值，刚好低于这个阈值，这可能是作者进行 P 值操控的一个迹象；

有趣的是，除了统计数据方面的问题外，一些不被认为可复制成功的研究还有另一个共同点: 

**新闻价值。**也就是说，这些研究报告了有趣、引人注目、至少符合社会某些部分口味的偏见。

不过，Adam Russell 更希望，他们的资助能找到表现超越投注者的程序。

## **算法会更聪明吗？**

在 DARPA 看来，算法工具有可能学会预测再现性，而且跨学科数据库的庞大规模可能会揭示出各种各样的新变量。

这在以前是不可想象的。**「我们想要接收到大量超出人类带宽的微弱信号，并将它们结合起来，帮助我们做出更好的决定。」**Russell 说。

而且，吃螃蟹的人已经出现。

今年年初，加州理工等顶尖大学研究人员发表了一篇研究，测量了用黑盒统计模型（机器学习技术）预测实验结果的准确性。

他们用来自实验心理学和经济学中，四个大规模的复制项目的数据，训练了一个预测模型，研究哪些变量可以帮助预测实验的可重复性。

结果，发现一些基础性特征能够帮助预测实验可重性的成功与否: 

比如，**原初论文的样本和效应的大小**，被报告的效应是单变量的主效应，还是双变量的交互效应。

而且，他们提出的模型还可以产生廉价、可预测的可复制性指标，有助于使评价新发现的过程制度化，并指导资源进行可能最有益的直接复制。

不过，由于 SCORE 项目要求之一是算法可解释性 (而不是神秘的黑盒子)，这也意味着更大的挑战。
![](https://pic3.zhimg.com/v2-193584c0dd32013f8c819c0fac3ca39e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)
**参考来源：**

[https://www.darpa.mil/program/systematizing-confidence-in-open-research-and-evidence](https://link.zhihu.com/?target=https%3A//www.darpa.mil/program/systematizing-confidence-in-open-research-and-evidence)

[https://osf.io/preprints/bitss/zamry/](https://link.zhihu.com/?target=https%3A//osf.io/preprints/bitss/zamry/)

[https://www.theatlantic.com/science/archive/2018/08/scientists-can-collectively-sense-which-psychology-studies-are-weak/568630/](https://link.zhihu.com/?target=https%3A//www.theatlantic.com/science/archive/2018/08/scientists-can-collectively-sense-which-psychology-studies-are-weak/568630/)


