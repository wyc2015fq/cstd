# 密集对象网络：通过机器人操作学习密集的视觉对象描述符 - 知乎
# 



选自arXiv，作者：Peter R. Florence、Lucas Manuelli、Russ Tedrake，机器之心编译。

> 机器人操作中，针对特定任务的强化学习方法可以在给定的任务中获得很好的技能，但是人们还没有找到高效完成多种不同任务的最佳途径。本文作者提出了密集对象网络——被训练用来提供密集对象描述的深度神经网络。密集对象网络可以区分多个对象，以完全机器人自监督的方式快速学习，并实现新的操作任务。

源代码地址：[https://github.com/ RobotLocomotion/pytorch-dense-correspondence](https://link.zhihu.com/?target=https%3A//github.com/)




**1 引言**

机器人操作中正确的对象表征方式是什么？尽管针对特定任务的强化学习方法可以在给定的任务中获得令人印象深刻的灵巧技能 [1]，人们还没有找到高效完成多种不同任务的最佳途径。一些其它的最新研究 [2，3] 可以提供普通的抓取物体的功能，但是没有进行特异性处理。为了实现特异性，即使用特定对象完成特定任务的能力，人们可能需要解决数据关联问题。粗略地说，识别和操作单个对象的任务可以通过语义分割来解决，正如亚马逊机器人挑战赛（ARC）[4，5] 或论文 [6] 所展示的那样。然而，语义分割提供的表征并不能提供关于对象本身丰富结构的任何信息。因此，这可能并非解决 ARC 中「取物-放置」问题之外的复杂问题的恰当表征方法。

除了视觉分割，近期的研究 [7] 标志着从原始 RGBD 数据中使用自监督方法学习密集像素级数据关联的一个进步。我们的工作受到了 [7] 的启发，但是我们需要额外的新方法去可靠地学习一致的对象描述符，并且开发一种适合机器人自动化的学习方法。此外，之前没有人研究密集描述符的对象唯一性，对不止一类对象演示密集描述学习，或者使用密集描述符对操作任务进行研究。在本文中，我们提出了密集对象网络，它是被训练用来提供密集（像素级）对象描述的深度神经网络。密集对象网络可以区分多个对象，以完全机器人自监督的方式快速学习，并实现新的操作任务。

本文贡献：我们认为，本文最大的贡献在于，我们引入了针对对象的密集描述符的机器人学习，并且展示了其在机器人操作中的普适性和实用性。本文的一个主要贡献是新开发了针对多对象的不同密集描述符，为此我们引入了三种获取描述符的方法：跨对象损失、直接多对象训练、合成多对象训练。通过修改损失函数和采样过程，我们可以得到在各类对象之间泛化的描述符，或者对每个对象实例都有不同表示的描述符。此外，我们已经证明，机器人自监督密集视觉描述符学习可以应用于各种潜在的非刚性对象和类（目前包含 3 个不同类别的 47 个对象），并且可以快速学习（大约 20 分钟）。我们还贡献了学习密集描述符的一般训练技术（见 3.2 节），这对于在实践中获得良好表现至关重要。最后，我们演示了学习到的密集描述符在机器人操作中的新应用。在示例任务中，我们抓取对象在可能变形的配置上的特征点，在杂乱环境中利用对象实例特异性做到这一点，或者在类中的对象间迁移特定的抓取动作。
![](https://pic3.zhimg.com/v2-a265f793b9f61e4d8d8cc5c8c7fa56ca_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='280'></svg>)
图 1：数据收集和训练过程概览。（a）通过机器臂自动收集数据（b）使用密集 3D 重构技术实现变化检测（c）-（f）绿色表示匹配，红色表示不匹配。




**5 实验结果**
![](https://pic3.zhimg.com/v2-e215a18631ffb2724df9ddcb1f8ff136_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='506'></svg>)
图 2：学习到的对象描述符在经过显著变形（a）后保持一致，如果需要，还可以在（b-d）对象类别间保持一致。图中（a）和（b-d）顶部显示出来的是 RGB 帧，相应的在底部显示出来的是通过训练网络前馈传播直接输出的描述符图像。（e）-（f）表明，我们可以学习低纹理对象的描述符，并对描述符进行掩膜操作以得到清晰的可视化结果。在图的右侧我们对对象集合进行了总结。




**5.1 单个对象的密集描述符**
![](https://pic4.zhimg.com/v2-08f611a27f397fc6f55e7aff1fec347b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='291'></svg>)
图 3:（a）表显示了实验中参考的不同类型网络。列标签与第 3 节中讨论过的技术相对应。（b）图描绘了最佳匹配 u_b hat 和实际匹配 u_b*之间 L2 像素距离（利用图像的对角线进行归一化，640*480 的图像取 800）的累积分布函数，例如：对于 93% 的使用「standard-SO」训练过程的图像对而言，u_b* 和 u_b hat 之间归一化后的像素距离小于 13%。所有网络都使用（a）中标记的训练过程在同一数据集上训练。（c）图描绘了在 
![](https://pic2.zhimg.com/v2-6945a0fe34b6da3297c86ce19eadb3b1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='548' height='42'></svg>)
的情况下（即它们在描述符空间中与 u_a* 的距离比实际匹配的 u_b*的距离更近）对象像素点 u_b 部分像素的累积分布函数。
![](https://pic3.zhimg.com/v2-bc56d7c371d126e791462dcef13b9082_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='305'></svg>)
图 4：没有任何明显对象损失（a）和使用跨对象损失（b）的训练结果对比。在（b）中，50% 的训练迭代过程中应用了跨对象损失，其余 50% 则应用场景内的单个对象损失，而（a）中 100% 使用了场景内单个对象损失。这些图显示了三个不同对象中对每个对象的 10,000 个随机选择的像素点的描述符的散点图。该网络是在 D=2 的环境下训练的，使其可以直接进行聚类可视化。（c）图与 3（b）中的坐标相同。所有的网络都是在相同的 3 个对象数据集上进行训练的。带有数字标签的网络是通过跨对象损失进行训练的，数字表示了描述符的维度。非跨对象网络（红线）是不使用跨对象损失训练的网络。




**5.2 多个对象的密集描述符**
![](https://pic4.zhimg.com/v2-b53b52f8cdc3030af182ea08dce0cedf_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1041' height='303'></svg>)
图 5:（a）图与图 3（a）的坐标相同，将训练过程「standard-SO」和「without-DR」进行对比，其中唯一的区别是，「without-DR」在训练时不使用背景域随机化技术。（a）中使用的数据集包含 3 个对象，每个对象有 4 个场景。（b）图说明，对于一个包含 10 个训练场景的数据集，在训练过程中，不带背景和方向随机性的情况下学习到的描述符并不一致（中间），但是带有背景和方向随机性时学到的描述符是一致的（右侧）。




**5.4 机器人操作示例应用：抓取特定点**
![](https://pic2.zhimg.com/v2-17792e49402fc09d202a7d5b6b813a69_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='377'></svg>)
图 6：描述「抓取特定点」过程的示意图。用户为每张参考图像指定一个像素点，接着机器人自动地抓取测试环境下最佳匹配的点。对于单个对象而言，可以看到机器人抓取了毛毛虫对象两个不同的点：尾巴（i）和右耳（ii）。请注意，「右耳」示范是在相当对称的对象上打破对称性的一个例子。在类间泛化能力上（iii），通过一致训练，机器人在各种实体上抓取类间泛化点（具有公共特征的点）。这项工作仅仅通过 4 只鞋子进行训练并且扩展到机器人没有见过的鞋子实例上，例如（c）。对于「实体特异性」问题，机器人经过了对特定物体的训练，并且通过合成多对象场景（3.3 iii）进行了数据增强处理，从而使机器人甚至能够在杂乱的环境中抓取特定实例的这个点。




**论文：Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation**
![](https://pic2.zhimg.com/v2-e68d980ba27237c6175510f1ad713609_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='348'></svg>)
论文链接：[https://arxiv.org/pdf/1806.08756.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1806.08756.pdf)

**摘要：**机器人操作的正确对象表征应该是怎样的？我们希望机器人能够直观地感知场景，理解其中的对象，并满足以下要求:（i）与具体任务无关，能够作为构建模块用于各种操作任务中，（ii）广泛适用于刚性和非刚性对象，（iii）利用 3D 图像提供的丰富先验信息，（iv）完全通过自监督方法学习。这通过以前的方法是很难实现的，具体而言：许多近期在抓取物体方面的工作没有扩展到抓取特定物体或其他的任务上，而针对特定任务的学习可能需要经过大量的尝试来取得在不同的对象配置和其它任务上很好的泛化能力。在本文中，我们以自监督密集描述符学习的最新进展为基础提出了密集对象网络，作为对视觉理解和操作任务的一致对象表征。我们已证明它们可以被快速地训练（大约 20 分钟），用于操作各种事先未知并且可能是非刚性的物体。此外，我们还提出了一些新的贡献来实现多对象描述符学习，并且说明了可以通过修改训练过程得到能够在不同类别的对象间泛化的描述符，或者对每个对象实例都不同的描述符。最后，我们演示了学习到的密集描述符在机器人操作中的新应用。我们演示了在可能变形的对象配置下抓取一个对象的特定点的过程，以及使用类通用的描述符对一个类中不同对象的特定抓取动作进行迁移。
*![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)*





