# 谷歌发布全新端到端语音识别系统：词错率降低至5.6% - 知乎
# 



> *近日，谷歌发表博客介绍了他们对端到端语音识别模型的最新研究成果，新模型结合了多种优化算法提升 LAS 模型的性能。相较于分离训练的传统系统，新方法充分地发挥了联合训练的优势，在语音搜索任务中取得了当前业内最低的词错率结果。*

**当前最佳语音搜索模型**

传统自动语音识别系统（ASR）一直被谷歌的多种语音搜索应用所使用，它由声学模型（AM）、发音模型（PM）和语言模型（LM）组成，所有这些都会经过独立训练，同时通常是由手动设计的，各个组件会在不同的数据集上进行训练。AM 提取声学特征并预测一系列子字单元（subword unit），通常是语境依赖或语境独立的音素。然后，手动设计的词典（PM）将声学模型生成的音素序列映射到单词上。最后，LM 为单词序列分配概率。独立地训练各个组件会产生额外的复杂性，最终得到的性能低于联合训练所有的组件。过去几年来出现了越来越多开发中的端到端系统尝试以单个系统的方式联合学习这些分离的组件。虽然相关文献表明这些端到端模型具有潜在价值 [2,3]，但对于这样的方法是否能提升当前最佳的传统系统的性能，尚无定论。

最近，谷歌发布了其最新研究，「使用序列到序列模型的当前最佳语音识别系统」（State-of-the-art Speech Recognition With Sequence-to-Sequence Models[4]）。这篇论文描述了一种新型的端到端模型，它的性能优于目前已商用的传统方法 [1]。在谷歌的研究中，新的端到端系统的词错率（WER）可以降低到 5.6%，相对于强大的传统系统有 16% 的性能提升（6.7%WER）。此外，该端到端模型可以在任何的假设再评分（hypothesis rescoring）之前输出初始词假设。该模型的大小只有传统模型的 1/18，因为它不包含分离的 LM 和 PM。

谷歌的新系统建立在 Listen-Attend-Spell（LAS，在文献 [2] 中首次提出）端到端架构之上。LAS 架构由三个组件组成。listener 编码器组件，和标准的 AM 相似，取输入语音信号 x 的时间-频率表征，然后使用一系列的神经网络层将输入映射到一个高级特征表示，henc。编码器的输出被传递到 attender，其使用 henc 学习输入特征 x 和预测子字单元的 {y_n,...y_0} 之间的对齐方式，其中每个子字通常是一个字素或字片。最后，attention 模块的输出被传递给 speller（即解码器），speller 和 LM 相似，可以生成一系列假设词的概率分布。






![](https://pic1.zhimg.com/v2-b9d933a64f9266e5587c6b909786dba0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='208' height='329'></svg>)



*LAS 端到端模型的组件*




LAS 模型的所有组件通过单个端到端神经网络联合地训练，相较于传统系统的分离模块更加简单。

此外，因为 LAS 模型都是神经网络类型，因此并不需要添加外部的手动设计组件，例如有限状态转换器、词典或文本归一化模块。最后，和传统模型不同，训练端到端模型不需要决策树的引导或一个分离系统生成的时间序列，给定了文本副本和相关的声学特征之后，就可以进行训练。

在文献 [4] 中，谷歌引入了一种新型的结构化提升，包括优化传递给解码器的注意力向量，以及优化更长的子字单元（即字片，wordpieces）的训练过程。此外，谷歌在新模型中还引入了大量的优化训练过程的方法，包括最小词错率训练法（minimum word error rate training[5]）。正是这些结构化和优化提升使新模型取得了相对于传统模型 16% 的性能提升。

这项研究的另一个潜在应用是多方言和多语言系统，仅需优化单个神经网络所带来的简单性是很有吸引力的。所有的方言/语言可以被组合以训练一个网络，而不需要为每个方言/语言分配分离的 AM、PM 和 LM。谷歌生成这些模型在 7 种英语方言 [6] 和 9 种印度方言 [7] 上都工作得很好，优于分离地训练模型的性能。

虽然结果很吸引人，但是研究人员认为目前的探索还尚未完成。第一，这些模型还不能实时地处理语音 [8,9,10]，而实时处理对于延迟敏感的应用如语音搜索而言是必要的。第二，这些模型在实际生产数据上进行评估的时候表现仍然不佳。第三，谷歌目前的端到端模型是在 22,000 个录音-文本对上学习的，而传统系统通常可以在显著大得多的语料库上进行训练。最后，新模型还不能为生僻词学习合适的拼写，例如专有名词（一般还需要使用手动设计的 PM）。谷歌接下来的目标将是解决这些问题。

**论文：**State-of-the-art Speech Recognition With Sequence-to-Sequence Models






![](https://pic4.zhimg.com/v2-ccd9783b7ea688e7e7b9b098b0abc1e3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='861' height='198'></svg>)






论文链接：[https://arxiv.org/abs/1712.01769](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.01769)

**摘要：**基于注意力机制的编码器-解码器架构，如 Listen、Attend 和 Spell（LAS）可以将传统自动语音识别（ASR）系统上的声学、发音和语言模型组件集成到单个神经网络中。在我们以前的工作中，我们已经证明了这样的架构在听写任务中与业内顶尖水平的 ASR 系统具有相当水平，但此前还不清楚这样的架构是否可以胜任语音搜索等更具挑战性的任务。

在本研究中，我们探索了多种优化和提升 LAS 模型的方法，其中的一些显著提升了系统表现。在结构上，我们证明了词块模型可以用来代替字素。我们引入了新型的多头注意力架构，它比常用的单头注意力架构有所提升。在优化方面，我们探索了同步训练、定期采样、平滑标签（label smoothing），也应用了最小误码率优化，这些方法都提升了准确度。我们使用一个单向 LSTM 编码器进行串流识别并展示了结果。在 12,500 小时的语音搜索任务中，我们发现新模型将 LAS 系统的词错率（WER）从 9.2% 降低到了 5.6%，相对于目前业内最佳系统的 6.7% 提高了 16% 的水平。

参考文献

[1] G. Pundak and T. N. Sainath,「Lower Frame Rate Neural Network Acoustic Models," in Proc. Interspeech, 2016.

[2] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals,「Listen, attend and spell,」CoRR, vol. abs/1508.01211, 2015

[3] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and N. Jaitly,「A Comparison of Sequence-to-sequence Models for Speech Recognition,」in Proc. Interspeech, 2017.

[4] C.C. Chiu, T.N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R.J. Weiss, K. Rao, K. Gonina, N. Jaitly, B. Li, J. Chorowski and M. Bacchiani,「State-of-the-art Speech Recognition With Sequence-to-Sequence Models,」submitted to ICASSP 2018.

[5] R. Prabhavalkar, T.N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.C. Chiu and A. Kannan,「Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models,」submitted to ICASSP 2018.

[6] B. Li, T.N. Sainath, K. Sim, M. Bacchiani, E. Weinstein, P. Nguyen, Z. Chen, Y. Wu and K. Rao,「Multi-Dialect Speech Recognition With a Single Sequence-to-Sequence Model」submitted to ICASSP 2018.

[7] S. Toshniwal, T.N. Sainath, R.J. Weiss, B. Li, P. Moreno, E. Weinstein and K. Rao,「End-to-End Multilingual Speech Recognition using Encoder-Decoder Models」, submitted to ICASSP 2018.

[8] T.N. Sainath, C.C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu, P. Nguyen and Z. Chen,「Improving the Performance of Online Neural Transducer Models」, submitted to ICASSP 2018.

[9] C.C. Chiu and C. Raffel,「Monotonic Chunkwise Attention,」submitted to ICLR 2018.

[10] D. Lawson, C.C. Chiu, G. Tucker, C. Raffel, K. Swersky, N. Jaitly.「Learning Hard Alignments with Variational Inference」, submitted to ICASSP 2018.

[11] T.N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan, D. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen and C.C. Chiu,「No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models,」submitted to ICASSP 2018.

[12] A. Kannan, Y. Wu, P. Nguyen, T.N. Sainath, Z. Chen and R. Prabhavalkar.「An Analysis of Incorporating an External Language Model into a Sequence-to-Sequence Model,」submitted to ICASSP 2018. 




*原文链接：[https://research.googleblog.com/2017/12/improving-end-to-end-models-for-speech.html](https://link.zhihu.com/?target=https%3A//research.googleblog.com/2017/12/improving-end-to-end-models-for-speech.html)*




