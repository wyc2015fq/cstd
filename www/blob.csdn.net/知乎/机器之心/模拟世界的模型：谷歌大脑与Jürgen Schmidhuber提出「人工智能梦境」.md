# 模拟世界的模型：谷歌大脑与Jürgen Schmidhuber提出「人工智能梦境」 - 知乎
# 



文章选自arXiv，作者：David Ha、Jürgen Schmidhuber，机器之心编译

> 人类可以在应对各种情况时在大脑中事先进行充分思考，那么人工智能也可以吗？近日，由谷歌大脑研究科学家 David Ha 与瑞士 AI 实验室 IDSIA 负责人 Jürgen Schmidhuber（他也是 LSTM 的提出者）共同提出的「世界模型」可以让人工智能在「梦境」中对外部环境的未来状态进行预测，大幅提高完成任务的效率。这篇论文一经提出便吸引了人们的热烈讨论。

论文在线交互地址：[https://worldmodels.github.io/](https://link.zhihu.com/?target=https%3A//worldmodels.github.io/)

人类基于有限的感官感知开发关于世界的心智模型，我们的所有决策和行为都是基于这一内部模型。系统动力学之父 Jay Wright Forrester 将这一心智模型定义为： 

> 「我们周围的世界在我们的大脑中只是一个模型。没有人的大脑可以想象整个世界、所有政府或国家。他只选择概念及其之间的关系，然后使用它们表征真实的系统。」[4]

为了处理我们日常生活中的海量信息，大脑学习对信息进行时空抽象化表征。我们能够观察一个场景，并记住其抽象描述 [5, 6]。有证据表明我们在任意时刻的感知都由大脑基于内部模型所做的未来预测而决定 [7, 8]。
![](https://pic4.zhimg.com/v2-c53454a50d2806471e68c7ac7ffe38b3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='938' height='473'></svg>)图 2：我们看到的事物基于大脑对未来的预测 (Kitaoka, 2002; Watanabe et al., 2018)
一种理解大脑中预测模型的方式是：它可能不是预测未来，而是根据给出的当前运动动作预测未来的感官数据 [12, 13]。在面对危险时，我们能够本能地根据该预测模型来行动，并执行快速的反射行为 [14]，无需有意识地规划一系列动作。

以棒球为例 [15]。棒球击球手只有几毫秒时间来决定如何挥动球棒，而眼睛的视觉信号传到大脑所需时间比这更少。击球手能够快速根据大脑对未来的预测来行动，无需有意识地展开多个未来场景再进行规划 [16]。

在很多强化学习（RL）[17, 18, 19] 问题中，人工智能体还受益于过去和现在状态的良好表征，以及优秀的未来预测模型 [20, 21]，最好是在通用计算机上实现的强大预测模型，如循环神经网络（RNN）[22, 23, 24]。

大型 RNN 是具备高度表达能力的模型，可以学习数据丰富的时空表征。但是，文献中很多无模型 RL 方法通常仅使用具备少量参数的小型神经网络。RL 算法通常受限于信用分配问题（credit assignment problem），该挑战使传统的 RL 算法很难学习大型模型的数百万权重，因此在实践中常使用小型网络，因为它们在训练过程中迭代速度更快，可以形成优秀策略。

理想情况下，我们希望能够高效训练基于大型 RNN 网络的智能体。反向传播算法 [25, 26, 27] 可用于高效训练大型神经网络。本研究中，我们试图通过将智能体分为大型世界模型和小型控制器模型，来训练能够解决 RL 任务的大型神经网络。我们首先用无监督的方式训练一个大型神经网络，来学习智能体世界的模型，然后训练小型控制器模型来使用该世界模型执行任务。小型控制器使得算法聚焦于小搜索空间的信用分配问题，同时无需牺牲大型世界模型的容量和表达能力。通过世界模型来训练智能体，我们发现智能体学会一个高度紧凑的策略来执行任务。

尽管存在大量与基于模型的强化学习相关的研究，但本文并不是对该领域当前状况进行综述。本文旨在从 1990—2015 年一系列结合 RNN 世界模型和控制器的论文 [22, 23, 24, 30, 31] 中提炼出几个关键概念。我们还讨论了其他相关研究，它们也使用了类似的「学习世界模型，再使用该模型训练智能体」的思路。

本文提出了一种简化框架，我们使用该框架进行实验，证明了这些论文中的一些关键概念，同时也表明这些思路可以被高效应用到不同的 RL 环境中。在描述方法论和实验时，我们使用的术语和符号与 [31] 类似。




## **2. 智能体模型**

我们提出一种由人类认知系统启发而来的简单模型。在该模型中，我们的智能体有一个视觉感知模块，可以把所见压缩进一个小的表征性代码。它同样有一个记忆模块，可以根据历史信息对未来代码做预测。最后，智能体还有一个决策模块，只基于由其视觉和记忆组件创建的表征来制定行动。
![](https://pic3.zhimg.com/v2-8537ea4305a0e282452e107dca6baf56_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='714'></svg>) 图4：我们的智能体包含紧密相连的三个模块： 视觉 (V)、记忆 (M) 和控制器 (C)
**2.1. VAE (V) 模型**

环境在每一时间步上为我们的智能体提供一个高维输入观测，这一输入通常是视频序列中的一个 2D 图像帧。VAE 模型的任务是学习每个已观测输入帧的抽象压缩表征。
![](https://pic1.zhimg.com/v2-5d701491a509829d4923d80e1a0e6cfc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='985' height='326'></svg>)图 5：VAE 的流程图
在我们的试验中，我们使用一个变分自编码器 (VAE) (Kingma & Welling, 2013; Jimenez Rezende et al., 2014) 作为 V 模型。




**2.2. MDN-RNN (M) 模型**

尽管在每一时间帧上压缩智能体的所见是 V 模型的任务，我们也想压缩随着时间发生的一切变化。为达成这一目的，我们让 M 模型预测未来，它可以充当 V 预期产生的未来 z 向量的预测模型。由于自然中的很多复杂环境是随机的，我们训练 RNN 以输出一个概率密度函数 p(z) 而不是一个确定性预测 z。
![](https://pic1.zhimg.com/v2-151a82c6faa1f7bcd65bc2eae124a214_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='758' height='519'></svg>)



**2.3. 控制器 (C) 模型**

在环境的展开过程中，控制器 (C) 负责决定动作进程以最大化智能体期望的累加奖励。在我们的试验中，我们尽可能使 C 模型简单而小，并把 V 和 M 分开训练，从而智能体的绝大多数复杂度位于世界模型（V 和 M）之中。




**2.4. 合并 V、M 和 C**

下面的流程图展示了 V、M 和 C 如何与环境进行交互：
![](https://pic4.zhimg.com/v2-db3799469a99a08556a31eff6b2901c7_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='796' height='592'></svg>)
图 8：智能体模型的流程图。原始的观察每个时间步 t 到 zt 首先在 V 上进行处理。C 的输入是隐向量 zt 在每个时间步上与 M 隐藏态的串接。随后 C 会输出动作矢量以控制 motor，这会影响整个环境。随后 M 会以 zt 作为输入，生成时间 t+1 的状态 ht+1。 




## **3.Car Racing 实验**

在这一章节中，我们描述了如何训练前面所述的智能体模型，并用来解决 Car Racing 任务。就我们所知，我们的智能体是解决该任务并获得预期分数的第一个解决方案。
![](https://pic2.zhimg.com/v2-8d66491fc89584def50b4d870238f96d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='915' height='666'></svg>)
总结而言，Car Racing 实验可以分为以下过程：

1. 从随机策略中收集 10000 个 rollouts。

2. 训练 VAE（V）将视频帧编码为 32 维的隐向量 z。

3. 训练 MDN-RNN（M）建模概率分布 P(z_{t+1} | a_t, z_t, h_t)。

4. 定义控制器（C）为 a_t = W_c [z_t, h_t] + b_c。

5. 使用 CMA-ES 求解 W_c 和 b_c 而最大化预期累积奖励。
![](https://pic1.zhimg.com/v2-5eb66cb7acc22f29ada1a69245cdc34c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='887' height='362'></svg>)表 1：多种方法实现的 CarRacing-v0 分数
因为我们的世界模型能够对未来建模，因此我们能自行假设或预想赛车场景。给定当前状态，我们可以要求模型产生 z_{t+1} 的概率分布，然后从 z_{t+1} 中采样并作为真实世界的观察值。我们可以将已训练的 C 放回由 M 生成的预想环境中。下图展示了模型所生成的预想环境，而该论文的在线版本展示了世界模型在预想环境中的运行。
![](https://pic2.zhimg.com/v2-9554ddab02a9d883d67858eb192067b1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='809' height='811'></svg>)
图 13：我们的智能体在自己的预想环境或「梦」中学习驾驶。在这里，我们将已训练策略部署到从 MDN-RNN 生成的伪造环境中，bintonggu 并通过 VAE 的解码器展示。在演示中，我们可以覆盖智能体的行动并调整τ以控制由 M 生成环境的不确定性。




## **4. VizDoom 实验**

如果我们的世界模型足够准确，足以处理手边的问题，那么我们应该能够用实际环境来替换世界模型。毕竟，我们的智能体不直接观察现实，而只是观察世界模型呈现给它的事物。在该实验中，我们在模仿 VizDoom 环境的世界模型所生成的幻觉中训练智能体。
![](https://pic3.zhimg.com/v2-2f43c215fc6124f6afa7024758e2286e_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='320' height='240'></svg>)
经过一段时间训练后，我们的控制器学会在梦境中寻路，逃离 M 模型生成怪兽的致命火球攻击（fireballs shot）。
![](https://pic3.zhimg.com/v2-a96fb3e79782e09b90e003857dca470e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='669' height='667'></svg>)图 15：我们的智能体发现一个策略可以逃避幻境中的火球
我们把在虚拟幻境中训练的智能体放在原始 VizDoom 场景中进行测试。
![](https://pic1.zhimg.com/v2-e11eb3acbef723de2ccdaa6ea987f8a8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='540'></svg>)图 16：将智能体在幻觉 RNN 环境中学到的策略部署到真实的 VizDoom 环境中
由于我们的世界模型只是该环境的近似概率模型，它偶尔会生成不遵循真实环境法则的轨迹。如前所述，世界模型甚至无法确切再现真实环境中房间另一端的怪兽数量。就像知道空中物体总会落地的孩子也会想象存在飞越苍穹的超级英雄。为此，我们的世界模型将被控制器利用，即使在真实环境中此类利用并不存在。
![](https://pic4.zhimg.com/v2-b8f77fa702caeadd895a863fd169417b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='859' height='861'></svg>)图 18：智能体在多次运行中被火球击中后，发现了自动熄灭火球的对抗策略
## **5. 迭代训练过程**

在我们的实验中，任务相对简单，因此使用随机策略收集的数据集可以训练出较好的世界模型。但是如果环境复杂度增加了呢？在难度较大的环境中，在智能体学习如何有策略地穿越其世界后，它也仅能获取世界的一部分知识。

更复杂的任务则需要迭代训练。我们需要智能体探索自己的世界，不断收集新的观测结果，这样其世界模型可以不断改善和细化。迭代训练过程（Schmidhuber, 2015a）如下：

1. 使用随机模型参数初始化 M、C。

2. 在真实环境中试运行 N 次。智能体可能在运行过程中学习。将运行中的所有动作 a_t 和观测结果 x_t 保存在存储设备上。

3. 训练 M 对 P(x_t+1, r_t+1, a_t+1, d_t+1|x_t, a_t, h_t) 进行建模。

4. 如果任务未完成，则返回步骤 2。




## **论文：World Models**
![](https://pic2.zhimg.com/v2-cef3273ca79616d1d35121bb039344cd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='896' height='443'></svg>)






论文链接：[https://arxiv.org/pdf/1803.10122.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.10122.pdf)

**摘要**：我们探索构建流行的强化学习环境之下的生成神经网络。我们的「世界模型」可以无监督方式进行快速训练，以学习环境的有限时空表征。通过使用提取自世界模型的特征作为智能体的输入，我们可以训练一个非常紧密且简单的策略，解决目标任务。我们甚至可以完全通过由世界模型本身生成的虚幻梦境训练我们的智能体，并把从中学会的策略迁移进真实环境之中。


