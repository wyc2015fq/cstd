# NIPS 2018 | Spotlight论文：凭借幻想的目标进行视觉强化学习 - 知乎
# 



选自bair.Berkeley，作者：Vitchyr Pong、 Ashvin Nai，机器之心编译，参与：乾树、王淑婷。

> 为了让智能体在测试时自动实现用户指定的广泛目标，它必须能够学习广泛适用的通用技能。此外，为了提供必要的通用性，这些技能必须处理原始的传感输入，如图像。在本文中，作者提出了一种算法，通过结合无监督表征学习和目标条件策略的强化学习来获得这种通用技能。

我们想构建一个能够在复杂的非结构化环境中完成任意目标的智能体，例如可以做家务的机器人。一种有前景的方法是使用深度强化学习，这是一种用于教授智能体最大化奖励函数的强大框架。然而，典型的强化学习范例一般需要手动设计奖励函数来训练智能体解决独立任务。

例如，你可以通过设计奖励函数来训练机器人摆放餐桌，而这个函数则基于每个盘子或器具与其目标位置之间的距离来设定。这种设置需要为每个任务设计奖励函数，还需要诸如目标检测器之类的额外系统，这些系统可能昂贵又易坏。此外，如果想要能够执行大量琐事的机器，我们必须在每个新任务上重复这个强化学习训练步骤。
![](https://pic2.zhimg.com/v2-b8b48fd0175c745ea748ea74aee3e569_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='758' height='376'></svg>)尽管设计奖励函数和设置传感器（门角测量，目标检测器等）在模拟中可能很容易做到，但在现实世界中它很快变得不切实际（右图）![](https://pic2.zhimg.com/v2-07d84c0b92f3bbbbb84bd26be71c17b9_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='776' height='384'></svg>)我们训练智能体在没有额外仪器的情况下仅凭视觉来解决各种任务。第一行展示了目标图像，底行展示了达到这些目标的策略
在本文中，我们讨论了可以同时学习多个不同的任务而无需额外人工监督的强化学习算法。如果智能体要在没有人为干预的情况下获取技能，那它必须能够为自己设定目标、与环境交互，并评估其是否实现了改善行为的目标，所有这些都要通过图像等原始观测数据来实现，而不能借助目标探测器等手动设计的额外组件。

我们引入了一个设定抽象目标并自主学习以实现这些目标的系统；然后发现可以使用这些自主学习的技能来执行各种用户指定的目标，例如推动物体，抓取物体以及开门，而无需任何额外的学习。最后，我们证明这种方法在现实世界中能够应用在 Sawyer 机器人上。该机器人仅将图像作为系统的输入, 学会了设置并实现将物体推到特定位置的目标。




**目标条件强化学习**

我们该如何表示环境和目标的状态？在多任务设置中，枚举出机器人可能需要注意的所有物体可能不切实际：物体的数量和类型可能会发生变化，检测它们需要专用的视觉处理器。

不过，我们可以直接在机器人的传感器上操作，用机器人拍摄的图像来表示状态，而用我们想要实现状态的图像来表示目标。用户只需提供目标图像来指定新任务。我们发现，这项工作可以扩展为指定目标的更复杂方式，例如通过语音或演示，或者通过优化以前博客中的目标。



![](https://pic4.zhimg.com/v2-ea697f4819302a7f875aaf70aab4efaf_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='632'></svg>)任务：让环境看起来像图片中这样
强化学习的本质是训练智能体最大化奖励总和。对于目标条件强化学习来说，获得奖励的一种方法是减少当前状态和目标状态之间的距离，因此，要最大化奖励，就得最小化当前状态到目标状态的距离。

首先，我们可以通过学习目标条件 Q 函数来训练单一策略，以最大化奖励并因此达到目标状态。在给定当前状态 s 和目标 g 的情况下，目标条件 Q 函数 Q（s，a，g）会告诉我们动作 a 有多正确。例如，一个 Q 函数告诉我们，「如果我拿着一个盘子（状态 s）并且想把它放在桌子上（目标 g），那么举手（动作 a）有多正确？」一旦训练好这个 Q 函数，你就可以通过执行以下优化函数来获取目标条件策略：
![](https://pic3.zhimg.com/v2-ac2fa2b4465bb9c52b8e166c87fe898e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='460' height='66'></svg>)
通俗点说，「根据这个 Q 函数选择最佳动作。」通过使用这个程序，我们获得了一个最大化奖励总和的策略，可以实现各种目标。

Q 学习受欢迎的一个原因是，它能以非策略方式完成，这意味着我们训练 Q 函数所需的仅仅是当前状态、动作、下一状态、目标和奖励（分别为 s,a,s′,g,r）的样本。此数据可以通过任何策略收集，并且可以在多个任务中重复使用。因此，一个简单的目标条件 Q 学习算法如下所示：
![](https://pic3.zhimg.com/v2-d3a563ed9bc120f28140d9e1279d454a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='172'></svg>)
该训练过程的主要瓶颈是收集数据。如果我们可以人工生成更多数据，理论上来说，我们甚至可以在不与环境互动的情况下学习解决各种任务。但是，学习一个精确的环境模型很困难，所以我们通常需要依靠抽样来获得状态—动作—下一状态 (s,a,s′) 的数据。

然而，如果能够访问奖励函数 r(s,g)，我们就可以重新标记目标并重新计算奖励，从而在给定单个 (s,a,s′) 元组的情况下人工合成更多数据。因此，我们可以像这样修改此训练过程：
![](https://pic2.zhimg.com/v2-a3b1b7c4c4e3fc425df090fdddd12a0d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='152'></svg>)
目标重采样的好处在于，我们可以同时学习如何一次实现多个目标，而无需从环境中获取更多数据。总的来说，这种简单的修改可以大大加快学习速度。上述方法有两个主要假设：（1）你能够访问奖励函数;（2）你知道目标采样分布 p(g) 的状况。

以前使用此目标重新标记策略的工作（Kaelbling 『93 , Andrychowicz 『17 , Pong 『18）基于真实状态信息（例如，物体的笛卡尔位置）进行操作，很容易手动设计目标分布 p(g) 和奖励函数。然而，当转向基于视觉的任务（其中目标是图像）时，这两个假设都不成立。

首先，我们不清楚应该使用哪种奖励函数，因为与目标图像的像素距离在语义上可能没有意义。其次，因为我们的目标是图像，我们需要一个可以从中进行采样的目标图像分布 p(g)。手动设计目标图像的分布是一项非常重要的任务，图像生成仍然是一个活跃的研究领域。因此，我们希望智能体能够自主设定自己的目标并学习如何实现这些目标。




**凭借幻想的目标进行视觉强化学习**

我们可以通过学习图像的表征，并将这种表征而非图像本身用于强化学习，来减轻与目标图像条件 Q 学习相关的挑战。关键问题变成了：我们的表征应满足哪些属性？为了计算语义上有意义的奖励，我们需要一种能够捕捉影响图像变化的潜在因素的表征。

此外，我们需要一种轻松生成新目标的方法。我们首先通过训练生成隐变量模型来实现这些目标，在我们的例子中用的是变分自编码器（VAE）。该生成模型将高维观察数据 x（如图像）转换为低维隐变量 z，反之亦然。

对模型进行训练，以便隐变量捕获图像中潜在的变化因素，类似于人类用于解释环境和目标的抽象表征。给定当前图像 x 和目标图像 x_g，我们将它们分别转换为隐变量 z 和 z_g。

然后我们用这些隐变量来表示强化学习算法的状态和目标。在低维隐空间而不是直接在图像上学习 Q 函数和策略，可以加快学习速度。
![](https://pic1.zhimg.com/v2-d3a5d2251aa74ece1bf9c545a067e080_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='393'></svg>)智能体将当前图像（x）和目标图像（xg）编码到隐空间中，并使用该隐空间中的距离进行奖励
使用图像和目标的隐变量表征还解决了另一个问题：如何计算奖励。我们利用隐空间中的距离，而不是像素误差来作为训练智能体达到目标的奖励。在描述该方法的完整研究论文中，我们发现这样做能够最大化取得目标的概率并提供更有效的学习信号。

这种生成模型也很重要，因为它让智能体在隐空间中可以轻松生成目标。特别是，我们的生成模型被设计为忽视采样隐变量：我们之前只是从 VAE 中抽取隐变量。使用这种抽样机制有两个原因：

首先，它为智能体设定自己的目标提供了一种机制。智能体只是从我们的生成模型中对隐变量的值进行采样，并尝试达到该隐目标。其次，如上所述，该重采样机制还用于重新标记目标。因为我们的生成模型经过训练可以将真实图像编码到之前的图像中，所以之前隐变量的样本对应于有意义的隐目标。
![](https://pic1.zhimg.com/v2-1a59383588e7f86d017addb91ff55784_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='393'></svg>)即使不提供目标，我们的智能体仍然可以为探索和目标重新标记生成自己的目标
总之，图像的隐变量表示（1）捕获场景的潜在因素，（2）提供有意义的优化距离，（3）提供一种有效的目标抽样机制，使我们能够有效地训练一个直接对像素进行操作的目标条件强化学习智能体。我们将这整个方法称为凭借幻想目标进行强化学习（RIG）。




**实验**

我们进行了实验来测试 RIG 是否具有足够的样本效率，能够在合理的时间内训练现实世界的机器人策略。我们测试了机器人到达用户指定位置、将物体推动到所需位置的能力，如目标图像所示。

机器人只能接收 84x84 RGB 图像，无法知道关节角度或物体位置。机器人首先通过在隐空间中设置自己的目标来学习。我们可以使用解码器来可视化机器人想象的目标。在下面的 GIF 中，上面的图片展示了解码的「想象」目标，而下面的图片展示了做出的实际策略。
![](https://pic3.zhimg.com/v2-acdc722fbca11e889ed6e93b8c262f6e_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='88' height='172'></svg>)机器人设定了自己的目标（上图）以及实现目标（下图）
通过设定自己的目标，机器人可以自主地练习到达不同的位置而无需人为干预。唯一的人为干预发生在人们想要机器人执行特定任务的时候。这种时候，先给机器人输入目标图像。因为机器人已经通过练习达到了很多目标，我们发现它能够在没有额外训练的情况下达到这个目标：
![](https://pic1.zhimg.com/v2-0229a766984e0f67b10fcc661454748c_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='88' height='172'></svg>)人类给出一个目标图像（顶部），机器人实现它（底部）
我们还使用 RIG 来训练将物体推动到目标位置的策略：
![](https://pic1.zhimg.com/v2-17203f19cdd5c129d39d4093994dbbb0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='620' height='612'></svg>)左：Sawyer 机器人设置。右：人类给出一个目标图像（顶部），机器人完成它（底部）
直接从图像训练一个策略可以轻松地将任务从到达指定位置改为推送物体。我们只需添加一个物体、一个桌子，然后调整相机。最后，尽管直接从像素级开始工作，这些实验并没有花费很长时间。到达指定位置实际大约需要一个小时的机器人交互时间，而推动物体到目标位置需要大约 4.5 小时。

许多真实世界的机器人强化学习结果使用真实状态信息，如物体的位置。但是，这通常需要额外的机器，比如购买和设置额外的传感器或训练目标检测系统。相比之下，我们的方法只需要 RGB 相机并直接在图像上工作。

更多结果，包括对比实验，请阅读论文原文，地址如下：[https://arxiv.org/abs/1807.04742](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.04742)




**未来展望**

我们已经证明，直接在图像上训练现实世界机器人的策略是可行的，并且能以高效的方式完成各种任务。这个项目有许多令人兴奋的后续研究。目标图像可能无法表示所有任务，但是可以使用其它模式（例如语音和演示）来表示目标。

此外，尽管我们提供了一种机制来对自主探索的目标进行抽样，但是否存在更科学的方式选择这些目标来进行更好的探索？结合内在动机，我们的政策可以积极地选择目标，以更快地了解它能不能达到目标。

未来的另一个方向是训练我们的生成模型，使其了解动态。编码有关环境动态的信息可以使隐空间更适合强化学习，从而加快学习速度。

最后，实际生活存在各种机器人任务，其状态表征难以用传感器捕获，例如操纵可变形物体或处理具有可变数量物体的场景。放大 RIG 以解决这些任务将是令人兴奋的下一步。
*![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)*
原文链接：[https://bair.berkeley.edu/blog/2018/09/06/rig/](https://link.zhihu.com/?target=https%3A//bair.berkeley.edu/blog/2018/09/06/rig/)




