# 这种两阶段深度着色模型，为黑白照披上了彩衣 - 知乎
# 



选自arXiv，**作者：Mingming He、Dongdong Chen、Jing Liao、Pedro V. Sander、Lu Yuan，机器之心编译。**

> 图像着色一直是比较困难的任务，近日港科大、中科大和微软研究提出了一种结合了图像检索与图像着色的模型。该模型首先会从大量参照图像中检索和灰度图相似的图像，然后再将该参照图像的配色方案迁移到灰度图中。这种深度模型实现了非常好的着色效果，感兴趣的读者也可以查看原论文与 GitHub 项目。

图像着色的目的是为灰度图像增添色彩，使图像更具视觉感知力和吸引力。由于图像的灰度像素可以被输入许多种颜色（例如树叶可能是绿色、黄色或棕色），所以这个问题难以解决，它本质都是模棱两可的。因此目前没有唯一正确的解决方案，人工介入往往在着色过程中起着重要的作用。

引导着色的手动标注信息通常有两种形式：用户引导的涂鸦或样本参照图。在第一种形式中，为了得到可信的结果，必须仔细选择涂鸦或调色板的颜色。这需要丰富的经验和良好的审美，因此对于未经训练的使用者来说是一个挑战；在第二种形式中，为了便于处理，我们需要给出了与灰度图同类的彩色图作为色彩基准。为了抽取这种色彩基准，首先需要建立对应关系，然后在最可靠的对应关系上传播色彩。然而，结果的质量很大程度上取决于参照图片的选择。光照、视角、内容不一致等因素会导致参照和目标间有巨大差异，进而误导着色算法。

利用巨大的参考图像数据库搜索最相似的图像块或像素进行着色不失为一种可靠的方法。最近，深度学习技术在大型数据建模方面多有建树。图像着色问题可以被看作回归问题，并利用深度神经网络直接求解。目前很多方法都可以全自动地给新照片附色，无需任何参考，但是这些方法都无法实现多模态着色 [Charpiat et al. 2008]。他们的模型主要利用从数据中学到的主色调，阻碍了使用者对其它色调的选择与使用。另一个缺点是，这些模型必须在一个涵盖所有可能参考图像的巨大数据库中进行训练。

最近的研究都试图在交互的可控性和学习的鲁棒性两方面都做到最好。Zhang 和 Sangkloy 等人在深度神经网络中以色点和笔画的形式添加人工信息，以便为用户推荐绘制时最可能需要的颜色。这极大促进了传统基于涂鸦的交互，并实现了通过大规模数据学习到更自然的颜色。然而，涂鸦对于获得高质量的结果仍然必不可少，所以还需要一定的试错。

本文中采取了另一种类型的混合解决方案。研究者提出了第一种基于样本的局部着色方法。与现有的着色网络相比，该网络可以通过选择不同的参照图来控制着色的输出。如图 1 所示，参考图像可能与目标相似也可能不同，但最后总能得到差不多的色彩结果，这些颜色在视觉上忠于参照图，并且色彩也非常有意义。
![](https://pic4.zhimg.com/v2-5ae7a1c516b9739c8fbc2622b6c19c83_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='272'></svg>)图 1：黑白照片着色结果。研究人员通过提供不同的参照图片，能为目标图像生成多个贴近真实的着色效果
为了实现这一目标，研究者提出了一个卷积神经网络（CNN），它可以从对齐的参照图像中直接选择、传播和预测灰度图像颜色。此方法在质量上优于已有基于样本的方法，它的成功之处在于使用基于样本着色框架中的两个新型子网络。

首先，相似性子网络（Similarity sub-net）是着一种预处理步骤，它为端到端着色网络提供了输入。相似性网络度量的是在灰度图像目标识别任务中，利用 VGG-19 网络预训练的参照图和目标图间的语义相似性。与之前基于低级特征的度量比，它为不同的语义图像提供了更具鲁棒性和可靠性的相似性度量。

然后着色子网络（Colorization sub-net）为相似或不同的块/像素对提供更一般的着色方案。它利用多任务学习训练两个不同的分支，两个分支共享相同的网络和权重，但损失函数不同：1）色度损失（Chrominance loss），激励网络选择性传播满足色彩一致性的相关块/像素；2）感知损失（Perceptual loss），使着色结果和真色彩图像在高级特征表示空间上紧密匹配。即使在参照图中没有合适匹配区域的情况下（参见图 2），也能确保从大规模数据中学习到适当的着色。因此，该方法与其它基于样本的方法不同，它可以大大放宽需要选择良好参照图的限制。
![](https://pic2.zhimg.com/v2-10b10ae695b9435ad9bbda8dc8f6e051_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='747' height='219'></svg>)图 2：目标是有选择地向相关图像块/像素传播正确参照图颜色（红点表示），并在参照图中没有合适匹配区域（红色轮廓线表示）时，从大规模数据中预测自然的颜色
为引导使用者进行有效的参照图选择，系统会根据本文所提出的图像检索算法推荐最可能的参照图。它利用高级语义信息和低级亮度统计信息来搜索 ImageNet 数据集中最相似的图像 [Russakovsky et al. 2015]。在这个检索算法的帮助下，研究者提出的方法可以看作一个全自动着色系统。实验表明，该自动着色系统在数量和质量上都优于现有的着色方法，甚至可以和当前最先进的交互方法 [Zhang et al. 2017; Sangkloy et al. 2016] 产生的高质量结果相媲美，此方法也可以扩展到视频着色。

研究成果如下：（1）提出第一个基于样本着色的深度学习方法，它具备可控性并对参照图的选择有鲁棒性。（2）提出一种全新的端到端双支路网络架构，当无法获取优秀的参照图时，模型会联合学习有意义的参照图局部着色和近似合理的色彩预测。（3）提出用于推荐的参照图像检索算法，也可用于实现全自动着色。（4）提出一种能迁移到非自然图像的方法，即使网络只在自然图像训练集上训练。（5）可以扩展到视频着色。




**论文：Deep Exemplar-based Colorization**
![](https://pic4.zhimg.com/v2-86ac4ac7d5154a77b4b532e572b1d88f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='206'></svg>)- 论文地址：[https://arxiv.org/pdf/1807.06587v2.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1807.06587v2.pdf)
- 项目地址：[https://github.com/msracver/Deep-Exemplar-based-Colorization](https://link.zhihu.com/?target=https%3A//github.com/msracver/Deep-Exemplar-based-Colorization)

**摘要：**我们提出了第一个基于样本的局部着色深度学习方法。当给定一个参照彩色图像时，我们的卷积网络将直接将灰度图像映射到输出的彩色图像中。与传统基于样本方法的手动标注规则不同，我们的端到端着色网络会学习如何从大规模数据中选择、传播和预测颜色。即使参照图像与输入灰度图像无关，该方法仍然有较强的鲁棒性和泛化能力。更重要的是，与其他基于学习的着色方法不同，我们的网络允许使用者简单地输入不同参照图片，就可实现对应的结果。为了进一步减少人工选择参照图像的工作量，系统采用我们提出的图像检索算法自动推荐参照图像，该检索算法同时兼顾到语义信息和亮度信息。通过简单地选取推荐参照图像，即可实现全自动着色。通过用户调查和与目前最优方法定量比较，我们对该方法进行了验证。此外，我们的方法可以自然地拓展到视频着色。代码和模型都会开源给大家。
![](https://pic3.zhimg.com/v2-c17d11b862095b21e12a56a41ca97a6e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='228'></svg>)
图 3：系统结构图（推断阶段）。该系统由两个子网络构成。相似性子网络作为预处理步骤使用 Input 1，Input 1 包含两个分别来自目标图和参照图的亮度通道 T_L 和 R_L、双向映射函数Φ_(T↔R) 和两个来自参照图像的亮度通道 R_ab。相似性子网络计算了双向相似映射 sim_(T↔R) 和与参照对应的色度通道 R'_ab，两者连同 T_L 作为 Input 2 输入到着色子网当中。着色子网是一个用来预测目标色度通道的端到端 CNN，结合 TL 生成最终的着色结果 P_Lab。
![](https://pic4.zhimg.com/v2-deb51b672ec0b5a7f1aa8d62414db69b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='543'></svg>)
图 10：我们的方法在不同参照图中的着色表现：手动选择、自动推荐、在同等目标中随机选择、在同类中随机选择、在同类之外随机选择。输入图像基本都选自 ImageNet 数据集，除了 Andreas Mortonus/flickr 和 Indi Samarajiva/flickr 的两张参照图片。
![](https://pic3.zhimg.com/v2-17313e5444e2578037dc2f769c443286_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='395'></svg>)
图 12：在 ImageNet 上训练的着色网络的迁移能力比较。输入图像（从左到右，从上到下）：Charpiat et al. [2008], Snow64/wikimedia 和 Ryo Taka/pixabay。
![](https://pic2.zhimg.com/v2-12a9ede62875318710189800bb1f906d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='895' height='763'></svg>)
图 14：与基于学习的方法进行比较。输入图像：ImageNet 数据集。
![](https://pic1.zhimg.com/v2-0001e3ba1d8b7066b0b9dcfe9d2ca000_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='752' height='594'></svg>)
图 9：包含手动阈值选择颜色样本和交叉匹配的端到端网络与 Zhang 等人着色方法对比。输入图像：ImageNet 数据集。
*![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)*





