# MIT的「少数派报告」，预测未来图像的深度学习算法 - 知乎
# 



> *通过一张静止的图片，CSAIL 的[深度学习系统](https://link.zhihu.com/?target=http%3A//web.mit.edu/vondrick/tinyvideo/)可以生成一个视频，预测该场景中接下来将发生什么。*


我们生活在动态的物理世界中，很容易忘记感知是一件多么复杂的事。不需要多少精力，人类就可以轻松对事物的变化做出反应。
但人类的第二天性对于机器而言是一个巨大的课题。自然界中的每一个对象都有无限多种运动的可能性，让计算机来预测事物未来的行动是非常困难的。
最近麻省理工学院（MIT）的计算机科学与人工智能实验室（CSAIL）让梦想距离我们更近了，他们的新一代深度学习算法可以从一张静止图像中学习特征，创建一个简短的视频来预测场景的未来变化。

![](http://shp.qpic.cn/qqvideo_ori/0/l0350wovy7v_228_128/0)预测未来的深度学习算法 - 腾讯视频https://v.qq.com/x/page/l0350wovy7v.html
研究人员让该算法训练了 200 万个未分类标记的视频，相当于两年的影像时间，该算法生成的视频中，人物的真实程度相对其他模型要真实 20%。
该论文的研究者认为，预测未来的图像可以应用到安保领域，也可以帮助自动驾驶汽车进行预测，增加其安全性。该论文的第一作者，CSAIL 的博士生 Carl Vondrick 认为，该算法也可以在未经协助的情况下帮助机器更快地识别人类的活动。
「这些深度学习生成的视频能告诉我们在计算机认知下，下一刻会发生什么，」Vondrick 说道。「如果你想预测未来，你必须先感知现在。」
Vondrick 的论文是与 MIT 教授 [Antonio Torralba](https://link.zhihu.com/?target=http%3A//web.mit.edu/torralba/www/) 和 [Hamed Pirsiavash](https://link.zhihu.com/?target=http%3A//www.csee.umbc.edu/~hpirsiav/) 合作完成的，Pirsiavash 曾在 CSAIL 进行博士后研究，现在是马里兰州巴尔第摩大学（UMBC）的助理教授。他们的研究成果将在下周的巴塞罗那巴塞罗那神经信息处理系统（NIPS）大会上提出。


**工作原理**


在之前的研究中，一些研究人员们已经解决了类似的问题，其中包括 MIT 教授 Bill Freeman，他的新成果「视觉动力学（visual dynamics）」可以创造一个场景中未来的一帧图像。但 Freeman 的模型只能预测动态视频中的未来，而 Torralba 的模型则可以从静态图片中创造出前所未有的视频。
之前的系统逐帧生成视频，这会产生很大的误差边界；而这个算法会一次生成整段视频，它现在可以创作出每秒 32 帧的视频，共生成大约 1.5 秒。
「逐帧生成视频就像一个巨大的『打电话』游戏，信息随着你在房间中的走动逐渐传播出去，」Vondrick 解释道。「现在我们尝试一次预测所有帧，看起来就像同时在和房间里的所有人说话。」
当然，这种同步生成的算法也有缺点：随着预测的准确性越来越高，计算机模型也随着视频的增长而变得越来越复杂。当然，对于复杂的预测而言，这是值得的。为了创造更多的框架，研究人员训练模型生成与背景分离的前景图像，随后将前景对象放回场景中，以此告知深度学习网络哪些物件会移动，哪些不会。
该团队使用了被称为「对抗学习（adversarial learning）」的深度学习方式，同时训练两个神经网络。一个网络生成视频，另一个尝试分辨真实图像和新视频。随着实验的进行，生成视频的神经网络学会了如何欺骗辨认者。
现在，该模型可以创作包括海滩、火车站、医院和高尔夫球场的视频场景。在海滩视频中会出现波浪，在高尔夫球场中，会有人在草地上行走。


**测试场景**


该小组将新方法生成的视频和其他模型的输出做了比较，并向人们询问哪种方法的结果看起来更真实。在 150 名用户的 13000 个意见中，新模型的辨识度高出了 20%。
 Vondrick 强调道，这个模型仍然缺少一些比较简单的常识性原则。比如说，它经常无法理解在物体移动时还仍然存在这件事情，就像是当火车从画面中驶过时的场景。并且，这个模式倾向于将人和物体放大。
另一个局限是，生成视频的长度只有 1.5 秒，所以团队也希望在之后的工作中对这一点加以改进。而面临的挑战是，这需要追踪更加长期的依赖来保证场景长时间有效。一种实现的方式就是增加人力监管。
Vondrick 说，「整合视频中长时间的具体信息是非常困难的」，「如果视频中的活动既包括烹饪，又包括吃饭，你必须能够把这两项活动连接起来，并且要保证场景合理有意义」。
这些模型不仅限于预测未来。生成式视频能够用来为静止的图像添加动画，就像是哈利·波特书中有动画效果的报纸。它们可以帮助在安全录像中侦查异常现象，或是在储存、发送长视频时压缩数据。
Vondrick 说道，「在未来，这会扩大我们的视觉系统，仅仅通过视频训练，而不需要任何监管，就可以让我们对物体或是场景进行辨认。」
这一研究也得到了美国国家科学基金会，马里兰大学巴尔的摩分校（UMBC）的 START 项目和谷歌一位博士生的支持。


**视频生成**

我们的模型生成了一些视频。这些视频是深度学习的预测，是由生成式视频模型输出的。尽管不是真实的，但里面的动作看起来在所属的分类中存在合理性。

**条件式视频生成（Conditional Video Generations）**

通过训练模型生成未来行为，我们也能使用模型为静态图片增加动画。当然，未来行为是不确定的，所以模型难以生成「正确的」未来，但我们认为这种预测有一定的合理性。

**视频表征学习（Video Representation Learning）**

生成视频的学习模型也可能成为学习表征的一种方式。例如，我们能在大型无标记视频库上训练生成器，然后在小型标记数据集上微调判别器，从而在较小的监督情况下识别一些行为。我们给出了模型在 UCF 101 上的准确率，并与其它面向视频的无监督学习方法进行了对比：

![](https://pic2.zhimg.com/v2-da85af863954f1f00e0da6e77b204829_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1008' height='290'></svg>)对未来进行预测时，我们也能可视化表征中的内容。同时不是所有的单元都是语义上的，我们发现有一些隐藏单元瞄准了用作运动来源的目标。因此，生成未来行为需要理解运动的目标，该网络可能在内部学习识别这些目标，即使不是通过监督的方式进行的。![](https://pic1.zhimg.com/v2-7796cf59a5db02da0d057f682e931820_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='384' height='256'></svg>)*面向人的隐藏单元![](https://pic1.zhimg.com/v2-7796cf59a5db02da0d057f682e931820_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='384' height='256'></svg>)**面向火车轨迹的隐藏单元*

以上图像展示了特定的卷积隐藏单元触发的区域。

**简要技术综述**

我们的方法建立在使用生成式对抗网络的生成式图像模型的基础上，我们将其应用到了视频中。背后的基本思路是将两个深度网络相互对比。一个网络（生成器）试图生成合成视频，另一网络（判别器）试图将合成视频与真实视频进行判别。训练出的生成器要能骗过判别器。

在生成器上，我们使用深度卷积网络输入低维随机噪音，输出视频。为了建模视频，我们使用时空上的（spatiotemporal） up-convolutions 网络（2D 空间，1D 时间）。生成器也独立于前景对背景进行建模，网络产生一个静态背景（随时间被复制），也生成一个移动的前景（前景同时也使用了一个 mask）。下图我们对此进行了展示：![](https://pic3.zhimg.com/v2-c761493f28900115b8f873db3a2d5f52_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1397' height='663'></svg>)

我们同时训练一个判别器网络区别真假视频。我们使用深度时空卷积网络作为判别器，如下图：
![](https://pic3.zhimg.com/v2-eb3e397e9686a0fb4d0796ac3b98f3ba_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1222' height='453'></svg>)
我们从 Flicker 上下载了总时长约2年的视频，同时通过场景分类对其进行了稳固与自动过滤，然后用它进行训练。

为了预测未来行为，可以将一个编码器附属到生成器上，如下图：
![](https://pic3.zhimg.com/v2-954692f5018ecc1eb8407eb743a229de_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1495' height='546'></svg>)
**目前的局限性**

对抗学习（adversarial learning）难以做到正确，我们的模型有如下一些不足之处：
- 
可以明显的区别生成的视频与真实视频。生成的视频有着相当低的分辨率：64×64,32帧。

- 
评估生成式模型很难，我们使用一个心理物理学 2AFC 测试在 Mechanical Turk 上问工作人员哪个视频更真实。我们认为该评估还可以，但社区内选定稳健的自动评估标准更加重要。

- 
为了更好的生成视频，我们通过场景分类自动过滤掉了一些事，并为每个分类单独训练一个模型。我们在最初的几帧上使用 PlacesCNN 获取视频的场景分类。

- 
未来的推测并不总是很好的匹配第一帧，之所以这样可能是因为瓶颈太大。


**论文：生成带有场景动态的视频（[Generating Videos with Scene Dynamics](https://link.zhihu.com/?target=http%3A//web.mit.edu/vondrick/tinyvideo/paper.pdf)）**

摘要：我们利用了大量未标记的视频来学习在视频识别（例如动作分类）和视频生成（例如预测未来）两方面都使用的场景动态模型。我们提出一个面向视频的生成式对抗网络，该网络利用时空卷积架构（spatio-temporal convolutional architecture ）从背景中整理出场景的前景。实验表明，该模型能瞬间生成在全帧率上优于简单基准的小视频。而且它还能用于预测静态图片的合理未来。此外，实验和可视化结果显示，该模型可以在内部学习有用的特征，在最小限度的监督情况下识别动作，这表明场景动态是表征学习的一个有效信号。我们相信生成式视频模型能影响视频理解和视频模拟中的许多应用。

选自MIT News  **作者：Adam Conner-Simons、Rachel Gordon  机器之心编译**


