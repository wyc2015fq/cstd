# 从最大似然估计开始，你需要打下的机器学习基石 - 知乎
# 



选自Medium

**作者：Jonny Brooks-Bartlett**

**机器之心编译**

> 概率论是机器学习与深度学习的基础知识，很多形式化的分析都是以概率的形式进行讨论。而这些讨论或多或少都离不开最大似然估计，因为它是参数估计的基础之一，也是构建模型的基石。在本文中，我们从最大似然估计到贝叶斯推理详细地讨论了机器学习的概率论基石，并希望能为读者的预习与复习提供优秀的参考资源。




**什么是参数？**

在机器学习中，我们经常使用一个模型来描述生成观察数据的过程。例如，我们可以使用一个随机森林模型来分类客户是否会取消订阅服务（称为流失建模），或者我们可以用线性模型根据公司的广告支出来预测公司的收入（这是一个线性回归的例子）。每个模型都包含自己的一组参数，这些参数最终定义了模型本身。

我们可以把线性模型写成 y = mx + c 的形式。在广告预测收入的例子中，x 可以表示广告支出，y 是产生的收入。m 和 c 则是这个模型的参数。这些参数的不同值将在坐标平面上给出不同的直线（见下图）。
![](https://pic4.zhimg.com/v2-7e84b678801fcb81a08525ed2de2961b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='739'></svg>)
*参数值不同的三个线性模型。*

因此，参数为模型定义了一个蓝图。只有将参数选定为特定值时，才会给出一个描述给定现象的模型实例。




**最大似然估计的直观解释**

最大似然估计是一种确定模型参数值的方法。确定参数值的过程，是找到能最大化模型产生真实观察数据可能性的那一组参数。

上述的定义可能听起来还是有点模糊，那么让我们通过一个例子来帮助理解。

假设我们从某个过程中观察了 10 个数据点。例如，每个数据点可以代表一个学生回答特定考试问题的时间长度（以秒为单位）。这 10 个数据点如下图所示：
![](https://pic2.zhimg.com/v2-cbd7bbbd4c98138a08e3ce63c43e1235_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='763'></svg>)
*我们观察到的 10 个（假设的）数据点。*




我们首先要决定哪个模型最适合描述生成数据的过程，这一步至关重要。至少，我们应该对使用哪种模型有一个不错的想法。这个判断通常来自于一些领域内专家，但我们不在这里讨论这个问题。

对于这些数据，我们假设数据生成过程可以用高斯分布（正态分布）进行充分描述。对以上数值目测一番就可以得知，高斯分布是合理的，因为这 10 个点的大部分都集中在中间，而左边和右边的点都很少。（因为我们只使用了 10 个数据点，做出这样的草率决定是不明智的，但考虑到我是用某个确定的分布函数生成这些数据点，我们就凑合着用吧）。

回想一下高斯分布有两个参数：均值μ和标准差σ。这些参数的不同值会对应不同的曲线（就像上面的直线一样）。我们想知道「哪条曲线最可能产生我们观察到的数据点」？（见下图）。用最大似然估计法，我们会找到与数据拟合得最好的 μ、σ 的值。
![](https://pic2.zhimg.com/v2-1016565bb33c57abb2c3eaa8518582dd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='765'></svg>)
*10 个数据点和可能得出这些数据的高斯分布。f_1 是均值为 10、方差为 2.25（方差等于标准偏差的平方）的正态分布，也可以表示为 f_1∼N(10, 2.25)。其它曲线为 f_2∼N(10, 9)、f_3∼N(10, 0.25)、f_4∼N(8,2.25)。最大似然的目标是找到最有可能生成已知观察数据分布的参数值。*

我生成这 10 个数据的真实分布是 f_1~N(10, 2.25)，也就是上图中的蓝色曲线。




**计算最大似然估计**

现在我们对最大似然估计有了直观的理解，我们可以继续学习如何计算参数值了。我们找到的参数值被称为最大似然估计（maximum likelihood estimates，MLE）。

我们同样将用一个例子来演示这个过程。假设这次有三个数据点，我们假设它们是从一个被高斯分布充分描述的过程生成的。这些点是 9、9.5 和 11。那么如何用最大似然估计逼近这个高斯分布的参数 μ 和 σ 呢?

我们要计算的是同时观察到所有这些数据的概率，也就是所有观测数据点的联合概率分布。因此，我们需要计算一些可能很难算出来的条件概率。我们将在这里做出第一个假设，假设每个数据点都是独立于其他数据点生成的。这个假设能让计算更容易些。如果事件（即生成数据的过程）是独立的，那么观察所有数据的总概率就是单独观察到每个数据点的概率的乘积（即边缘概率的乘积）。

从高斯分布中生成的单个数据点 x 的（边缘）概率是：
![](https://pic2.zhimg.com/v2-b423c2465c6cb48d4c19026d224edbd5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='205'></svg>)
在表达式 P(x; μ, σ) 中的分号是为了强调在分号后的符号都是概率分布的参数。所以千万不要把这个与条件概率相混淆。条件概率一般会用竖线来表达，比如说 P(A| B)。

在我们的例子中，同时观察到这三个数据点的总（联合）概率是：
![](https://pic3.zhimg.com/v2-3bf5b7676af468b37ca401cf432c2076_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='179'></svg>)
我们只要找出能够让上述表达式最大化的μ、σ值就可以了。

如果你在数学课上学过微积分，那么你可能会意识到有一种技巧可以帮助我们找到函数的最大值（和最小值）。我们所要做的就是求出函数的导数，把导函数设为零然后重新变换方程，使其参数成为方程的未知数。然后就这样，我们将得到参数的 MLE 值。我将串讲一下这些步骤，但我假设读者知道如何对常用函数进行微分。




**对数似然函数**

上述的总概率表达式实际上是很难微分，所以它几乎总是通过对表达式取自然对数进行简化。这完全没问题，因为自然对数是一个单调递增的函数。这意味着，如果 x 轴上的值增加，y 轴上的值也会增加（见下图）。这一点很重要，因为它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，我们可以用更简单的对数概率来代替原来的概率。
![](https://pic1.zhimg.com/v2-64336047372e620975fba0add5cef680_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='428'></svg>)
*原函数的单调性，左边是 y = x，右边是（自然）对数函数 y = ln(x)。*
![](https://pic3.zhimg.com/v2-bdff68ae3b0d975decd51b1a7b1eabc6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='706'></svg>)
*这是一个非单调函数的例子，因为从左至右 f(x) 会上升，然后下降，然后又上升。*




取初始表达式的对数能得到：
![](https://pic4.zhimg.com/v2-0045665d4830a9fe1c79261256a444f7_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='175'></svg>)
我们可以用对数的运算法则再一次简化这个表达式，得到：
![](https://pic4.zhimg.com/v2-a7a80654d04e58394781884299af2233_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='77'></svg>)
这个表达式可以通过求导得到最大值。在这个例子中，我们要找到平均值 μ。为此我们对函数求 μ 的偏导数，得到：
![](https://pic2.zhimg.com/v2-6ba1c1054be1814bb9abed6b2600b211_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='161'></svg>)
最后，设置等式的左边为零，然后以μ为未知数整理式子，可以得到：
![](https://pic3.zhimg.com/v2-a11ee91a9b81bc3489aed9b60326e92e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='152'></svg>)
这样我们就得到了 μ 的最大似然估计。我们可以用同样的方法得到 σ 的最大似然估计，这留给有兴趣的读者自己练习。




**最大似然估计小结**




**最大似然估计总是能精确地得到解吗？**

简单来说，不能。更有可能的是，在真实的场景中，对数似然函数的导数仍然是难以解析的（也就是说，很难甚至不可能人工对函数求微分）。因此，一般采用期望最大化（EM）算法等迭代方法为参数估计找到数值解，但总体思路还是一样的。




**为什么叫「最大似然（最大可能性）」，而不是「最大概率」呢？**

好吧，这只是统计学家们卖弄学问（但也是有充分的理由）。大多数人倾向于混用「概率」和「似然度」这两个名词，但统计学家和概率理论家都会区分这两个概念。通过观察这个等式，我们可以更好地明确这种混淆的原因。
![](https://pic1.zhimg.com/v2-19e714809e2b1b654501aabfe820ce1c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='55'></svg>)
这两个表达式是相等的！所以这是什么意思？我们先来定义 P(data; μ, σ) 它的意思是「在模型参数μ、σ条件下，观察到数据 data 的概率」。值得注意的是，我们可以将其推广到任意数量的参数和任何分布。

另一方面，L(μ, σ; data) 的意思是「我们在观察到一组数据 data 之后，参数 μ、σ 取特定的值的似然度。」

上面的公式表示，给定参数后数据的概率等于给定数据后参数的似然度。但是，尽管这两个值是相等的，但是似然度和概率从根本上是提出了两个不同的问题——一个是关于数据的，另一个是关于参数值的。这就是为什么这种方法被称为最大似然法（极大可能性），而不是最大概率。




**什么时候最小二乘参数估计和最大似然估计结果相同？**

最小二乘法是另一种常用的机器学习模型参数估计方法。结果表明，当模型向上述例子中一样被假设为高斯分布时，MLE 的估计等价于最小二乘法。

直觉上，我们可以通过理解两种方法的目的来解释这两种方法之间的联系。对于最小二乘参数估计，我们想要找到最小化数据点和回归线之间距离平方之和的直线（见下图）。在最大似然估计中，我们想要最大化数据同时出现的总概率。当待求分布被假设为高斯分布时，最大概率会在数据点接近平均值时找到。由于高斯分布是对称的，这等价于最小化数据点与平均值之间的距离。
![](https://pic3.zhimg.com/v2-ef7c7891b7431b25607e5e83dd48ca3a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='709'></svg>)
*有随机高斯噪声的回归线*




上一部分讨论了机器学习和统计模型中参数估计的最大似然法。在下文我们将讨论贝叶斯推理的参数估计，并解释该方法如何可作为最大似然法的推广，以及两者等价的条件。

阅读本文需要理解一些基本的概率论知识，例如边缘概率和条件概率。此外，了解高斯分布有助于理解，但并不是必要的。




**贝叶斯定理**

在介绍贝叶斯推理之前，理解贝叶斯定理是很有必要的。贝叶斯定理的意义在于使我们能利用已有的知识或信念（通常称为先验的）帮助计算相关事件的概率。例如，如果想知道在炎热和晴朗的天气中卖出冰淇淋的概率，贝叶斯定理可以使用「在其它类型天气中可能卖出冰淇淋数量」的先验知识。




**数学定义**

贝叶斯定理的数学定义如下：
![](https://pic2.zhimg.com/v2-e0fb5f4981f9e1141719b69f70fe4945_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='243'></svg>)
其中，A 和 B 是事件，P(A|B) 是给定事件 B 发生时，事件 A 发生的条件概率，P(B|A) 同理。P(A) 和 P(B) 分别是事件 A 和事件 B 的边缘概率。




**示例**

假定一副扑克牌里有 52 张牌，其中 26 张是红色的，26 张是黑色的。那么当牌是红色的时候，牌上数字为 4 的概率是多少？

我们将牌为数字 4 设为事件 A，将牌为红色设为事件 B。因此我们需要计算的概率是 P(A|B)=P(4|red)，接下来，我们使用贝叶斯定理计算这个概率值：

1. P(B|A) = P(red|4) = 1/2

2. P(A) = P(4) = 4/52 = 1/13

3. P(B) = P(red) = 1/2

然后根据贝叶斯定理可得到：P(4|red)=P(red|4)·P(4)/P(red)=1/13。




**为什么贝叶斯定理能结合先验信念？**

仅仅看数学公式很难理解这一点。我们将再次借用冰淇淋和天气的例子。

令 A 为卖出冰淇淋的事件，B 为天气的事件。我们的问题是「给定天气的类型，卖出冰淇淋的概率是多少？」用数学符号表示为 P(A=ice cream sale | B=type of weather)。

贝叶斯定理右边的 P(A) 被称为先验概率。在我们的例子中即 P(A = ice cream sale) 是卖出冰淇淋的边缘概率（其中天气是任何类型）。一般而言，这个概率都是已知的，因此其被称为先验概率。例如我通过查看数据了解到 100 个人中有 30 个买了冰淇淋，因此 P(A = ice cream sale)=30/100=0.3，而这都是在了解任何天气的信息之前知道的。

注意：先验知识本身并不是完全客观的，可能带有主观成分，甚至是完全的猜测。而这也会对最终的条件概率计算产生影响，我将在后面解释。




**贝叶斯推理**




**定义**

首先，（统计）推理是从数据中推导群体分布或概率分布的性质的过程。最大似然法也是同样的，如可以通过一系列的观察数据点确定平均值的最大似然估计。

因此，贝叶斯推理不过是利用贝叶斯定理从数据中推导群体分布或概率分布的性质的过程。




**使用贝叶斯定理处理数据分布**

以上例子使用的都是离散概率，有时可能需要使用连续的概率分布。即卖出冰淇淋的概率可能不只是 0.3，还可能是 0.25 或 0.4 以及其它任何可能值，每个概率对应一个先验信念，因而是一个函数 f(x)，如下图所示。该分布被称为先验分布（prior distribution）。
![](https://pic2.zhimg.com/v2-3b2812b4529209f7807e5c1d79c4c651_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='720'></svg>)



*上图中的两个分布曲线都可以作为上述例子的先验分布，其中两者的峰值都在 x=0.3 处。在 x≠0.3 处，f≠0，意味着我们并不完全确定 0.3 就是卖出冰淇淋的真实概率。蓝线表示先验概率的值更可能在 0-0.5 之间，而黄线表示先验概率可能在 0-1 之间的任何值。相对而言，黄线表示的先验分布比蓝线的「更加不确定」。*

在处理模型的时候，大部分都需要用到概率分布的形式。




**贝叶斯定理的模型形式**

模型形式的贝叶斯定理将使用不同的数学符号。

我们将用Θ取代事件 A。Θ是我们感兴趣的事件，它代表了参数的集合。因此如果要估计高斯分布的参数值，那么Θ代表了平均值μ和标准差σ，用数学形式表示为Θ = {μ, σ}。

我们用 data 或 y={y1, y2, …, yn} 取代事件 B，它代表了观察数据的集合。
![](https://pic4.zhimg.com/v2-252c02f380d619f35a9353bad8ce1d37_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='219'></svg>)
其中 P(Θ) 是先验分布，它代表了我们相信的参数值分布，和上述例子中代表卖出冰淇淋的概率分布类似。等式左边的 P(Θ|data) 称为后验分布，它代表利用观察数据计算了等式右边之后的参数值分布。而 P(data| Θ) 和似然度分布类似。

因此我们可以使用 P(data|Θ) 更新先验信度以计算参数的后验分布。




**等等，为什么忽略了 P(data)？**

因为我们只对参数的分布感兴趣，而 P(data) 对此并没有任何参考价值。而 P(data) 的真正重要性在于它是一个归一化常数，它确保了计算得到的后验分布的总和等于 1。

在某些情况下，我们并不关心归一化，因此可以将贝叶斯定理写成这样的形式：
![](https://pic1.zhimg.com/v2-d40c8d7f440086fa16b1050a1f35dc38_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='90'></svg>)
其中 ∝ 表示符号左边正比于符号右边的表达式。




**贝叶斯推断示例**

现在我们来展示一个贝叶斯推断的示例。该示例要算出氢键键长。你无需知道什么是氢键（hydrogen bond），我只是用它举例。
![](https://pic3.zhimg.com/v2-2721ec885fb5f82f4f971ef034bd2b4e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='1045'></svg>)
*我用上图因为它有助于拆分密集文本，且与我们要展示的示例有某种关联。不要担心，无需理解上图也可以理解贝叶斯推断。*




假设氢键是 3.2Å—4.0Å。该信息将构成我的先验知识。就概率分布而言，我将将其形式化为均值 μ = 3.6Å、标准差 σ = 0.2Å 的高斯分布（见下图）。
![](https://pic4.zhimg.com/v2-def28bce91936e28ce0ea1292ddc71cf_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='751'></svg>)
*氢键键长的先验分布*




我们现在选取一些数据（由均值为 3Å 和标准差为 0.4Å 的高斯分布随机生成的 5 个数据点），代表了氢键的测量长度（图 3 中的黄色点）。我们可以从这些数据点中推导出似然度分布，即下图中黄色线表示的似然度分布。注意从这 5 个数据点得到的最大似然度估计小于 3Å（大约 2.8Å）。
![](https://pic3.zhimg.com/v2-84e7af0f116343757ee484b8b00a9f66_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='780'></svg>)
*氢键长度的先验分布（蓝线），和由 5 个数据点导出的似然度分布（黄线）。*




现在我们有两个高斯分布。由于忽略了归一化常数，因此已经可以计算非归一化的后验分布了。高斯分布的一般定义如下：
![](https://pic3.zhimg.com/v2-1b8931cbf2c74b519b1593ad0fa09bf6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='205'></svg>)
我们需要将上述的两个分布乘起来，然后得到下图的粉线所示的后验分布。
![](https://pic1.zhimg.com/v2-30ec636f50d18782fa62d0571d0c3cf0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='749'></svg>)
*蓝色分布和黄色分布的乘积得到粉色的后验分布。*




现在我们得到了氢键键长的后验分布，可以从中推导出统计特征。例如，我们可以使用分布的期望值估计键长，或者计算方差以量化对结果的不确定度。对后验分布的最常用的统计计算是众数，它被用于估计感兴趣参数的真实值。在这个例子中，后验分布是一个高斯分布，因此平均值等于众数（以及中位数），而氢键长度的 MAP 估计在分布的峰值处，大约 3.2Å。




**结语**




**为什么我经常使用高斯分布？**

你将注意到在我所有涉及分布的实例中，我使用了高斯分布。其中一个原因是它使数学变的更容易。但是对贝叶斯推理实例来说，它需要计算 2 个分布的乘积。此外，因为高斯分布有一个特殊的属性，使其易于计算分布的乘积。对于高斯似然函数来说，高斯分布与它自己共轭，因此如果我把一个高斯似然函数乘以一个高斯先验分布，我将得到一个高斯后验函数。事实是后验和先验都来自相同的分布族（均为高斯），这意味着它们可被称为共轭分布。在这种情况下，先验分布被称为共轭先验。

在很多推理情景中，似然和先验被选择，从而得到的分布是共轭的，因为它使数学变的更简单。数据科学中的一个实例是隐狄利克雷分配（LDA），它是一个无监督学习算法，可以发现若干个文本文档（语料库）中的主题。




**当我们获取新数据，会发生什么？**

贝叶斯推理的最大优势之一是使用它无需有大量数据。事实上贝叶斯框架允许你有数据后实时、迭代地更新你的信念。其工作如下：你有一个关于什么的先验信念（比如参数值），接着你接收到一些数据。你可以通过计算后验分布更新你的信念，就像上面我们做的那样。随后，甚至有更多的数据进来。因此我们的后验成为新的先验。我们可以通过从新数据中获得的似然更新的新的先验，并再次获得一个新后验。这一循环可无限持续，因此你可以不断更新你的信念。

卡尔曼过滤器（及其变体）是很好的一个实例。它在很多场景中使用，可能数据科学中最醒目就是其在自动驾驶汽车上的应用。在我的数学蛋白质晶体学博士学位期间，我曾使用一种名为 Unscented 卡尔曼过滤器的变体，并为实现它们的开源软件包做出了贡献。为了更好地视觉描述卡尔曼过滤器，请查看 Tim Babb 的这篇文章：[http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/](https://link.zhihu.com/?target=http%3A//www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)。




**把先验用作 regulariser**

我们在上述氢键长度实例中产生的数据表明，2.8Å是最佳估计。但是，如果我们的估计只依据数据，则存在过拟合的风险。如果数据收集过程出现差错，这将是一个严重的问题。我们可以在贝叶斯框架中使用先验解决这一问题。在我们的实例中，使用一个以 3.6Å为中心的高斯先验得到了一个后验分布，给出的氢键长度的 MAP 估计为 3.2Å。这表明我们的先验在估计参数值时可以作为 regulariser。

先验与似然上的权重数量取决于两个分布之间的相对不确定性。在下图中我们可以看到这一点。颜色与上面一样，蓝色表征先验分布，黄色表征似然分布，粉红表征后验分布。左图中我们看到蓝线不如黄线那么延展。因此后验要远比似然更相似于先验。右图中则情况相反。
![](https://pic1.zhimg.com/v2-1980f4d185d8e051daa13472eccd06a0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='353'></svg>)
因此如果我们愿意增加参数的正则化，我们可以选择缩小与似然性相关的先验分布。




**什么时候 MAP 估计与最大似然估计相等？**

当先验分布均匀之时，MAP 估计与 MLE 相等。下图是均匀分布的一个实例。
![](https://pic3.zhimg.com/v2-3b7bec2162584eedccf02729e7467ec2_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='718'></svg>)
*均匀分布*




我们可以看到均匀分布给 X 轴（水平线）上的每个值分布相同的权重。直观讲，它表征了最有可能值的任何先验知识的匮乏。在这一情况中，所有权重分配到似然函数，因此当我们把先验与似然相乘，由此得到的后验极其类似于似然。因此，最大似然方法可被看作一种特殊的 MAP。
![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)



**本文为机器之心编译，转载请联系本公众号获得授权。**


