# CVPR 2019「准满分」论文：英伟达推出首个跨摄像头汽车跟踪数据集 - 知乎
# 



> 这篇论文主要介绍了英伟达新推出的 CityFlow（流动之城）数据集，是目前世界上第一个支持跨摄像头汽车跟踪及再识别的大型数据集，同时拥有最多的摄像头数量（40）以及最大的空间跨度（> 3 km^2），为智慧城市的解决方案提供了最好的测试平台。目前，该论文已经被 CVPR 2019 接收为 Oral 论文，拿到了 2 个 Strong Accepts 和 1 个 Accept 的「准满分」成绩。

城市利用交通摄像头作为全市范围内的传感器来优化交通流量和管理交通事故潜力巨大。但现有技术缺乏大范围跟踪车辆的能力，这些车辆跨越多个摄像机，分布在不同的十字路口，天气条件也各不相同。

要克服这一难题，必须解决三个截然不同但又密切相关的研究问题：1）单摄像头内目标的检测和跟踪，即多目标单摄像头（MTSC）跟踪；2）跨多摄像头目标重识别，即 ReID；3）跨摄像头网络对目标进行检测和跟踪，即多目标跨摄像头跟踪（MTMC tracking）。MTMC 跟踪可以看作是相机内部 MTSC 跟踪与基于图像的 ReID 的结合，连接相机之间的目标轨迹。

如图 1 所示，多目标跨摄像头跟踪包含三大组成部分：基于图片的再识别、单摄像头内的多目标跟踪以及摄像头之间的时空分析。
![](https://pic2.zhimg.com/v2-be3893b4d4dfe431c8db347bb3ffdd71_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='387'></svg>)图 1：多目标跨摄像头跟踪
相比于最近发展火热的行人再识别，车辆再识别主要面临两大挑战：一是类内部的高变化性（因为不同视角的车辆变化比人更大），二是类之间的高相似性（因为不同汽车厂商生产的车辆模型很相近）。目前已有的车辆再识别数据集（北邮的 VeRi-776、北大的 VehicleID 以及同样来自北大的 PKU-VD）都没有提供原始视频和相机校正信息，所以无法用它们开展基于视频的跨摄像头车辆跟踪研究。

本文作者提出的「流动之城」数据集包含高清的同步视频，涵盖最多的路口（10）和最大数量的摄像头（40），收集于一个中等规模的美国城市，场景也很多样，包括了住宅区和高速公路等等。本文的主要贡献有以下三点：
- 在现有数据集中，本数据集有最大的空间跨度和摄像头/路口数量，包括多样的城市场景和交通流量，为城市规模的解决方案提供了最佳平台。
- 「流动之城」也是第一个支持（基于视频的）跨摄像头多目标车辆跟踪的数据集，提供了原始视频、相机分布及相机校正信息，将打开一个全新研究领域的大门。
- 分析了各种最先进算法在该数据集上的表现，比较了各种视觉和时空分析结合的算法，证明该数据集比现有其他数据集更具挑战性。
![](https://pic3.zhimg.com/v2-d78f584a9d7abc1085a24163591e9d7e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='875'></svg>)图 2：摄像头空间分布示意图。红色箭头表示摄像头的位置和方向
**论文：CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification**
![](https://pic3.zhimg.com/v2-b119b6105666aa132adde6b4f2d53432_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='641' height='204'></svg>)
论文链接：[https://arxiv.org/abs/1903.09254](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1903.09254)

**摘要**：使用交通摄像头作为传感器的城市交通优化需要更强大的多目标跨摄像头跟踪支持。这篇论文介绍了 CityFlow（流动之城），是一个城市规模的交通摄像头数据集，包括了从 10 个路口提取的 40 个摄像头收集到的超过 3 个小时的同步高清视频，两个同步摄像头间的最长距离是 2.5 千米。据我们所知，从空间跨度和摄像头/视频数量来看，「流动之城」是目前都市环境中最大规模的数据集。该数据集包含超过 20 万个目标框，并且涵盖了多样的场景、视角、车辆模型和城市车流状况。

我们提供了相机分布和校正信息来辅助时空分析。此外，我们也提供这个数据集的子集用作基于图像的车辆再识别。我们进行了大量的实验分析，测试了各种各样的跨摄像头多目标跟踪、单摄像头多目标跟踪、目标检测和再识别的基准/最先进算法，并分析了不同的网络结构、损失函数、时空模型和它们的结合。

该数据集和线上评估服务器都已经在 2019 年的 AI 城市大赛发布（[https://www.aicitychallenge.org/](https://link.zhihu.com/?target=https%3A//www.aicitychallenge.org/)），研究者可以在服务器上测试自己的最新算法技术。我们期待这个数据集能促进该领域的研究，提升现今算法的效果，并优化现实世界的交通管理。为保护隐私，数据集中的所有车牌及人脸都进行过遮挡处理。

**「流动之城」与相关基准的对比**
![](https://pic3.zhimg.com/v2-6423cdc4378739248188a39d71399f4e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='517'></svg>)表 1：现有的目标再识别数据集的总结
可以看出，「流动之城」是目前唯一支持跨摄像头基于车辆跟踪的数据集，而且拥有目前最多的相机数量，有超过 20 万个目标框，并提供原始视频、相机分布和多视角分析。

**「流动之城」基准数据集**

整个数据集包括 5 个不同场景和 40 个摄像头，视频总长度大概 3 小时 15 分钟，标注了 666 辆车的跨摄像头轨迹。以下是这些场景的总结（部分场景摄像头有重合）。
![](https://pic3.zhimg.com/v2-dd9ac510200098e46a513366b199dd52_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='969' height='353'></svg>)
下图是车辆颜色及车型的分布情况。
![](https://pic3.zhimg.com/v2-34f7a5b675103e0b3c4e681f30fcd7f6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='555'></svg>)
以下是跟踪标注结果的示例。研究者首先采用目前先进的目标检测和单摄像头跟踪方法得到粗略的目标轨迹，并手动修复轨迹中的错误，在此基础上进行跨摄像头间的信息标注。
![](https://pic3.zhimg.com/v2-a040e180e74f13bf371952f8f9e2f8da_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
同时，他们用谷歌地图的三维信息和图像上的二维投影结果进行匹配和优化，获得了较准确的单应性矩阵，提供给参赛队伍进行三维时空分析。
![](https://pic4.zhimg.com/v2-009c4327fa0e8eedb6da167ee6975cbb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='391'></svg>)
他们的实验分析分为三大部分：基于图片的车辆再识别、单摄像头多目标跟踪和有时空分析结合的跨摄像头跟踪。

首先是再识别的部分，研究者比较了去年 AI 城市大赛上的获奖方法、行人再识别的目前最优方法（整理于伦敦大学玛丽女王学院的 deep-person-reid 项目）还有车辆再识别的最优方法（来自英伟达内部，刚被 IJCNN 录用）。下面是这几种方法的 CMC 曲线比较（包围面积越大效果越好），可以看到行人再识别和车辆再识别的方法在该数据集上不相伯仲，但是这些方法整体的精确度还是很低的，Rank-1 的命中率只有 50% 左右，相比较下目前 VeRi 数据集上同样方法能拿到 90% 以上的 Rank-1 命中率，这说明该数据集的挑战还是很大的
![](https://pic1.zhimg.com/v2-d16f2ba0ae8ca1511643ebba846a5964_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='581'></svg>)
下面是这几种方法的排名结果对比，可以看到相机的视角非常多样，也带来了更大的难度。
![](https://pic2.zhimg.com/v2-2ab2266eecb4f501c0ad49ab6eb59119_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='1398'></svg>)
下表对比了目前比较先进的单摄像头跟踪算法及目标检测方法的结合。其中 DS 代表德国科布伦茨-兰道大学的 Deep SORT，TC 是去年 AI 城市大赛上的获奖方法，MO 是目前 MOTChallenge（多目标跟踪大赛）的三维跟踪数据集上的领先方法 MOANA。目标检测部分比较了 YOLO、SSD 和 Faster R-CNN。目前最好的结果来自于 TC 和 SSD 的结合。
![](https://pic4.zhimg.com/v2-197e4b303d7187821cc1155a2fe0d2bb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='965' height='473'></svg>)
最后，下表加入了时空分析的比较，对比跨摄像头多目标跟踪的最终结果。其中 PROVID 是 VeRi 数据集作者的方法。2WGMMF 是作者实验室之前提出的方法，用高斯分布来学习摄像头之间的时空关系。最后 FVS 还是作者去年 AI 城市大赛上获奖方法的一部分，用手动来设定跨摄像头间的高斯分布，所以也更加准确一些。
![](https://pic2.zhimg.com/v2-bd980763f2b7b96d82518327959bd9b1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='374'></svg>)
**作者简介**

本文的第一作者汤政是华盛顿大学（西雅图）电子计算机工程学院的博士生，预计今年 6 月毕业。作者目前在英伟达实习，毕业后将入职亚马逊，加入无人商店「购」项目。该论文是其在英伟达实习期间的成果。

汤政在 2017 年及 2018 年两度带领自己实验室的团队参加英伟达主办的 AI 城市大赛，他们的队伍连续两届成为该项赛事冠军，击败了包括加州大学伯克利分校、伊利诺伊大学厄巴纳-香槟分校、马里兰大学帕克分校、北京邮电大学、国立台湾大学等在内的全球近 40 支队伍，其中第二届赛事是 CVPR 2018 的 workshop。因为团队的出色表现，汤政受邀进入英伟达实习，负责协助筹办第三届 AI 城市大赛（同样是今年 CVPR 2019 的 workshop）并准备基准数据集，也就是本文介绍到的「流动之城」数据集。

今年的 AI 城市大赛共有三个分赛：跨摄像头多目标车辆跟踪、基于图片的车辆再识别以及交通异常检测。目前已经有全球超过 200 支参赛队伍报名（合计超过 700 名参赛者），是前两年比赛总和的四倍之多。英伟达会在今年加州长滩的 CVPR 会议上公布获奖队伍和颁发奖品（一台 Quadro GV100、三台 Titan RTX 和两台 Jetson AGX Xavier）。目前比赛仍然接受参赛队伍报名和 workshop 投稿，比赛截止时间是 5 月 10 日。另外，论文的其他作者包括英伟达 AI 城市项目的 CTO - Milind Naphade、英伟达研究院的 GAN 领域专家 - 劉洺堉、同样来自英伟达研究院的杨晓东（今年有三篇 CVPR oral 中稿）、英伟达雷蒙德分公司的首席研究员 - Stan Birchfield、汤政的导师黄正能教授等。

汤政个人网站：[https://sites.google.com/site/zhengthomastang/](https://link.zhihu.com/?target=https%3A//sites.google.com/site/zhengthomastang/)


