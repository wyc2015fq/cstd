# GMIS 2017嘉宾李佳：从Snapchat到谷歌，人工智能领域的「Another Badass Woman」 - 知乎
# 



> 
*李佳，谷歌云机器学习/人工智能研发负责人（Head of R&D, AI/ML, Senior Director at Cloud，Google）。本科毕业于中国科大自动化系。新加坡南洋理工大学硕士。在伊利诺伊大学香槟分校、普林斯顿大学与斯坦福大学跟随李飞飞从事计算机视觉研究。2011 年获斯坦福大学计算机科学博士。曾在谷歌实习，2011 年加入雅虎，2014 年成为雅虎资深研究员，开始领导雅虎实验室的视觉计算和机器学习部门。2015 年 2 月，Snapchat 聘请李佳担任公司研发主管。2016 年加入谷歌。*


在结束乌镇的活动之后，李佳将亮相机器之心 GMIS 2017，分享精彩的主题演讲。



在乌镇，中国媒体首次见到被李飞飞描述为「斯坦福大学、计算机科学和人工智能领域里另一位女性狠角儿（Another badass woman in Stan, CS and AI）」的李佳。她参与了一场名为「AI 的未来」活动。「看到自己的技术成果能影响到各行各业，让我感觉非常兴奋。」李佳对机器之心说。之所以加入谷歌云，是因为看到很多优秀的人工智能人才聚集在几家高科技公司或者独角兽公司中，其他传统公司没有这个机会，谷歌云能够把机器学习和人工智能带给更多的公司。


去年 11 月，在谷歌 CEO Sundar Pichai 宣布谷歌战略转型后不久，作为谷歌云业务改组的一部分，公司宣布李飞飞和李佳加入谷歌，负责云业务机器学习/人工智能研发工作。公司表示，李飞飞和李佳是谷歌正式将人工智能集团业务正式化的一部分。该团队不会只专注于人工智能研究，而是致力于将尖端技术融入各种 Google Cloud 产品。


当时，人们对李佳的印象来自谷歌云业务负责人 Diane Greene 一句分量不轻的评价上——「世界领先的研究科学家，从业人员和领导者」，但相比李飞飞，她并不太善于在公开场合侃侃而谈。不过，对于人工智能技术应用的使命感，她们想法相似——云平台的数据资源和计算资源都是最大的，希望能把人工智能带到千家万户。


**一**


和李飞飞加入谷歌不同，李佳的选择并不让人感到意外。2011 年获得斯坦福大学计算机科学博士学位后，李佳一直在公司从事研究工作。人工智能领域产生的科技与人类生活的方方面面都有关。因此，人工智能业界人士应该让这个技术关系到每个人的生活，这十分重要。


2011 年，李佳加入雅虎担任研究员。雅虎拥有当时全球最有趣、最大资料库之一——相片社群网站 Flickr，网站图片数量足够拿来训练人工智能。作为 ImageNet 主要参与者之一，李佳深知数据对于深度学习技术的重要性。


2012 年，AlexNet 在 ImageNet 中获胜，惊艳世人。雅虎也在尝试不是靠图片描述或是用户输入的关键词来搜索图片，而是教会计算机如何识别照片的某种特征。李佳的工作开始有了显著进展。2013 年，李佳研发了一个算法，可以让用户从大规模 Flickr 图片集中发现具有视觉审美性的天气图片。2014 年，李佳担任雅虎实验室的计算机视觉和机器学习部门负责人。这一年，因为对台湾电子商业产品推荐和产品图像搜索所做出的贡献，李佳荣获 2014 年 Superstar Individual Award，这也是公司设立的最高奖项。每年，公司会从 1 万多名员工选出 11 名获奖者。


这一年，正好是深度学习丰收的一年。VGG、GoogLeNet 接连出现，深度学习模型在图像内容分类方面和视频分类方面有了极大的应用。深度学习不再是海市蜃楼，越来越接地气。许多曾对深度学习和神经网络都保持怀疑态度的人开始涌入深度学习。2015 年，在 1000 类的图像识别中，ResNet 超过人类水平。很多拥有丰富数据的公司开始组建自己的人工智能团队。


Snapchat 就是其中之一。这家以不断创新著称的公司打算建设一支研发团队（亦即后来的 Snap Research），通过复杂的算法对图片和视频等用户数据进行深入分析，做出更多更好玩的应用产品。李佳成为这家公司启动研发的关键人物。2015 年 2 月，李佳担任公司研发主管并领导该团队。


*![](https://pic2.zhimg.com/v2-2e0a9870de9169450f683cbc584551a1_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1213' height='652'></svg>)Snapchat 的创新*



2015 年，Snap 分别上线了「滤镜」、「赞助滤镜」（sponsored lenses）等一系列新功能都离不开 Snap 在人脸识别、3D 建模等方面获取的技术。


![](https://pic1.zhimg.com/v2-d81b53a35bec9647915b900de49fc440_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1653' height='961'></svg>)2015年，Snapchat 为 Live Stories 增加 Story Explorer 功能，用户观看某个视频时，可以通过不断滑动视频，借助其他用户的拍摄，从多个角度来观看同一事件或场景，比如，一场秀的其他情况或者一辆车的，而不仅仅局限在自己的拍摄角度里。功能的实现需要很多昂贵的计算机分析视频，进而选择要在 Story Explorer 里展示的内容。这些算法会考虑视频中的物体、递交时间和定位等数据。

![](https://puui.qpic.cn/qqvideo_ori/0/h0506653hos_228_128/0)story explorer_腾讯视频https://v.qq.com/x/page/h0506653hos.html
期间，李佳团队的 ACM 论文 *Multi-view face detection using deep convolutional neural networks* 提出了一个新的多角度面部检测方法 Deep Dense Face Detector（DDFD），这个方法不需要标注，单个基于 CNN 的模型就可以实现多角度检测面部，比当前最新的方法方便很多。


2016 年 4 月份，Snap 又推出了一种新功能 3D stickers，用户可以将 emoji 贴到视频中任何一个目标物体上，emoji 还可以一直跟踪移动的目标物体，技术也出自李佳团队。

![](https://pic2.zhimg.com/v2-78bbd58c872f5aff62555270e3395719_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1246' height='855'></svg>)![](https://pic4.zhimg.com/v2-c57ad800c15a6403affeb4f7efd1ae97_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='680' height='356'></svg>)
担任 Snapchat 研究负责人期间，李佳还担任了 2016 CVPR 产业关系（Industrial Relationship）主席，以及 Springer 的* International Journal of Computer Graphics* 的计算机副主编。


**二**


技术融入产品固然重要，但是研究成果也需要肥沃的土壤。


1998 年，贝尔实验室，LeCun 首次将 CNN 用于实践，打造出一个可以识别手写数字的系统 LeNet5。不过，后来一段时期，模型没能火起来。20 世纪 90 年代末，神经网络和反向传播被机器学习社区大量遗弃，同时也被计算机视觉和语音识别领域忽略。人们普遍认为，学习有用的、多层级的、几乎不靠先验知识的特征提取器并不现实可行。此时，李佳正从重庆一所中学进入中科大自动化系学习。


2006 年，Hinton 在深层神经网络训练上取得了突破，向学界了展示了深度学习的可靠性。他发表在* Science* 上的 *Reducing the Dimensionality of Data with Neural Networks *成为深度学习史上一个里程碑。此时，李佳已经在美国跟随李飞飞从事计算机视觉学习和研究。


> 
*这些研究人员引入无监督学习程序——无需标记数据便可创建特征检测器层。各层特征检测器的学习目标便是在下一层重构或模拟特征检测器（或原始输入）的活动。利用这种重构学习目标来「预训练（pre-training）」几层复杂度递增的特征检测器，深层网络的权重可以被初始化为合理值。接着，最终层的输出单元可被添加到网络顶端，整个深度系统可被微调至使用标准的反向传播。在识别手写数字或检测行人时，特别是当标记的数据量非常有限的时候，这一程序非常有效。*



深度学习正值爆发前夜，剩下要做的就是让世人看到深度学习的实际效果。但是，又遇到数据集这个老问题。


早在 1986 年，Rumelhart、Hinton 等人就在* Nature* 上发表 *Learning Internal Representations by Error Propagation*，将反向传播算法用于训练神经网络，使得神经网络的训练变得简单可行。但是，由于训练数据集规模太小，加上计算资源有限，训练一个较小的网络也需要很长的时间。与其它模型相比，在识别准确率上也没有明显优势，更多的学者更青睐浅层机器学习模型，比如支持向量机、Boosting。


幸运的是，与二十年前不同。2000 年以来，互联网开始大量产生各种各样的图片数据。大规模数据集也相伴而生，这为通过机器学习的方法来做计算机视觉提供了土壤。2007 年，在普林斯顿大学，李飞飞等研究人员开始着手一项庞大的任务。他们通过众包的方式，标注了 1400 万张图片，分了大概 2 万多个类别，这些类别包罗万物，比如像动物，里边可能分为鸟类、鱼类等；植物，里面可能会分为树和花。他们希望提供一个数据集，为计算机视觉算法提供一个数据土壤，让未来的机器能够认识世界上一切物品。


李佳是这个项目的主要参与者。2009 年，在斯坦福大学读博士期间，李佳以第四作者发表了论文* ImageNet: A large-scale hierarchical image database*，被 CVPR 2009 接收。论文开创了迄今为止被应用最广泛的图像数据集 ImageNet，它也是全球最大的图像识别数据库。这篇论文也是李佳本人（到目前为止）影响力最大、引用率最高的论文。


2012 年，ImageNet 遇见深度学习，结果几乎重新定义了计算机视觉研究。这一年，Hinton 和学生 Alex Krizhevsky 参加了 ImageNet。在此之前，卷积神经网络在很大程度上并未获得主流计算机视觉和机器学习团体青睐。Alex Krizhevsky 用 GPU 做出一个 Deep Learning 模型，摘取 ILSVRC 2012 桂冠，在 ImageNet 上，效果大幅度超过传统方法，从传统的 70% 多提升到 80% 多。这个 Deep Learning 模型就是 AlexNet。AlexNet 的突破，除了得益于 GPU 和算法改进，大量数据也功不可没，Deep Learning 领域应该感谢 ImageNet。在 AlexNet 获得冠军后，第二年 ImageNet ILSVRC 2013 大赛上，排名前 20 的小组使用的都是深度学习技术。如今，卷积神经网络几乎覆盖所有识别和检测任务。


李佳的领英中写着这样一句话「人工智能民主化（Democratize AI）」，或许为她在数据集建设方面的工作做了最好的总结。正如李飞飞所说，数据是民主化的另一部分，像人类一样，人工智能需要大量数据为自我发展提供洞见。因此，数据集是人工智能需要克服的最大障碍中的一个。


**三**


当被问及如何评价 Visual Genome 时，李佳说「这是升级版的 ImageNet。」数据集的搭建，往往包含着参与者对所处领域现状和未来的思考。


2010 年到 2017 年，计算机视觉研究的主要进步发生在感知领域，比如物体识别、图像标注、物体检测等，我们也有了应用产品，比如谷歌照片、行人检测系统等。接下来，更重要的课题是认知方面。如果说 ImageNet 是参与者对 2016 年之前深度学习研究状态的思考和判断，那么，接下来计算机视觉的研究方向是什么？


李飞飞认为，语言中的很大一部分都是有关描述视觉世界的。视觉在交流和语言等方面占据着很重要的地位，我们讲故事、辨别事物、区分视频类别都是在进行图像识别。因此，将计算机视觉和语言结合在一起会非常有趣。「一个人坐在办公室里，但布局是怎样的，那个人是谁，他在干什么，周围有什么物体，在发生着什么事？」


在分析更复杂场景的任务中，深度学习很有可能将扮演关键角色。但是，技术会对数据提出更多的要求。因为理解一个视觉场景（visual scene）不只是要理解单独的一个个物体。物体之间的关系也能提供丰富的有关这个场景的语义信息（semantic information）。


> 
*尽管感知任务方面进展显著，比如图像分类，但是，计算机仍无法很好完成认知任务，比如图像描述以及问答。认知不仅仅是识别任务的核心，对视觉内容进行推理也离不开认知。但是，在认知任务中，被用来理解图像丰富内容的模型的训练数据集，仍然是那些被设用来解决认知任务的数据集。然而，要在漂亮完成认知任务，模型就要搞理解图像中物体之间的关系。比如，当被问道「这个人骑的什么交通工具？」，计算机需要识别出图像中物体，以及骑（人，车）与牵引（马，车）的关系，这样才能做出正确的回答：人坐马车。*



谷歌、Facebook 和其它公司正在推进人工智能算法解析视觉场景的能力。2014 年，谷歌发布的研究展示了一种能够在多种精度上为图像提供基本说明的算法。Facebook 也展示过一个能够回答有关图像的基本问题的问答系统。有趣的是，2017 年 IJCAI 计算机和思想奖获得者 Devi Parikh 也是从事 VQA 研究。


「我们也正在搭建（从理解）到语言的桥梁，因为交流的方式并不是将数字分配到像素上——你需要将感知和认知与语言连接起来。」2016 初，李佳参与了 Visual Genome 项目（*Visual genome: Connecting language and vision using crowdsourced dense image annotations*），旨在帮助研究人员为这种关系建模。研究人员收集了物体、特征、以及关系的密集标注，用来学习这些模型。较之于 ImageNet（也由 Stanford 大学维护），Visual Genome 图像标签更为丰富，包括名字、图片的不同细节，以及在对象和动作信息之间的关系，语义信息更丰富，可用以拓展更加丰富的基于图像及语义信息的人工智能应用。


> 
*目前，这个数据集包括 108249 张图片、420 万区域内容描述（Region Descriptions）、170 万图像内容问答（Visual Question Answers）、210 万对象案例（Object Instances）、180 万属性（Attributes）、180 万关系（Relationships）。我们规范转化了从物体、属性、关系、区域描述里的名词短语和问答对到 WordNet 同义词集的关系。这些注释代表了图像描述、物体、属性、关系和问答里最密集、规模最大的数据集。*



这是第一个能够提供结构化地对图片进行形式化表示的数据集，在这种形式下能够大量用于 NLP 的基于知识的展示中。「这个数据集为两种模式结合与新模型测试提供了一个新的可扩展的方法。」


可以想见，它们还能训练计算机了解物理可能的概念或不可能发生的概念，从而让计算机拥有更多常识。Richard Sochar 说，这可能就是这一项目中最重要的一面。


*![](https://pic4.zhimg.com/v2-94614ba66a1cd9c0ec762799a9aa456f_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1024' height='768'></svg>)Visual Genome 是一个数据集，知识库，不断努力把结构化的图像概念和语言连接起来*


和建立了两个被人工智能研究者广泛用来教会机器分类物体的数据库的李飞飞相似，李佳也着手了另一个重要多媒体数据库的建立。2016 年，李佳 ACM 论文* YFCC100M: the new data in multimeesearch* 介绍了 Yahoo Flickr Creative Commons 100 Million Dataseta（YFCC100M）数据集，这是一个包含令人关注且科学上实用的数据集的参考库，也是有史以来最庞大的公共多媒体集合，包含总计 1 亿个媒体对象（大约 9920 万张照片，80 万个视频）和标签，它们全都是上传至 Flickr 并根据 CC 商用或非商用许可证发布。与许多数据集一样，YFCC100M 也处于不断演进之中。YFCC100M 数据集克服了影响现有多媒体数据集的许多问题，例如在形态、元数据、许可以及最主要的体积大小方面。


**四**


除了数据集方面的重要贡献，李佳在图像识别和场景理解领域也做出了自己的贡献。下图给出了 Semantic Scholar 统计的李佳的学术影响力和影响关系，可以看到，李飞飞和李佳的研究合作关系非常紧密。

![](https://pic4.zhimg.com/v2-24050c59618568832cefb067de6c101b_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1671' height='749'></svg>)

李佳发表过的其它一些重要论文，涉及高层特征分析、场景理解等多个方向。尤其是在 2010 年，带有李佳的姓名的论文出现在了 NIPS、ECCV、CVPR 等顶级学术会议上，其中一些研究结果达到了当时的最佳水平，甚至有的已经成为了后来进一步研究的常用方法。


比如在 NIPS 2010 上，李佳等人（其他作者包括李飞飞、邢波）在论文 *Object Bank: A High-Level Image Representation for Scene Classification and Semantic Feature Sparsification *中提出的 Object Bank 描述方法。这种方法的主要思想是在更高的层面上提取出一张图片中尽可能多的物体的信息，将其组合起来，使其有助于场景的分类（尽管低水平特征在场景分类以及物体识别中有着比较好的应用，但其包含的语义信息却更少）。用 Object 作为特征，计算图像对不同特征的响应，并根据其响应情况进行分类。该方法不仅能够用于图像分类，还能说明图像中存在哪些种类的 Object。

![](https://pic2.zhimg.com/v2-9095bb489f5e0e2ee7d0f0b76b9ec3c1_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1469' height='615'></svg>)
而在复杂场景理解方面，李佳等研究者的另一项 NIPS 2010 研究 *Large Margin Learning of Upstream Scene Understanding Models *试图带来一些进步。在这篇论文中，研究者提出了一种最大边界（max-margin）和最大似然学习（max-likelihood learning）的联合方法来解决复杂场景的理解问题，其中隐主题发现和预测模型估计是紧密耦合的，并且得到了很好的平衡。通过使用一个变分 EM 流程，这个优化问题得到了有效的解决，其能迭代式地解决在线损失增强的 SVM。


另外在场景分类方面。李佳等研究者在 ECCV 2010 的论文* Objects as Attributes for Scene Classification *中提出了将物体用作场景属性以进行场景分类的方法。这些表征带有高层面语义信息，使其更加适合高层面的视觉识别任务。该研究表明我们可以使用支持向量机等已有的分类器来在物体层面上进行场景分类等高层面的视觉任务。在当时的标准下，该方法的表现超越了之前的其它方法。


场景中的内容也是李佳的研究目标之一。在 CVPR 2010 上，李佳第一作者的论文 *Building and Using a Semantivisual Image Hierarchy *提出了一种通过整合图像和标签信息来自动发现「语义视觉（semantivisual）」图像层次的新方法。该方法可结合图像的语义和视觉信息来有效创建图像层次，且相对于其它的分层方法，本论文所提出的语义视觉层次更有意义、更精确。


**结语**


在刚刚结束不久的谷歌 I/O 开发者大会上，Google Lens 引发不少关注。除了拍照识物，还能随便扫描一家餐馆，然后自动在 Google 的数据库里找到对应资料显示出来，包括点名、菜式、评分、打烊时间等。Google Photos 也变得更加聪明。利用机器学习技术，它能提醒用户自动与照片中的人共享照片 ，据说未来还将整合 Google Lens 的图像识别能力，提供一系列新特性，比如识别照片中的电话号码。


这些技术的发展和应用会继续改善我们的生活，李佳为计算机视觉领域所做出的基础性贡献也仍然将继续推动这一领域的发展，其女性身份也在激励着新一代女性进入计算机科学等前沿研究领域和更广泛地参与到科技领域的发展进步中。


谷歌云业务负责人 Diane Greene 曾说，最令人欣喜的一点是李飞飞和李佳均为女性。在人工智能领域，各大高校和技术公司的重要研究岗位上一直缺乏女性。她们两人代表着人工智能热门领域的、世界领先的研究科学家、实践者和领导者。

**机器之心原创**


