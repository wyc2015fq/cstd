# 人工智能重塑芯片设计 - 知乎
# 



> 技术增加了芯片粒度，但随着架构需要处理更大数据量，设计起点也变了。

作者：ED SPERLING；来源：semiengineering；编译：孙茜茜、张玺。

随着架构师开始利用 AI 提高性能和降低功耗，并为未来芯片的开发、制造和更新奠定基础，人工智能也开始影响半导体设计。

人工智能、机器学习、深度学习可以极大改善芯片某项特定功能的控制和性能。架构师既可以基于现有设备进行分层设计，也可以整合到新设计中，以实现更多功能或某个特定功能。

人工智能技术带来很多好处，比如：

通过稀疏算法或数据压缩来改变特定函数精度，增加粒度，提高芯片性能和降低功耗。

识别数据模式而不是单个比特，有效提高计算的抽象性，增加软件密度。

允许以矩阵的形式执行处理和内存读/写操作，大大加快操作速度。

但是我们也需要好好反思如何在芯片上或者在芯片之间迁移（或不迁移）数据。毕竟，无论是用于边缘计算还是数据中心，训练还是推断，需要加以处理和存储的数据量都是最大的。

## **新起点**

从好的方面来说，通过使用更多更低精度的元素，人工智能提供了一种平衡结果精度与准度的方法。比如，语音识别对精度的要求，远没有自动驾驶中安全应用与目标识别严苛。根据特定需要而展现的自适应能力，才是人工智能的价值所在。

与其说人工智能的起点是硬件和软件，不如说是数据的质量、数量和迁移。这需要用一种不同的方式来看待设计，包括过去通常没有合作的团队之间的协作。

「计算真的很便宜，压缩/解压数据也很便宜，但在内存中存储和加载数据却一点不便宜。要构建这些系统，需要特定领域的专家、机器学习专家、优化与性能专家，这三个领域的专家都需要。」Arm 研究员杰姆•戴维斯（Jem Davies）表示。

他指出，机器学习可以影响系统中的所有东西，其中很多东西隐藏在视线之外。「有些是用户看不见的，」戴维斯说，「它被用来延长电池寿命。相机里也有机器学习。」

采用神经形态计算和不同的内存架构，AI 效果最好，因为在这些情况下，数据可以进行矩阵处理。为达到最优工作状态，除了对处理器有要求外，还需要良好的系统架构、超大的数据吞吐量及内存变化过程中的数据对齐。

「许多架构改进是软硬件的结合，虽然不一定会提高单个处理器的整体性能，但会更节能，内存效率也更高。缩小一点，内存大小就能减半。」铿腾电子（Cadence）音频和语音 IP 产品市场总监 Gerard Andrews 表示。

实际上，这使得软件设计密度更高，并加速了数据在内存中的移动。「问题是，内存不会有效地收缩，单词识别的错误率正在上升，我们都在探索算法的稀疏性，以降低功耗、提高性能。」Andrews 说。

这仅仅是快速变革的皮毛。

「内存子系统中发生的变化是不连续的、突然的。这一切与延迟和带宽，以及如何满足芯片内外的庞大需求有关。由于需要大量数据管道，因此，我们开发了许多关于如何移动数据的架构。在此之前，你要考虑的是添加多少内存，如何高效使用内存。但是，现在要建造巨大的管道，较少地使用内存。」Achronix 的系统架构师 Kent Orthner 说。

尝试减少数据流量的新方法之一是脉冲神经网络。它们不是持续发射信号，而是以类似大脑峰值的方式发射信号。

「脉冲神经网络是下一代神经网络。」BrainChip 营销和业务开发高级副总裁 Bob Beachler 说，「卷积使用线性代数。出现峰值时，数据以尖峰的形式输入。你可以通过尖峰进行训练，如果有很多尖峰，可以选择加强或抑制它们。对于专用于训练阈值的位，你可以用非常低的权重来做到这一点。」

据估计，约有 70 家人工智能初创公司正在研究不同办法。最重要的是，几乎所有主要的芯片制造商、IP（知识产权）供应商和工具公司都有涉足 AI 的某个方面。
![](https://pic3.zhimg.com/v2-5391d54b9de7a03eb3c3f37cd102c60a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='368'></svg>)数据压缩，资料来源：谷歌
## **人工智能的风险和困惑**

但是，人工智能也存在一定的风险，这取决于应用程序和精确度。

过去，电子系统的设计是建立在完全可预测性的逻辑之上的，其中大部分是硬连线。人工智能用可接受行为的分布代替了计算精度，人们也在会议上讨论这对芯片设计意味着什么。目前尚不清楚的是，现有工具或方法提供的置信度能否满足设备需求，特别是在系统遭到破坏或退化的情况下，检测任何异常行为的速度如何。

对于如何应用人工智能，人们也有一定困惑。有专门为人工智能设计的芯片，以及一些不是专门为 AI 开发但可用于 AI 的芯片，对这些芯片进行修改和叠加后，就能更有效地利用人工智能。

总的来说，这符合人工智能的主题，全行业都在争相以相同或更低功率来提升性能。根据摩尔定律，在 16/14 纳米工艺后，每个节点的能耗和性能的提高比例都下降到 20%，因此，大家都在寻找替代或补充的新方法。

对于针对 AI 训练或推理的芯片，或者芯片中发挥 AI 能力的处理器和加速器，人们的普遍共识是，不同量级的程序指令可能使用不同的芯片架构。但它不适用于所有情况，还有一些变量，比如训练数据的大小和价值，它们可能会使 AI 在某些应用程序中失效，而在其他情况下，性能提升 100 倍甚至被认为过于保守。

这就是人们要花很长时间才能把一些新架构推向市场的原因。随着芯片行业初见端倪，人们也在进行大量的架构探索和实验。

「应用程序和算法都面临挑战，处理器和内存芯片也面临挑战。」Synopsy 的战略营销经理 Ron Lowman 说：

「这使得对 AI 架构的探索变得更加重要，这也是 CCIX（缓存一致性互联加速器）变得如此流行的原因之一。探索新架构的客户越来越多。每个人都在尝试建立人脑仿生的新架构。

除此之外，有一些新的非易失性存储器技术正在开发中。还有一种趋势是，将更小的处理器置于较小的存储器旁边，有时，这种处理器会与针对不同数据类型、定制的新型加速器相关联。另外，还有很多关于数据压缩和量化的工作。

「人们正在研究从 32 位浮点数到 8 位浮点数，」Lowman 说，「现在的问题是，你是否能精确到单比特量化。」

量化涉及到将一大组输入值映射到一小组输出值，最大的问题是，什么是可接受的精度损失。

理论上，有了足够传感器或数据输入，就可将错误率的影响降到最低，但这非常依赖于应用程序。

沿着这些思路的另一种方法涉及到源代码同步，特别是针对数据中心的 AI 芯片，促使芯片的网络拓扑结构发生变化。网络中的所有目标都是接收相同的数据，较之广播，使用多播方法能更好地针对性使用数据。

「通过多播，可以向多个目的地发送一封邮件。」Arteris IP 的营销副总裁 Kurt Shuler 说，「它通常被用来做权重。好处是，你可以更好地利用片上网络带宽，因此路上的车也越来越少了。

AI 芯片有一个问题：它们往往非常大。「最大的问题是时钟树，」Shuler 说，「这需要同步通信，异步处理通信会占用很多空间。另外，大型芯片更容易出现路由堵塞。解决这个问题的方法是创建虚拟通道连接，减少线路数量并通过一组线路共享通信。这就需要通过仲裁来匹配数据流。」
![](https://pic2.zhimg.com/v2-55b3632b79dd839bb54a4b5188b73c6d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='974' height='922'></svg>)芯片上的映射端口，资料来源：Arteris IP
## **计划性淘汰**

这只是设计的一部分。另一方面，还要保持算法的时效性。

目前，深度学习算法还在定期更新，这会影响到 AI 芯片添加何种处理器。每一次变化，都可能对芯片内部数据迁移、以及处理这些数据的处理器造成影响。

CPU 和 GPU 具有软件可编程性，DSP 和 FPGA 具有固件/硬件可编程性。嵌入式 FPGA 将可编程性直接添加到 SoC 或多芯片包中。

处理器的选择也取决于终端市场的应用。例如，汽车或工业环境中的重要安全应用，也需要有足够通用性与反应性，以便与其他车辆或设备兼容。

「当我们讨论未来时，问题不在于它是否有效。」eSilicon 的创新高级主管 Carlos Macián 说，「TPU（张量处理单元）是一个开拓者，它表明性能可以得到数量级提高。但是对于新的工作负载，如果没有 ASIC 的优化，你可能只会提高 3 倍。」

前提是，假设数据是干净、有用的，这也是情况变得复杂的地方。

「AI 非常适用于处理非结构化的数据，」Macián 说，「如果你给出现在 Facebook 上的人打标签，你就知道这很适合人工智能。但它不是结构化数据。所以，AI 天生就不准确，有时它还是错的。」

并非所有事情都要面向未来。在一些市场，比如手机，消费者希望每隔几年就更换一次手机。在其他市场，电子产品被寄予厚望——全部的功能能够顺畅运行二十年之久。

提高数据质量是有帮助的，这有助于解释算法为何变化如此之快，也有助于解释为什么对于一些设备而言，现场升级的能力至关重要。但是，这些变化也会影响性能，如果不在硬件中添加一些可编程性，就无法解释这些变化。问题是，可编程性有多高，因为可编程逻辑明显慢于（软件）已调优的硬件。

## **结论**

与其他许多成长型半导体市场不同，AI 是一种横向技术，可以应用于各种垂直市场，也可以用来为这些市场开发芯片，还可以用来提高现有芯片的效率。

这只是 AI 革命的开始，但其影响已经很大了。

随着设计团队越来越精通这项技术，这将对如何设计芯片、以及这些芯片如何与其他芯片交互产生重大影响，也会给工具开发人员、硬件开发人员、软件开发人员创造新的机会，也可能带来一个全新的市场。
![](https://pic3.zhimg.com/v2-193584c0dd32013f8c819c0fac3ca39e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)
原文链接：[https://semiengineering.com/ai-begins-to-reshape-chip-design/](https://link.zhihu.com/?target=https%3A//semiengineering.com/ai-begins-to-reshape-chip-design/)




