# CVPR论文经不起复现推敲？是学术会议水了还是我飘了 - 知乎
# 



机器之心报道。

> 很多人工智能学者都在抱怨目前的 AI 顶级会议接收论文数量大幅膨胀，论文质量也显著下降，其中有一部分经不起复现的推敲。最近，在 Reddit 上一个「较真」的网友就对 CVPR2018 的一篇大会接收论文进行复现，发现了其中的问题。此贴在 Reddit 上引发了众人的热烈讨论，其中包括对学术会议同行评审机制的审视。
![](https://pic2.zhimg.com/v2-573ede28207e768d505bc25fdf2f7525_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='503'></svg>)
po 主在 Reddit 中称，「《Perturbative Neural Networks》一文提出用 1x1 卷积代替 3x3 卷积，输入中应用了一些噪声。作者称这种做法表现良好，但我的复现结果并不理想，因此我决定对其进行测试。作者提供了他们用的代码，但是经过仔细检查，我发现他们的测试准确率计算有误，导致得出的所有结果无效。」
- 原论文链接：[https://arxiv.org/abs/1806.01817](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1806.01817)
- po主测试地址：[https://github.com/michaelklachko/pnn.pytorch](https://link.zhihu.com/?target=https%3A//github.com/michaelklachko/pnn.pytorch)




**CVPR 论文复现发现问题**

下面，先让我们细致过一遍 po 主在复现这篇论文时发现的问题，他主要发现测试准确率计算无效，因此加噪声或采用 1×1 的卷积也没有更好的效果。

原始实现在第一层上使用常规卷积，其余各层使用 fanout=1，这意味着每个输入通道都被一个噪声掩码扰乱。

然而，原始实现最大的问题在于测试准确率计算有误。作者没有使用常用的计算方法算出测试集中正确样本与全部样本的比值，而是在以每批为基础计算准确率，并应用平滑权重（测试准确率=0.7 * prev_batch_accuracy + 0.3 * current_batch_accuracy）。

以下是论文中所用方法与正确准确率计算的比较：
![](https://pic1.zhimg.com/v2-f9338519bb0e76abe60667d109d4643c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1004' height='711'></svg>)
运行这个模型（在 CIFAR10 上训练 noiseresnet18），原始代码得出的最佳测试准确率为 90.53%，而实际最佳测试准确率为 85.91%。

在纠正了这个问题之后，我进行了大量的测试，想看看扰乱的输入和噪音覆盖会不会带来什么好处，但最终我得出的结论是：并没有。

下面的例子展示了几个 ResNet-18 变体的不同：降低过滤器数量以保证相同参数数量的基线模型；除了第一层，其它层都只使用 1×1 卷积（无噪音）的模型；以及除第一层外所有层使用扰动后接 1×1 卷积。这三个模型大约都有 550 万个参数：
![](https://pic4.zhimg.com/v2-45f610b805dab55c4dffcfad14fb210b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1008' height='685'></svg>)
*在整个训练过程中，常规 resnet 基线模型和 PNN 之间的准确率差值保持在 5% 左右，并且添加噪声掩码比没有任何噪声的等效「受损」resnet 提升不到 1 %。*




作者对源代码，尤其是 PerturbLayer class 进行了一些修正，包括 --nmasks、--mix_maps 和--train_masks 等等。最后，作者使用了基线模型 resnet18 和原实现中的 noiseresnet18。此外，perturb_resnet18 在使用合适的参数下有足够的能力替代基线模型与 noiseresnet18。

CIFAR-10：
- 基线测试准确度：91.8%；采用带有 3×3 卷积的常规 ResNet-18，卷积核的数量降低到与 PNN 参数量相当的水平。
- 原始实现的测试准确度：85.7%；相当于下载原始 GitHub 实现项目的代码并运行。
- 与原始实现相同，但是将 first_filter_size 修正为 3 会将准确度提升到 86.2%。
- 与上面实现相同，但是不带任何噪声会实现 85.5% 的测试准确度；resnet18 在第一层使用 3×3 的卷积，在后面会保留 1×1 的卷积。
- 在 PNN 的所有层级使用均匀噪声，测试准确度为 72.6%。
- 在 PNN 除第一层外的所有层使用噪声掩码，测试准确度为 82.7%；其中第一层使用常规的 3×3 卷积。
- 与上面实现相同，但是使用—no-unique_masks 参数，这表示相同一组掩码会作用于每一个输入通道，测试准确率为 82.4%
- 训练噪声掩码，测试准确率为 85.9%，噪声掩码每一个批量更新一次，与常规模型参数同时更新。

**问题与实验结论**

原论文中的第 3.3 节和 3.4 节阐述了如何通过找到 PNN 参数来复现常规 CNN 的输出。问题在于，它只适用于单个输入 x。因此，对于任何给定的输入 x，PNN 可能找到计算正确输出 y 所需的权重，但这并不代表它可以为数据集中的所有输入样本找到权重。

直觉告诉我，PNN 似乎缺乏常规 CNN 最主要的特征提取特性：它不能通过卷积核直接匹配任何空间模式。

似乎用噪声干扰层输入没有任何显著的好处，不带噪声掩码的简单 1x1 卷积性能没有显著提升。不管我们如何应用噪声掩码，使用 1x1 卷积造成的准确率损失都很严重（即使不修改第一层，在 CIFAR-10 上的准确率也要下降约 5% )。由于准确度计算方法有误，作者发表的结果无效。




**作者回应与 Reddit 讨论**

其实，在收到反馈后，该论文作者也对 po 主提出的问题作出了积极回应：

> 我是原文一作。大约三周前，我们才知道这件事，并展开了调查。我感激 Michael 复现 PNN 论文并把这个问题带给我们。我们计划透彻分析下这篇文章，在十分确定之后给出进一步的回应。可视化器中光滑函数的默认配置是疏忽点，我们进行了修改。如今，我们正在重新运行所有实验，并将把最新结果更新到 arXiv 论文与 Github 上。此外，如果分析表示我们的结果远不如 CVPR 论文中报告的，我们会撤回此论文。话虽如此，基于我的初步评估，在他的实现中选择合适的过滤器、噪声等级和优化方法，我目前能够在 CIFAR-10 上达到 90%-91% 的准确率，用的也是他选择的参数。但没有仔细过一遍之前，我也不想谈论太多。

Felix Juefei Xu 的回应得到了大家的支持与认可，但同样也引出了另一个问题：同行评审流程中对实验结果的复现。也许，这才是背后的关键。




**用户 RoboticGreg 表示：**

> 总的来说，论文评审过程不包含复现实验结果。根据我的经验，评审者不得不在很大程度上依靠作者的诚信。评审者经常自问，作者是否令人信服地、准确地陈述了问题，并根据实验的结构和他们提出的结果得出了正确的结论。

代码错误、精细的实验过程中存在的问题通常在论文发表之后才会被发现，就像本文中提到的这一案例。




**用户 tkinter76 也认为：**

> 我认为（CVPR2018 的）论文评审者根本没有运行代码，原因如下：
- 实现有时不会随论文一起提交
- 把一切都弄妥当太耗时间
- 通常人们缺乏在合理的时间内重新运行所有步骤所需的资源

因此同行评审通常更关心论文中描述的方法。潜在的解决办法是要求作者提交现成的实现（如通过 docker）。然而，在哪里运行仍然是一个问题。也许 AWS 资源等可以从提交费用中提取，供评审人员重新运行模型。然后，问题是确保评审人员不会「滥用」资源进行他们自己的实验等。在任何情况下，「通过计算的方法」进行 DL 论文评审都很棘手。

其实，学术社区已经注意到这一问题。去年 ICML 学术会议上，「机器学习复现 Workshop」就对这一问题进行过讨论，并于 2018 年举办了 ICLR 2018 复现挑战赛，目的就是保证接收论文公布的结果是可靠的、可复现的。
![](https://pic2.zhimg.com/v2-e14398fe62163f8b5f974e8788073649_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='228'></svg>)
在机器学习愈发火热的今天，如何在论文提交与参会人数激增的情况下保证接收论文与大会的质量，是学术会议面临的巨大挑战，也急需社区能给出可行的解决方案。

参考链接：[https://www.reddit.com/r/MachineLearning/comments/9jhhet/discussion_i_tried_to_reproduce_results_from_a/](https://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/9jhhet/discussion_i_tried_to_reproduce_results_from_a/)

[https://github.com/michaelklachko/pnn.pytorch](https://link.zhihu.com/?target=https%3A//github.com/michaelklachko/pnn.pytorch)




