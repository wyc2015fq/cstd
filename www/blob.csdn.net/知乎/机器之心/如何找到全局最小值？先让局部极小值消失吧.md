# 如何找到全局最小值？先让局部极小值消失吧 - 知乎
# 



> 通过添加一个神经元，就可以让一种二分类深度神经网络的损失函数局部极小值消失。早在去年 5 月就有人发现了这个有趣的结果，MIT 将其进一步推广到了更广泛的损失函数类。
选自arXiv，作者：Kenji Kawaguchi、Leslie Pack Kaelbling，机器之心编译，参与：Geek AI、刘晓坤。

目前，深度神经网络在计算机视觉、机器学习和人工智能等领域取得了巨大的实际成功。然而，从理论上对深度神经网络的理解相对于其在经验上的成功来说是较为缺乏的。在理论上，理解深度神经网络的一个主要难点是用于训练网络的目标函数的非凸性以及高维度。由于非凸性和高维度，能否保证深度神经网络在训练过后具有理想的性质，而不是陷入一个随机的糟糕的局部极小值点附近，往往还不清楚。实际上，寻找一个通用的非凸函数（Murty & Kabadi, 1987）以及用于训练特定种类神经网络的非凸目标函数（Blum & Rivest, 1992）的全局极小值是 NP-Hard 的问题，这引起了研究人员对高维问题的关注（Kawaguchi et al., 2015）。在过去，这类理论问题被认为是人们偏向于选择只需要进行凸优化的经典机器学习模型（无论带不带有核方法）的原因之一。尽管深度神经网络近来取得了一系列的成功，但始终绕不开一个问题：能否在理论上保证深度神经网络避开糟糕的[局部极小值](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650756878%26idx%3D4%26sn%3D8767c5a32b92b275c2e85dbfcf7a3030%26chksm%3D871a9370b06d1a66a3ea39aa346477ddf7e00796cf43b10259d552ce9f60946ab80c3157edcf%26token%3D1881952741%26lang%3Dzh_CN)点？

近来，有许多研究分析了神经网络的训练中目标函数的变化情况和局部极小值。一些研究在假设深度神经网络被显著简化（Choromanska et al.，2015；Kawaguchi，2016；Hardt & Ma，2017）和显著过参数化（Nguyen & Hein，2017；2018）的条件下取得了积极的结果。对于仅仅包含一个隐藏层的浅层网络，已经取得了许多积极的结果，但往往带有很强的假设，例如，需要使用显著的过参数化处理、简化处理和高斯化的输入（Andoni et al., 2014; Sedghi & Anandkumar, 2014; Soltanolkotabi, 2017; Brutzkus & Globerson, 2017; Ge et al., 2017; Soudry & Hoffer, 2017; Goel & Klivans, 2017; Zhong et al., 2017; Li & Yuan, 2017; Du & Lee, 2018）。

除了强假设之外，深度神经网络中渐渐出现了两种明显的积极结果。首先，某些深度神经网络在损失处于所有局部极小值点时的性能已经被证明并不亚于相应的经典机器学习模型在全局最小值点的性能（Shamir, 2018; Kawaguchi & Bengio, 2018; Kawaguchi et al., 2018）；通过不可忽略的残差表征（Kawaguchi & Bengio, 2018）以及对网络深度和宽度的增加，即使并没有经过显著的过参数化处理，也能够保证模型性能可以被进一步提升。其次，研究表明，增加一个神经元可以为一个带有特定类型的平滑的铰链损失（hinge loss）函数（Liang et al., 2018）的二分类器消除所有的次优局部极小值（即不是全局最小值的局部极小值）。第二种类型的结果已经被多篇不同的论文提及（Nguyen et al., 2018; Wang et al., 2018; Zhang et al., 2018）。然而，由于假设二分类问题带有特定的损失函数，目前这种技术还不适用于许多常见的深度学习任务，而这也正是该技术的一个主要的局限性（如 Fessler, 2018 的论文所述）。

在本文中，作者证明了，在没有任何强假设的情况下，对于带有任意损失函数的多分类、二分类以及回归任务，为每个输出单元增加一个神经元可以消除所有次优局部极小值。据作者所知，这是第一个在没有任何典型的未满足的假设的情况下，能够保证许多常见的深度学习任务没有次优局部极小值的结果。此外，作者还展示了用这种方法消除次优局部极小值的局限性。

**论文：Elimination of All Bad Local Minima in Deep Learning**
![](https://pic3.zhimg.com/v2-d5dd5fd23480c00ba116a6b3d79833ee_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='202'></svg>)
论文地址：[https://arxiv.org/abs/1901.00279](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.00279)

**摘要：**本文从理论上证明了，对于带有一个任意的损失函数的多分类、二分类以及回归问题，我们可以通过向任意深度神经网络的每个输出单元添加一个神经元，消除所有的次优局部极小值。在任意加入了神经元的深度神经网络的每一个局部极小值处，可以保证原神经网络（不增加神经元）的参数设置可以使原神经网络处于全局极小值。本文证明了，新加入的神经元的作用效果可以自动地在每个局部极小值消失（从而不影响神经网络的输出）。与先有工作中的许多相关结果不同，我们的理论结果直接适用于常见的深度学习任务，因为这些结果仅仅依赖于在常见任务中自然成立的假设。此外，我们还提供了额外的理论结果和几个例子，讨论了以这种方式消除次优局部极小值的几个限制。
*![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)*





