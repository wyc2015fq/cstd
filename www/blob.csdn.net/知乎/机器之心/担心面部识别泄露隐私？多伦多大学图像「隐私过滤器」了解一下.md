# 担心面部识别泄露隐私？多伦多大学图像「隐私过滤器」了解一下 - 知乎
# 



选自neuro science news，机器之心编译，参与：李诗萌、张倩。

> 随着面部识别系统越发成熟，个人隐私问题也引发了越来越多的担忧。多伦多大学的研究人员利用对抗式训练的深度学习技术开发了一种新的算法，这种算法可以动态地扰乱面部识别系统，有助于保护用户隐私。研究者表示，他们的系统可以将可检测的面部比例从原先的近百分之百降低到 0.5%。

每当用户将照片或视频上传到社交媒体平台时，这些平台的面部识别系统都会对用户有一定的了解。这些算法会提取包括用户的身份、所在地以及认识的人在内的数据，而且还在不断提升。

随着对社交网络隐私和数据安全的担忧不断增加，Parham Arabia 教授和研究生 Avishek Bose 带领多伦多大学工程部的研究人员创建了一种可以动态扰乱面部识别系统的算法。

Aarabi 认为，「当面部识别系统做得越来越好时，个人隐私就成为了一个真正的问题。这种反面部识别的方法可以有力地保护个人隐私。」

他们的算法利用了所谓对抗式训练的深度学习技术，这种方法使两种人工智能算法相互对抗。Aarabi 和 Bose 设计的方法中有两个神经网络：第一个用来进行面部识别，第二个用来扰乱第一个做出的面部识别任务。这两个网络不断对抗，也不断地相互学习，从而开始了一场持续的 AI 竞赛。

这场竞赛的结果是建立了一个与 Instagram 有些相似的过滤器，这种过滤器可以应用在照片上从而达到保护隐私的目的。该算法改变了图像中的特定像素，做出了一些人眼几乎察觉不到的变化。Bose 说，「扰乱性 AI 可以『攻击』面部识别神经网络正在识别的东西。例如，如果识别性 AI 正在识别眼睛的角落，扰乱性 AI 就会对这个部位做出几乎无法察觉的调整。它在照片中创造了一些非常微妙的干扰，但是却足以欺骗系统。」Aarabi 和 Bose 在 300-W 面部数据集上测试了他们的系统，300-W 是一个包含 600 多张面部图像的产业标准库，这些面部图像来自不同的种族，照明条件及环境也有所不同。研究者表示，他们的系统可以将可检测的面部比例从原先的近百分之百降低到 0.5%。

该项目的主要作者 Bose 说：「这个项目的重点在于训练两个相互对抗的神经网络——一个用来创建越来越强大的面部识别系统，另一个用来创建更强大的、用来禁用面部检测系统的工具。」该团队的研究将于今年夏天在 2018 年 IEEE 国际多媒体信号处理研讨会（International Workshop on Multimedia Signal Processing）上发布。
![](https://pic1.zhimg.com/v2-c3855718f8f9f2073ff1f821b639e5b4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='500'></svg>)多伦多大学工程部的研究人员设计了一个用于扰乱面部识别算法的「隐私过滤器」。该系统依赖于两个基于 AI 创建的算法：一个用于连续进行面部识别，另一个用于对第一个进行扰乱
除了禁用面部识别外，这项新技术还可以扰乱基于图像的搜索、特征识别、情感和种族评估以及其他自动提取面部属性的功能。

接下来，该团队希望隐私过滤器可以以 app 或网页的形式为大众所用。

Aarabi 说：「十年前，这些算法还需要人为定义，但是现在神经网络已经可以自行学习了——除了训练数据，无需提供其他东西。最终，它们可以做出一些真正了不起的东西。在这个领域中这是一段非常有趣的时光，而且这个领域还有很大的潜力有待发掘。」




**论文：Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization**
![](https://pic2.zhimg.com/v2-a55b377ac2d59392f2490cb6c7f057b1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='500'></svg>)
论文链接：[https://joeybose.github.io/assets/adversarial-attacks-face.pdf](https://link.zhihu.com/?target=https%3A//joeybose.github.io/assets/adversarial-attacks-face.pdf)

摘要：本文所述算法通过对抗式攻击在输入中添加几乎无法察觉的扰乱，从而达到使机器学习模型对输入进行错误分类的目的。尽管在图像分类模型中已经提出了许多不同的对抗式攻击策略，但一直难以打破目标检测的途径。本文作者提出的新策略可以通过使用对抗式生成器网络解决约束优化问题，制作对抗的例子。该方法快速而且可拓展，只需要通过训练好的生成器网络的正向通路制作对抗性样例。与许多攻击策略不同的是，本文所述的相同的训练后的生成器可以攻击新图像但不会明显优化它们。文中用 300-W 面部数据集对训练好的 Faster R-CNN 面部识别器结果进行了评估，本文所述方法成功将面部检测数降低到原始面部检测数的 0.5%。同样是用 300-W 数据集，我们还在不同的实验中证明了我们的攻击对基于 JPEG 压缩图的防御的鲁棒性，在 75% 的压缩等级的情况下，我们的攻击算法的有效性从 0.5%（可检测的面部比例）降低到 5.0%。




