# 6D目标姿态估计，李飞飞夫妇等提出DenseFusion - 知乎
# 



选自 arxiv，作者：Chen Wang 等，机器之心编译，机器之心编辑部。

> 根据 RGB-D 图像进行 6D 目标姿态估计的一个主要技术挑战是如何充分利用两个互补数据源——颜色和深度。为此，李飞飞夫妇等研究者提出了 DenseFusion——一种可单独处理两个数据源的异质架构。实验表明，DenseFusion 在 YCB-Video 和 LineMOD 两个数据集上的表现都优于当前最先进的方法。此外，研究者还将该方法应用于真实的机器人上，使其可以根据估计的姿态来抓取和操纵目标。

**1 引言**

6D 目标姿态估计对许多重要的现实应用都很关键，例如机器人抓取与操控、自动导航、增强现实等。理想情况下，该问题的解决方案要能够处理具有各种形状、纹理的物体，且面对重度遮挡、传感器噪声、灯光条件改变等情况都极为稳健，同时还要有实时任务需要的速度。RGB-D 传感器的出现，使得在弱灯光环境下推断低纹理目标姿态的准确率比只用 RGB 方法的准确率更高。尽管如此，已有的方法难以同时满足姿态估计准确率和推断速度的需求。

传统方法首先从 RGB-D 数据中提取特征，完成对应的分组和假设验证。但是，对手动特征的依赖和固定的匹配程序限制了它们在重度遮挡、灯光变化环境下的表现。近来在视觉识别领域取得的成果激发了一系列数据驱动方法，即使用 PoseCNN[40] 和 MCN [16] 这样的深度网络对 RGB-D 输入做姿态估计。

但是，这些方法需要精心制作后分析微调步骤，从而完整利用 3D 信息，例如 PoseCNN 中生成的高级定制的迭代最近点（ICP）和 MCN 中多视角假设验证规划。这些微调步骤不能与最终目标函数联合优化，在现实应用中也极为缓慢。在自动驾驶中，有一种第三方解决方案被提出，它能够通过 Frustrum PointNet[22] 和 PointFusion[41] 这样的端到端深度模型很好地利用 RGB-D 数据中颜色和深度信息的补充性质。在驾驶场景中，这些模型取得了非常好的表现，也有很好的实时推理能力。但是，根据经验可知，这些方法在重度遮挡环境下不符合标准，这是实际操控领域中非常常见的一种情况。

在本文中，研究者提出一种端到端的深度学习方法，对 RGB-D 输入的已知物体进行 6D 姿态估计。该方法的核心是在每个像素级别嵌入、融合 RGB 值和点云，这和之前使用图像块计算全局特征 [41] 或 2D 边界框 [22] 的研究相反。这种像素级融合方法使得本文的模型能够明确地推理局部外观和几何信息，这对处理重度遮挡情况至关重要。此外，研究者还提出了一种迭代方法，能够在端到端学习框架中完成姿态微调。这极大地提高了模型性能，同时保证了实时推理速度。

研究者在两个流行的 6D 姿态估计基准——YCB-Video 和 LineMOD 上评估了他们的方法。结果表明，在经过 ICP 改进后，该方法的性能超越了当前最佳的 PoseCNN，其姿态估计准确率提高了 3.5%，推断速度提高了 200 倍。值得一提的是，这一 dense fusion 新方法在高度凌乱的场景中表现出了鲁棒性。最后，研究者还在一个真实的机器人任务中展示了它的用途，在这项任务中，机器人估计目标的姿态并抓取它们以清理桌面。

总而言之，本文的贡献主要分为两个方面：首先，研究者提出了一种将 RGB-D 输入中的颜色和深度信息结合起来的原则性方法。他们利用为该任务学习的嵌入空间中的 2D 信息来增加每个 3D 点的信息，并使用这个新的颜色深度空间来估计 6D 姿态。其次，他们在神经网络架构中集成了一个迭代的微调过程，消除了之前的后处理 ICP 步骤方法的依赖性。

**论文：DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion **
![](https://pic1.zhimg.com/v2-abedd7f045fc29a64fed5ce003b4c444_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='264'></svg>)
论文地址：[https://arxiv.org/abs/1901.04780](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.04780)

**摘要**：根据 RGB-D 图像进行 6D 目标姿态估计的一个主要技术挑战是如何充分利用两个互补数据源。先前的研究要么分别从 RGB 图像和深度中提取信息，要么使用代价较高的后处理步骤，限制了它们在高度混乱的场景和实时应用中的性能。在本文中，研究者提出了 DenseFusion。这是一个用于估计 RGB-D 图像中已知目标 6D 姿态的通用框架。DenseFusion 是一种异质架构，可单独处理两个数据源，并且使用新颖的 dense fusion 网络来提取像素级的密集特征嵌入，从中估计姿势。此外，研究者整合了端到端的迭代姿态微调程序，该程序进一步改善了姿态估计，同时实现了近实时推理。研究实验表明 DenseFusion 在 YCB-Video 和 LineMOD 两个数据集上的表现优于当前最先进的方法。研究者也将该方法应用于真实的机器人上，使其可以根据估计的姿态来抓取和操纵目标。

**3 模型**

研究者的目标是在混乱场景的 RGB-D 图像中估计出一组已知目标的 6D 姿态。通常情况下，将 6D 姿势视为齐次变化矩阵，p ∈ SE(3)。换句话说就是，6D 姿态是由旋转 R ∈ SO(3) 和平移 t ∈ R 3 , p = [R|t] 组成的。既然是从拍摄图像中对目标进行 6D 姿态的估计，那么目标姿态就要相对于相机的坐标框架来定义。

要想在不利的条件下（例如，重度遮挡，光线不足等）估计已知目标的姿态，只有结合颜色和深度图像通道中的信息才有可能。但是，这两个数据源是不同空间的。因此，从异质数据源中提取特征并把它们恰当地融合在一起是这个领域中的主要技术挑战。

研究者通过以下方式来应对这一挑战：（1）一个能够分别处理颜色和深度图像信息并且可以保留每个数据源原始结构的异质框架；（2）一个通过利用数据源间的内在映射融合颜色-深度图像信息的密集像素级融合网络。最后，姿态估计可以通过可微分的迭代微调模块进一步微调。相较于昂贵的事后微调步骤，本文中的微调模块能够和主架构一起训练，并且只会占用总推理时间的一小部分。

**3.1 架构概览**
![](https://pic2.zhimg.com/v2-db1f6eec16be542370677b8f3b87dad5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='982' height='496'></svg>)
图 2. 本研究中的 6D 姿态估计模型概览。该模型从 RGB 图像中生成目标分割掩码和边界框。深度图中的 RGB 颜色和点云被编码为嵌入并在每个对应的像素上进行融合。

上述模型的架构主要包含两个阶段。第一个阶段将彩色图像作为输入，为每个已知的目标类别执行语义分割。接下来，对于每个分割后的目标，研究者将掩码深度像素（转换为 3D 点云）及掩码边框裁剪的图像块导入到第二阶段。

第二个阶段处理分割的结果并估计目标的 6D 姿态。它包含四个部分：a）一个处理颜色信息的全卷积网络，该网络将图像块中的每个像素映射到一个颜色特征嵌入中；b）一个基于 PointNet 的网络，该网络将带有掩码的 3D 点云中的每个点处理为一个几何特征嵌入；c）一个像素级的 fusion 网络，该网络将两个嵌入结合起来并基于无监督置信度得分输出目标的 6D 姿态估计；d）一个迭代的自微调方法，该方法以课程学习的方式对网络进行训练，并迭代地微调估计结果。a、b、c 见图 2。d 见图 3。
![](https://pic4.zhimg.com/v2-b2916634c4449532cc77a0138a7e275b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='663' height='332'></svg>)图 3：迭代姿态微调。研究者引入了一个网络模块来改进迭代过程中的姿态估计
**4 实验**

在实验部分，研究者想解答以下几个问题：1）dense fusion 网络与单纯的整体 fusion-by-concatenation 相比如何？2）dense fusion 和预测方案对重度遮挡和分割误差是否鲁棒？3）迭代微调模块能够改善最终的姿态估计吗？4）本文的方法对下游任务（如机器人抓取）来说是否足够鲁棒和高效？

为了回答前面三个问题，研究者在两个具有挑战性的 6D 目标姿态估计数据集上评估了这一方法：YCB-Video 数据集 [40] 和 LineMOD [12] 数据集。YCB-Video 数据集包含不同遮挡程度、形状和纹理级别的目标。因此它对该抗遮挡、多模态融合的方法来说是一个理想的试验台。LineMOD 数据集是一个广泛使用的数据集，允许我们与更多现有方法进行比较。研究者将本文的方法与最先进的方法 [14, 30] 以及模型变体进行比较。为了解答最后一个问题，研究者在真实的机器人平台上部署其模型，然后在使用该模型预测的任务中评估了机器人在抓取任务中的性能。
![](https://pic3.zhimg.com/v2-beeb7a028f63f517574465cf70fd17d6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='530' height='390'></svg>)图 1：研究者开发了一个端到端的深度网络模型，用于根据 RGB-D 数据进行 6D 姿态估计。该模型能够在实时应用（如机器人抓取和操控）中进行快速准确的预测![](https://pic3.zhimg.com/v2-bb0877fa3b3dcefb770bf68bce4b1b56_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='629'></svg>)表 1：在 YCB-Video 数据集上对 6D 姿态（ADD-S[40]）进行定量评估。加粗的目标是对称的。![](https://pic2.zhimg.com/v2-0489dca51087c34c98940e378debdc29_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='586' height='370'></svg>)图 5：遮挡程度不断加大时模型性能的变化。通过计算图像帧中每个目标不可见表面的百分比来估计遮挡程度。与基线方法相比，本文的方法在重度遮挡的情况下表现更稳健


![](https://pic2.zhimg.com/v2-96bc0e2d087da3d419e4f7e12f0dcdd1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='572' height='95'></svg>)
表 3：运行时分解（YCBVideo 数据集上每帧的秒数）。本文的方法几乎比 PoseCNN+ICP 快了 200 倍。Seg 表示 Segmentation（分割），PE 表示 Pose Estimation（姿态估计）。
![](https://pic4.zhimg.com/v2-e30361e8f182a7802110bd946fcbe627_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='522'></svg>)
图 4：在 YCB-Video 数据集上的定性结果。用和 PoseCNN 中相同的分割掩码对三种方法进行测试。不同颜色中的每个目标点云通过预测的姿态来变换，然后投影到 2D 图像帧上。上两行用的是以前的 RGB-D 方法，最后一行用的是本文提出的 dense fusion 与迭代微调方法（迭代 2 次）。
![](https://pic3.zhimg.com/v2-1280d72a1174a5f641bc36ea91b533ca_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='925' height='476'></svg>)
表 2：在 LineMOD 数据集上对 6D 姿态（ADD[13]）进行定量估计。加粗的目标是对称的。
![](https://pic4.zhimg.com/v2-23141eef626e419f84bfa8c8f2faaa5f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='573' height='291'></svg>)
图 6：在 LineMOD 数据集上的迭代微调性能。可视化该研究中的迭代微调程序如何纠正最初的次优姿态估计。
*![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)*





