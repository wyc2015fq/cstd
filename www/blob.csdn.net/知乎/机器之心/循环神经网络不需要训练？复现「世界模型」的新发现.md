# 循环神经网络不需要训练？复现「世界模型」的新发现 - 知乎
# 



选自GitHub，作者：Corentin Tallec、Léonard Blier、Diviyan Kalainathan，机器之心编译。

> 由谷歌大脑研究科学家 David Ha 与瑞士 AI 实验室 IDSIA 负责人 Jürgen Schmidhuber（他也是 LSTM 的发明者）共同提出的[「世界模型」](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650740072%26idx%3D1%26sn%3D9c7a1fb39de042c3488ed06f1dae896a%26chksm%3D871ad116b06d58005ade4798c9fee43b8d514d9149f1c47bdd951b016c01f6910b7097681d2d%26scene%3D21%23wechat_redirect)可以让人工智能在「梦境」中对外部环境的未来状态进行预测，大幅提高完成任务的效率。这篇论文在今年 3 月份出现时引起了人们的热烈讨论。本文深入探讨了这篇基于模型的强化学习的研究，该论文在颇具挑战的 CarRacing-v0 环境中的表现令人惊喜。

除此之外，研究人员还大胆提出了一个问题：循环网络的训练真的有必要吗？

除简介之外，作者还提供了该论文的 PyTorch 实现及额外的实验，以探讨训练过程在循环网络中的重要程度。
- PyTorch 实现：[https://github.com/ctallec/world-models](https://link.zhihu.com/?target=https%3A//github.com/ctallec/world-models)




**「世界模型」总结**

论文《World Models》介绍了一种基于模型的强化学习方法，主要围绕模型的三个部分来介绍：
- 变分自编码器（VAE, Kingma et al., 2014），这是一种可以学习编码器和解码器的生成模型。编码器的任务是将输入图像压缩为一种紧密的潜在表征。解码器的任务是从潜在表征中还原原始图像。
- 混合密度循环网络（MDN-RNN, Graves, 2013），训练用于在给出之前的潜在编码和动作的情况下预测下一帧的潜在编码。混合密度网络输出高斯混合，用于预测下一次观测的分布密度。
- 简单线性控制器（C）。它将当前帧的潜在编码和给定之前编码和动作的 MDN-RNN 的隐藏状态作为输入，并输出动作，经过训练后，它能够利用一种通用的黑箱优化算法——协方差矩阵自适应进化策略（CMA-ES，Hansen，2006）来将累积奖励最大化。

论文中解释这一结构的图解如下：
![](https://pic3.zhimg.com/v2-d83947c56783f43cc57da439e4ff0e3e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='936' height='617'></svg>)
在给定环境下，模型按以下步骤进行训练：
- 对从恰当的随机策略中生成的 rollout 进行随机采样。
- 在从 rollout 中得到的图像上训练 VAE。
- 在使用 VAE 编码器编码的 rollout 上训练 MDN-RNN。为了减少计算量，我们在 rollout 的固定大小子序列上训练 MDN-RNN。
- 使用 CMA-ES 在与环境交互的同时训练控制器。在每个时间步上，控制器将编码的当前帧和 MDN-RNN 的循环状态作为输入，MDN-RNN 包含关于所有先前帧和动作的信息。

或者，如果 MDN-RNN 在建模环境方面足够优秀，则控制器可以直接在梦境中的模拟 rollout 上进行训练。




**在 CarRacing 环境中的复现性**

在 CarRacing-v0 环境中，结果相对容易复现。我们惊喜地发现，相对于深度强化学习算法通常的复现性标准，该模型在第一次尝试中取得了良好的结果。我们自己的实现达到的最高分是 860 分，低于该论文报道的 906 分，但比第二优基准的 780 分要好得多。我们认为，结果的差距与我们降低了的计算能力有关，导致 CMA-ES 的超参数不如世界模型那篇论文中的那么合适。下图展示了我们训练的最好模型的行为。
![](https://pic2.zhimg.com/v2-b78c89e53b9ad7393e638886bf8f49cd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='788'></svg>)
**额外的实验**

我们想测试 MDRNN 对结果的影响。事实上，我们在训练过程中发现，该模型能够快速学习动态的简单部分，但大部分没有考虑到长期效应和多模态性。

在原始论文中，作者将其结果与没有 MDRNN 的模型进行比较，并获得以下分数：
![](https://pic4.zhimg.com/v2-7426256873287402bcd578bbe2c1bc5f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='992' height='276'></svg>)
我们做了一个额外的实验，在不训练 MDRNN 的情况下测试完整的「世界模型」架构，并保持其随机初始权重。结果如下：
![](https://pic3.zhimg.com/v2-da870ac78e8b485c69125cab8711cb62_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='944' height='270'></svg>)
我们展示了我们训练出的最优模型（使用未经训练的 MDRNN）的行为：
![](https://pic2.zhimg.com/v2-055ca88bd3126a984ca2610cb7121b21_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='788'></svg>)
MDRNN 的训练似乎并没有提升性能。我们对这一现象的解释是：即使循环模型并不能预测环境的下一个状态，但它的循环状态依然包含一些关于环境动态的关键信息。如果没有循环模型，则单个帧中就不包含车速等一阶信息，如此一来，隐藏代码中也不会包含这些信息。因此，在没有 MDRNN 的情况下学习到的策略就无法利用这些信息。很明显，即使是一个随机的 MDRNN 也包含一些有用的时间信息，这对于学习一个解决该问题的优秀策略来说已经足够。




**结论**

我们在 CarRacing 环境中复现了论文「世界模型」提出的方法，并进行了一些新的实验。总的来说，我们得出了两大结论：
- 「世界模型」的结果很容易复现。这很可能意味着解决该问题的方法不仅表现良好，而且非常稳定。这一点对于评价深度强化学习方法来说至关重要。
- 在 CarRacing-v0 环境中，似乎循环网络只能作为循环库来使用，能够为访问高阶信息提供路径，如速度和加速度。这一观察需要一些论点的支持，以下是对此的一些评论：
-   在「世界模型」的论文中，作者在 VizDoom 模拟环境中进行训练时报告了不错的结果。如果没有经过训练的循环前向模型，我们无法期待可以获得这样的结果。
-   而在 CarRacing-v0 上，未经训练的 MDRNN 已经可以达到接近最佳的结果。是因为这个任务太过简单以至于无需好的循环前向模型吗？
-   学习一个高维的环境，训练一个好模型很难。在长时间的范围内（如预测两种未来：一个是左转，一个是右转），获取连贯的多模态行为显然是困难的。在视觉上，除了隐高斯混合模型（latent gaussian mixture），我们的模型看起来并没有克服这个困难。正确地处理多模态行为对于利用「世界模型」的可用性至关重要吗？
![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)
原文链接：[https://ctallec.github.io/world-models/](https://link.zhihu.com/?target=https%3A//ctallec.github.io/world-models/)


