# 千人签署阻止「杀人机器」承诺，马斯克、哈萨比斯位列其中 - 知乎
# 



**选自Guardian，作者：Ian Sample，机器之心编辑部。**

> 在斯德哥尔摩举行的 IJCAI 2018 大会上，众多学者与从业者们共同签署了阻止致命性人工智能武器的承诺，Google DeepMind 的联合创始人丹尼斯·哈萨比斯和 SpaceX 的首席执行官埃隆·马斯克位列其中。该承诺誓言：绝不将人工智能技术应用于自主杀人机器的研发。
![](https://pic2.zhimg.com/v2-76da36b1316a272ea08f9ec5b6e7f8ad_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='620' height='372'></svg>)一名男子在武器交易会上走过武装机器人系统。摄影：Brendan Smialowski/ Bloomberg /Getty Images
数千名专门从事人工智能（AI）的科学家宣称，他们将不参与开发或制造能够在无人监督的情况下识别和攻击人类的机器人。

 Google DeepMind 公司的丹尼斯·哈萨比斯和美国火箭公司 SpaceX 的埃隆·马斯克与 2400 多人和 170 多个组织共同签署了一项承诺书，旨在阻止军事公司和各个国家建立致命性自主武器系统。签署协议的其他人包括 Yoshua Bengio、Jeff Dean、Stuart Russell 等人；而 UCL、NNAISENSE 和 ElementAI 等大学和研究机构也位列其中。

完整名单：[https://futureoflife.org/lethal-autonomous-weapons-pledge/?cn-reloaded=1&cn-reloaded=1](https://link.zhihu.com/?target=https%3A//futureoflife.org/lethal-autonomous-weapons-pledge/%3Fcn-reloaded%3D1%26cn-reloaded%3D1)

此举是相关科学家和组织的最新举措，旨在强调将生死交付给 AI 机器是极度危险的事情。接下来是呼吁颁布技术禁令，参与此活动的人认为，AI 技术可能会导致新一代大规模杀伤性武器的出现。

该承诺书由总部位于波士顿的未来生命研究所精心策划，要求各国政府商定规范、法律和法规，以有效扼制杀人机器人的发展。在今天没有此类措施的情况下，签署方承诺「既不参与也不支持致命性自主武器的研发、制造、交易或使用。」今天斯德哥尔摩的人工智能联合会议（IJCAI）宣布，超过 150 个与 AI 相关的公司和组织在此承诺书中签署了他们的名字。

来自蒙特利尔算法学习研究所的 AI 先驱 Yoshua Bengio 告诉《卫报》，如果这项承诺能够让那些制造自主武器的公司和军事组织自惭形秽，舆论也不会倾向于他们。他说：「尽管美国等主要国家没有签署禁雷条约，但由于国际条约和公众的约束，这种做法实际上在禁雷上已起到作用。美国公司已经停止制造地雷。」Bengio 签署了承诺书，表示会「强烈关注致命性自主武器。」

军方是人工智能技术的最大资助者和采纳者之一。有了先进的计算机系统，机器人可以在敌方领域执行任务，在地面导航，在海底巡逻。更先进的武器系统正在酝酿中。周一，英国国防部长 Gavin Williamson 公布了一项 20 亿英镑的新皇家空军战斗机计划——Tempest，使战斗机不需要飞行员操控。承诺书签署者呼吁各国政府采取更多措施，规范和限制这种自主杀人机器的使用，担心各国将展开一场失控并威胁世界稳定的军备竞赛。他们警告说，自主杀人机器可能比「核生化武器」更加危险，因为军备竞赛很容易失控，脱离国际组织的管理。他们指出，政府监管还不够完善，无法应对如此严重的威胁。

对于许多研究人员来说，让机器决定人的生死跨越了道德底线。

英国的部长们表示，英国不会发展致命性自动武器系统，其部队将始终监督、控制其部署的武器。但活动人士警告说，人工智能和其他领域的迅速发展意味着，人类现在可以制造未经人类同意就自主识别、跟踪和射击人类目标的尖端武器。对于许多研究人员来说，让机器决定人的生死跨越了道德底线。

「我们需要创造一项国际惯例，拒绝接受自动武器。」悉尼新南威尔士大学（University of New South）的人工智能教授 Toby Walsh 在承诺书上签了字，他说:「人必须永远遵守这项惯例」。

他补充说:「「我们不能阻止意志坚定的人制造自动武器，就像我们不能阻止意志坚定的人制造化学武器一样。」但如果我们不希望无赖国家或恐怖分子轻易就能获得自动武器，我们必须确保武器公司不公开出售这些武器。」

研究人员可以选择不做自主武器研究，但其他人如何利用他们的研究突破就不在他们的控制范围之内了。兰开斯特大学（Lancaster University）科学与技术人类学教授 Lucy Suchman 也签署了这份承诺书，他指出，虽然研究人员无法决定别人如何利用他们的研究成果，但如果他们对该技术抱有疑虑时，他们可以介入并干预。

她说:「如果我是一名签署了承诺书的机器视觉研究人员，我会首先承诺追踪自己所研发技术的后续用途，并公开反对将这些技术应用于自动化目标识别，其次拒绝直接协助将这些技术用于自动武器系统的研究，并拒绝为其提供建议。」

除了伦理问题之外，许多批评者担心这些武器还可能易受黑客攻击，最终进入黑市或落入 ISIS 等恐怖组织手中。

人工智能的发展引起了许多人的担忧，包括埃隆·马斯克。2017 年 9 月，马斯克发推特称人工智能可能引发第三次世界大战。此外，马斯克还敲响了人工智能的警钟，称未来几十年内，人工智能将「全面击败人类」，并称其为人类面临的「最大风险」。
![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)
原文链接：[https://www.theguardian.com/science/2018/jul/18/thousands-of-scientists-pledge-not-to-help-build-killer-ai-robots](https://link.zhihu.com/?target=https%3A//www.theguardian.com/science/2018/jul/18/thousands-of-scientists-pledge-not-to-help-build-killer-ai-robots)




