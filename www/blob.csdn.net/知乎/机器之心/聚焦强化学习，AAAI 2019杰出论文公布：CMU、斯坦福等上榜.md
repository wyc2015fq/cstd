# 聚焦强化学习，AAAI 2019杰出论文公布：CMU、斯坦福等上榜 - 知乎
# 



> 2019 年首场 AI 顶会 AAAI 2019 即将开幕，本文介绍此届大会的获奖信息，包括：杰出论文奖、杰出学生论文奖、经典论文奖、蓝天理念奖、Feigenbaum 奖、杰出程序委员会成员等。

AAAI 2019，进入 2019 年后人工智能领域的首场顶会，将于明天正式拉开序幕；而今天正有无数学者跨越山川海洋，乘坐数十小时的飞机陆续抵达美国夏威夷首府檀香山。

正如大家所熟知，今年论文提交数量高达 7745 篇，创下了 AAAI 历史新高；而同时论文录取率仅有 16.2%，也创下 AAAI 的历史新低。但一成不变的是，AAAI 2019 将毫无疑问成为新年首场人工智能的盛宴，值得每一位 AI 研究人员瞩目关注。

目前 AAAI 2019 的各项奖项已全部公布，包括：杰出论文奖、杰出学生论文奖、经典论文奖、蓝天理念奖、Feigenbaum 奖、杰出程序委员会成员等。

**杰出论文奖（Outstanding Paper）**

今年 AAAI 的杰出论文同样比较关注强化学习，它们代表了高标准的技术贡献和阐述。
- 论文：How to Combine Tree-Search Methods in Reinforcement Learning 
- 作者：Yonathan Efroni、Gal Dalal、Bruno Scherrer 和 Shie Mannor
- 机构：以色列理工学院、法国国家信息与自动化研究所
- 论文地址：[https://arxiv.org/abs/1809.01843](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.01843)




**杰出论文荣誉提名奖（Honorable Mention）**
- 论文：Solving Imperfect-Information Games via Discounted Regret Minimization 
- 作者：Noam Brown、Tuomas Sandholm
- 机构：卡内基·梅隆大学
- 论文地址：[https://arxiv.org/abs/1809.04040](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.04040)




**杰出学生论文奖**
- 论文：Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference
- 作者：Mike Wu、Milan Mosse、Noah Goodman 和 Chris Piech
- 机构：斯坦福大学
- 论文地址：[https://arxiv.org/abs/1809.01357](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.01357)




**杰出学生论文荣誉提名奖**
- 论文：Learning to Teach in Cooperative Multiagent Reinforcement Learning 
- 作者：Shayegan Omidshafiei、Dong-Ki Kim、Miao Liu、Gerald Tesauro、Matthew Riemer、Christopher Amato、Murray Campbell 和 Jonathan P. How
- 机构：麻省理工学院、IBM 研究院、美国东北大学
- 论文地址：[https://arxiv.org/abs/1805.07830](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.07830)




**经典论文奖（Classic Paper）**

今年的经典论文奖颁给了 2002 年提交到 AAAI 的优秀论文，该论文发现当时基于协同过滤和基于内容的推荐系统各有优缺点，因此他们提出了一种新的框架以结合两种方法来完成推荐任务。论文的获奖词为「为推荐系统中基于内容和协同过滤的方法提供互补性框架而获奖」。
- 论文：Content-Boosted Collaborative Filtering for Improved Recommendations 
- 作者：Prem Melville、Raymond J. Mooney 和 Ramadass Nagarajan 
- 机构：德克萨斯大学奥斯汀分校
- 论文地址：[https://www.cs.utexas.edu/~ml/papers/cbcf-aaai-02.pdf](https://link.zhihu.com/?target=https%3A//www.cs.utexas.edu/~ml/papers/cbcf-aaai-02.pdf)




**2019 年 Feigenbaum 奖**

AAAI Feigenbaum 奖旨在表彰和鼓励通过计算机科学实验方法取得的杰出人工智能研究进展。2019 年的奖项授予加州大学伯克利分校的 Stuart Russell，以表彰他在概率知识表示、推理和学习上的创新与成就。

**2019 年蓝天理念奖（Blue Sky Idea）**

AAAI 与计算机研究协会计算社区协会（CCC）合作，从众多论文遴选出三篇提名为「蓝天奖」，这些论文提出了可以激发研究界寻求新方向的想法和愿景，例如新问题、新应用领域或新方法。包括：
- 第一名：Explainable, Normative, and Justified Agency（Pat Langley）
- 第二名：Building Ethically Bounded AI（Francesca Rossi、Nicholas Mattei）
- 第三名：Recommender Systems: A Healthy Obsession（Barry Smyth）




**AAAI-19 杰出程序委员会成员**

每年，AAAI 都会选出几位得到认可的杰出程序委员会成员，基于其在达成共识决策时表现出的判断力、清晰度、知识丰富度和领导力。

今年 AAAI 选出了 10 名获奖的程序委员会成员，其中来自华中科技大学的白翔被授予杰出高级程序委员会奖，来自南京航空航天大学的黄圣君被授予杰出委员会奖。
![](https://pic1.zhimg.com/v2-d17d4f48ee3cf92cd22e6381d2d8eadc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='533' height='521'></svg>)
**获奖论文简介**

如下展示了四篇杰出论文和一篇经典论文的摘要，读者可以了解它们大致都描述并解决了什么问题。因为机器学习和深度学习常见的概念涉及得比较少，所以这几篇论文看起来就很「强大」，阅读这些论文可能还需要额外的背景知识。

**论文：How to Combine Tree-Search Methods in Reinforcement Learning**

摘要：有限时域前瞻策略（Finite-horizon lookahead policies）被大量用于强化学习，并得到了令人印象深刻的实证成果。通常，前瞻策略是使用特定的规划方法实现的，例如[蒙特卡罗树搜索](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650756158%26idx%3D1%26sn%3Dc6d15f41e2616b0425606c34b16cacee%26chksm%3D871a9040b06d195662e73b723213bd2bfd896958b6c095de5ef2cf2cbbcd3e008e514482e3ee%26scene%3D38%26key%3D814ec3562518101fe22e73e84a0f0af9fede5355a6065f7f5a0b835bb5508d0d7516ea26dc2137fa03e28d8372f833e5ffd15f42a40c258bb8184f7b2419e0f42db0df0d050600a94dec4c8117e0b070%26ascene%3D7%26uin%3DMTE2NDA4MjczOQ%253D%253D%26devicetype%3DWindows%2B10%26version%3D62060619%26lang%3Dzh_CN%26pass_ticket%3D9T6Ehy%252BLuD4uwrjfH6xZ%252BcjIM%252Fl2bggS%252BfuoWSIIFxzJqCDLbu5eRduoncGrZLcP%26winzoom%3D1)（例如在 AlphaZero 中）。这些实现中有一种合理的做法是将规划问题视为树搜索，其仅在叶节点处备份值，而在根节点下获取的信息不用于更新策略。在本文中，我们对这种方法的有效性提出质疑。即，后一个过程通常是非收缩的，并且其收敛性不能保证。

我们提出的增强方法是简单明了的：使用最佳树路径的返回值来备份根节点的后代的值。这导致了一个γ ^ h 收缩过程，其中γ是折扣因子（discount factor），h 是树深度。为了实现我们的结果，我们首先介绍一种称为多步贪婪一致性（multiple-step greedy consistency）的概念。然后，在存在树搜索阶段和值估计阶段的注入噪声的情况下，我们展示了上述增强方法的两个算法实例的收敛速率。

**论文：Solving Imperfect-Information Games via Discounted Regret Minimization**

摘要：反事实后悔最小化（Counterfactual regret minimization / CFR）是一系列迭代算法，是最受欢迎、也是实际上也是逼近解决大型不完美信息博弈的最快方法。在这篇论文中，我们介绍了一种新型 CFR 变体，它能：1) 以各种方式从早期迭代中贴现后悔值（regrets），且在某些情况下对于正后悔值和负后悔值是不同的；2) 以各种方式重新加权迭代而获得输出策略；3) 使用非标准后悔值最小化器；4) 利用「optimistic regret matching」。

这种变体能在许多环境中显著提升性能。首先，我们在每一个测试的博弈中引入一个优于 CFR+（先前最先进的算法）的变体，这些测试博弈还会包含大规模现实设定。其中 CFR+是一种强大的基准：还没有其他算法能够超越它。最后，我们表示很多重要的新变体与 CFR+不同，它们与现代不完美信息博弈的剪枝技术相兼容，并且还与[博弈树](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650756158%26idx%3D1%26sn%3Dc6d15f41e2616b0425606c34b16cacee%26chksm%3D871a9040b06d195662e73b723213bd2bfd896958b6c095de5ef2cf2cbbcd3e008e514482e3ee%26scene%3D38%26key%3D814ec3562518101fe22e73e84a0f0af9fede5355a6065f7f5a0b835bb5508d0d7516ea26dc2137fa03e28d8372f833e5ffd15f42a40c258bb8184f7b2419e0f42db0df0d050600a94dec4c8117e0b070%26ascene%3D7%26uin%3DMTE2NDA4MjczOQ%253D%253D%26devicetype%3DWindows%2B10%26version%3D62060619%26lang%3Dzh_CN%26pass_ticket%3D9T6Ehy%252BLuD4uwrjfH6xZ%252BcjIM%252Fl2bggS%252BfuoWSIIFxzJqCDLbu5eRduoncGrZLcP%26winzoom%3D1)中的采样相兼容。

**论文：Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference**

摘要：在现代计算机科学教育中，大规模开放在线课程（MOOCs）记录了数千小时关于学生如何解决编码挑战赛的数据。由于数据非常丰富，这些平台已经引起了机器学习社区的兴趣，许多新算法试图自主地提供反馈以帮助之后的学生学习。但那些之前的数十万学生呢？在大多数教育环境（即教室）中，作业没有足够的历史数据用于监督学习。在本文中，我们介绍了一种人机环路（human-in-the-loop）的「量规采样/rubric sampling」方法，以解决「[零样本](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650756158%26idx%3D1%26sn%3Dc6d15f41e2616b0425606c34b16cacee%26chksm%3D871a9040b06d195662e73b723213bd2bfd896958b6c095de5ef2cf2cbbcd3e008e514482e3ee%26scene%3D38%26key%3D814ec3562518101fe22e73e84a0f0af9fede5355a6065f7f5a0b835bb5508d0d7516ea26dc2137fa03e28d8372f833e5ffd15f42a40c258bb8184f7b2419e0f42db0df0d050600a94dec4c8117e0b070%26ascene%3D7%26uin%3DMTE2NDA4MjczOQ%253D%253D%26devicetype%3DWindows%2B10%26version%3D62060619%26lang%3Dzh_CN%26pass_ticket%3D9T6Ehy%252BLuD4uwrjfH6xZ%252BcjIM%252Fl2bggS%252BfuoWSIIFxzJqCDLbu5eRduoncGrZLcP%26winzoom%3D1)」反馈挑战。

我们能够为第一批做入门编程作业的学生提供自主反馈，其准确性大大优于 data-hungry 的算法，并接近人类的保真度。量规采样只需要很少的教师工作量，可以将反馈与学生解决方案的特定部分相关联，并能够用教师的语言表达学生的错误观念。深度学习推断使得量规采样能够在获得更多任务特定的学生数据时进一步提高。我们在世界上最大的编程教育平台 [http://Code.org](https://link.zhihu.com/?target=http%3A//Code.org) 的新数据集上展示了我们的结果。

**论文：Learning to Teach in Cooperative Multiagent Reinforcement Learning**

摘要：人类集体知识显然得益于个人创新能通过交流传授给他人。与人类社会群体类似，分布式学习系统中的智能体可能会从沟通中受益，它们可以分享知识和教授技能。先前的工作已经研究了改进智能体学习的教学问题，但是这些方法做出的假设阻碍了将教学方法应用于一般的[多智能体](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650756158%26idx%3D1%26sn%3Dc6d15f41e2616b0425606c34b16cacee%26chksm%3D871a9040b06d195662e73b723213bd2bfd896958b6c095de5ef2cf2cbbcd3e008e514482e3ee%26scene%3D38%26key%3D814ec3562518101fe22e73e84a0f0af9fede5355a6065f7f5a0b835bb5508d0d7516ea26dc2137fa03e28d8372f833e5ffd15f42a40c258bb8184f7b2419e0f42db0df0d050600a94dec4c8117e0b070%26ascene%3D7%26uin%3DMTE2NDA4MjczOQ%253D%253D%26devicetype%3DWindows%2B10%26version%3D62060619%26lang%3Dzh_CN%26pass_ticket%3D9T6Ehy%252BLuD4uwrjfH6xZ%252BcjIM%252Fl2bggS%252BfuoWSIIFxzJqCDLbu5eRduoncGrZLcP%26winzoom%3D1)问题，或者需要领域专业知识来解决应用的问题。这种学习教学问题具有与度量教学的长期影响相关的固有复杂性，加剧了标准的多智能体协调挑战。

与现有工作相比，本文提出了智能体在多智能体环境中学习教学的第一个通用框架和算法。我们的算法，学习协调和教学强化（Learning to Coordinate and Teach Reinforcement，LeCTR），解决了合作多智能体强化学习中的点对点教学。我们的方法中的每个智能体都会学习何时何地提供建议，然后使用收到的建议来改善本地学习。重要的是，这些角色并不是固定的；这些智能体学会在适当的时刻承担学生和/或教师的角色，请求并提供建议，以提高整个团队的绩效和学习。对最先进教学方法的实证比较表明，我们的教学智能体不仅学得更快，而且学会协调现有方法失败的任务。

**论文：Content-Boosted Collaborative Filtering for Improved Recommendations**

摘要：大多数推荐系统使用协作过滤或基于内容的方法来预测用户感兴趣的新项目。虽然这两种方法各有优势，但若单独使用它们，在大多数情况下都无法提供好的建议。若将两种方法结合起来构成一个混合推荐系统，则可以克服这些缺点。在本文中，我们提出了一个漂亮且有效的框架，用于结合内容和协作。我们的方法使用了基于内容的预测器来增强现有用户的数据，然后通过协作过滤提供个性化建议。我们的实验结果显示这种方法（内容增强的协作过滤）比纯内容预测器、纯协作过滤器或简单混合方法的性能都要更好。

最后，值得一提的是，本届大会程序主席由南京大学周志华教授联合担任；此外香港科技大学的杨强教授以及京东集团的郑宇博士将作为特邀讲者在大会期间做特邀报告。机器之心将持续关注 AAAI 2019，为大家带来精彩的内容报道。


