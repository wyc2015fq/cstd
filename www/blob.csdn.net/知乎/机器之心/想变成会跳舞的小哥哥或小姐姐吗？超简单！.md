# 想变成会跳舞的小哥哥或小姐姐吗？超简单！ - 知乎
# 



**选自arXiv，机器之心编译，作者：Caroline Chan、Shiry Ginosar、Tinghui Zhou、Alexei A. Efros。**

> 此前，机器之心整理了 GitHub 上一篇关于用 DanceNet 生成舞蹈视频的文章（[DanceNet：帮你生成会跳舞的小姐姐](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650746918%26idx%3D2%26sn%3Db3425beb4419f19131aa4a7ce9ba9cce%26chksm%3D871af458b06d7d4ea25b941bc4117a7aa721246f353152729db5756908e97ad835e472db3df6%26scene%3D21%23wechat_redirect)），吃瓜群众直呼厉害。这回，UC 伯克利的一项研究更不得了——给你一个舞蹈视频，按照本文提出的方法，你可以直接把视频里的人换成自己了。就问你，厉不厉害，流不流皮？

该研究提出一种迁移不同视频中人物动作的方法。给出两个视频，一个视频中是研究者想要合成动作的目标人物，另一个是被迁移动作的源人物，研究者通过一种基于像素的端到端流程在人物之间进行动作迁移（motion transfer）。该方法与这二十年来使用最近邻搜索 [4, 9] 或 3D 重定位运动 [7, 13, 26, 30] 的方法不同。研究者用该框架创建了多个视频，使未经训练的业余人员也能像专业芭蕾舞演员那样做出旋转等舞蹈动作，像流行明星那样跳舞。
![](https://pic2.zhimg.com/v2-fa8800e49d85f9e42c26540568cefa11_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='483'></svg>)图 1：从源人物到两个目标人物的动作迁移
为了逐帧地实现两个视频中人物之间的动作迁移，研究者必须学习两个人物图像之间的映射。该研究的目标是发现源集合和目标集合之间的图像转换方法 [14]。但是，研究者并不具备执行同样动作的两个人物的对应图像对，来直接监督图像转换。即使两个人物按同样的方式执行动作，仍然有可能不具备帧到帧的身体姿态对应关系，因为每个人物的身材和风格都存在差异。

研究者观察到基于关键点的姿势可以作为两个人物之间的中间表示，关键点姿势本质上编码了身体位置而非外表。这些姿势与该研究的目标一致，能够保持随时间变化的运动特征，同时尽可能多地抽象人物特质。因此研究者把中间表示设计成姿势简笔画（见图 2）。研究者从目标视频中获取每一帧的姿势检测 [5, 27, 35]，得到对应（姿势简笔画，目标人物图像）对集合。使用该对齐数据，研究者以监督的方式学习姿势简笔画和目标人物图像之间的图像转换模型。因此，该模型经过训练后可以生成特定目标人物的个性化视频。接下来就是动作迁移，研究者将姿势简笔画输入训练好的模型中，以获取目标人物持同样姿势的图像。研究者在模型中添加了两个组件来改善结果的质量：为了增强生成视频的时间流畅度（temporal smoothness），研究者每一帧的预测都基于前一个时间步的预测结果；为了增强结果的面部逼真程度，研究者在模型中纳入了一个专门化 GAN，用于生成目标人物的面部。
![](https://pic3.zhimg.com/v2-f5aeb30fa047a1dad2b267fc01cb4b2e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='596'></svg>)图 2：姿势简笔画和目标人物帧之间的对应
该方法可以生成不同视频人物之间的动作迁移视频，无需昂贵的 3D 或运动捕捉数据。该研究的主要贡献是提出了一种基于学习的流程，用于视频间人物的动作迁移，结果在真实、详细的视频中实现了复杂的动作迁移。研究者还对提出的模型进行了模型简化测试（ablation study），并与基线模型进行对比。




**论文：Everybody Dance Now**
![](https://pic2.zhimg.com/v2-34cc5fe084b8f7a65f43289457f2547d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='304'></svg>)
论文链接：[https://arxiv.org/abs/1808.07371](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.07371)

**摘要：**本文提出了一种简单的方法，用于「do as I do」的动作迁移：给出一个人跳舞的源视频，我们可以在目标人物执行标准动作的几分钟之后，将该表演迁移到一个新的（业余）目标人物上。我们将该问题看作一个具有时间平滑的逐帧图像转换问题。我们使用姿势检测作为源和目标之间的中间表示，学习从姿势图像到目标人物外观图像的映射。我们将此设置用于时间相干视频生成，包括逼真的人脸合成。
[Everybody Dance Now_腾讯视频​v.qq.com![图标](https://pic3.zhimg.com/v2-a772a2982020f0c43d39432a93d041da_180x120.jpg)](https://link.zhihu.com/?target=https%3A//v.qq.com/x/page/g0765m4o7th.html)
**方法**

给出一个源人物视频和一个目标人物视频，我们的目标是生成目标人物执行源视频同样动作的新视频。为了完成这一任务，我们将工作流程分成以下三个步骤：姿势检测、全局姿势归一化、从归一化的姿势简笔画映射到目标人物。在姿势检测阶段，我们用一个预训练的当前最优姿势检测器，基于源视频中的帧来创建姿势简笔画。全局姿势归一化阶段考虑了帧内源人物和目标人物的身材、位置差异。最后，我们设计了一个系统，通过对抗训练学习从归一化姿势简笔画到目标人物图像的映射。
![](https://pic2.zhimg.com/v2-9b78bc00554c9cf5ee9b3885690d2bf5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='553'></svg>)
图 3.（上图）训练：我们的模型利用姿势检测器 P，基于目标人物的视频帧创建姿势简笔画。训练期间，我们学习映射 G 和对抗鉴别器 D，后者尝试区分「真」对应对 (x, y) 和「假」对应对 (G(x), y)。（下图）迁移：我们使用姿势检测器 P：Y ′ → X ′来获取源人物的姿势关节，然后通过归一化过程 Norm 将这些姿势关节迁移到目标人物的关节，为目标人物创建对应的姿势简笔画。接下来我们使用训练好的映射 G 来生成目标人物的新视频。




**图像转换的对抗训练**

我们修改了 pix2pixHD [33] 的对抗训练设置来生成时间相干视频帧、合成逼真的人脸图像。

为了创建视频序列，我们修改了单个图像生成设置来增强相邻帧之间的时间连贯性，如图 4 所示。
![](https://pic2.zhimg.com/v2-bd6ecdfe9ca89e36d8e40e15675cfe19_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='823'></svg>)
图 4：时间平滑设置。当合成当前帧 G(x_t ) 时，我们基于对应的姿势简笔画 x_t 和之前合成的帧 G(x_t−1)，获得时间平滑的输出。然后鉴别器 D 尝试将「真」时间相干序列 (x_t−1, x_t , y_t−1, y_t ) 与「假」序列 (x_t−1, x_t , G(x_t−1), G(x_t )) 区分开来。




我们添加了专门的 GAN 设置，旨在为面部区域添加更多细节和真实感，如图 5 所示。结果显示该 GAN 设置产生了比较真实的面部特征，模型简化测试显示其改善了完整图像 GAN 的结果。
![](https://pic1.zhimg.com/v2-5b516fb808a322b79641d114244e5d14_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1049' height='588'></svg>)
图 5：Face GAN 设置。残差由生成器 G_f 来预测，并从主生成器添加到原始人脸预测中。
![](https://pic1.zhimg.com/v2-7d0382fd272d581a69119fd28e28d44c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1004' height='1217'></svg>)
图 6：迁移结果。每一部分展示了五个连续帧。顶部一行显示源人物，中间一行显示标准化的姿势简笔画，底部一行显示目标人物的模型输出。
![](https://pic1.zhimg.com/v2-59b4305797292021b5855d1810ac271c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='204'></svg>)
表 4：每张图像的漏检平均值，数值越小越好。
![](https://pic2.zhimg.com/v2-6b7c65147b42d9356ff9ddfd967150c1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='849'></svg>)
图 7：不同模型合成结果的比较。图像被裁剪成围绕原始姿势的边界框。T.S. 表示具有时间平滑设置的模型，T.S. + Face 是具有时间平滑设置和 Face GAN 的完整模型。时间平滑设置为手、头、上衣和阴影添加了细节。这些细节在完整的模型中延续，包括脸部和周围区域的附加细节，从而产生最逼真的合成结果。
![](https://pic4.zhimg.com/v2-631412138f3458164e44fe8734d1d35f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='893' height='687'></svg>)
图 8：验证集上不同模型的人脸图像对比。T.S. 表示具有时间平滑设置的模型，T.S. + Face 是具有时间平滑设置和 Face GAN 的完整模型。时间平滑设置和 Face GAN 的增加使得细节得到改善，失真程度得以减少。
*![](https://pic1.zhimg.com/v2-18cc987d5f379a82f1208b6d90722318_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='73' height='24'></svg>)*





