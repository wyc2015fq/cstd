# 28 天自制你的 AlphaGo (5) : 结合强化学习与深度学习的 Policy Gradient（左右互搏自我进化的基础） - 知乎
# 


知乎对于人机大战的关注度很高，所以写这个系列，希望能让大家对于人工智能和围棋有更多的了解。如果有收获，请记得点一下赞。
本系列已更新多篇，其它几篇的传送门：
- *(1) : 围棋 AI 基础 [知乎专栏](https://zhuanlan.zhihu.com/p/24801451)*
- *(2) : 安装 MXNet 搭建深度学习环境 [知乎专栏](https://zhuanlan.zhihu.com/p/24879716)*
- *(3) : 训练策略网络，真正与之对弈 [知乎专栏](https://zhuanlan.zhihu.com/p/24885190)*
- *(4) : 对于策略网络的深入分析（以及它的弱点所在） [知乎专栏](https://zhuanlan.zhihu.com/p/24939269)*

- *(4.5)：后文预告（Or 为什么你应该试试 Batch Normalization 和 ResNet）**[知乎专栏](https://zhuanlan.zhihu.com/p/25051435)*


本篇提前回答一个大家经常问的问题：**强化学习在**** AlphaGo ****中究竟是怎么用的？**比如说，SL策略网络，是怎么变成 RL 策略网络的？

## **Policy Gradient：简单而有效**

很有意思的是，很少见到有人回答上述问题（可能是因为 AlphaGo 论文在此写得很简略）。**其实，****这个问题的答案特别简单：**
- **如果我赢了棋，就说明这次我选择的策略是正确的。所以可以对于这次所经历的每一个局面，都加强选择这局的走法的概率。**

- **如果我输了棋，就说明这次我选择的策略是错误的。所以可以对于这次所经历的每一个局面，都减少选择这局的走法的概率。 **



举个例子，比如说电脑左右互搏，黑棋开局走星位，白棋回应走小目，最后白棋输了，那么黑棋就加强开局走星位的概率（以及后续的每一步选择这局的走法的概率），白棋就减少在黑棋开局走星位的情况下走小目的概率（以及后续的每一步选择这局的走法的概率）。

等一下，这里好像有问题。这是不是太傻了？也许白棋并不是败在开局，而是败在中盘的某一步？也许黑棋并不是真的这次走对了策略，而是白棋看漏了一步（而且白棋如果走对是可以赢的）？

以上说的很正确。但是，反过来想，如果黑棋的走法可以让白棋后面打勺的概率增加，那也不错啊。另一方面，如果白棋发现自己目前的策略容易进入自己不容易掌握的局面，那么尽管确实可能有完美的招数隐藏在里面，那白棋也不妨一开始就去避免这种局面吧。而且，胜和负的影响可以相互抵消，所以在经过大量对局后，这个过程是比较稳定的。比如说如果某个开局的后续胜率经统计是50%，那它就不会被改变；但如果不是50%，这种改变就有一定道理。

这个过程，有点像人类棋手的“找到适合自己的棋风”的过程。**毫无疑问，现在的 AlphaGo 已经找到了十分适合自己的棋风，它确实是会扬长避短的。**

以上是最简单的 Policy Gradient 的例子，它的问题是有可能陷入局部的最优（对付自己有效，不代表对付其他人有效），因此 AlphaGo 论文中会建立一个对手池（包括整个进化过程中形成的所有策略），保证新策略尽量对于不同对手都有效。在这个基础上，可以做各种各样的改进，例如配合未来的价值网络，更清楚地看到自己的败着在哪里，而不是傻傻地把所有概率都同样修改 。 

## **Deepmind 的相关研究**

其实 Deepmind 自创始以来就在做类似的研究，在此简单说说。经典的一系列论文是学会玩 Atari 游戏：


[https://arxiv.org/abs/1312.5602](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.5602)


[http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html](https://link.zhihu.com/?target=http%3A//www.nature.com/nature/journal/v518/n7540/full/nature14236.html)


例如最经典的 Pong：
![](https://pic3.zhimg.com/v2-d3913140a2749cea1657cd1235513226_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='483' height='216'></svg>)
这里也有一个策略网络，它输入的是目前的屏幕图像（实际上要输入几幅图像，或者前后两幅图像的差，用于判断运动情况），输出的是此时应该往上移动的概率。用这里所说的训练方法就可以让它无师自通，自己学会玩游戏，最终达到相当高的水准（可以想象，这个学习过程会比较慢）。

但是如果我们仔细想想，这个办法恐怕很难自己学会玩好星际！一个重要原因是星际的决策中有太复杂的“层次结构”。因此尽管 Deepmind 此前说星际是下一个目标，目前我们尚未看到 Deepmind 在这方面发表的进展。如果真的成功实现，将是相当大的成就。

最后，如果对于这方面感兴趣，这是一篇很好的介绍：

[Deep Reinforcement Learning: Pong from Pixels](https://link.zhihu.com/?target=http%3A//karpathy.github.io/2016/05/31/rl/)

这篇就到此吧，我们下一篇见。

> 如需转载本系列，请先与本人联系，谢谢。小广告：晚上工作学习是否觉得光线不够舒服，精神不够集中，眼睛容易疲劳？不妨点击看看我们的自然全光谱灯系列：[Blink Sunshine护眼LED灯泡 高显指97显色指数无频闪学习台灯床头](https://link.zhihu.com/?target=https%3A//item.taobao.com/item.htm%3Fid%3D40134613056) 如果需要好用的耳机或钱包，我们也有 :-)


