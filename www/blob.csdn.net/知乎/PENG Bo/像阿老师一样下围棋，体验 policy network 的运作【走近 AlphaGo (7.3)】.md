# 像阿老师一样下围棋，体验 policy network 的运作【走近 AlphaGo (7.3)】 - 知乎
# 



AlphaGo的神秘之处在于策略网络和价值网络，它们可以实现一个惊人的事情：**即使你对围棋基本一窍不通**（只要你会数气）**，也没有任何逻辑推理能力，只要你会做加法和乘法，就可以达到相当强的棋力。**

不过确实要做相当多的加法和乘法！在此我们简化一下，看一个最简化的策略网络是如何具体运作，虽然它的棋力很低，但也能让读者体验到一点“半人半狗”的感觉。

**如果觉得此文有帮助，请记得点个赞噢。**如需转载本文，请先与本人联系，谢谢。

## **1. 局面**

看一个相对简单的局面，来自于AlphaGo与柯洁的第2局对局的第27手。图中带红点的Q15是刚下的一手，来自于持黑的AlphaGo。
![](https://pic4.zhimg.com/v2-61a604de5f61d44c782f8d1ffb2ad777_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1054' height='1054'></svg>)
如果读者对于围棋的基本规则有了解，此时持黑的AlphaGo在叫吃白方的P17 P16 Q16这3子，因为这3子此时只有1口气了，如果白置之不理，黑再走P15就会把它们彻底围起来，吃掉。

弃这3个子的代价太大，所以持白的柯洁的下一手可以说只有2种选择：
- 要么走P15，逃出3子。如果黑再走O15叫吃，则白可走P14成功逃出。
- 要么走R15，把黑的R16吃掉，也可以暂时解除威胁。但黑一定会再走P15叫吃，则白必须走R16连回，然后黑可走O18把自己连上，于是白有“一团棋被封在右边”的感觉。

柯洁最终的选择是P15。让我们看策略网络对此有何看法。




## **2. 最简化的策略网络**

前文讲了将使用的8个特征层（见：[走近 AlphaGo 系列 (7.1) ：从围棋盘看卷积神经网络CNN的具体工作过程 (上)](https://zhuanlan.zhihu.com/p/27807838)），于是有最简化的策略网络模型：

## **【注：如果不懂这里的内容，可以直接跳过看后面，后面有例子，会容易理解】**

> 8个特征层 => 3*3卷积（只有1个神经元） => SoftMax => 输出

确实简单得不能再简单了，整个网络只有1个神经元，接近于一个Logistic回归模型。关于卷积请看：[走近 AlphaGo (7.2) ：教你快速理解卷积和卷积神经网络（图像篇）](https://zhuanlan.zhihu.com/p/27897220)。

这个网络，对于人类高手棋谱，平均可以实现16.49%的预测准确率。

这是什么概念？

整盘棋下来，棋盘上平均有一百多个点可以走，柯洁会选择其中一个，所以乱猜猜对的可能性只有一百多分之一。

**现在你用下面的特别傻的方法，只要做一点加法（乘法都不用做了！），竟然可以有16.49%的概率和柯洁的选择一模一样**（实际来说，由于柯洁的棋力强，预测准确率会略低一些）**。**




## **3. 特征层：本方棋子（白棋）**

第一个特征层，是本方棋子。它对应的卷积核，以及解释：
![](https://pic1.zhimg.com/v2-f8c39ee4840ed25ea5734118cc18e440_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='889' height='185'></svg>)
看图（为了清晰，只展示右上角）。特征层，特征层单独显示，以及特征层在卷积和SoftMax后的效果：
![](https://pic2.zhimg.com/v2-8c8e96c36416fcd0edf66602012d5e79_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='792' height='181'></svg>)
说说这是什么意思。**策略网络是给每个点打分，分数越高就代表越应该走在这里。**

上面左边和中间是先把本方棋子（白棋）标记出来。然后在此基础上做“卷积”，就可以给每个点打分。打的分数在上面的右图展示，分数越高就蓝色越深。

“卷积”具体怎么做，就是：
- 每个点的初始分数为 0 分。
- 每颗白子给自己所在的点贡献 -2.47 分。
- 每颗白子给自己上下左右邻近的4个点贡献 0.55 分。
- 每颗白子给自己斜位的4个点（可以叫肩位）贡献 0.47 分。

看几个例子：
- L15 R19这样的点，周围没有白子，所以分数为0。
- L19，它收到斜位的白子（M18）的影响，所以分数为0.47。
- O18，它收到N18（直接接触，贡献为 0.55）和 N17 P17（斜位接触，贡献都为 0.47）的影响，所以分数为 0.55 + 0.47 + 0.47 = 1.49分，分数很高啊，所以在右图的颜色很深。
- O17的分数更高，是 0.55+0.55+0.47+0.47=2.04分！等一下！这个地方已经有棋子了！怎么可以走在现有的棋子上面！没关系，后面有修正的办法。
- M18的分数是多少？首先它本身是白子，白子自己给自己所在的点贡献 -2.47，然后它的两个白子邻居给它贡献 0.47+0.55，最后分数是 -1.45分，低于正常值，确实不适合走。
- 读者会问，为什么不干脆让白子自己给自己所在的点共享负无穷大分？这样就肯定不会走在自己上面了！这是因为我们这里的贡献值都是训练出来的，后面会看到 -2.47 其实已经足够小了，网络在这种情况下会“没有什么动力”再去缩小这个值。

最后我们把结果美化一下，显示出来就是上面的右图。




## **4. 特征层：对方棋子（黑棋）**

卷积核及解释：
![](https://pic2.zhimg.com/v2-23878a9acda14e0006d47ddb70317161_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='627' height='145'></svg>)
看图：
![](https://pic1.zhimg.com/v2-497a4610d93ea9ab169ecbbbf7e011d4_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='764' height='168'></svg>)
和前面一样：
- 每个点的初始分数为 0 分。
- 每颗黑子给自己所在的点贡献 -2.27 分。
- 每颗黑子给自己上下左右邻近的4个点贡献 0.49 分。
- 每颗黑子给自己斜位的4个点（可以叫肩位）贡献 0.23 分。

例如 P17 的得分很高，因为它周围都是黑子，而且自己不是黑子。等一下，这里好像也已经有棋子了吧！

说明一下，总共有8个特征层，最后会把8个特征层的分数加起来，加起来之后就会发现这个会得到解决。




## **5. 特征层：空点**
![](https://pic3.zhimg.com/v2-9069f0983dc2c695b39a0f921fe41176_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='629' height='145'></svg>)


![](https://pic3.zhimg.com/v2-bd2a36e2470fa0b60ed6eb1de412de6a_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='757' height='166'></svg>)
这个来解决问题了。由于空点给自己所在的位置贡献5.78分，因此非空点的位置就享受不到这5.78分，这意味着什么？这意味着我们会非常偏向于选择空点。

策略网络最后是看哪个点的分数相对最大（SoftMax是负责这个），所以“给空点加5.78分”，其实等价于“给非空点减5.78分”。

上面右边的输出很有意思，我们发现，它避开有棋子的地方，避开拥挤的空点（如P15 R15），也避开棋盘最外圈（一路），因为最外圈周围的空点不够多。

不信就算一下：
- L15是空点，附近8个点也是空点，所以它的分数是 5.78+0.09+0.09+0.09+0.09+0.41+0.41+0.41+0.41=7.78分。
- T19是空点，但是因为在角落，所以附近的空点只有S19 S18 T18。所以它的分数是5.78+0.09+0.09+0.41=6.37分，确实低了一些，低了整整1.41分。
- 说明一下，如果两个点的分数差X，选择的概率就差e^X倍。所以，如果差1.41分，选择的概率是差e^1.41=4.10倍！

那么，请看将前3个特征平面的打分相加后的结果：
![](https://pic3.zhimg.com/v2-985818281adf6d4ef40149a691f8d87a_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1054' height='1054'></svg>)
震惊了。颜色最蓝的，分数最高的，是O18 R15这样的与双方棋子都有直接接触的空点。这两个地方确实有重要性，鉴于我们的方法如此简单，真是令人惊讶地准确。

其实很久以前的围棋程序就是类似这样写的（例如“手谈”）。但是当时的计算机处理能力太低，也没有人收集这么多的高手棋谱，没有人拿这么多的棋谱去训练，所以当时的开发者只能去手工一个个实验各种参数（就是像这里的具体要加多少分数），无疑是很低效率和效果差的过程。

这就是数据的威力。**深度学习的威力，很大程度上是数据的威力。**




## **6. 特征层：剩下的几个**

只有1气的棋子：
![](https://pic3.zhimg.com/v2-54d6a598b000280fd69709b93d540e36_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='687' height='194'></svg>)


![](https://pic4.zhimg.com/v2-4435cb3139458e75e3eea8744980f663_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='766' height='177'></svg>)
可见，倾向于走在它们的旁边。另外，如前所述，P16在加起来的时候会消失。

==========================

只有2气的棋子： 
![](https://pic1.zhimg.com/v2-9beb6ace0f9a08d7e3a01167fa78aeb4_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='633' height='143'></svg>)


![](https://pic2.zhimg.com/v2-cc406787c94e294018937ee65146aa09_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='762' height='171'></svg>)
由于这里最高分数的点也只有0.42分（是L17 N17 M18 M16这四个点），所以周围普通点（它们的分数显然为0）的蓝色相对没有显得很浅，看上去差别不是很明显。

==========================

只有3气的棋子：
![](https://pic4.zhimg.com/v2-d5a6f0b43065448f61764797fccd33db_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='628' height='144'></svg>)


![](https://pic3.zhimg.com/v2-c7d4250b617ebbfad7f101ef053a8afa_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='760' height='166'></svg>)
由于P16的分数达到0.80分（0.41+0.41-0.02=0.80），所以把普通点的分数拉开了差距，让它们显得浅了。

==========================

有4气及以上的棋子： 
![](https://pic2.zhimg.com/v2-977e52834e2a36027cee8076fe7baa75_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='838' height='229'></svg>)


![](https://pic2.zhimg.com/v2-c08fbea404463821a4073d96b6a34eb1_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='755' height='171'></svg>)
可以看到“避免直接接触”的效应（如M17）。

==========================

最后一手的位置：
![](https://pic1.zhimg.com/v2-318a060d228b3cc0214d72ba28c41530_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='705' height='138'></svg>)


![](https://pic4.zhimg.com/v2-3e3d1231c0c03f91ed5d026b335c4eaf_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='758' height='171'></svg>)
在卷积后，基本只剩下最后一手的附近棋子。




## **7. 最终结果**

最终，将8个特征平面的打分相加后的结果：
![](https://pic2.zhimg.com/v2-307cb55253db53b052021a9b80b145d5_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1054' height='1054'></svg>)
成功地将目标锁定在了P15和R15。可见“最后一手”的效应非常强，其它地方的蓝色都已经看不到了（所以这个网络本身其实没有棋力，因为它只会靠着你走，预测准确率要达到30%才开始有一点棋力，达到50%就下得颇为不错）。

这是浅层网络的局限性，如果我们用更加复杂的网络，“最后一手”的效应会减少到正常的程度。然而，复杂的网络需要做的加法和乘法太多，这里写不下了！需要想想该怎么写。

如果去掉“最后一手”的特征层呢？结果如图：
![](https://pic3.zhimg.com/v2-9e6a079e9a990dfa88a96eb71a0b6afa_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1054' height='1054'></svg>)
可见此时最起关键作用的是“走在只有1气的棋子的旁边”。读者可以把这里的过程写成代码，实验更多的局面看效果。

此外，由于这里的卷积核只有3*3，而且只有1层网络，所以这个网络会特别喜欢走在现有棋子的附近。通过使用多层网络可以解决这个问题。




## **8. 总结**

之前知乎有问题问“AlphaGo真的会下围棋吗？”，看完这个过程，读者也许会有自己的答案。

那么，这里的“应该加多少分”是如何通过棋谱训练出来的？我在未来会写一写。

如果觉得本文有帮助，请记得点个赞噢。

**如需转载本文，请与本人联系，谢谢。**

最后，欢迎关注我的专栏： [技术备忘录 - 知乎专栏](https://zhuanlan.zhihu.com/mathNote)，其中有更多文章。 












