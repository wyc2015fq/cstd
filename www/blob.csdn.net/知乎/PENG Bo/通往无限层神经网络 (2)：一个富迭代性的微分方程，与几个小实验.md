# 通往无限层神经网络 (2)：一个富迭代性的微分方程，与几个小实验 - 知乎
# 


在上一篇我们看了残差结构在深层网络中的必要性：[对于残差网络（Residual Network）的一种理解方法，与深层网络的训练 - 知乎专栏](https://zhuanlan.zhihu.com/p/26595791)。
现在网络越来深了，那么最深就是无限深。我们在此运用残差结构，构造一个无限层的神经网络，或者说，一个"层的深度可以连续变化"的神经网络（不知道之前有没有人做过这个，如果您有什么想法，欢迎讨论）。

还是拿上一次的例子，令网络只有 1 个输入，1 个输出。令网络的输出为 f，层的深度是从 0 到 1 连续标记。


于是这里核心的方程是：
![f(0,x)=x,\;\;\;\frac{\partial f(t,x)}{\partial t} = N(t,\,\{w_t\},\, f(t,x))](https://www.zhihu.com/equation?tex=f%280%2Cx%29%3Dx%2C%5C%3B%5C%3B%5C%3B%5Cfrac%7B%5Cpartial+f%28t%2Cx%29%7D%7B%5Cpartial+t%7D+%3D+N%28t%2C%5C%2C%5C%7Bw_t%5C%7D%2C%5C%2C+f%28t%2Cx%29%29)

这里用 ![N(t,\, \{w_t\},\, f(t,x))](https://www.zhihu.com/equation?tex=N%28t%2C%5C%2C+%5C%7Bw_t%5C%7D%2C%5C%2C+f%28t%2Cx%29%29) 代表 "第 t 层" 的操作，其中 ![\{w_t\}](https://www.zhihu.com/equation?tex=%5C%7Bw_t%5C%7D) 是权重，![f(t,x)](https://www.zhihu.com/equation?tex=f%28t%2Cx%29) 是之前的层给出的输出。这个微分方程比较有趣，如果你把它展开会发现它具有很强的迭代性，因为神经网络具有迭代性，所以它的解会出现 ![e^t](https://www.zhihu.com/equation?tex=e%5Et) 这样的项。

## 例子1：

由于 ![f(t,x)](https://www.zhihu.com/equation?tex=f%28t%2Cx%29) 的情况比较复杂，我们不妨先看更简单的情况：
![f(0,x)=0,\;\;\;\frac{\partial f(t,x)}{\partial t} = N(t,\, \{w_t\},\, x)](https://www.zhihu.com/equation?tex=f%280%2Cx%29%3D0%2C%5C%3B%5C%3B%5C%3B%5Cfrac%7B%5Cpartial+f%28t%2Cx%29%7D%7B%5Cpartial+t%7D+%3D+N%28t%2C%5C%2C+%5C%7Bw_t%5C%7D%2C%5C%2C+x%29)

这里的残差结构不是用之前的输出 ![f(t,x)](https://www.zhihu.com/equation?tex=f%28t%2Cx%29)，而是直接用原始数据 ![x](https://www.zhihu.com/equation?tex=x) 作为输入。这已经足以给出相当复杂的最终网络输出。不妨看个最简单的例子。令：
![N(t,\, \{w_t\},\, x) = Max[0,\, x + (w_1 t + w_2)]](https://www.zhihu.com/equation?tex=N%28t%2C%5C%2C+%5C%7Bw_t%5C%7D%2C%5C%2C+x%29+%3D+Max%5B0%2C%5C%2C+x+%2B+%28w_1+t+%2B+w_2%29%5D)

即，它拥有 1 个权重固定为 ![1](https://www.zhihu.com/equation?tex=1)，而偏置随着层的深度 ![t](https://www.zhihu.com/equation?tex=t) 的加深会线性变化的 ReLU 单元。于是我们积分一下就可以得到网络的输出：
![f(x) = f(1,x) = \int_0^1 N(t,\, \{w_t\},\, x) \,dt = ?](https://www.zhihu.com/equation?tex=f%28x%29+%3D+f%281%2Cx%29+%3D+%5Cint_0%5E1+N%28t%2C%5C%2C+%5C%7Bw_t%5C%7D%2C%5C%2C+x%29+%5C%2Cdt+%3D+%3F)

答案是：
![](https://pic1.zhimg.com/v2-a010093fe05137be08677f9e2d07f7f0_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1380' height='218'></svg>)
这个输出就比较复杂，而且还出现了 ![x^2](https://www.zhihu.com/equation?tex=x%5E2) 这样的高次项。

## 例子2：

如果我们把 ![w_t](https://www.zhihu.com/equation?tex=w_t) 和 ![t](https://www.zhihu.com/equation?tex=t) 的关系取得更复杂，答案更是会非常复杂。举例，如果加一个看似不起眼的 ![t^2](https://www.zhihu.com/equation?tex=t%5E2) 因子：
![N(t,\, \{w_t\},\, x) = Max[0,\, x + (t^2 + w_1 t + w_2)]](https://www.zhihu.com/equation?tex=N%28t%2C%5C%2C+%5C%7Bw_t%5C%7D%2C%5C%2C+x%29+%3D+Max%5B0%2C%5C%2C+x+%2B+%28t%5E2+%2B+w_1+t+%2B+w_2%29%5D)

最终网络的输出将是：

![](https://pic1.zhimg.com/v2-a5a883f3a335876feee75dfd7cc00668_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1816' height='878'></svg>)可以说它会很容易进入混沌状态。毕竟它是个无限深的神经网络。

## 例子3：

再看残差结构用之前的输出 ![f(t,x)](https://www.zhihu.com/equation?tex=f%28t%2Cx%29) 的情况。例如，最简单的例子是这样：

![f(0,x)=x,\;\;\;\frac{\partial f(t,x)}{\partial t} = Max(0, f(t,x))](https://www.zhihu.com/equation?tex=f%280%2Cx%29%3Dx%2C%5C%3B%5C%3B%5C%3B%5Cfrac%7B%5Cpartial+f%28t%2Cx%29%7D%7B%5Cpartial+t%7D+%3D+Max%280%2C+f%28t%2Cx%29%29)
这个解出来是：
![f(t,x) = IF\;\;x < 0\;\;THEN\;\;x\;\;ELSE\;\;x \cdot e^t](https://www.zhihu.com/equation?tex=f%28t%2Cx%29+%3D+IF%5C%3B%5C%3Bx+%3C+0%5C%3B%5C%3BTHEN%5C%3B%5C%3Bx%5C%3B%5C%3BELSE%5C%3B%5C%3Bx+%5Ccdot+e%5Et)


但是如果仔细思考，会发现这种残差结构有点问题，容易丢失信息。举例，如果在中间出现了![x_1](https://www.zhihu.com/equation?tex=x_1) 和 ![x_2](https://www.zhihu.com/equation?tex=x_2) 满足 ![f(x_1,t)=f(x_2,t)](https://www.zhihu.com/equation?tex=f%28x_1%2Ct%29%3Df%28x_2%2Ct%29)，那么在后续它们就会被"锁定"，也就是对于任何 ![T>t](https://www.zhihu.com/equation?tex=T%3Et) 都有 ![f(x_1, T) = f(x_2, T)](https://www.zhihu.com/equation?tex=f%28x_1%2C+T%29+%3D+f%28x_2%2C+T%29)。

**这是否意味着我们也应该试试****在中间层引入原始数据？**其实我训练围棋策略网络时试过这个，但好像作用不大。对于图像的问题，也许可以试试在中间层引入缩小到相应尺寸的原始图像，迟点试试。

## 总结：

下一步的问题是如何训练这种网络，自然的想法是 BP 是否也存在一个连续的形式。我们在后续看这个问题。

如果本文对你有启发，请点个赞，谢谢！~ 如果您有什么想法，也很欢迎讨论。


