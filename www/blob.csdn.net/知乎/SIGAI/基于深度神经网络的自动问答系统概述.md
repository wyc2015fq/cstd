# 基于深度神经网络的自动问答系统概述 - 知乎
# 

> **SIGAI特邀作者**
[@穆文](https://www.zhihu.com/people/c7cdf56b61d269758b8ddcc8c212a6d5)
NLP从业者
目前兴趣点：问答系统、序列标注、文本生成
个人知乎ID：[数据挖掘机养成记](https://zhuanlan.zhihu.com/DataMiner-X)

本文由 [@穆文](https://www.zhihu.com/people/c7cdf56b61d269758b8ddcc8c212a6d5)  独家授权刊登，禁止一切形式转载

本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
- [书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
- [书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)

> **本文受众 **
对 NLP 感兴趣 想入门阅读理解/自动摘要/自动问答
- 1. 引子
- 2. 门派
- 2.1. 背景
- 2.2. 脉络
- 3. 章法
- 3.1. GA-Reader
- 模型结构
- 杀手锏
- 3.2. Match-LSTM
- 模型结构
- 杀手锏
- 3.3. Bi-DAF
- 模型结构
- 杀手锏
- 3.4. R-Net
- 模型结构
- 杀手锏
- 3.5. QA-Net
- 模型结构
- 杀手锏
- 3.6. S-Net
- 模型结构
- 杀手锏
- 3.7. R3
- 模型结构
- 杀手锏

## 引子

学术圈的生存之道，无非『挖坑灌水』四字真言。大仙鼻祖挖坑，徒子徒孙灌水。坑越挖越大，水越灌越多，终成一片繁荣的小江湖。而『深度神经网络+自动问答系统』，就是这样一个好坑。而本文，将带着各位读者，趟一趟这个大坑，会一会各路大仙们的套路。作为一篇科普文，

**本文会探讨：**模型结构和对比、简单的公式。

**本文不会涉及: **复杂的公式；比赛刷榜的奥义（包括：数据集、特征工程、调参技巧）。

## 门派

2.1. 背景

问答系统的实现机制多种多样，基于信息检索 (IR: Information Retrieval) 的、基于问答知识库 (KB: Knowledge Base) 的、基于知识图谱 (KG: Knowledge Graph) 的等等，一个相对完善的问答系统往往是多种机制的组合。

而本文提到的问答系统，特指:

`给定一个问题 Q 和一个与 Q 相关的文档 D，自动得到 Q 对应的答案 A`

这样的系统有一个更学术的名字: 机器阅读理解 (MRC: Machine Reading Comprehension)，它既可以作为一个独立的问答系统，也可以看成是一个完整问答系统的一部分: 即，在 IR 检索出跟 Q 相关的文档 D 的基础上，机器去阅读并理解 Q 和 D，得到答案 A。

将问答系统用一图以蔽之:
![](https://pic1.zhimg.com/v2-95e633d34999168cd5601a4926d3c950_b.jpg)(图片来自网络，侵权请告知)
2.2. 脉络

用简单的数学符号来表达问答系统要做的事情：

`f(Q, D) = A`

(题外话: 如果我们去掉 Q，便成了 f(D) = A，也就是一个摘要系统。所以，问答系统的模型稍加改造，可以无缝对接到『自动摘要』上去。)

回想一下，我们人类是怎么解决阅读理解问题的？

我们会仔细阅读问题和文章，带着问题去文章里找答案，如果文章里有现成的答案片段，我们直接抽取出来，如果没有，我们需要组织自己的语言去生成一个回答。

机器其实跟我们的套路是一样的：
![](https://pic1.zhimg.com/v2-af7fce11b3ff7ef3662908cac1c203ac_b.jpg)
这，便是深度问答系统、机器阅读理解的『**套路**』，可以用一个更简单明了的图来表示：
![](https://pic1.zhimg.com/v2-1528c70f769f5b8f70aa5267921636c0_b.jpg)
因此，按照模型各个组成部分的不同，可以将其大致分成以下几类『门派』：
![](https://pic1.zhimg.com/v2-13f06030841b845e6ceb9534e0714bc0_b.jpg)
每种模型都或多或少融合了不同门派的风格，既有各自的特色，更有相互的传承，且待下文慢慢道来。

这里要说明一下，decoder 的不同主要因为数据集的不同，有些数据集里的答案 A 可以直接从 D 里面抽取出来，比如 SQuAD 数据集, 有些则需要抽取+生成，比如 MS-MARCO 、 DuReader 和最新的 SQuAD 2.0 数据集。（P.S. 我们在 DuReader 比赛中，用单模型（不做特征工程），刷到过第三）

## 章法

我们将一网打尽当前常见问答系统、阅读理解的各路模型，细数他们套路章法的特别之处。 若有漏网之鱼，读者朋友务必留言指出。
为避免不必要的江湖纷争，特此声明：

`模型有门派之异，但无优劣之分。以下模型的排序乃随机抽样的结果，与诞生时间、模型先进性无任何关系。`

鉴于每种模型的大框架都是上节提到的 『套路』，所以接下来我们只提及模型的特别之处，具体的公式细节乃至源代码，请各位自行参考相关论文。

3.1. GA-Reader

秘籍传送门: GA-Reader

模型结构
![](https://pic1.zhimg.com/v2-81cca435ca4c639c0710379687797f18_b.jpg)
杀手锏

1.) 在每层 encoder 里都加入了一个 『GA (Gated Attentnion)』 结构，其公式如下：
![](https://pic1.zhimg.com/v2-49f8c453be2ee99629de47c74f2da300_b.jpg)
顾名思义，GA 就是用 attention 的思想，用 Q 的 encoding 给 D 里每个词 di 的 encoding 加一个 『gate』 ai，再传入下一层。

 2.) 加深 encoder 的深度，在每一层都设置 GA 结构，使得 Q 能在不同粒度上被融合进 D。

 3.) decoder 用 softmax 函数来预测答案里的所有词，比如文档 D 是 [ '水', '能', '载舟', '亦', '可', 'citing']， A 是 ['能', '载舟', '亦']，则 label 就是 [0, 1, 1, 1, 0, 0]，本质上就是个 multi-label 问题，损失函数用 cross entropy。

3.2. Match-LSTM

秘籍传送门: Match-LSTM

模型结构
![](https://pic1.zhimg.com/v2-aa13cf3a6b493c51290d1978bc79cf9c_b.jpg)
杀手锏

1.) encoder 第一部分将 D (图中用 P 表示，代表 Passage) 、Q 分别经过 bi-LSTM 得到的 hp、 hq 拼接在一起，实现 Q 和 D 的第一次融合 2.) encoder 第二部分是重头戏，应用到前人提出的 match-LSTM 模块，公式如下：
![](https://pic1.zhimg.com/v2-cb360a05930bb06bf5ffb08e84acfdd0_b.jpg)
通过 attention 机制，得到 D 中每个词 i 关于 Q 里所有词的权重，即公式中的 ai，接着
![](https://pic3.zhimg.com/v2-8c5246753168fb1f5258649d6d38eeee_b.jpg)
将 ai 作用到 Q 的 encoding Hq 上，得到一个新的向量，可以认为这个向量是 D 融合了 Q 之后的结果，再跟 Hp 拼接在一起，实现 Q 和 D 的第二次融合。

注意，这里的 attention 机制跟 GA-Reader 有区别，这里得到的是 D 里每个词对 Q 里每个词 两两的权重，而不像 GA-Reader 里 GA 的，得到的是整个 Q 对 D 里每个词的权重。 这也是 Match-LSTM 的特别之处。 输入到下一时刻的向量包含三部分： attention vector, 上一层的 hidden state, 上一时刻的 hidden state，如下所示：
![](https://pic2.zhimg.com/v2-88db7b97ec611b055eb40e385696b0bd_b.jpg)![](https://pic2.zhimg.com/v2-d9d03246a93630e52a1e4c952debb685_b.jpg)

3.) 设计了两种 decoder，即图中的 answer pointer layer 模块：
![](https://pic3.zhimg.com/v2-bde659982248410074372590997d4356_b.jpg)
这两种 decoder 将答案预测看做是一个序列问题，而不是像 GA-Reader 一样看做 multi-label，所以用到了 pointer network 去选择答案。 第一种 decoder 是按顺序预测整个答案序列，仍以 D 是 [ '水', '能', '载舟', '亦', '可', 'citing']， A 是 ['能', '载舟', '亦'] 为例，在第一种 decoder 下，label 是 [1, 2, 3]，代表 A 在 D 中的位置，而第二种 decoder 是预测答案边界，即起始位置， label 是 [1, 3]。 文章的实验表明，预测边界效果最好，这也几乎成了后续抽取式模型的『标配』。

(题外话: 关于 pointer network，简言之就是用 attention 的机制，预测目标序列在原文中的位置，每一步解码的概率，就是 attention score 经过 softmax 之后的概率分布。 具体请参考原论文 pointer-network .)

3.3. Bi-DAF

秘籍传送门: Bi-DAF

模型结构
![](https://pic2.zhimg.com/v2-61d7a635a2fa9ac0f7611c65ec248d81_b.jpg)
杀手锏

1.) encoder 跟前面俩模型一样， 考虑到了用 Q 融合进 D、 去 attend D 里的每个词，即图中的 Context2Query 模块，这部分没什么特别的，只是在计算 attention 的时候，多加了 h 和 u 的点乘项，如下：
![](https://pic2.zhimg.com/v2-93fbdb22c61fd554b925407a785115ad_b.jpg)
除此之外，还创新性地用 D 去 attend Q，即图中的 Query2Context 模块，跟 Context2Query 的本质一样，唯一区别是对 attention score 取了 max 后再对 h 加权平均，得到的是单个向量，可以看成是 D 融合进 Q 之后的结果，参与到 encoder 的每个时间步的输入。
![](https://pic3.zhimg.com/v2-f106068b47d0a3cc7dbec88548df4f9a_b.jpg)
2.) encoder 输出层做了个类似 『特征工程』 的 trick，将 h u 等向量进行拼接、两两相乘
![](https://pic1.zhimg.com/v2-cb6e4f8b1ddc98ccdd9b1c4b143b89bc_b.jpg)
3.) decoder 部分看似是新的，但我个人认为本质上还是个 pointer network，即利用 Q 和 D 通过 decoder 每一步出来的 M，去 attend encoder 的输出 G，然后用 softmax 作用，去预测起始位置。 只不过在预测终止位置时，M 又额外经过了一层 LSTM 得到 M2。 损失函数仍然是 cross entropy。
![](https://pic4.zhimg.com/v2-d5fd3caf8b0444b154a5ba1e61817d2f_b.jpg)![](https://pic3.zhimg.com/v2-fd4c668b69da7bece71bb430706b7eea_b.jpg)![](https://pic2.zhimg.com/v2-f65c88e6137236a96e4b368d1ba73acd_b.jpg)

3.4. R-Net

秘籍传送门: R-Net

模型结构
![](https://pic2.zhimg.com/v2-14ef408fb2aa1353bfbb3ca326497405_b.jpg)
杀手锏

1.) encoder 中加入了一个 Gated Match-LSTM 模块，这也是模型主要 claim 的一个点， 但我认为相对 Match-LSTM，改动并不大。 回顾下 Match-LSTM 里的每步输入：
![](https://pic4.zhimg.com/v2-ad8573a33f034a47f4f66e8344fff4b3_b.jpg)![](https://pic2.zhimg.com/v2-8c4ed9f8c13c1d2bfa326b1ec143fe6d_b.jpg)
再看看 Gated Match-LSTM 的输入：
![](https://pic4.zhimg.com/v2-ae04c9c7da4a9e381d91d5043f89870f_b.jpg)![](https://pic2.zhimg.com/v2-7e952751224e268c0474cbab2a630c15_b.jpg)
可见唯一的区别就是对输入多加了个 gate gt。 这么做的直观解释是，通过 gt 来过滤掉输入中跟问题和答案不相关的部分。

2.) encoder 中还加入了 self-attention 机制，使得文章内部的词与词之间相互融合，通过自身的上下文信息辅助筛选有价值的词，公式如下：
![](https://pic4.zhimg.com/v2-d2d7c90e01db8d372fe3eccb517843cf_b.jpg)![](https://pic3.zhimg.com/v2-5f0671fdfa184599905ee6cb20844e2e_b.jpg)
注意这里的 attention vector ct，是由当前词 vj 和所在文章里的所有词 vt 做 attention 得到的：

3.) decoder 仍然采用 pointer network， 但额外引入了 question 的信息，作为 decoder hidden state 的初始输入，如下图：
![](https://pic1.zhimg.com/v2-80f57f6f6c33e4a654d162575c11eb80_b.jpg)

3.5. QA-Net

秘籍传送门: QA-Net

模型结构
![](https://pic3.zhimg.com/v2-20ea0102a81383e92dbd7ff15b846f7a_b.jpg)
杀手锏

相信在看过四个 rnn 派的模型后，这个模型一定让大家耳目一新，所以他的杀手锏就是：
1.) encoder 用 cnn 来实现，由多个结构一样的 encoder block 来实现，每个 encoder block 长下面这样：
![](https://pic2.zhimg.com/v2-8f3a075135f90882fdf7b01ced8f1a09_b.jpg)
每个 block 里又包含多个小单元，包含了 layer-norm、 convolution 和 self-attention 等，其中：
a.) layer-norm 可以看做是 batch-norm 的变种，简单理解，batch-norm 是对一个 batch 里 x 的特征进行归一化，而 layer-norm 是对一层的 x 输出归一化。

b.) convolution 采用了 depthwise separable convolution，如下图：
![](https://pic2.zhimg.com/v2-ba1b7a93961399b03e92ae0294c9423d_b.jpg)
看似复杂，其实简单理解，就是把一个 H x W x D 的卷积核，分解成 H x W x 1 和 1 x D 俩矩阵，从而降低矩阵的秩，减少参数量。 这是模型压缩很常见的一个做法。

2.) encoder 里也有一层 context to query 的 attention，做法与 Bi-DAF 有点类似但不一样，作者说他是借鉴自一个叫 DCN 的模型，计算 attention score 的公式如下，额外加入了 q 和 c 的点乘。
![](https://pic1.zhimg.com/v2-14b30911d6c4c3270d15050cbe2f2858_b.jpg)
3.) decoder 沿用 Bi-DAF 的，这里不赘述了。
4.) 除了模型层面的改进，QA-Net 还额外做了数据扩充，采用 English-French-English 的双向翻译模型来扩充语料。 作者号称效果变好，对此我表示存疑，毕竟翻译误差的累积会导致语料变差。
![](https://pic1.zhimg.com/v2-57d68db25b1ab475b79d5a3195cb9928_b.jpg)

3.6. S-Net

秘籍传送门: S-Net

模型结构
![](https://pic1.zhimg.com/v2-9cf731a52df563dc5497c2140471694c_b.jpg)
杀手锏

前面所有模型都是端到端的抽取式模型，而 S-Net 是分成了两部分的生成式模型：先从文章中抽取出所谓的 evidence snippets， 再通过生成式的 decoder 生成答案，具体如

1.) 第一部分： 直接 train 一个抽取式模型，如下：
![](https://pic2.zhimg.com/v2-a8d4d68506117699bed69d1a3edc4a55_b.jpg)
a.) encoder 部分借鉴了 R-Net 里的 Gated Match-LSTM。
b.) decoder 部分采用了 multi-task training，是一个双任务结构，一个目标是传统的抽取式过程，采用了 R-Net 的 decoder 用来预测答案，一个是 passage ranking，将目标 passage 排序靠前，辅助抽取式 decoder。

2.) 第二部分: 将第一部分预测的答案起始位置作为重要信息放到词的 embedding 里，然后采用 seq2seq 的结构来生成答案。
![](https://pic3.zhimg.com/v2-0042fa9d4fdd5d11cea648a4412b19e2_b.jpg)
这里用 seq2seq 的方法比较简单粗暴，encoder 部分直接拿 passage 和 question 的 last hidden state 做个拼接作为 decoder 的初始状态，decoder 就是一个采用了 attention 机制的 GRU。

S-Net 这种设计方式使得模型结构非常灵活，既可以应用在生成式答案的数据集，如 MS-MARCO, SQuAD 2.0，也可以用于传统的抽取式数据集，如 SQuAD。 值得注意的是，虽然 S-Net 号称处理多篇文档，但在 encoder 层面依然是把多篇文档拼接在一起作为一整篇文档，并没有做文档排序。

3.7. R3

秘籍传送门: R3-Net

模型结构
![](https://pic1.zhimg.com/v2-3c23aa259e0f3aa1973dd70d1a5cd9f4_b.jpg)
杀手锏

回顾一下我们在前言部分提到的，一个相对完整的问答系统——包含了文章检索、文章排序和答案生成三部分，前面我们讨论的所有问答模型主要集中在答案生成，所针对的数据集都是：

`给定一个问题 Q 和一个与 Q 相关的文档 D，自动得到 Q 对应的答案 A`

即便是 S-Net，也是基于这样的数据集，文章排序不过是一个辅助任务，但 R3 要解决的问题是，

`给定一个问题 Q 和通过 IR 检索出的相关文档集合 Dset，自动得到 Q 对应的答案 A`

这是个典型的 open-domain QA 系统，一般都会先对 Dset 排序，再做答案生成，但 R3 用强化学习的框架统一了文章排序 (Ranker) 和答案生成 (Reader) 两部分，是一个端到端的模型。

1.) Ranker 和 Reader share 一个 encoder， 用来对输入文章建模，结构如下：
![](https://pic3.zhimg.com/v2-137fbc5fb84d1938a1e65203b266f2de_b.jpg)
这里有个小 trick，跟 Bi-DAF 有点类似，对 Hp 和 Hq 做了些类似特征工程的操作—— 将 Hp Hq 进行点乘和相减，再作为下一层 layer 的输入：
![](https://pic3.zhimg.com/v2-962651dfe52070902563ad18d2ff8682_b.jpg)
2.) 把 Ranker 和 Reader 放到强化学习(RL) 的框架中， Ranker 负责产生 policy，而 Reader 负责执行 policy 并产生 reward。 Ranker 产生 policy 的过程如下图：
![](https://pic3.zhimg.com/v2-e05350843d54ea2dc81e8b1ab35a3366_b.jpg)
对应的公式如下：
![](https://pic4.zhimg.com/v2-0db6781ea542278f136fc2d8d38fb4e7_b.jpg)
最后的 policy 其实就是 Dset 里文章的概率分布。 在得到 policy 后，所要采取的 action 就是根据概率分布抽取文章，作为 Reader 的输入，如下所示：
![](https://pic2.zhimg.com/v2-95327278052bd36ef8b2b8accfb70481_b.jpg)
这部分其实就是典型的阅读理解模型，可以用前面任何一个模型替代，只不过文中的 encoder 和 decoder 来自 Match-LSTM。 注意到除了 sample 概率最高的文章，还额外加进去一些概率不高的文章作为『负样本』，一起输入 Reader，这么做的原因是使得 softmax 出来的正确答案位置更 sharp，加快模型收敛：
![](https://pic3.zhimg.com/v2-8cb095d0493842dc98516ed482711ee2_b.jpg)
Ranker 跟 Reader 分别用 policy gradient 和 supervised gradient 这两种更新策略来训练，这部分没太多好说的，都是典型的强化学习框架。
![](https://pic2.zhimg.com/v2-eed6e9f83095941b2e30573640e12ab9_b.jpg)
唯一注意的是这里 reward 不能直接用 reader 的 decoder 出来的 cross entropy：
![](https://pic1.zhimg.com/v2-35c0f7cebe48e52c51992c6ff8291774_b.jpg)
因为这个 reward 是没有上下界的，梯度变化会很大，所以需要重新设计一个 reward，文中简单采用了真实答案和预测答案序列的 f1 score，如下所示：
![](https://pic1.zhimg.com/v2-ab728b4a2537a956910482dfca53a374_b.jpg)
当真实答案和预测答案有交集时，用 f1 score 作为 reward；当两者完全一致时，f1 score 是1，但这里 reward 直接设为 2，我理解是为了使收敛更快，更快地找到最优策略；当两者没有交集时，f1 score 是0，但这里 reward 设为 -1 。

参考文献链接

[GA-Reader]

([http://www.aclweb.org/anthology/P17-1168](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/P17-1168)) 

[Match-LSTM]

([http://www.aclweb.org/anthology/P17-1018](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/P17-1018))

[Bi-DAF]

([https://arxiv.org/abs/1611.01603](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.01603))

[R-Net]([https://www.microsoft.com/enus/research/wpcontent/uploads/2017/05/r-net.pdf](https://link.zhihu.com/?target=https%3A//www.microsoft.com/enus/research/wpcontent/uploads/2017/05/r-net.pdf)) 

[QA-Net]

([https://arxiv.org/pdf/1804.09541.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1804.09541.pdf))

[S-Net]

([https://arxiv.org/pdf/1706.04815.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.04815.pdf))

[R3-Net]

([https://arxiv.org/pdf/1709.00023.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1709.00023.pdf))

推荐阅读

[1][机器学习-波澜壮阔40年](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483705%26idx%3D1%26sn%3Dc6e7c4a2e14a2469308b41eb60f155ac%26chksm%3Dfdb69caecac115b8712653600e526e99a3f6976fdaa2f6b6a09388fa6f9677ccb57b40c40ae3%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0413.

[2][学好机器学习需要哪些数学知识？](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483713%26idx%3D1%26sn%3D1e7c81381d16806ac73e15691fe17aec%26chksm%3Dfdb69cd6cac115c05f1f90b0407e3f8ae9be8719e454f908074ac0d079885b5c134e2d60fd64%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0417.

[3] [人脸识别算法演化史](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483726%26idx%3D1%26sn%3D9fef4cc1766ea4258749f8d40cc71a6e%26chksm%3Dfdb69cd9cac115cf4eba16081780c3b64c75e1e55a40bf2782783d5c28f00c6f143426e6f0aa%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0420.

[4][基于深度学习的目标检测算法综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483731%26idx%3D1%26sn%3D237c52bc9ddfe65779b73ef8b5507f3c%26chksm%3Dfdb69cc4cac115d2ca505e0deb975960a792a0106a5314ffe3052f8e02a75c9fef458fd3aca2%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0424.

[5][卷积神经网络为什么能够称霸计算机视觉领域？](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483816%26idx%3D1%26sn%3Dfc52765b012771d4736c9be4109f910e%26chksm%3Dfdb69c3fcac115290020c3dd0d677d987086a031c1bde3429339bb3b5bbc0aa154e76325c225%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0426.

[6] [用一张图理解SVM的脉络](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483937%26idx%3D1%26sn%3D84a5acf12e96727b13fd7d456c414c12%26chksm%3Dfdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0428.

[7] [人脸检测算法综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483950%26idx%3D1%26sn%3Da3a5b7907b2552c233f654a529931776%26chksm%3Dfdb69fb9cac116af5dd237cf987e56d12b0d2e54c5c565aab752f3e366c0c45bfefa76f5ed16%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0503.

[8] [理解神经网络的激活函数](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483977%26idx%3D1%26sn%3D401b211bf72bc70f733d6ac90f7352cc%26chksm%3Dfdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI2018.5.5.

[9] [深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484037%26idx%3D1%26sn%3D13ad0d521b6a3578ff031e14950b41f4%26chksm%3Dfdb69f12cac11604a42ccb37913c56001a11c65a8d1125c4a9aeba1aed570a751cb400d276b6%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0508.

[10] [理解梯度下降法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484111%26idx%3D1%26sn%3D4ed4480e849298a0aff828611e18f1a8%26chksm%3Dfdb69f58cac1164e844726bd429862eb7b38d22509eb4d1826eb851036460cb7ca5a8de7b9bb%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0511.

[11] [循环神经网络综述—语音识别与自然语言处理的利器](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484310%26idx%3D1%26sn%3D0fc55a2784a894100a1ae64d7dbfa23d%26chksm%3Dfdb69e01cac1171758cb021fc8779952e55de41032a66ee5417bd3e826bf703247e243654bd0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0515

[12] [理解凸优化](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484439%26idx%3D1%26sn%3D4fa8c71ae9cb777d6e97ebd0dd8672e7%26chksm%3Dfdb69980cac110960e08c63061e0719a8dc7945606eeef460404dc2eb21b4f5bdb434fb56f92%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】 SIGAI0518

[13] [【实验】理解SVM的核函数和参数](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484495%26idx%3D1%26sn%3D4f3a6ce21cdd1a048e402ed05c9ead91%26chksm%3Dfdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0522

[14][【SIGAI综述】行人检测算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484523%26idx%3D1%26sn%3Dddaa70c4b92f6005d9bbd6b3a3fe4571%26chksm%3Dfdb699fccac110ea14e6adeb873a00d6ee86dd4145ddf8e90c9b878b908ac7b7655cfa51dab6%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0525

[15] [机器学习在自动驾驶中的应用—以百度阿波罗平台为例(上)](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484540%26idx%3D1%26sn%3D733332042c31e1e18ad800f7f527893b%26chksm%3Dfdb699ebcac110fd6607c1c99bc7ebed1594a8d00bc454b63d7f518191bd72274f7e61ca5187%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0529

[16][理解牛顿法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484651%26idx%3D1%26sn%3Da0e4ca5edb868fe3eae9101b71dd7103%26chksm%3Dfdb6997ccac1106a61f51fe9f8fd532045cc5d13f6c75c2cbbf1a7c94c58bcdf5f2a6661facd%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0531

[17] [【群话题精华】5月集锦—机器学习和深度学习中一些值得思考的问题](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484658%26idx%3D1%26sn%3Df5c9f92c272c75883bf8e6f532559f11%26chksm%3Dfdb69965cac11073f49048caef5d7b9129614090a363d9ef7f3d1b9bc59948d2217d2bca7b7b%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI 0601

[18] [大话Adaboost算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484692%26idx%3D1%26sn%3D9b389aa65208c778dddf17c601afbee1%26chksm%3Dfdb69883cac1119593934734e94c3b71aa68de67bda8a946c1f9f9e1209c3b6f0bf18fed99b8%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0602

[19] [FlowNet到FlowNet2.0：基于卷积神经网络的光流预测算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484711%26idx%3D1%26sn%3Dbb7644e101b5924f54d6800b952dc3aa%26chksm%3Dfdb698b0cac111a6605f5b9b6f0478bf21a8527cfad2342dbaaf624b4e9dcc43c0d85ae06deb%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0604

[20] [理解主成分分析(PCA)](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484754%26idx%3D1%26sn%3Db2c0d6798f44e13956bb42373e51d18c%26chksm%3Dfdb698c5cac111d3e3dca24c50aafbfb61e5b05c5df5b603067bb7edec8db049370b73046b24%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0606

[21] [人体骨骼关键点检测综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484784%26idx%3D1%26sn%3Dceafb54203f4e930ae457ad392b9f89c%26chksm%3Dfdb698e7cac111f13d8229d7dcc00b4a7305d66de3da1bd41e7ecc1d29bfa7be520d205c53e9%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0608

[22][理解决策树](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484827%26idx%3D1%26sn%3D043d7d0159baaddfbf92ed78ee5b1124%26chksm%3Dfdb6980ccac1111a9faeae7f517fee46a1dfab19612f76ccfe5417487b3f090ab8fc702d18b8%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0611

[23] [用一句话总结常用的机器学习算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484859%26idx%3D1%26sn%3D2c4db22fb538953a62a90983e3e1f99d%26chksm%3Dfdb6982ccac1113a82e92be325bb07a947d54090274654375f3b50e11e1abd809fb7358bde16%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0611

[24] [目标检测算法之YOLO](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484909%26idx%3D1%26sn%3Dc02ee17e5175230ed39ad63e73249f5c%26chksm%3Dfdb6987acac1116c0108ec28424baf4ea16ca11d2b13f20d4a825d7b2b82fb8765720ebd1063%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0615

[25] [理解过拟合](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484954%26idx%3D1%26sn%3Dc28b7f07c22466e91b1ef90e9dbe3ad1%26chksm%3Dfdb69b8dcac1129bc6e78fca1d550e2b18238ad1c240c73b280d4e529f9f93c4626b3ac45ea2%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0618

[26][理解计算：从√2到AlphaGo ——第1季 从√2谈起](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484981%26idx%3D1%26sn%3Dd3003468b9853851923844812993e060%26chksm%3Dfdb69ba2cac112b4dac620d52100ebd033eb679f29340726a67297c4d6980b16c7cc91122028%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0620

[27] [场景文本检测——CTPN算法介绍](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485005%26idx%3D1%26sn%3D0d4fb43b8db2a8046c64a9cfcbf3f478%26chksm%3Dfdb69bdacac112cce05c8b735b4f8b1ccf2348bea55a30af2055fc328958bb8f1ffd0f819bd2%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0622

[28] [卷积神经网络的压缩和加速](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485042%26idx%3D1%26sn%3Dcdcf8d4b07acf64c7a6f5f7c1a731a12%26chksm%3Dfdb69be5cac112f377766984afb87313c1e1c58d94c80005f0f6f6af61ee5a4bd1bf6c6157b6%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0625

[29] [k近邻算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485074%26idx%3D1%26sn%3D0ebf1bf8f49e9c46075fe3803d04c95d%26chksm%3Dfdb69b05cac112132d280c70af3923ca4c3cccfa5fcd8628b79d4b246b3b2decbc80a180abb3%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0627

[30][自然场景文本检测识别技术综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485142%26idx%3D1%26sn%3Dc0e01da30eb5e750be453eabe4be2bf4%26chksm%3Dfdb69b41cac11257ae22c7dac395e9651dab628fc35dd6d3c02d9566a8c7f5f2b56353d58a64%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0627

[31] [理解计算：从√2到AlphaGo ——第2季 神经计算的历史背景](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485155%26idx%3D1%26sn%3D990cc7400751c36e9fef0a261e6add2a%26chksm%3Dfdb69b74cac112628bdae14c6435120f6fece20dae9bf7b1ffc8b8b25e5496a24160feca0a72%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0704

[32] [机器学习算法地图](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485306%26idx%3D1%26sn%3Dfc8cc8de313bdb61dcd39c1dedb240a4%26chksm%3Dfdb69aedcac113fb4b18c74248a313536ded50bade0e66b26f332ab247b148519da71ff2a3c0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0706

[33] [反向传播算法推导-全连接神经网络](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485446%26idx%3D1%26sn%3D57d7d866443810c20c4ea2c6ee8018cc%26chksm%3Dfdb69591cac11c8773638b396abe43c0161e4d339f0fa845e54326be3e8c4933a3b6a2713dae%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0709

[34] [生成式对抗网络模型综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485551%26idx%3D1%26sn%3D213f48c4e55bee688cf0731097bb832c%26chksm%3Dfdb695f8cac11ceef3ef246c54d811dd64d8cc45fc75488c374c7aa95f72c1abfb55555ef0b7%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0709.

[35][怎样成为一名优秀的算法工程师](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485570%26idx%3D1%26sn%3D5e71437a6d3ddf6d05144fc99e7633cc%26chksm%3Dfdb69515cac11c030cf713ec85293b7ad3bbe0f20d43fc2729cc976ff628aabf636834ccd904%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0711.

[36] [理解计算：从根号2到AlphaGo——第三季 神经网络的数学模型](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485592%26idx%3D1%26sn%3D1c5236972402ea8cb168161bc41e8e7e%26chksm%3Dfdb6950fcac11c19ad047e7cb9ced96447a85b41e21b10789a86ae4a211e4fb2ca1f911a7fc5%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0716

[37][【技术短文】人脸检测算法之S3FD](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485609%26idx%3D1%26sn%3Dd9068aecfbf150b40103210de538fea9%26chksm%3Dfdb6953ecac11c28361435306a7a09632ea79000abf1bf626e50afb3cda48eb3e47b96c6e7cd%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0716

[38] [基于深度负相关学习的人群计数方法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485617%26idx%3D1%26sn%3D7955bfefc8e4b28221ae5247812f8235%26chksm%3Dfdb69526cac11c30e1051edc4378d7dfdedf46925236dbe33e7912b4bea882e515f074eee4c9%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0718

[39] [流形学习概述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485668%26idx%3D1%26sn%3Df70547fc671d164e28fddcea6473524d%26chksm%3Dfdb69573cac11c65ee9fc98ac8fad093282a3d244748e7c88541c133ac55a32cb07472dc80e0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0723

[40] [关于感受野的总结](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485700%26idx%3D1%26sn%3Dc0425495fe0ae9cb2120dbcb246f49b1%26chksm%3Dfdb69493cac11d8542f7a8e662a7ecdeece4fd2270c71504023e8b58128575d1e4fdadf60cf5%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0723

[41] [随机森林概述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485718%26idx%3D1%26sn%3Dc05c217af81173ae2c1301cbda5f7131%26chksm%3Dfdb69481cac11d975d86ff2e280371963d04b5709dfa0a9e874d637b7cf3844cad12be584094%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0725

[42] [基于内容的图像检索技术综述——传统经典方法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485741%26idx%3D1%26sn%3Df8217e523d54842daaa4be38fa1792eb%26chksm%3Dfdb694bacac11dacfd8d7e4882166e2774c4685c043379ce0d2044e99c3b3c8b0140480bbf8e%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0727

[43] [神经网络的激活函数总结](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485762%26idx%3D1%26sn%3De1e9fc75b92999177d3c61c655b0e06e%26chksm%3Dfdb694d5cac11dc37dac1a7ce32150836d66f0012f35a7e04e3dceaf626b8453dc39ee80172b%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0730

[44] [机器学习和深度学习中值得弄清楚的一些问题](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485811%26idx%3D1%26sn%3Da87082f0e47b01bb8c22443ba5b1139c%26chksm%3Dfdb694e4cac11df20ea1deb8b55cf297ad44e48a4c7ca45cfce387284143403fcd342ac98ec0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0802

[45] [基于深度神经网络的自动问答系统概述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485842%26idx%3D1%26sn%3Dd7485054d6e93225b6ac0c77f8706cf7%26chksm%3Dfdb69405cac11d1355b84f692c2cbe49a3852a10e074b6941c95618598caea6ed64103c4ee4c%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0803

[46] [反向传播算法推导——卷积神经网络](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485971%26idx%3D1%26sn%3D632e3c135ead5f0ac730eee5b6a05647%26chksm%3Dfdb69784cac11e9258f67312fa2798e9c8210b8f77741a3d7bab4c1ccfa1c1f66632183f4f26%26token%3D1469111007%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0806

[47] [机器学习与深度学习核心知识点总结 写在校园招聘即将开始时](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486105%26idx%3D1%26sn%3Dd0b33e7e23b0e2fc442bd6b3e2a9d952%26chksm%3Dfdb6970ecac11e18085ea36f3b654028b2d4ba33a0cdc89c4ea25ac81570969f95f84c6939ac%26token%3D1065243837%26lang%3Dzh_CN%23rd) 【获取 码】SIGAI0808

[48] [理解Spatial Transformer Networks](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486133%26idx%3D1%26sn%3D31c64e83511ad89929609dbbb0286890%26chksm%3Dfdb69722cac11e34da58fc2c907e277b1c3153a483ce44e9aaf2c3ed468386d315a9b606be40%26token%3D283993194%26lang%3Dzh_CN%23rd)【获取码】SIGAI0810

[49][AI时代大点兵-国内外知名AI公司2018年最新盘点](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486140%26idx%3D1%26sn%3D6157951b026199422becca8863f18a17%26chksm%3Dfdb6972bcac11e3d7427847df818bd6cc7893a261daa0babe0b11bd01a403dc4f36e2b45650e%26token%3D283993194%26lang%3Dzh_CN%23rd)【获取码】SIGAI0813

[50] [理解计算：从√2到AlphaGo ——第2季 神经计算的历史背景](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486202%26idx%3D1%26sn%3Dbf66be1e71272be42508e557ed93acbf%26chksm%3Dfdb6976dcac11e7b9d0f878233e8d9907825e57851e7a095f1be3a23abc9327370a77f4e2c03%26token%3D283993194%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0815

[51] [基于内容的图像检索技术综述--CNN方法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486268%26idx%3D1%26sn%3Ddff53feb3d78ea7698f70bede08b3b19%26chksm%3Dfdb696abcac11fbdcde5f4acc72d34c14119a21234b9e6cd0c1886b85c330e7f77d6e3da9122%26token%3D283993194%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0817

[52][文本表示简介](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486289%26idx%3D1%26sn%3Da312c951c943ad104c722e6c681823b6%26chksm%3Dfdb696c6cac11fd01224a68b9d67fcf0043ff2de0ec67f7bfd98bacf691c8eaf221cbca9b4d6%26token%3D1485183944%26lang%3Dzh_CN%23rd) 【获取码】SIGAI0820

[53][机器学习中的最优化算法总结](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486354%26idx%3D2%26sn%3D1afd5f272536b6771b5e1e224e8414ec%26chksm%3Dfdb69605cac11f13f2a16c8748e333045e99497dc03ca9f4741723204e7d8348e969375379ae%26token%3D1580317399%26lang%3Dzh_CN%23rd)【获取码】SIGAI0822

[54][【AI就业面面观】如何选择适合自己的舞台？](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486371%26idx%3D1%26sn%3D955286243fe23ff58d5a090c166d83f6%26chksm%3Dfdb69634cac11f2251793b40432b5429828f5e72411e2ae48c1ae282117f2d1b4067c14fc889%26token%3D1383761413%26lang%3Dzh_CN%23rd)【获取码】SIGAI0823

[55][浓缩就是精华-SIGAI机器学习蓝宝书](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486392%26idx%3D1%26sn%3D81cc65e42418bf846167ab80ae1dd4f4%26chksm%3Dfdb6962fcac11f39d3c62bc06e8a0708a24a024e53aef38e1bd716dcd1433fa89a0c0fe422df%26token%3D867271861%26lang%3Dzh_CN%23rd)【获取码】SIGAI0824

[56][DenseNet详解](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486433%26idx%3D2%26sn%3D9858164d290b60c03081ee0b04d57a76%26chksm%3Dfdb69676cac11f60f48b9bce3017e2776767f3f9c2cc980e03c07dabc076a9ce8da4b906760d%26token%3D1199765642%26lang%3Dzh_CN%23rd)【获取码】SIGAI0827

原创声明：本文为 SIGAI 原创文章，仅供个人学习使用，未经允许，不能用于商业目的

