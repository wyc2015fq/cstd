# 哪些成为了经典-引用次数最多的10篇机器学习文献 - 知乎
# 

获取全文PDF请查看：[哪些成为了经典-引用次数最多的10篇机器学习文献](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_91.html)

近40年来机器学习领域产生了数以万计的论文，并以每年上万篇的速度增长。但真正能够称为经典、经受住历史检验、能投入实际应用的并不多。本文整理了机器学习历史上出现的经典论文，按照被引用次数对它们进行了排序，分为top10，被引用次数超过2万，被引用次数超过1万，未来有潜力的文章4部分。它们已经或者在未来具有资格被写入机器学习、深度学习、人工智能的教科书，是一代又一代研究人员为我们留下的宝贵财富。需要说明的是，引用次数对近几年新出现的文章是不公平的，它们还处于高速增长期，但好酒就是好酒，随着时间的沉淀会越来越香。

引用次数最高的10篇文献

第1名-EM算法

Arthur P Dempster, Nan M Laird, Donald B Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the royal statistical society series b-methodological, 1976.

被引用次数：55989

令笔者惊讶的是排名第一的居然不是支持向量机，集成学习，深度学习，决策树等历史上赫赫有名的算法，而是EM。这是EM算法的原文，引用次数高达5万多！EM算法在很多版本的排名中都被称为机器学习的10大算法之一。它在数学上优美，实现起来也很简单，是求解含有隐变量的最大似然估计、最大后验概率估计的有力工具，在高斯混合模型，隐马尔可夫模型等问题上得到了成功的应用。在SIGAI之前的公众号文章“理解EM算法”中对其原理进行了详细的介绍。

第2名-logistic回归

David W Hosmer, Stanley Lemeshow. Applied logistic regression. Technometrics. 2000.

被引用次数：55234

代表了线性模型这一山头。这不是logistic回归的原文，logistic回归在这之前几十年就已经被提出，但这篇文献的引用次数却达到了，虽然它不是论文而是书的形式，但其引用次数比著名的PRML还要高。这也符合我们的直观认识，logistic回归虽然简单，但却实用，在工程上，往往是越简单的东西越有用。

第3名-随机森林

Breiman, Leo. Random Forests. Machine Learning 45 (1), 5-32, 2001.

被引用次数：42608

代表了集成学习这一大山头。Breiman的随机森林，分类与回归树分列第3/4名。而随机森林的排名比AdaBoost算法要高。同样的，随机森林也很简单，但却好用。在SIGAI之前的公众号文章“随机森林概述”中对集成学习，bagging，随机森林进行了详细的介绍。

第4名-分类与回归树

Breiman, L., Friedman, J. Olshen, R. and Stone C. Classification and Regression Trees, Wadsworth, 1984.

被引用次数：39580

这是分类与回归树的原文，代表了决策树这一山头。在各种决策树中，分类与回归树（CART）应当是用的最广的，现在还被用于充当随机森林，AdaBoost，梯度提升算法的弱学习器。Breiman老爷子在2005年已经逝去，但他留给我们大片的树和森林。在SIGAI之前的公众号文章“理解决策树”中对这一算法进行了详细的介绍。

第5名-支持向量机开源库libsvm

C.-C. Chang and C.-J. Lin. LIBSVM: a Library for Support Vector Machines. ACM TIST, 2:27:1-27:27, 2011.

被引用次数：38386

这篇文章介绍了libsvm开源库。引用次数超过了支持向量机的原文，应该算是公开的最经典的支持向量机实现，其作者是台湾大学林智仁教授及其学生。相信很多做机器学习研究和产品的同学都用过它。在SIGAI之前的公众号文章“用一张理解SVM”，“理解SVM核函数和参数的作用”中对SVM进行了详细的介绍。

第6名-统计学习理论

An overview of statistical learning theory. VN Vapnik - IEEE transactions on neural networks

被引用次数：36117

Top10中唯一一篇理论层面的文章，出自Vapnik之手。他最有影响力的成果是支持向量机，VC维。但机器学习理论文章，整体来说引用次数相对较少，应该与做这些方向的研究者更少，文章更少有关，大部分人还是在做某些具体的算法。

第7名-主成分分析

Ian T. Jolliffe. Principal Component Analysis. Springer Verlag, New York, 1986.

被引用次数：35849

代表了降维算法这一山头。这篇文献不是主成分分析的原文，其原文发表于1个多世纪以前。这个排名对得起主成分分析的江湖地位，在各种科学和工程数据分析中，PCA被广为应用。在SIGAI之前的公众号文章“理解主成分分析（PCA）”中对PCA进行了介绍。

第8名-决策树树-C4.5

J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco, CA, 1993.

被引用次数：34703

又是决策树。决策树简单实用，可解释性强，是机器学习早期的重要成果。

第9名-深度卷积神经网络

Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton. ImageNet Classification with Deep Convolutional Neural Networks. 2012.

被引用次数：34574

代表了深度学习这一山头。深度卷积神经网络的开山之作，将严乐村的卷积神经网络发扬光大。这篇2012年才发表的文章能有如此的引用次数实属不易，刚酿造出来的酒就是名酒，出自Hinton之手这也不奇怪。如此的被引用次数是当前炙手可热的深度学习造就的。同样的，没有复杂的公式和理论，但却出奇的好用。

第10名-支持向量机

Cortes, C. and Vapnik, V. Support vector networks. Machine Learning, 20, 273-297, 1995.

被引用次数：33540

代表了线性模型、核技巧的山头，这是SVM正式的原文。支持向量机才排到第10位让人有些奇怪，它可是在机器学习的江湖中风光了近20年的算法，当年言必称SVM。

总结这top10的文献可以看出，简单才是美。这些文献提出的算法没有复杂的数学公式和晦涩难解的理论，但确实最经典的，因为有用！它们体现的是更深层次的哲学思想。其实在其他科学领域也是如此，数学领域中最经典的一些定理和公式也是非常的优美而简洁，类似的还有物理。在top10中，Breiman和Vapnik两次上榜。


引用次数超过2万的文献

除了top10之外，还有一些被引用次数超过2万的文章 ，也堪称经典。

Lawrence R. Rabiner. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE. 77 (2): 257–286. 1989.

被引用次数：26466

代表了概率图模型这一山头。终于见到了概率图模型，过去几十年中，引用最广的概率图模型当属隐马尔可夫模型（HMM）。这篇文章不是HMM的原文，但却写成了经典，对HMM的原理，在语音识别中的建模方法讲解得清晰透彻。

MacQueen, J. B. Some Methods for classification and Analysis of Multivariate Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. 1. University of California Press. pp. 281–297, 1967

被引用次数：24035

代表了聚类算法的山头。k均值算法的开山之作，它也在各种排名中都被称为机器学习10大经典算法，同样是简单而易于理解，我相信中学生都能看懂它！

J. Ross Quinlan. Induction of decision trees. Machine Learnin, 1(1): 81-106, 1986.

被引用次数：20359

介绍决策树的文献，不过多解释，地位摆在这里。Quinlan也是决策树的一大山头。


引用次数超过1万的文献

Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500). 2000: 2323-2326.

被引用次数：12941

Tenenbaum, Joshua B and De Silva, Vin and Langford, John C. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500). 2000: 2319-2323.

被引用次数：11927

流形学习的双雄，两篇代表作，开这一领域之先河。流形学习是当年非常热门的方向。这两篇文章都发在Science上，要知道，计算机科学的论文发Science和Nature是非常难的。在SIGAI之前的公众号文章“流形学习概述”中对这类算法进行了介绍。

Ronald A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7 Part 2: 179-188, 1936.

被引用次数：15379

线性判别分析的原文，1936年就已经发表了，那时候二战还没有爆发。

Burges JC. A tutorial on support vector machines for pattern recognition. Bell Laboratories, Lucent Technologies, 1997.

被引用次数：19885

介绍支持向量机在模式中应用的文章，SVM当年真是灌水的好方向！

Yoav Freund, Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. computational learning theory. 1995.

被引用次数：16431

AdaBoost算法的经典之作，与SVM并列为当年的机器学习双雄。这是集成学习第一个有广泛影响力的算法，在人脸检测等问题上取得了成功。在SIGAI之前的公众号文章“大话AdaBoost算法”，“理解AdaBoost算法”中对它进行了详细的介绍。

Lafferty, J., McCallum, A., Pereira, F. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proc. 18th International Conf. on Machine Learning. Morgan Kaufmann. pp. 282–289. 2001.

被引用次数：11978

条件随机场的经典之作，这种方法在自然语言处理，图像分割等问题上得到了成功的应用，如今还被与循环神经网络整合在一起，解决自然语言处理等领域中的一些重点问题。

David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by back-propagating errors. Nature, 323(99): 533-536, 1986.

被引用次数：16610

严格意义上的反向传播算法的原文，发在Nature上，重要性就不解释了。现在的深度学习还是使用它。Hinton的名字再一次出现。在SIGAI之前的公众号文章“反向传播算法推导-全连接神经网络”，“反向传播算法推导-卷积神经网络”中进行了详细的讲解。

Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators. Neural Networks, 2, 359-366, 1989.

被引用次数：16394

神经网络的理论文章，著名的万能逼近定理，从理论上证明了至少有1个隐含层的神经网络尅逼近闭区间上任意连续函数到任意指定精度，为神经网络和深度学习提供了强有力的理论保障。

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, november 1998.

被引用次数：16339

LeNet网络的原文，被引用次数比严乐村同志在1989年，1990年提出卷积神经网络的论文还多。也让严乐村得到了卷积神经网络之父的称号。

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Going Deeper with Convolutions, Arxiv Link: [http://arxiv.org/abs/1409.4842](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.4842).

被引用次数：11268

GoogLeNet网络的原文，做深度学习的同学都知道。发表于2015年的文章能有如此的引用次数，当然得利于深度学习的火爆。

K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. international conference on learning representations. 2015.

被引用次数：18980

VGG网络的原文，经典的卷积网络结构，被用在各个地方，引用次数比GoogLeNet多不少。

Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. computer vision and pattern recognition, 2015.

被引用次数：17285

残差网络的原文，做深度学习的同学都知道，终于用中国人名字上榜，加油！

S. Hochreiter, J. Schmidhuber. Long short-term memory. Neural  computation, 9(8): 1735-1780, 1997.

被引用次数：15448

LSTM的原文，让循环神经网络真正走向了实用。作者在深度学习领域做出了重要的贡献，但却非常低调，以至于很多人都不知道。

Martin Ester, Hanspeter Kriegel, Jorg Sander, Xu Xiaowei. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pp. 226–231, 1996.

被引用次数：13817

又是聚类算法，著名的DBSCAN-基于密度的聚类算法的典型代表。这算法也非常简单，但也非常强大，没有一个超过中学数学范围之外的公式。在SIGAI之前的公众号文章“聚类算法概述”中对它进行了介绍。

Dorin Comaniciu, Peter Meer. Mean shift: a robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002.

被引用次数：12146

大名鼎鼎的mean shift算法，同样是及其简洁优美，但非常好用。做机器学习，机器视觉的同学肯定都知道，尤其是做视觉领域中目标跟踪算法的。


未来可能有潜力的文献

下面这些文章的被引用次数目前还没有超过1万，但它们都还很年轻，未来很有前途，因此单独列出。需要强调的是有几篇强化学习的文章虽然是1990年代发表的，但我们也列出来了，它们会随着深度学习研究的进展而逐渐体现出更重要的价值。

Goodfellow Ian, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets. Advances in Neural Information Processing Systems, 2672-2680, 2014.

被引用次数：6902

生成对抗网络的开山之作，代表了深度生成模型这一山头。生成对抗网络的思想简单而优美，而且有效，出现了大量改进算法和各种应用。变分自动编码器（VAE）是仅次于GAN的深度生成模型，但其原文的被引用次数远不及GAN。

Richard Sutton. Learning to predict by the methods of temporal differences. Machine Learning. 3 (1): 9-44.1988.

被引用次数：5108

时序差分算法的开山之作，地位就不多解释了。

Mnih, Volodymyr, et al. Human-level control through deep reinforcement learning. Nature. 518 (7540): 529-533, 2015.

被引用次数：4570

深度强化学习的重量级作品，出自DeepMind公司之手。第一篇文章发表于2013年，引用次数远不及这篇，这篇可是发在Nature上，开创了DQN算法。

David Silver, et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature, 2016.

被引用次数：4123

AlphaGo的原文，就不解释了，地球人都知道。

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

被引用次数：8308

Q学习的原文，奠定了这一算法的基础，也是DQN的基础。

