# 循环神经网络综述 -语音识别与自然语言处理的利器（下篇） - 知乎
# 

接 循环神经网络综述-语音识别与自然语言处理的利器（上篇）

本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
- [书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
- [书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)

**机器视觉**

对于机器视觉中的某些问题，循环神经网络也取得了很好的效果。在这些问题中，数据都被抽象成一个时间序列，如物体运动的动作，状态等。

**字符识别**

如果我们知道每个字符的笔画信息，即整个字的书写过程，则可以将手写字符识别看成是一个轨迹分类问题。每个手写字符是一个序列数据，每个时刻的坐标连接起来，在平面上构成一个字符的图像。手写字符识别属于序列标记问题中的序列分类问题，即给定一个字符的坐标点序列，预测这个字符的类别。在这里，循环神经网络的输入为坐标点序列，输出值为类别，为了达到这个目的，我们可以将最后一个时刻的循环层输出值映射为类别概率，这可以通过softmax层实现。

另外，也可以直接以图像作为输入，在这里，将图像看作是一个序列，序列中的每一个向量是图像中的一个行的像素。依次将每一行输入循环神经网络，最后时刻的隐含层状态输出作为提取的字符特征，送入softmax层进行分类。

**目标跟踪**

运动跟踪可以抽象为已知目标在之前时刻的坐标，预测出它在当前时刻的坐标，这同样是一个序列预测问题。

文献[42]提出了一种用循环神经网络进行目标跟踪的方法，称为RTT。RTT主要目标是解决目标遮挡问题。循环神经网络的作用是得到置信度图，即每个点处是目标的概率。下面介绍这种方法的处理流程。
![](https://pic2.zhimg.com/v2-fe556026d23a148cc01d3bc5138c8acd_b.jpg)
在对每一帧进行跟踪时，给定目标在上一帧中的矩形框，以目标的中心为中心，以目标宽高的2.5倍为宽高，即将目标矩形放大2.5倍，得到一个矩形的候选区域。然后，将这个候选区域划分成网格。然后对每个矩形框提取特征，可以使用HOG特征，也可以使用更复杂的卷积网络提取的特征。在这里，划分网格而不是对整个候选区域计算特征的原因是这样做能够更好的处理遮挡，以及目标外观的变化。最后我们得到候选区域的特征。

然后，以这个特征作为输入，用多维RNN对特征进行处理，得到置信度图。最后根据置信度图完成对目标位置的预测。

和单个目标跟踪不同，多目标跟踪需要解决数据关联问题，即上一帧的每个目标和下一帧的哪个目标对应，还要解决新目标出现，老目标消失问题。多目标的跟踪的一般流程为每一时刻进行目标检测，然后进行数据关联，为已有目标找到当前时刻的新位置，在这里，目标可能会消失，也可能会有新目标出现，另外目标检测结果可能会存在虚警和漏检测。联合概率滤波，多假设跟踪，线性规划，全局数据关联，MCMC马尔可夫链蒙特卡洛算法先后被用于解决数据关联问题来完成多个目标的跟踪。

首先我们定义多目标跟踪的中的基本概念，目标是我们跟踪的对象，每个目标有自己的状态，如大小、位置、速度。观测是指目标检测算法在当前帧检测出的目标，同样的，它也有大小、位置、速度等状态值。在这里，我们要建立目标与观测之间的对应关系。下图是数据关联的示意图：

![](https://pic3.zhimg.com/v2-6d101951c6c80e5b315ee05ddb4298aa_b.jpg)

在上图中，第一列圆形为跟踪的目标，即之前已经存在的目标；第二列圆为观测值，即当前帧检测出来的目标。在这里，第1个目标与第2个观察值匹配，第3个目标与第1个观测值匹配，第4个目标与第3个观测值匹配。第2个和第5个目标没有观测值与之匹配，这意味着它们在当前帧可能消失了，或者是当前帧被漏检，没有检测到这两个目标。类似的，第4个观测值没有目标与之匹配，这意味着它是新目标，或者虚警。

文献[43]提出了一种用循环神经网络在线跟踪多个目标的算法。这种方法实现了完全端到端的训练。在这里，用LSTM循环神经网络同时解决数据关联、新目标出现、老目标消失问题。

首先定义状态向量 ![X_{t}](https://www.zhihu.com/equation?tex=X_%7Bt%7D) ，这是一个NxD维向量，表示t时刻所有目标的状态值，其中，D为每个目标的状态个数，在这里值为4，分别为目标的位置和宽高。定义N为某一帧中能够同时跟踪的最大目标个数。 ![X_{t}^{i}](https://www.zhihu.com/equation?tex=X_%7Bt%7D%5E%7Bi%7D) 为第i个目标的状态。

类似的定义观测向量 ![Z_{t}](https://www.zhihu.com/equation?tex=Z_%7Bt%7D) ，这是一个MxD维向量，表示t时刻所有观测值。其中M为每一帧中最大检测目标个数。需要注意的是，我们对模型能够处理的最大目标个数并没有限制。

接下来定义分配概率矩阵A，这是一个Nx(M+1)的矩阵，元素取值0和1之间的实数。矩阵的每一行为一个目标的分配概率向量，即元素 ![A_{i j}](https://www.zhihu.com/equation?tex=A_%7Bi+j%7D) 表示将第i个目标分配给第j个观测的概率。分配概率矩阵满足约束条件：
![](https://pic2.zhimg.com/v2-b0a3675c89680b5bb0da08c2e27b8be5_b.jpg)
在这里矩阵的列数不是M而是M+1，这是因为一个目标可能不和任何一个观测向匹配。

最后定义指示向量 ![\varepsilon](https://www.zhihu.com/equation?tex=%5Cvarepsilon) ，这是一个N维向量，每个元素表示一个目标存在的概率值。

跟踪问题被分成两个部分来解决：状态预测与更新，以及跟踪管理；数据关联。前一部分负责单个目标的状态跟踪；后一部分解决目标之间的对应关系。
![](https://pic4.zhimg.com/v2-6255e843dd5cd35a22d1cd03f628a837_b.jpg)
对于第一个问题，用一个时序循环神经网络来学习N个目标的运动模型，以及目标的指示变量，指示变量用于处理目标的出现与消失。在时刻t，循环神经网络输出四种值：

1.包括所有目标的状态预测值 ![X_{t+1}^{*}](https://www.zhihu.com/equation?tex=X_%7Bt%2B1%7D%5E%7B%2A%7D) ，前面已经介绍过。

2.所有目标状态的更新值 ![X_{t+1}](https://www.zhihu.com/equation?tex=X_%7Bt%2B1%7D) 。

3.指示向量 ![\varepsilon_{t+1}](https://www.zhihu.com/equation?tex=%5Cvarepsilon_%7Bt%2B1%7D) ，其每个元素的值位于(0,1)之间，表示目标是一个真实轨迹的概率。

4. ![\varepsilon_{t+1}^{*}](https://www.zhihu.com/equation?tex=%5Cvarepsilon_%7Bt%2B1%7D%5E%7B%2A%7D) ，这是与 ![\varepsilon_{t}](https://www.zhihu.com/equation?tex=%5Cvarepsilon_%7Bt%7D) 的差值。

神经网络的输入为前一个时刻的状态值 ![X_{t}](https://www.zhihu.com/equation?tex=X_%7Bt%7D) ，前一个时刻的指示向量值 ![\varepsilon_{t}](https://www.zhihu.com/equation?tex=%5Cvarepsilon_%7Bt%7D) ，当前时刻的观测值 ![Z_{t+1}](https://www.zhihu.com/equation?tex=Z_%7Bt%2B1%7D) ，以及当前时刻的数据关联矩阵 ![A_{t+1}](https://www.zhihu.com/equation?tex=A_%7Bt%2B1%7D) ，数据关联矩阵的计算方法将在后面介绍。
![](https://pic2.zhimg.com/v2-f991e0a9836032a5cee6b7363de6a259_b.jpg)
这个功能模块有三个目标：

1.预测。为指定数量的目标学习一个复杂的运动模型，这个模型包含了每个目标的运动参数，包括速度，加速度信息等。

2.更新。根据当前的观测数据，对预测值进行校正，修正物体的状态值，包括运动状态值。

3.目标的出现与消失。学习到如何根据目标的状态值、当前时刻的观测值，以及数据关联信息来处理新目标的出现，已有目标的消失问题。

预测值 ![X_{t+1}^{*}](https://www.zhihu.com/equation?tex=X_%7Bt%2B1%7D%5E%7B%2A%7D) 只取决于状态值 ![X_{t}](https://www.zhihu.com/equation?tex=X_%7Bt%7D) 和循环神经网络隐含层的状态值 ![h_{t}](https://www.zhihu.com/equation?tex=h_%7Bt%7D) 。一旦数据关联矩阵 ![A_{t+1}](https://www.zhihu.com/equation?tex=A_%7Bt%2B1%7D) 已经确定，即已经知道了目标和观测之间的对应关系，我们就可以根据观测值来更新状态值，完成校正。接下来，将观测值和预测的状态值拼接在一起：
![](https://pic2.zhimg.com/v2-231e2b764340ec555b004b2656485ec9_b.jpg)
然后乘以矩阵 ![A_{t+1}](https://www.zhihu.com/equation?tex=A_%7Bt%2B1%7D) 。同时 ![\varepsilon_{t+1}](https://www.zhihu.com/equation?tex=%5Cvarepsilon_%7Bt%2B1%7D) 也被计算出来。在确定了网络的输出和输出之后，我们需要定义训练时的损失函数。损失函数定义为：
![](https://pic3.zhimg.com/v2-b30eabb8ad98f86ce00bf2bed75955ea_b.jpg)

其中 ![X^{*}，X，\varepsilon](https://www.zhihu.com/equation?tex=X%5E%7B%2A%7D%EF%BC%8CX%EF%BC%8C%5Cvarepsilon) 为预测值， ![\tilde{X}，\tilde{\varepsilon}](https://www.zhihu.com/equation?tex=%5Ctilde%7BX%7D%EF%BC%8C%5Ctilde%7B%5Cvarepsilon%7D) 为真实值。上面损失函数的第一部分为预测误差，第二部分为更新误差，第三和第四部分为目标消失、出现以及回归值误差。这里只是定义了某一个时刻的误差值，训练时需要将每一帧的误差值累加起来，然后计算平均值。第一部分误差的意义是在没有观察值的情况下，预测值要和目标的真实运动轨迹尽可能接近。第二部分的意义是得到观测值之后，要将预测值校正到和观测值尽可能接近。

第三部分损失反应了目标的出现与消失。如果 ![\varepsilon =1](https://www.zhihu.com/equation?tex=%5Cvarepsilon+%3D1) ，表示一个目标存在，如果 ![\varepsilon =0](https://www.zhihu.com/equation?tex=%5Cvarepsilon+%3D0)

，表示这个目标不存在。为此我们定义交叉熵损失函数：
![](https://pic2.zhimg.com/v2-607660777fb773dff8c6a04e7b15d691_b.jpg)
最后一个问题是数据关联。数据关联的目标是为每个目标分配一个唯一的观测值，这是一个组合优化问题，直接求解的话是NP完全问题。在这里，采用LSTM网络通过学习来解决此问题。在这里，网络的输入是成对距离矩阵C，这是一个NxM的矩阵，矩阵元素定义为：
![](https://pic1.zhimg.com/v2-742d6bf0f5f3530c5034c835e21a841c_b.jpg)
即第i个目标的预测状态与第j个观察值之间的欧氏距离。当然，我们也可以使用更多的信息，如目标的外观或其他相似度。网络的输出值为概率向量 ![A^{i}](https://www.zhihu.com/equation?tex=A%5E%7Bi%7D) ，表示第i个目标与所有观测值之间的分配概率，这可以通过softmax层输出。这里的![A^{i}](https://www.zhihu.com/equation?tex=A%5E%7Bi%7D)是数据关联矩阵的第i行。最后，我们定义网络训练时的损失函数为：
![](https://pic3.zhimg.com/v2-f55b3223174d6086b1f8605465553282_b.jpg)
其中 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 是一个标量，是目标i的真实分配值，即将目标i分配给观测
![](https://pic3.zhimg.com/v2-12c09aaaee72f4b452051c2b720438ca_b.jpg)

**视频分析**

视频动作识别是机器视觉领域的一个重要问题，它的目标是对运动物体的动作进行分类，如人的站立，坐下等动作。动作识别在诸多领域有实际的应用，如视频监控、人机交互、游戏控制等。这个问题可以抽象成一个时间序列分类问题。以人的动作识别为例，它的输入是目标关键点坐标序列，如人体一些关键点的2D或3D坐标，输出值为动作类别，即序列的标签值。

文献[45]提出了一种整合了卷积神经网络和循环神经网络的框架进行人体动作分类的方法。整个系统包括一个3D卷积神经网络和一个循环神经网络。其中，3D卷积神经网络的输入为多张图像，用于提取一段视频的时空特征。然后将提取的特征序列送入循环神经网络中进行分类。

在这里，卷积神经网络的输入为3D图像。整个视频被分成一系列的固定长度片段，每个片段包括相同数量的帧，被处理成固定大小的输入图像。第三个卷积层后面是两个全连接层，最后一个全连接层有6个神经元，即卷积网络的输出向量为6维。
![](https://pic1.zhimg.com/v2-4f7a455a5e708c640449982b173b4200_b.jpg)
接下来将卷积得到的固定长度的特征向量序列送入LSTM循环神经网络。用循环神经网络的输出完成对视频的分类。

文献[46]提出了一种用双向LSTM循环神经网络进行3D手势分类的方法。在这里，每个时刻用加速度计和陀螺仪测量出手在3D空间的加速度和角速度，形成一个6D的向量，作为循环神经网络的输入，这是一个序列数据。循环神经网络采用双向LSTM网络。循环神经网络的输出向量维数和要分类的手势类型数相同，最后通过softmax层产生概率输出用于分类。这些都是标准的做法，不再详细讲述。

文献[47]提出了一种用分层循环神经网络进行人体动作识别的方法，在这里，利用了人体骨架的关键点信息，对骨架关键点的运动轨迹进行分析。

整个人体被分成5个部分进行建模，分别为四肢和躯干。整个处理流程为：

1.将5个部分分别送入5个子网络中进行处理

2.将四肢和躯干在第一步中的处理结果分别进行融合，送入4个子网络中进行处理

3.将两只胳膊，两条腿，躯干在第二步中的处理结果进行融合，送入2个子网络中进行处理

4.将上一步中的两个结果融合，送入第4层子网络中进行处理

5.将上一步的结果送入全连接层中进行处理

6.最后用softmax层进行计算，得到分类概率

在这里，所有循环层都使用双向循环结构，前面3个循环层都采用tanh激活函数，最后一个循环层采用LSTM单元。循环层和全连接层的计算方式和前面介绍的标准结构相同，在这里不详细讲述。

![](https://pic1.zhimg.com/v2-1bde64dc5a5d736d155295f3956c978c_b.jpg)

全连接层在各个时刻的输出向量被累计起来，然后用softmax层进行概率输出。整个网络的输入为人体各个部位关键点的3D坐标，送入网络之前，对坐标进行了归一化处理；要识别的动作类型根据实际应用而定。

文献[48]提出了一种整合卷积神经网络和循环神经网络的视频识别方法。在这里，用卷积网络提取单帧图像的特征，多个帧的特征依次被送入循环神经网络中进行处理。这种结构不仅在空间上具有深度，在时间上也具有深度，称为Long-term Recurrent Convolutional Networks，简称LRCNs。

整个系统的输入是一系列的视频帧，对于每一帧，首先经过卷积网络的作用，产生固定长度的输出向量。经过这一步，我们得到一个固定长度的序列数据：
![](https://pic2.zhimg.com/v2-6aa559fc47933aaeef142e86f9c40325_b.jpg)
这个序列数据被送入循环神经网络中进行处理，得到输出值。最后，经过softmax层，得到概率输出。这里的卷积网络和循环神经网络的变换和前面介绍的标准做法一致，不再重复介绍。

![](https://pic4.zhimg.com/v2-759a82505fd0fddcda11d02feece52ff_b.jpg)

假设循环神经网络的学习参数为V和W，训练时的损失函数定义为：
![](https://pic1.zhimg.com/v2-0f68ea29fdf9afca74f7ca418dddf9ac_b.jpg)
这一框架可以用于以下三种情况：

1.序列输入，固定长度输出。即实现映射 ![(X_{1},...,X_{T})\rightarrow Y](https://www.zhihu.com/equation?tex=%28X_%7B1%7D%2C...%2CX_%7BT%7D%29%5Crightarrow+Y) ，典型的是视频动作识别。在这里输入是多个视频帧，输出是动作类别。

2.固定长度输入，序列输出。即实现映射 ![X \rightarrow (Y_{1},...,Y_{T})](https://www.zhihu.com/equation?tex=X+%5Crightarrow+%28Y_%7B1%7D%2C...%2CY_%7BT%7D%29) ，典型的是生成图像的描述，如给图像生成文字说明。

3.序列输入，序列输出。即实现映射 ![(X_{1},...,X_{T})\rightarrow (Y_{1},...,Y_{T})](https://www.zhihu.com/equation?tex=%28X_%7B1%7D%2C...%2CX_%7BT%7D%29%5Crightarrow+%28Y_%7B1%7D%2C...%2CY_%7BT%7D%29) ，典型的是视频描述，如为一段视频生成一段文字解说。

参考文献

[1] Mikael Boden. A guide to recurrent neural networks and backpropagation. 2001.

[2] Ronald J Williams, David Zipser. A learning algorithm for continually running fully recurrent neural networks. 1989, Neural Computation.

[3] Fernando J Pineda. Generalization of back-propagation to recurrent neural networks. 1987, Physical Review Letters.

[4] Paul J Werbos. Backpropagation through time: what it does and how to do it. 1990, Proceedings of the IEEE.

[5] Xavier Glorot, Yoshua Bengio. On the difficulty of training recurrent neural networks. 2013, international conference on machine learning.

[6] Y. Bengio, P. Simard, P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.

[7] S. Hochreiter, J. Schmidhuber. Long short-term memory. Neural  computation, 9(8): 1735-1780, 1997.

[8] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau , Fethi Bougares, Holge. Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation. 2014, empirical methods in natural language processing.

[9] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681, 1997.

[10] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio. Gated Feedback Recurrent Neural Networks. 2015, international conference on machine learning.

[11] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio. How to Construct Deep Recurrent Neural Networks. 2014, international conference on learning representations.

[12] Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks.

[13] Alex Graves, Santiago Fernandez, Faustino J Gomez, Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.2006, international conference on machine learning.

[14] Alex Graves, Navdeep Jaitly. Towards End-To-End Speech Recognition with Recurrent Neural Networks. 2014, international conference on machine learning.

[15] Ilya Sutskever, Oriol Vinyals, Quoc V Le. Sequence to Sequence Learning with Neural Networks.2014, neural information processing systems.

[16] Oriol Vinyals, Suman Ravuri, and Daniel Povey. Revisiting Recurrent Neural Networks for Robust ASR. ICASSP, 2012.

[17] A. Graves, A.Mohamed, G. Hinton, Speech Recognition with Deep Recurrent Neural Networks, ICASSP 2013.

[18] Alex Graves, Santiago Fernandez, Juergen Schmidhuber. Multi-dimensional recurrent neural networks. 2007, international conference on artificial neural networks.

[19] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg. Deep speech 2: end-to-end speech recognition in English and mandarin. 2016, international conference on machine learning.

[20] Hasim Sak, Andrew W Senior, Kanishka Rao, Francoise Beaufays. Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition. 2015, conference of the international speech communication association 

[21] Miao, Yajie, Mohammad Gowayyed, and Florian Metze. EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding. 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015.

[22] Bahdanau, Dzmitry, et al. End-to-end attention-based large vocabulary speech recognition. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.

[23] Chan, William, et al. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.

[24] H. Sak, Hasim, Senior, Andrew, and Beaufays, Francoise. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In Inter speech, 2014.

[25] Sainath, Tara, Vinyals, Oriol, Senior, Andrew, and Sak, Hasim. Convolutional, long short-term memory, fully connected deep neural networks. In ICASSP, 2015.

[26] Chorowski, Jan, Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. End-to-End continuous speech recognition using attention-based recurrent nn: First results. abs/1412.1602, 2015. [http://arxiv.org/1412.1602](https://link.zhihu.com/?target=http%3A//arxiv.org/1412.1602)

[27] Hannun, Awni, Case, Carl, Casper, Jared, Catanzaro, Bryan, Diamos, Greg, Elsen, Erich, Prenger, Ryan, Satheesh, Sanjeev, Sengupta, Shubho, Coates, Adam, and Ng, Andrew Y. Deep speech: Scaling up end-to-end speech recognition. 1412.5567, 2014a. [http://arxiv.org/abs/1412.5567](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.5567).

[28] A.Graves. Sequence transduction with recurrent neural networks. ICML Representation Learning Workshop, 2012.

[29] Bahdanau, Dzmitry, Chorowski, Jan, Serdyuk, Dmitriy, Brakel, Philemon, and Bengio, Yoshua. End-to-end attention-based large vocabulary speech recognition. abs/1508.04395, 2015. [http://arxiv.org/abs.1508.04395](https://link.zhihu.com/?target=http%3A//arxiv.org/abs.1508.04395).

[30] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocký, Sanjeev Khudanpur. Recurrent neural network based language model. 2010, conference of the international speech communication association.

[31] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer. Neural architectures for named entity recognition. 2016, north american chapter of the association for computational linguistics.

[32] Peilu Wang, Yao Qian, Frank K Soong, Lei He, Hai Zhao. Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network. 2015, Computation and Language.

[33] Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao. Recurrent convolutional neural networks for text classification. 2015, national conference on artificial intelligence.

[34] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, Eduard H Hovy. Hierarchical Attention Networks for Document Classification. 2016, north american chapter of the association for computational linguistics.

[35] Konstantin Lopyrev. Generating News Headlines with Recurrent Neural Networks. 2015, arXiv: Computation and Language.

[36] Kyunghyun Cho,Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares. Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation. 2014, empirical methods in natural language processing.

[37] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. 2015, international conference on learning representations.

[38] Yonghui Wu, et al. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. Technical Report, 2016. 

[39] Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013).

[40] Shujie Liu, Nan Yang, Mu Li, Ming Zhou. A Recursive Recurrent Neural Network for Statistical Machine Translation. 2014, meeting of the association for computational linguistics.

[41] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. 2014, arXiv: Neural and Evolutionary Computing.

[42] Zhen Cui, Shengtao Xiao, Jiashi Feng, Shuicheng Yan. Recurrently Target-Attending Tracking. 2016, computer vision and pattern recognition.

[43] Anton Milan, Seyed Hamid Rezatofighi, Anthony R Dick, Ian D Reid, Konrad Schindler. Online Multi-target Tracking using Recurrent Neural Networks. 2016, national conference on artificial intelligence.

[44] Peter Ondruska, Ingmar Posner. Deep tracking: seeing beyond seeing using recurrent neural networks. 2016, national conference on artificial intelligence.

[45] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt. Sequential deep learning for human action recognition. In Human Behavior Understanding, pages 29-39. Springer, 2011.

[46] G. Lefebvre, S.Berlemont, F.Mamalet, and C.Garcia. Blstm-rnn based 3d gesture classification. In Artificial Neural Networks and Machine Learning, pages 381-388. Springer, 2013.

[47] Y.Du, W.Wang and L.Wang. Hierarchical recurrent neural network for skeleton based action recognition. CVPR 2015.

[48] J.Donahue, L.A.Hendricks, S.Guadarrama, M.Rohrbach, S.Venugopalan, K.Saenko, and T.Darrell. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014.

[49] A.Grushin, D.D.Monner, J.A.Reggia, and A.Mishra. Robust human action recognition via long short-term memory. In International Joint Conference on Neural Networks, pages 1-8, IEEE, 2013.

[50] Antoine Miech, Ivan Laptev, Josef Sivic. Learnable pooling with Context Gating for video classification. 2017, Computer Vision and Pattern Recognition.

推荐阅读

[1]  [机器学习-波澜壮阔40年](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483705%26idx%3D1%26sn%3Dc6e7c4a2e14a2469308b41eb60f155ac%26chksm%3Dfdb69caecac115b8712653600e526e99a3f6976fdaa2f6b6a09388fa6f9677ccb57b40c40ae3%26scene%3D21%23wechat_redirect) SIGAI 2018.4.13.

[2]  [学好机器学习需要哪些数学知识？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483713%26idx%3D1%26sn%3D1e7c81381d16806ac73e15691fe17aec%26chksm%3Dfdb69cd6cac115c05f1f90b0407e3f8ae9be8719e454f908074ac0d079885b5c134e2d60fd64%26scene%3D21%23wechat_redirect)SIGAI 2018.4.17.

[3]  [人脸识别算法演化史](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483726%26idx%3D1%26sn%3D9fef4cc1766ea4258749f8d40cc71a6e%26chksm%3Dfdb69cd9cac115cf4eba16081780c3b64c75e1e55a40bf2782783d5c28f00c6f143426e6f0aa%26scene%3D21%23wechat_redirect) SIGAI 2018.4.20.

[4]  [基于深度学习的目标检测算法综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483731%26idx%3D1%26sn%3D237c52bc9ddfe65779b73ef8b5507f3c%26chksm%3Dfdb69cc4cac115d2ca505e0deb975960a792a0106a5314ffe3052f8e02a75c9fef458fd3aca2%26scene%3D21%23wechat_redirect) SIGAI 2018.4.24.

[5]  [卷积神经网络为什么能够称霸计算机视觉领域？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483816%26idx%3D1%26sn%3Dfc52765b012771d4736c9be4109f910e%26chksm%3Dfdb69c3fcac115290020c3dd0d677d987086a031c1bde3429339bb3b5bbc0aa154e76325c225%26scene%3D21%23wechat_redirect) SIGAI 2018.4.26.

[6] [用一张图理解SVM的脉络](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483937%26idx%3D1%26sn%3D84a5acf12e96727b13fd7d456c414c12%26chksm%3Dfdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329%26scene%3D21%23wechat_redirect)SIGAI 2018.4.28.

[7] [人脸检测算法综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483950%26idx%3D1%26sn%3Da3a5b7907b2552c233f654a529931776%26chksm%3Dfdb69fb9cac116af5dd237cf987e56d12b0d2e54c5c565aab752f3e366c0c45bfefa76f5ed16%26scene%3D21%23wechat_redirect) SIGAI 2018.5.3.

[8] [理解神经网络的激活函数](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483977%26idx%3D1%26sn%3D401b211bf72bc70f733d6ac90f7352cc%26chksm%3Dfdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3%26scene%3D21%23wechat_redirect) SIGAI 2018.5.5.

[9] [深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484037%26idx%3D1%26sn%3D13ad0d521b6a3578ff031e14950b41f4%26chksm%3Dfdb69f12cac11604a42ccb37913c56001a11c65a8d1125c4a9aeba1aed570a751cb400d276b6%26scene%3D21%23wechat_redirect) SIGAI 2018.5.8.

[10] [理解梯度下降法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484111%26idx%3D1%26sn%3D4ed4480e849298a0aff828611e18f1a8%26chksm%3Dfdb69f58cac1164e844726bd429862eb7b38d22509eb4d1826eb851036460cb7ca5a8de7b9bb%26scene%3D21%23wechat_redirect) SIGAI 2018.5.11

原创声明

本文为 [SIGAI](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484037%26idx%3D2%26sn%3D82d761ec0765fb79cb491906e76acc52%26chksm%3Dfdb69f12cac11604db50c9bd8665094d3da6e368501588bf264b1777db6e1369483fd795fe98%26scene%3D21%23wechat_redirect)原创文章，仅供个人学习使用，未经允许，不能用于商业目的
![](https://pic1.zhimg.com/v2-ef555ac889611c34eb260854eb74c160_b.jpg)
更多干货请关注V X公众号：SIGAI

