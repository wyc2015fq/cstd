# 理解过拟合 - 知乎
# 

`原创声明：本文为 SIGAI 原创文章，仅供个人学习使用，未经允许，不能用于商业目的。 `

本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
- [书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
- [书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)

> 导言
在进行有监督的机器学习建模时，一般假设数据独立同分布（i.i.d，independently and identically distributed）。即样本数据根据通过一个概率分布采样得到，而且这些样本相互之间独立。我们使用历史数据集去训练模型，使得损失函数最小化，然后用训练得到的模型去预测未知数据。如果一味追求让损失函数达到最小，模型就会面临过拟合问题，导致预测未知数据的效果变差。如何判断自己的模型是否训练正常？怎么解决过拟合问题？大家先来听听我朋友小明的故事。
![](https://pic2.zhimg.com/v2-0ffe938f36755f6b34841e54ad1c9151_b.jpg)
小明的故事

小明是个机器学习爱好者，他很喜欢吃蛋糕。有一天他突然想到：能不能用蛋糕的直径来预测蛋糕的价格。于是他定了各种不同尺寸的蛋糕，然后把尺寸和价格的数据记录起来，接着使用回归函数来拟合这些训练数据。小明决定使用四次多项式：
![](https://pic4.zhimg.com/v2-685640ce643fc25b03f4374e486a979f_b.jpg)
和均方差损失函数：
![](https://pic3.zhimg.com/v2-e1f9b39e7dcd6def3bde900d3d81d09a_b.jpg)
在这里x是蛋糕的尺寸，f(x)是预测的蛋糕价格。四次多项式足够复杂，完全可以拟合这个问题；损失函数的含义是模型预测出的结果和真实值差的平方和求平均，预测越准确，损失函数就越小。接着小明使用随机梯度下降法更新参数。 他一直盯着损失函数的变化曲线，迭代若干次之后损失函数竟然到0了，小明异常兴奋，觉得自己训练出了最完美的模型。

接下来小明迫不及待的又定了几个尺寸的蛋糕，当作测试样本，来验证自己的模型准不准。结果发现测试集的损失函数不是0，而且很大。小明很是困惑。于是他去请教老师这是为什么，老师只说了一句话：把你之前的损失函数加上所有参数的平方和，再训练试试，效果也许会有改观。小明按照老师的意思训练新模型，但是这次之前的损失函数不能优化到0了，效果比之前差。小明心想，老师是不是在忽悠我？但奇怪的是，当他用新的模型去预测新蛋糕时，发现测试集损失函数真的更小了。他十分纳闷，再次去问老师，为什么我训练集的效果变差，预测的效果反而更好呢？

老师说：你一开始损失函数为0的情况叫做过拟合，训练集和测试集是有差异的，过分去拟合了训练集，放大了差异性，衰弱了共性，回归损失函数为0的情况说明你拟合了噪声，最后导致了效果差，换句话说拟合函数的过程中模型需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，但是损失函数附加了参数的平方和，导致整个算法不会让参数变的过大，使得拟合函数波动变小。这个参数的平方和就是一种正则化项，用来解决过拟合问题。损失函数加正则项，一般称为目标函数。小明若有所思，感觉自己认识还是太肤浅了,决定再去整理一下相关知识，下文就是他的笔记：

## 损失函数

损失函数（loss function）是用来度量模型预测值f(x)与样本真实标签值y的不一致程度。给定输入的样本数据x，模型函数输出一个f(x)，这个输出的f(x)与样本的真实值标签值y可能是相同的，也可能是不同的，为了表示我们拟合的好坏，就用一个函数来度量拟合的程度，比如平方损失：
![](https://pic2.zhimg.com/v2-2130cc619f4f53d90c8ba64af6212161_b.jpg)
上式中 ![y_{i}-f(x_{i})](https://www.zhihu.com/equation?tex=y_%7Bi%7D-f%28x_%7Bi%7D%29) 称为残差，整个式子就是样本的残差平方和，我们的目的是最小化此损失函数。在一些应用中通常会使用均方差（MSE）作为一项衡量标准。对于整个训练集来说，关于训练集的平均损失可以称作经验风险,即：
![](https://pic1.zhimg.com/v2-014b1ceac10688e9867dfa8d2b549584_b.jpg)
最小化损失函数，其实就是最小化经验风险，之所以叫经验风险，是因为这是对训练样本集的风险预估，及历史经验。

除了上面这种损失函数之外，还有交叉熵损失函数，对比损失函数，合页损失函数等类型，在这里我们不一一介绍，[SIGAI](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484909%26idx%3D1%26sn%3Dc02ee17e5175230ed39ad63e73249f5c%26chksm%3Dfdb6987acac1116c0108ec28424baf4ea16ca11d2b13f20d4a825d7b2b82fb8765720ebd1063%23rd)后续的公众号文章中会对它们做全面系统的讲解。

损失函数是一个实值函数，它的值越小，表示模型在训练样本集上拟合地越好。是不是训练集损拟合的越好，模型的效果会更好呢？ 答案是No。由于训练样本集和测试数据集是不一样的，在训练集上损失函数越小，并不代表测试集损失函数越小，我们希望模型在训练集上有高准确率的同时在测试集上也有高准确率。衡量这种能力的指标就是泛化能力，这时候不得不提过拟合和欠拟合的概念。

## 过拟合和欠拟合 

欠拟合（under-fitting）也称为欠学习，它的直观表现是算法训练得到的模型在训练集上表现差，没有学到数据的规律。引起欠拟合的原因有：模型本身过于简单，例如数据本身是非线性的但使用了线性模型；特征数太少无法正确的建立统计关系。下图是欠拟合的示意图：

![](https://pic1.zhimg.com/v2-5e1fefccbdff5c0b2fb1d10e90000878_b.jpg)图1 欠拟合
上图中数据是线性不可分的，三角形和圆形这两类样本的分界线是曲线而非直线，但是使用了线性分类器，导致大量的样本被错分类，这时更好的选择是非线性分类器。

过拟合（over-fitting）也称为过学习，它的直观表现是算法在训练集上表现好，但在测试集上表现不好，泛化性能差。过拟合是在模型参数拟合过程中由于训练数据包含抽样误差，在训练时复杂的模型将抽样误差也进行了拟合导致的。所谓抽样误差，是指抽样得到的样本集和整体数据集之间的偏差。直观来看，引起过拟合的可能原因有：

模型本身过于复杂，以至于拟合了训练样本集中的噪声。此时需要选用更简单的模型，或者对模型进行裁剪。

训练样本太少或者缺乏代表性。此时需要增加样本数，或者增加样本的多样性。

训练样本噪声的干扰，导致模型拟合了这些噪声，这时需要剔除噪声数据或者改用对噪声不敏感的模型。

下图是过拟合的示意图：

![](https://pic2.zhimg.com/v2-425f5443d94b4842939e0857eea3c025_b.jpg)图2 过拟合

在上图中训练样本存在噪声，为了照顾它们，分类曲线的形状非常复杂，导致在真实测试时会产生错分类。

过拟合是有监督的机器学习算法长期以来需要面临的一个问题。下表给出了实际应用时判断过拟合与欠拟合的准则：

表1  过拟合与欠拟合的判断标准
![](https://pic1.zhimg.com/v2-739f7b0e4c2f5dc9afb31d3427d12724_b.jpg)
如果发生了过拟合，需要根据产生过拟合的原因有针对性的采取措施。

## 方差与偏差

我们也可以从偏差和方差来看待过拟合和欠拟合，模型的泛化误差来自于两部分，分别称为偏差和方差。偏差（bias）是模型本身导致的误差，即错误的模型假设所导致的误差，它是模型的预测值的数学期望和真实值之间的差距。假设样本特征向量为x，标签值为y，要拟合的目标函数为h(x)，模型训练出来的函数为h(x)，则偏差为：
![](https://pic3.zhimg.com/v2-89c45e57b1e889429b02173fca5a0a96_b.jpg)
根据上面的定义，高偏差意味着模型本身的输出值与期望值差距很大，因此会导致欠拟合问题。方差（variance）是由于对训练样本集的小波动敏感而导致的误差。它可以理解为模型预测值的变化范围，即模型预测值的波动程度。根据概率论中方差的定义，有：
![](https://pic4.zhimg.com/v2-9dde5d73833e04cae226b85bb0d88503_b.jpg)
根据定义，高方差意味着算法对训练样本集中的随机噪声进行建模，从而出现过拟合问题。模型的总体误差可以分解为偏差的平方与方差之和：
![](https://pic1.zhimg.com/v2-41cd918142a3ce76e9b5ec8aa1f8a760_b.jpg)
这称为偏差-方差 分解公式。如果模型过于简单，一般会有大的偏差和小的方差；反之如果模型复杂则会有大的方差但偏差很小。这是一对矛盾，因此我们需要在偏置和方差之间做一个折中。如果我一模型的复杂度作为横坐标，把方差和偏差的值作为纵坐标，可以得到下图所示的两条曲线。
![](https://pic1.zhimg.com/v2-cb94206e458badd22c9a62b274db5b0c_b.jpg)图3 偏差方差与模型复杂度关系图
下面以一个简单的例子来形象的解释偏差和方差的概念。在打靶时，子弹飞出枪管之后以曲线轨迹飞行。
![](https://pic4.zhimg.com/v2-df5ad4ae1be55ea019785ad135b8d657_b.jpg)
如果不考虑空气的阻力，这是一条标准的抛物线，如果考虑空气阻力，是一条更复杂的曲线。
![](https://pic1.zhimg.com/v2-438bef14055d633839e06a28e26c4db8_b.jpg)
我们用弹道曲线作为预测模型，在给定子弹初速度的前提下，如果知道靶心与枪口的距离，可以通过调整枪口的仰角来让子弹命中靶心。

如果使用抛物线函数就会产生偏差，因为理论上子弹的落点不会在靶心而是在靶心偏下的位置，此时需要更换弹道曲线模型。

无论选用哪种弹道曲线模型，受风速、枪口震动等因素的影响，即使瞄准的是靶心，子弹还是会随机散布在靶心周围，这就是方差。 

## 正则化

有监督机器学习算法训练的目标是最小化误差函数。以均方误差损失函数为例，它是预测值与样本真实值的误差平方和：
![](https://pic2.zhimg.com/v2-d0567f99f2566c3e81a3dfe40a92ad5d_b.jpg)
其中 ![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D) 是样本的标签值， ![h_{\theta}(x_{i})](https://www.zhihu.com/equation?tex=h_%7B%5Ctheta%7D%28x_%7Bi%7D%29) 是预测函数的输出值， ![\theta](https://www.zhihu.com/equation?tex=%5Ctheta) 是模型的参数。在预测函数的类型选定之后，我们能控制的就是函数的参数。为了防止过拟合，可以为损失函数加上一个惩罚项对复杂的模型进行惩罚，即强制让模型的参数值尽可能小，加入惩罚项之后损失函数为：
![](https://pic1.zhimg.com/v2-b2c300edceda91dd61e53b93b2a5dc58_b.jpg)
函数的后半部分称为正则化项，这里的目标是让它的值尽可能小，即参数等于0或者接近于0。 ![\lambda](https://www.zhihu.com/equation?tex=%5Clambda) 为惩罚项系数，是人工设定的大于0的参数。正则化项可以使用L2范数即平方和，也可以使用其他范数如L1范数，即绝对值之和。L2范数在求解最优化问题时计算简单，而且有更好的数学性质，二次函数的导数为：
![](https://pic2.zhimg.com/v2-aec78c49a9a14e1f8b521cffff1aafad_b.jpg)
绝对值函数在0点不可导，如果不考虑这种情况，其导数为符号函数 ![sgn(x)](https://www.zhihu.com/equation?tex=sgn%28x%29) 。与L2相比L1正则化能更有效的让参数趋向于0，产生的结果更稀疏。

## 剪枝

剪枝是决策树类算法防止过拟合的方法。如果决策树的结构过于复杂，可能会导致过拟合问题，此时需要对树进行剪枝，消掉某些节点让它变得更简单。剪枝的关键问题是确定减掉哪些树节点以及减掉它们之后如何进行节点合并。决策树的剪枝算法可以分为两类，分别称为预剪枝和后剪枝。前者在树的训练过程中通过停止分裂对树的规模进行限制；后者先构造出一棵完整的树，然后通过某种规则消除掉部分节点，用叶子节点替代。

## 数据增广

数据增广是解决过拟合中思想比较朴素的方法。训练集越多，过拟合的概率越小，数据增广是一个比较方便有效屡试不爽的方法，但各类领域的增广方法都不同。
- 在计算机视觉领域中，增广的方式是对图像旋转，缩放，剪切，添加噪声等。
- 在自然语言处理领域中，可以做同义词替换扩充数据集。
- 语音识别中可以对样本数据添加随机的噪声。

## Dropout

Dropout是神经网络中防止过拟合的方法。dropout的做法是在训练时随机的选择一部分神经元进行正向传播和反向传播，另外一些神经元的参数值保持不变，以减轻过拟合。dropout机制使得每个神经元在训练时只用了样本集中的部分样本，这相当于对样本集进行采样，即bagging的做法。最终得到的是多个神经网络的组合。

## Early Stopping

提前停止的策略是在验证集误差出现增大之后，提前结束训练；而不是一直等待验证集 误差达到最小。提前停止策略十分简单,执行效率高，但需要额外的空间备份参数。

## 集成学习

集成学习算法也可以有效的减轻过拟合。Bagging通过平均多个模型的结果，来降低模型的方差。Boosting不仅能够减小偏差，还能减小方差。

原创声明：本文为 [SIGAI](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/G9gW2ghTII57jGmXSIaf7w) 原创文章，仅供个人学习使用，未经允许，不得转载，不能用于商业目的。

推荐阅读

[1]  [机器学习-波澜壮阔40年](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483705%26idx%3D1%26sn%3Dc6e7c4a2e14a2469308b41eb60f155ac%26chksm%3Dfdb69caecac115b8712653600e526e99a3f6976fdaa2f6b6a09388fa6f9677ccb57b40c40ae3%26scene%3D21%23wechat_redirect) SIGAI 2018.4.13.

[2]  [学好机器学习需要哪些数学知识？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483713%26idx%3D1%26sn%3D1e7c81381d16806ac73e15691fe17aec%26chksm%3Dfdb69cd6cac115c05f1f90b0407e3f8ae9be8719e454f908074ac0d079885b5c134e2d60fd64%26scene%3D21%23wechat_redirect)SIGAI 2018.4.17.

[3]  [人脸识别算法演化史](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483726%26idx%3D1%26sn%3D9fef4cc1766ea4258749f8d40cc71a6e%26chksm%3Dfdb69cd9cac115cf4eba16081780c3b64c75e1e55a40bf2782783d5c28f00c6f143426e6f0aa%26scene%3D21%23wechat_redirect) SIGAI 2018.4.20.

[4]  [基于深度学习的目标检测算法综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483731%26idx%3D1%26sn%3D237c52bc9ddfe65779b73ef8b5507f3c%26chksm%3Dfdb69cc4cac115d2ca505e0deb975960a792a0106a5314ffe3052f8e02a75c9fef458fd3aca2%26scene%3D21%23wechat_redirect) SIGAI 2018.4.24.

[5]  [卷积神经网络为什么能够称霸计算机视觉领域？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483816%26idx%3D1%26sn%3Dfc52765b012771d4736c9be4109f910e%26chksm%3Dfdb69c3fcac115290020c3dd0d677d987086a031c1bde3429339bb3b5bbc0aa154e76325c225%26scene%3D21%23wechat_redirect) SIGAI 2018.4.26.

[6] [用一张图理解SVM的脉络](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483937%26idx%3D1%26sn%3D84a5acf12e96727b13fd7d456c414c12%26chksm%3Dfdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329%26scene%3D21%23wechat_redirect)SIGAI 2018.4.28.

[7] [人脸检测算法综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483950%26idx%3D1%26sn%3Da3a5b7907b2552c233f654a529931776%26chksm%3Dfdb69fb9cac116af5dd237cf987e56d12b0d2e54c5c565aab752f3e366c0c45bfefa76f5ed16%26scene%3D21%23wechat_redirect) SIGAI 2018.5.3.

[8] [理解神经网络的激活函数](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483977%26idx%3D1%26sn%3D401b211bf72bc70f733d6ac90f7352cc%26chksm%3Dfdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3%26scene%3D21%23wechat_redirect) SIGAI 2018.5.5.

[9] [深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484037%26idx%3D1%26sn%3D13ad0d521b6a3578ff031e14950b41f4%26chksm%3Dfdb69f12cac11604a42ccb37913c56001a11c65a8d1125c4a9aeba1aed570a751cb400d276b6%26scene%3D21%23wechat_redirect) SIGAI        2018.5.8.

[10] [理解梯度下降法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484111%26idx%3D1%26sn%3D4ed4480e849298a0aff828611e18f1a8%26chksm%3Dfdb69f58cac1164e844726bd429862eb7b38d22509eb4d1826eb851036460cb7ca5a8de7b9bb%26scene%3D21%23wechat_redirect) SIGAI 2018.5.11

[11] [循环神经网络综述—语音识别与自然语言处理的利器](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484310%26idx%3D1%26sn%3D0fc55a2784a894100a1ae64d7dbfa23d%26chksm%3Dfdb69e01cac1171758cb021fc8779952e55de41032a66ee5417bd3e826bf703247e243654bd0%26scene%3D21%23wechat_redirect) SIGAI 2018.5.15

[12] [理解凸优化](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484439%26idx%3D1%26sn%3D4fa8c71ae9cb777d6e97ebd0dd8672e7%26chksm%3Dfdb69980cac110960e08c63061e0719a8dc7945606eeef460404dc2eb21b4f5bdb434fb56f92%26scene%3D21%23wechat_redirect)  SIGAI 2018.5.18

[13][【实验】理解SVM的核函数和参数](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484495%26idx%3D1%26sn%3D4f3a6ce21cdd1a048e402ed05c9ead91%26chksm%3Dfdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9%26scene%3D21%23wechat_redirect) SIGAI 2018.5.22

[14][【SIGAI综述】 行人检测算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484523%26idx%3D1%26sn%3Dddaa70c4b92f6005d9bbd6b3a3fe4571%26chksm%3Dfdb699fccac110ea14e6adeb873a00d6ee86dd4145ddf8e90c9b878b908ac7b7655cfa51dab6%26scene%3D21%23wechat_redirect) SIGAI 2018.5.25

[15] [机器学习在自动驾驶中的应用—以百度阿波罗平台为例](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484540%26idx%3D1%26sn%3D733332042c31e1e18ad800f7f527893b%26chksm%3Dfdb699ebcac110fd6607c1c99bc7ebed1594a8d00bc454b63d7f518191bd72274f7e61ca5187%23rd)（上） SIGAI 2018.5.29 

[16] [理解牛顿法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484651%26idx%3D1%26sn%3Da0e4ca5edb868fe3eae9101b71dd7103%26chksm%3Dfdb6997ccac1106a61f51fe9f8fd532045cc5d13f6c75c2cbbf1a7c94c58bcdf5f2a6661facd%26scene%3D21%23wechat_redirect) SIGAI 2018.5.31

[17] [【群话题精华】5月集锦—机器学习和深度学习中一些值得思考的问题](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484658%26idx%3D1%26sn%3Df5c9f92c272c75883bf8e6f532559f11%26chksm%3Dfdb69965cac11073f49048caef5d7b9129614090a363d9ef7f3d1b9bc59948d2217d2bca7b7b%26scene%3D21%23wechat_redirect)SIGAI 2018.6.1

[18] [大话Adaboost算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484692%26idx%3D1%26sn%3D9b389aa65208c778dddf17c601afbee1%26chksm%3Dfdb69883cac1119593934734e94c3b71aa68de67bda8a946c1f9f9e1209c3b6f0bf18fed99b8%23rd) SIGAI 2018.6.1

[19] [FlowNet到FlowNet2.0：基于卷积神经网络的光流预测算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484711%26idx%3D1%26sn%3Dbb7644e101b5924f54d6800b952dc3aa%26chksm%3Dfdb698b0cac111a6605f5b9b6f0478bf21a8527cfad2342dbaaf624b4e9dcc43c0d85ae06deb%26scene%3D21%23wechat_redirect) SIGAI 2018.6.4

[20] [理解主成分分析法(PCA)](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484754%26idx%3D1%26sn%3Db2c0d6798f44e13956bb42373e51d18c%26chksm%3Dfdb698c5cac111d3e3dca24c50aafbfb61e5b05c5df5b603067bb7edec8db049370b73046b24%23rd) SIGAI 2018.6.6

[21]  [人体骨骼关键点检测](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484784%26idx%3D1%26sn%3Dceafb54203f4e930ae457ad392b9f89c%26chksm%3Dfdb698e7cac111f13d8229d7dcc00b4a7305d66de3da1bd41e7ecc1d29bfa7be520d205c53e9%23rd) SIGAI 2018.6.8

[22] [理解决策树](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484827%26idx%3D1%26sn%3D043d7d0159baaddfbf92ed78ee5b1124%26chksm%3Dfdb6980ccac1111a9faeae7f517fee46a1dfab19612f76ccfe5417487b3f090ab8fc702d18b8%23rd) SIGAI 2018.6.11

[23] [用一句话总结各种机器学习算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484859%26idx%3D1%26sn%3D2c4db22fb538953a62a90983e3e1f99d%26chksm%3Dfdb6982ccac1113a82e92be325bb07a947d54090274654375f3b50e11e1abd809fb7358bde16%23rd) SIGAI 2018.6.13

[24] [目标检测算法之YOLO](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484909%26idx%3D1%26sn%3Dc02ee17e5175230ed39ad63e73249f5c%26chksm%3Dfdb6987acac1116c0108ec28424baf4ea16ca11d2b13f20d4a825d7b2b82fb8765720ebd1063%23rd) SIGAI 2018.6.

原创声明
本文为 [SIGAI](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484439%26idx%3D2%26sn%3Dd2fb3a3062d6d6abe4fef8ca8495bcbf%26chksm%3Dfdb69980cac11096dea7276ee62ce2fac42cd7a0e088256c5e2f5aec5aacacb0dcaea0575d67%23rd) 原创文章，仅供个人学习使用，未经允许，不得转载，不能用于商业目的。

![](https://pic2.zhimg.com/v2-6209626d2165f95155f3cfafa6cc544d_b.jpg)
更多干货请搜索关注 VX公众号：SIGAI

