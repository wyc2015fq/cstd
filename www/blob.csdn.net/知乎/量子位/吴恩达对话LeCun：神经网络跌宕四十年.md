# 吴恩达对话LeCun：神经网络跌宕四十年 - 知乎
# 



> 夏乙 栗子 发自 凹非寺
量子位 出品 | 公众号 QbitAI

Yann LeCun，深度学习三巨头之一。

最近，这位AI领域的传奇大牛，接受了另一位大牛吴恩达的视频专访。在这次对话中，LeCun回顾了卷积神经网络、反向传播的历史，以及他如何从一个默默无闻的“法国小孩”，一步步走到今天。

这是一段激荡四十年的故事。

是一段AI科学家的个人奋斗，也是一段AI复兴的历史进程。

这场访谈，也是DeepLearning.ai课程的一部分。

以下，就是吴恩达对话LeCun的主要内容，量子位听译整理，并且在不改变原意的基础上，进行了调整和注释。

完整访谈视频，文末有传送门。
![](https://pic1.zhimg.com/v2-1de8ef90d145c6bb39549bda554638c4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1200' height='800'></svg>)
## **入行记**

吴恩达：你投入神经网络的研究已经很久，我想听听你自己的故事，你是怎么开始入行的？怎么投入了AI、神经网络之中？

LeCun：我一直都对智能这件事感兴趣，例如人类的智能是如何出现的，我在孩提时代就对人类的进化很感兴趣。

吴恩达：当时你还在法国？

LeCun：对。

（量子位注：LeCun生于1960年，临近巴黎的一个地方。以下括号内文字，皆为量子位添加的注释。）

大概在初中左右，我开始对科技、宇宙等感兴趣，我最喜欢的电影是《2001太空漫游》（1968年上映），那里面有智能机器、星际旅行、人类进化等等让我着迷的东西。其中关于智能机器的概念特别吸引我。

后来我学了电气工程，在大二的时候无意中看到一本讨论哲学的书，是关于MIT计算语言学家乔姆斯基（Avram Noam Chomsky）和儿童心理学家皮亚杰（Jean Piaget）之间的一场辩论。

（这场辩论发生在1975年10月，就在法国巴黎附近的Royanmont。二人的辩论主题是：从人的语言机制和语言习得角度来探讨儿童发展问题）
![](https://pic3.zhimg.com/v2-f023c4ed492d48e2dd08765cfcaf82b6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='314' height='475'></svg>)
这本质上是一场关于自然（先天）还是使然（后天）的辩论。

乔姆斯基认为语言是基于一套天赋的习得机制（语言器官），而皮亚杰认为语言是后天练习的结果。

在皮亚杰这一边，还有来自MIT的Seymour Papert（人工智能先驱、LOGO语言创始人），他一直在研究感知器（Perceptron）模型。

我之前从来没听说过感知器，读了报道之后，才知道这是一个能学习的机器，这听起来太赞了。然后我开始在各个大学的图书馆里，寻找一切关于感知器的内容。

看了Seymour Papert那本合写的书（《感知器》，另一位作者是明斯基），我才知道很多论文都来自50年代，然后在60年代停滞了。

吴恩达：这是哪年？

LeCun：大约是1980年。当时在学校跟教授做过一些关于神经网络的项目，但基本上找不到人一起讨论，因为这个领域那时基本消失了。1980年，没人研究这些。与此相关的实验就是编写各种仿真软件，读神经科学的书。

当我完成工程方向的学习后，开始学习芯片设计。毕业之后，对我吸引力最大的一个问题是：如何训练多层的神经网络？

60年代的文献并没有解决这个问题。我读了福岛邦彦关于新认知机（Neo-cognition）的文章，里面讲述了分层结构，跟我们现在的卷积网络很像，但是没有反向传播学习算法。

后来我在法国一个很小的独立实验室里，遇到一群人，他们对自动机网络（Automata Networks）很感兴趣，他们给了我几份研究霍普菲尔德网络（Hopfield Networks）的论文。

霍普菲尔德网络第一次结合了存储系统和神经网络，这让一些研究人员在80年代初，重新燃起对神经网络的热情，其中大部分是物理学家、凝聚态物理学家和一些心理学家。那时工程师和计算机科学家才不会谈论神经网络。

他们还给我看了另一篇论文，当时还是预印版本，题目是：Optimal Perceptual Inference，这是第一篇关于玻尔兹曼机（BM）的论文，作者是Geoffrey Hinton和Terrence Sejnowski。
![](https://pic4.zhimg.com/v2-66c45b34fcd7f32f3326c977d90da4bb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='736' height='409'></svg>)
其中讨论了隐藏单元、多层神经网络等，虽然只是一个简单的分类器。但是看完之后，我说我十分想见作者，因为他们已经找到正确的方向。

几年之后，我开始读博的时候，在La Douche（位于加勒比海，法属瓜德罗普岛上）参加了一个workshop。Terry（即Terrence Sejnowski）是其中的一个演讲者，那是我们第一次见面。

吴恩达：当时是80年代初？

LeCun：1985年初。那是一个非常有趣的workshop，我还遇见了一些来自贝尔实验室的人，后来我也去了贝尔实验室，不过这是几年后的事情了。

我跟Terry说，我正在研究反向传播，那时还没有写出来反向传播的论文。他当时的谈的也是这个话题。那时Rumelhart、Hinton和Williams关于反向传播的论文还没发布。

（这篇论文即：Learning representations by back-propagating errors，1986年发布于《Nature》）

Terry是Hinton的朋友，信息都是互通的，所以他那会儿已经开始研究如何应用反向传播。但他当时没跟我说这些。

后来他回到美国，跟Hinton说法国有个小孩，也在研究跟我们一样的东西。几个月后，6月的法国又举办了另一个会议，Hinton发表了主题演讲，谈论了玻尔兹曼机以及他正在研究的反向传播论文。

演讲结束后，大约有50个人围住他想要交流。不过他跟主办方说的第一句话是：你们知道Yann LeCun么？因为他读了我的法语论文。Hinton能读一点法语，而且其中的数学部分他能看出来是哪种反向传播。

然后我们一起吃了午餐，从此变成朋友。
![](https://pic1.zhimg.com/v2-066beed61351c9fa6868619425db2a0c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='715'></svg>)Hinton和LeCun在2006年的一次相聚
吴恩达：我明白了。因为你们各自独立地（重新）发明了反向传播。

LeCun：是的。我们发现，其实反向传播最早是60年代，在最优控制的研究中发明出来的，它出自链式法则，或者按照搞最优控制的那些人的说法，叫joint state method。

反向传播的实质，是你可以将梯度下降用在很多阶段上的一个想法，这个概念在各个领域、各个时期出现了很多次。

但是我认为，是Rumelhart、Hinton、Williams那篇论文让这个概念流行了起来。

## **贝尔实验室的日子**
![](https://pic1.zhimg.com/v2-e0e2f4e2516b93490711e4e4b357f9f4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='600'></svg>)
吴恩达：你在AT&T贝尔实验室的时候，开发了许多东西，包括LeNet。还记得有一年夏天我在贝尔实验室实习的时候，还听说过你当年的战绩。可以帮我讲讲AT&T和LeNet的故事吗？

LeCun：故事要从我在多伦多大学做Geoff Hinton博士后的时候说起，那也是我开始研究卷积网络的时候。我最早写的代码和最早做的实验都在多伦多。

当年，数据集很小，也没有Endless之类的东西，我自己用鼠标画了一堆字母，然后做扩增，来测试系统的表现。

我对比了全连的网络，本地连接且无权值共享的网络与共享权值的网络，这就是最早的卷积网络。发现它对小数据集非常友好，有了卷积结构，就不会过度拟合。

1988年10月，我到了贝尔实验室，第一件要做的事就是扩展网络，因为有了更快的计算机。

在我入职的几个月前，老板Larry Jackel问我想要什么电脑。我说现在Sun 4最牛B，要是能来一台就好了。然后，我就有了一台，回想在多伦多整个系只有一台Sun 4。Larry告诉我，贝尔实验室没有省钱的风俗。简直不要太好。

我来之前，贝尔实验室已经在做字符识别的事情了。它们有一个巨大的数据集叫USPS，包含5000个训练样本。于是，我就设计了一个卷积网络，其实就是LeNet的前身。训练效果非常好，比之前其他人用其他方法得出的结果都要好。

这样一来，我们就知道自己有了一个非常超前的东西。我才到了贝尔三个月，就有了这样的成果。

那时的卷积网络，没有分开子采样和池化层，所以每个卷积都是直接做的子采样。之所以没有，是因为计算量太大了，每个位置都要一个卷积的话，我们承受不起。

不过，第二个版本就有了分开的池化层和子采样，我觉得这真的就是LeNet 1了。我们在这上面发表了挺多论文的。

有一个好玩的故事。关于LeNet，我在NIPS做过一个演讲，当时Geoff Hinton就坐在观众席。等我讲完回到座位，他就在我旁边。Hinton说，你的演讲只有一丢丢信息，那就是——如果把所有合理的事情都做了，就会有效果。
![](https://pic2.zhimg.com/v2-0024e1e4994dd41282bd580ec0e4995d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='200'></svg>)
吴恩达：LeNet后来改写了历史，那个时候AT&T就已经开始用它来读支票了。

LeCun：是啊。它在AT&T内部大规模使用，但在外面就没有。

我也不知道为什么，可能是80年代还没有互联网，FTP上有电子邮件，但不是互联网。随便两个实验室之间，用的硬件和软件平台都不一样。

那个时候，没有Python和MATLAB之类的东西，大家自己写自己的一套代码。我和Leon Bottou花了一年半，才写出一个神经网络模拟器。还要写一个翻译器，然后我们就写了自己的LISP翻译器。所以LeNet都是用LISP写的，后端和现在的TensorFlow之类已经很像了。

我们还开发了一堆别的应用。那时候，我们和一群高智商的工程师合作，比如Chris Burgess后来在微软研究院有了一片天地。和这些人在一起，我们开发了一些字符识别系统，和现在的CRF很像，可以读取一系列字符，而不只是单个字母了。

吴恩达：所以说LeNet论文是将神经网络和自动机结合了起来。

LeCun：对，论文的前半部分讲的是卷积神经网络，很多人都认为这部分很一颗赛艇；后半部分可能没几个人读过，讲的是一种不需要正则化的序列级判别学习和基本结构预测，实际上和CRF很像。

这个系统当时很成功。不过，就在我们和一家大银行达成合作部署了这个系统，吃饭庆祝的时候，当时是1995年，AT&T宣布拆分成3家公司：AT&T、朗讯和NCR。

我们实验室属于AT&T，技术团队去了朗讯，产品团队却在NCR。

特别特别不幸的是，AT&T的律师们使出了他们无穷的智慧，给卷积神经网络申请了一个专利，归属于NCR。可是NCR那些人甚至根本不知道卷积神经网络是怎么回事，我们和技术团队也没法继续开发这个系统。挺郁闷的。

谢天谢地这个专利2007年过期了。
![](https://pic4.zhimg.com/v2-201e40e7c5dc9782d1fd99951d5bde53_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='500'></svg>)2002年，后来大部分人都从AT&T离开了
## **神经网络寒冬**

吴恩达：在神经网络的寒冬里，你也还是在坚持这个研究方向，这是一种什么感觉？

LeCun：我在某些方面坚持了，有的方面也没坚持。

我始终相信这些技术最终会复兴，人们会想清楚怎么把它投入实际应用。这个想法始终在我脑海中。

但是1996年AT&T拆分之后，数字识别也没法继续做了，我升了职，成了一个部门主管。这个时候，我得思考这个部门接下来要干什么。

当时互联网刚刚兴起，我觉得互联网带来了一个巨大问题，就是怎样将之前保存在纸上的海量知识搬到数字世界里。

于是我搞了个叫DjVu的项目，压缩扫描图片的体积，方便通过互联网分发。

这个项目很好玩，也算取得了一些成功，但是AT&T没想出来该用它干什么。

（DjVu的主页：[http://yann.lecun.com/ex/djvu/](https://link.zhihu.com/?target=http%3A//yann.lecun.com/ex/djvu/)）

吴恩达：我记得这个，当时用来在线分享论文。

LeCun：对，我们扫描了整本的NIPS论文集，把它放到网上，来展示这项技术。一张高分辨率图片能压缩到只有几kb大小。

## **复兴**
![](https://pic1.zhimg.com/v2-c29c0b8ebe5fdfc0f0ea6deee14144c8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='600'></svg>)
吴恩达：你早期的工作已经占领了整个计算机视觉领域，甚至还扩展到了其他领域。你怎么看待这个进程？

LeCun：哈哈哈我早知道会这样。

首先，我始终相信这种方法有用，它需要快速的计算机和大量的数据，但我始终相信这是正确的方法。

最开始我还在贝尔实验室的时候，我认为它会随着机器变得更强，持续沿着某个方向发展下去，我们当时甚至在设计专门运行卷积神经网络的芯片。

所以你看，我们以为人们会逐渐对这个东西感兴趣，它会持续发展。

但实际上人们对神经网络的热情在20世纪90年代完全熄灭了，从1995年到大概2002年，有六七年的黑暗期，没人研究这个。

其实也有一点点研究，2000年左右，微软在研究用卷积神经网络来识别汉字。还有一些人脸检测之类的小研究。

我最近发现，在我们发表第一篇CNN论文之后，还有一些团队独立发展出了和卷积神经网路类似的方法，用在医疗影像的识别上。但他们没和我们一样去发表，也没进入专业研究的视野中。

就在最黑暗的几年里，有些人和我们有着同样的想法。

2012年ImageNet之后，人们对卷积神经网络的兴趣飞速提升，其实让我有点惊讶。
![](https://pic4.zhimg.com/v2-1aca3af6ad4cabb6c9ee689f60bf63bf_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='753'></svg>)
（Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton组成的SuperVision队，在ImageNet ILSVRC2012的分类和定位任务中夺冠，这个夺冠的卷积神经网络就是AlexNet，论文：Classification with Deep Convolutional Neural Networks）

2012年在佛罗伦萨的那届ECCV上，ImageNet的Workshop很有意思。现在大家都知道，那年Geoff Hinton的团队在ImageNet比赛中遥遥领先，Workshop上所有人都在等他的学生Alex Krizhevsky演讲。

不过那个时候，很多搞计算机视觉的人都不知道卷积神经网络是什么。他们应该听我讲过，2000年我还在CVPR上讲过一回，但是大多数人都没怎么注意到。

岁数大一点的人知道卷积神经网络，年轻人嘛，就完全没概念了。

Krizhevsky讲的时候，完全没解释卷积神经网络是什么，直接就开始说你们看这儿，这些东西都是连接起来的，我们怎么转换数据，怎么得到这个结果……

他是搞机器学习的，以为所有人都知道这个东西。很多人听着就非常惊讶，在他演讲的过程中，你都能从大家的脸上看到他们想法的变化。

吴恩达：所以，你认为那个Workshop是让计算机视觉界转变观念的决定性时刻？

Lecun：对，就是在那儿发生的。

## **FAIR**
![](https://pic1.zhimg.com/v2-d0650f49e753fad746e023f1618495e4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='600'></svg>)
吴恩达：现在你在NYU任教，同时还领导着FAIR，Facebook AI Research。我听说你对于企业应该怎样做研究有着独特的观点，愿意谈一谈吗？

LeCun：过去4年里，我在Facebook感觉最美好的一件事，就是我有很大的自由，能够按我认为最合适的方式来建立FAIR。

这是他们第一个研究院。Facebook是一家以工程为中心的公司，它已经10岁了，IPO也很成功，但是至今主要专注于生存，或者短期的事情。扎克伯格要思考下一个十年，对Facebook来说什么会很重要。公司的生存已经不再是个问题，终于有能力考虑下一个十年。

马克和他的团队认为在连接人类这件事上，AI是一项关键的技术。他们在内部建立了一个小组，用卷积神经网络在人脸识别等方面取得了不错的成绩，于是对这个方向更有兴趣了。

于是他们尝试了很多种方法，比如说雇一群年轻的研究员，收购AI公司，最后决定在这个领域雇个长者，建立一个研究机构。
![](https://pic1.zhimg.com/v2-ce1fcce8d07f47ac849987d142931c28_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
最开始还有一些文化冲突。

因为研究院和工程团队的运行方式完全不同，研究院有着更长的时间表、更宽的视野，科研人员对工作地点的选择又非常保守。

我在最开始的时候就决定，研究需要是开放的，要鼓励研究员们发表论文，甚至必须发表论文，研究成果要有一致的衡量标准。

马克和CTO迈克（Mike Schroepfer）就说，Facebook是个非常开放的公司，我们在开源领域有不少贡献，CTO来自Mozilla，是开源领域出来的，公司还有不少人以前是搞开源的，开放就刻在公司的DNA里，他们很自信能设立一个开放的研究机构。

Facebook对知识产权不像某些公司那么强迫症，于是和高校合作就更容易，让研究员们可以一脚踩在学术界，一脚踩在工业界。

如果你去看我过去四年发表的论文，大部分是和我在NYU的学生们一起完成的。在Facebook，我做的事情主要是组织实验室、招人、定科研方向、做顾问等等。但是，我没什么机会参与到某个研究项目里，把名字留在论文上。

你知道，我现在也不怎么在乎把名字留在论文上了，但是……

## **给年轻人的建议**
![](https://pic4.zhimg.com/v2-925da64263a744c98a2937af76c6369b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='600'></svg>)
吴恩达：对于想进入AI领域的人，你有什么建议？

LeCun：这个领域和我刚进入的时候，已经是两个完全不同的世界了。

我认为现在有一点非常好，人们想在某种程度上参与进来很容易了，工具很多、很好用，比如TensorFlow、PyTorch等等。你只需要一台不算贵的电脑，坐在卧室里，就能训练你的卷积神经网络、循环神经网络来做任何事情了。

你也可以用线上的学习资料学很多东西，也不是很麻烦。

所以现在连高中生都在搞神经网络了，我觉得很好，学生群体对机器学习和AI的热情在增长，看到年轻人参与进来，很一颗赛艇。

我的建议是，如果你想进入这个领域，让你自己有用，比如说去为开源项目做贡献。比如说你找不到某个算法的代码，可以自己实现，分享给其他人，找一篇你认为很重要的论文，实现其中的算法，开源出来。或者你也可以去为现有的那些开源项目做贡献。

如果你写的东西很有趣、很有用，人们就会注意到你，可能就会找到一份自己想要的工作，或者申请到最想读的PhD。

吴恩达：谢谢Yann，这些故事的细节今天听起来还是很有意思。

LeCun：有很多这样的故事，发生的时候，你根本不知道它们十到二十年后显示出怎样的重要性。

## **访谈视频**

这段访谈视频，现在可以在YouTube上查看。

我们也搬运了一份到国内，如果你不想翻墙，可以在量子位后台回复“**对话**”两个字，即可获得所有相关地址。

— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


