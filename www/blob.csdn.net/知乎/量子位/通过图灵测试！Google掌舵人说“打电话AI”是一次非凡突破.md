# 通过图灵测试！Google掌舵人说“打电话AI”是一次非凡突破 - 知乎
# 



> 夏乙 问耕 发自 凹非寺
量子位 出品 | 公众号 QbitAI
![](https://pic3.zhimg.com/v2-54bbf24e9736c7e6abe2086a518499a2_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='600'></svg>)
我们错了。

今天凌晨，Google I/O 2018大会最后一日，前不久刚刚获得[年度图灵奖](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzNjc1NzUzMw%3D%3D%26mid%3D2247495843%26idx%3D1%26sn%3D3fd0c1beef6421aea7dc8f9c3c49afe0%26chksm%3De8d047d1dfa7cec7632868f8731e87343aceae8d582080e433ffedd540187099f834ee9cfd9c%26scene%3D21%23wechat_redirect)的Alphabet新任董事长John Hennessy登上舞台。

原本以为这是一次例行公事的演讲。只是简单讲讲责任、做做科普等等。没想到，曾经的斯坦福校长Hennessy毫不打官腔。（他比上一版Alphabet董事长有趣太多）

听完之后，我们久久不能平静。

Hennessy一开场就花了大量的时间，仔细讲解了计算领域的发展、现状和困境，以及未来可能的突破方向等。句句干货。

当然亦有激动人心的情怀抒发。
![](https://pic1.zhimg.com/v2-44ef72efcc0cdf3378f5fe0b27edf388_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='634'></svg>)
比如，这位已经65岁的宗师泰斗说：你们可能不信，我搭建自己的第一台电脑，已经是几乎50年前的事了。

Hennessy说这五十年来，他亲眼目睹着不可思议的IT产业，上演一波又一波的革命，互联网、芯片、智能手机、电脑……展现着各自的魔力。但仍有一件事，他认为将会真正改变我们的生活。

“那就是机器学习和人工智能领域的突破。”

“人们投入这个领域的研究已经50年了。终于，终于，我们取得了突破。为了实现这个突破，我们所需要的基本计算能力，是之前设想的100万倍。但最终我们还是做到了。”

他说：这是一场革命，将会改变世界。

然后更震撼的一幕上演。

相信很多人已经见识过Google Duplex。就是前两天在Google I/O 2018大会首日，Google CEO展示的那个对话AI。

能够在真实的环境下，打电话给美发店、餐馆预约服务和座位。全程流畅交流，完美应对不知情的人类接线员。看看下面这段视频，再感受下Google的AI黑科技。
[Google Assistant打电话 | I/O 2018_腾讯视频​v.qq.com![图标](https://pic3.zhimg.com/v2-a772a2982020f0c43d39432a93d041da_180x120.jpg)](https://link.zhihu.com/?target=https%3A//v.qq.com/x/page/r0647360dsv.html)
Google Duplex一出，所有人都炸了。效果拔群的好。坊间观众们缓过神来一想：Google演示的这个AI，难不成就是通过了图灵测试？

没错，Alphabet董事长John Hennessy今天终于亲口承认：“**在预约领域，这个AI已经通过了图灵测试**。”

“这是一个非凡的突破。”

他进一步补充说，虽然这个AI不是在所有情境下取得突破，但仍然指明了未来的道路。
![](https://pic2.zhimg.com/80/v2-3c375f0081ddf8b322dbaa47064ac305_b.jpg)https://www.zhihu.com/video/978274694729924608
1950年，图灵发表了一篇划时代的论文，预言人类能创造出具备真正智能的机器。他还提出了著名的图灵测试：如果一台机器能与人类展开对话（通过电传设备）而不被识别出身份，那么这台机器就具有智能。

通过图灵测试，意味着机器可以思考。
![](https://pic1.zhimg.com/v2-53c3a1c7a0800f17418b484bfca49918_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1000' height='793'></svg>)图灵当年的惊世论文
了不起，再次为Google打Call（以及希望可以是AI接听）。

在演讲中，John Hennessy还问全场：有一件事，现在还能按照摩尔定律的速度快速增长，你们猜是什么？

答案是：机器学习论文的数量。

说完这个，全场都大笑起来。（感谢Jeff Dean、Dave Patterson、Cliff Young等贡献的数据。）

他还讲解了TPU的内部结构。下面有详细阐述。

那么，今天我们满怀敬意和激动发出这篇推送。我们觉得可以把这篇称为：《**Alphabet董事长在山景城科技座谈会上的讲话**》。

Hennessy讲话脱水全文如下，还没出版单行本。

## **讲话脱水全文**

今天我要讲一讲，未来40年我们在计算领域将面临的最大挑战是什么，但这同时也是一个巨大的机遇，让我们重新思考该如何构建计算机。

现在流行谈摩尔定律的终结，高登·摩尔自己曾经跟我说过：所有指数都会终结，只是迟早的问题。摩尔定律所遇到的正是这样的自然规律。

**摩尔定律终结意味着什么？**
![](https://pic3.zhimg.com/v2-724426556e5c64371b30adabf6dcd422_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
我们先来看看DRAM（动态随机存取存储器）的情况。很多年来，DRAM都在以每年50%的速度增长，比摩尔定律说的增速还要快一点。

但后来，就进入了平缓期。过去7年里究竟发生了什么？DRAM是一种比较特殊的技术，它需要深槽电容，因此也需要一种特殊的装配技术。
![](https://pic1.zhimg.com/v2-c24abab8fd5fb5807fdaab9e68fcabc0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
那么在处理器领域又发生了什么呢？处理器发展的减缓和DRAM是相似的。红线是摩尔定律的预测，蓝线是一个普通英特尔处理器上的晶体管数量。这两条线一开始只是略微分叉，但到2015、2016年，差距就很大了。

我们必须记住，这里还有个成本因素。生产线越来越贵了，芯片成本降得却没有那么快，因此每个晶体管的成本实际上在上升。我们去考虑架构的时候就会看到这些问题的影响。

在摩尔定律的减速之外，**还有一个更大的问题，登纳德缩放定律（Dennard Scaling）的终结。**

Bob Dennard是IBM员工，第一个晶体管DRAM的发明者，他多年之前曾经预测：每平方毫米的硅片需要的能量将保持恒定，因为电压电平、电流容量会下降。

这意味着什么呢？如果能量保持恒定，晶体管数却在指数型增长，那么每个晶体管的能量，实际上是在下降的，也就是说，从能耗的角度来看，计算的成本越来越便宜。
![](https://pic1.zhimg.com/v2-335e9c16fa01c41fa19c6db735931594_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
实际上登纳德缩放定律怎么样了呢？我们来看一下上图，红线是技术按标准摩尔定律曲线的发展，蓝线代表单位面积能耗的变化。

现在的处理器会调慢时钟频率、关闭核心，因为如果不这么干它就会烧起来。我从来都没想到竟然会看见这样一天，处理器给自己减速来防止过热，但是现在这种情况已经发生了。

实际上，登纳德缩放定律到2007年已经停止了，带来了芯片行业的剧变。忽然之间，关键限制因素不再是晶体管的数量，而是能耗。这就要求你完全重新思考架构、思考如何构建机器。

这意味着晶体管在计算中的低效、架构上的低效比以往危害更大。
![](https://pic3.zhimg.com/v2-71e1fe340f18a699d3538b115d99984a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
我们平时所用的、随身带着的各种设备，都离不开电池，忽然之间能量就成了关键资源。还有比手机没电了更糟糕的事吗？再想想即将到来的**IoT**时代，设备永远开着机，还指望依靠能量采集技术一块电池用10年。

我们越来越需要设备永远保持开机。比如说装了Google Assistant的东西，你可能不需要它的屏幕总亮着，但是需要CPU一直在工作。因此，我们需要越来越多地考虑能源效率。

让很多人惊讶的是，在**大型云计算数据中心里，能量效率也是个巨大的问题。**
![](https://pic1.zhimg.com/v2-6cf64fd22282088e83c2214c55ead450_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
这是Google数据中心的典型成本构成。红色的那一块是耗能加上冷却的成本，几乎和花在服务器上的成本持平。因此，能源效率是一个非常关键的问题，而登纳德缩放定律的终结意味着已经没有免费的午餐了，你也可以看出它的影响。
![](https://pic4.zhimg.com/v2-1fab4561bd04086bdbc0930ef3e9f3a3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
上图展示了**40年来处理器性能的变化**。最初几年，我们每年能够看到22%的进步；80年代中期RISC发明之后，每年的进步速度达到了大约50%；接下来，就是登纳德缩放定律的终结，芯片界所有人都转向了多核。多核有什么用呢？硬件设计者用它把能量效率的皮球踢给了软件设计者；现在我们进入了一个平台期，平均每年的性能增长只有3%左右，要20年才能翻倍。这是通用处理器性能增长的终结。

为什么会这样？
![](https://pic2.zhimg.com/v2-dc42dfe0e271376e87163e3b552452dd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
大量并行执行指令是不可行的，比如说在英特尔酷睿i7里，25%指令的执行结果都被扔掉了，但是，执行这些指令依然需要消耗能量。这也是为什么单处理器性能曲线终止了。
![](https://pic2.zhimg.com/v2-42cb423ab89611dacf21ae0a6edbe3d1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
但是多核处理器也面临着类似的问题。写一个大型的复杂软件，就必然有序列性的部分。

假设，你将来用一个64核的处理器，但它运行的代码中有1%是序列性的，于是，它的运算速度就只相当于一个40核的处理器，但是你却要为64核的能量买单。

我们要跨越这个能量效率的障碍，就要重新思考如何设计机器。
![](https://pic2.zhimg.com/v2-f0254292ddb6ef0c0a7aefb02309d0e1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
**我们还有什么方法，能让系统更加经济？**

以**软件**为中心的方法行得通吗？现代脚本语言对于使用它们的程序员来说很高效，但是在执行上效率很低。

以**硬件**为中心的方法呢？我和Patterson把它称为“**特定领域架构**”，这类架构不是通用的，但能非常好地处理一些领域的应用。
![](https://pic2.zhimg.com/v2-a4ff32e4444281fdfd86ec942eabdb1d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
根据上面提到的挑战，我们来看一下有哪些机会。

这个表格出自Charles Leiserson和他MIT同事们的论文“There’s Plenty of Room at the Top”。他们以矩阵乘法为例，在英特尔酷睿处理器上运行这种算法并优化它。用C语言重写、加上并行循环、加上内存优化都会带来速度的提升，最后他们用Intel AVX instructions重写了程序，和Python相比提速了6万多倍。

虽然只是一个很简单的算法，但是这体现了**软件优化能够带来的潜力**。
![](https://pic4.zhimg.com/v2-16eb6f499267a07b6c2ed8046b67d86b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
那么**特定领域架构（DSA）**呢？我们实际上要做的是要在硬件的能效上获得突破。“特定领域”指的是让一个处理器能实现一定范围内的用途，它们在运行的时候能够运用这个特定领域的知识，因此更加高效，比如专门用于运行神经网络的处理器，做的是机器学习相关的事情。
![](https://pic2.zhimg.com/v2-de010edd1e023ee77a68b34b13cdf081_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
DSA不是魔法，把架构限制在某个领域并不会自动让计算更快，我们需要做一些架构上的改变才能更高效。重要的有这样几点：

首先我们**更有效地实现了并行**，从现在多核处理器的多重指令多重数据变成了单指令多重数据，因此每个核心不必再单独从不同缓存中读取指令流，而是用一组功能单位存储一组指令。这是牺牲一些灵活性来换取的巨大的效率进步。我们用的更像是VLiW，让编译器来决定一组运算是不是要并行，将工作从运行时转移到编译时。

我们还**脱离了缓存**。缓存是个好东西，但是当空间局部性和时间局部性比较低的时候，它不仅没用，还会拖慢程序。于是，我们就把它转移到了用户控制的本地存储中。这里所做的取舍就是必须有人将他们的应用银蛇到用户控制的存储结构中。

另外，我们还**去除了不必要的精度**，转向低精度的浮点运算。

与这些相配合，我们还需要**特定领域语言**。用Python或C写的代码，无法提取出特定领域架构所需要映射到信息。
![](https://pic4.zhimg.com/v2-4b317eef7748143e81eda4964c085223_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
因此，我们需要**重新思考改如何为这些机器编程**，它们要用到高级运算，比如向量乘法、向量矩阵乘法或者系数矩阵的组织等，有了这些运算才能获得编译到架构中的高级信息。

设计特定领域语言的关键是要保持足够的机器独立性，不能换一台机器就要重写程序。应该有一个编译器能针对一种特定领域语言，既能映射到云端运行的架构，也能映射到手机上运行的架构，这是一种挑战。TensorFlow、OpenGL等正往这个方向发展，但这是一个船新的空间，我们刚刚开始理解，也刚刚开始理解该如何针对它来设计。

**为深度神经网络设计一种特定领域架构**，你们觉得应该什么样？
![](https://pic3.zhimg.com/v2-bee3eeefee6e34961267b74fee01a0a2_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
这张图是**TPU**内部的架构，我想说明的是，这上面占满硅片区域的不是控制，也不是缓存，而是与计算直接相关的东西。

也正因为如此，这个处理器每个clock可以完成256×256次，也就是64000次8位矩阵乘法，因此它可以轻松拿下推理任务。你不能用它来运行通用的C语言代码，而是要用它运行神经网络的推理。
![](https://pic1.zhimg.com/v2-bd915524947f372c8e972ea31e0af280_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
我们来看一看每瓦能耗实现的性能，TPU是通用处理器的30多倍，甚至比GPU强很多。这就体现了为特定领域来定制架构非常关键。
![](https://pic1.zhimg.com/v2-1b6e0dce3fe52e825419aaf93f16dc9c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)
这是一个新时代，但在某种意义上来讲也回到了过去。在计算机发展早期，应用领域的专家、研究软件环境、做编译器的人和做架构的人会组成一个垂直的团队。

而现在，我们也需要这样的**综合团队**，理解如何从应用来发展特定领域语言、发展特定领域架构，来思考如何重构机器。

对于行业发展来说，这是巨大的机遇，也是一种新挑战。我相信有足够多像这样有趣的应用领域，我们可以通过为领域定制机器，取得巨大的性能优势。

我想，如果能实现这种进步，就会为人们腾出时间，来担心信息安全问题，这也正是一个我们应该关注的重要问题。

## **现场视频**

Hennessy讲话全程在此，我们加了英文字幕。字幕来自YouTube，应该是人工智能听写完成的。
[谷歌董事长在I/O 2018上的讲话(字幕版)_腾讯视频​v.qq.com![图标](https://pic3.zhimg.com/v2-a772a2982020f0c43d39432a93d041da_180x120.jpg)](https://link.zhihu.com/?target=http%3A//v.qq.com/x/page/i0650kj5ar8.html)
## **One More Thing**

演讲结束后，John Hennessy老师还在现场，进行了20多分钟的答疑。

这次答疑涉及的领域包括：量子计算、神经网络，安全、计算架构的演变、行业未来发展、教育等等诸多领域。Hennessy都给出了诚恳而明确的回答。

这里先讲几个花絮。

现场有个提问者说：我上的处理器架构课程，用的就是你写的教材。

Hennessy立刻回答：“I hope it didn’t hurt you.”

又是一次全场大笑。
![](https://pic4.zhimg.com/v2-8ec971004aa7b77bb96af79413beabbb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='657'></svg>)
还有一个提问涉及比特币这类加密货币。

Hennessy回答：“确实，我能构建一个专门进行比特币挖矿的架构。”他说加密货币很重要，但还有一些问题需要解决。

如果你对问答部分感兴趣，可以量子位（公众号：QbitAI）对话界面，回复：“**轩尼诗**”三个中文字，即可获得传送门。

亦有英文字幕，推荐观看。

— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


