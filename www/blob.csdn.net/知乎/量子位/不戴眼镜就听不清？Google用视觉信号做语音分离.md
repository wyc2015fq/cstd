# 不戴眼镜就听不清？Google用视觉信号做语音分离 - 知乎
# 



> 雾中之栗 发自 凹非寺
量子位 出品 | 公众号 QbitAI
![](https://pic4.zhimg.com/v2-aabd143f6f790a26c537e6fb472874c3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='286' height='186'></svg>)△ 精神的力量
人类非常善于在嘈杂的环境里，集中注意力听某一个人说的话，从精神上“屏蔽”一切其他声音。这种现象便是“鸡尾酒会效应”，我们与生俱来。

不过，对于计算机来说，要把**一个音频信号分割成多个不同的语音来源**，依然有些棘手的问题需要解决。当许多人的语音**交叠**在一起的时候，AI时常措手不及。

谷歌团队建立了一个**深度学习视听模型**，用来把某个人的语音从一段混合声音里面提取出来。算法对视频做一些奇妙的改动，就可以让**观众需要的声音变得格外显著**，其他声音则被削弱。



![](https://pic2.zhimg.com/v2-20f3a4d7c1c4d79c7cf09628057d13c5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='240' height='201'></svg>)△ 不戴眼镜，AI也一样
这项技术的独到之处，就是把**听觉和视觉信号结合起来**用，而不单单靠声波来分离视频中的语音。直观来看，以嘴上的动作为例，人的**口型和语音相互对应**。这一点会帮助AI判断，哪些声音是哪个人发出的。




## **视听语音分离模型**

要生成训练样例，团队先搜集了**100,000条**高清视频，全部是油管上的讲座和演讲。第二步，是把**语音单纯** (比如，无音乐、无观众、无其他演讲者) 、且图像中**只有一人**出现的视频筛选出来。这样，就得到了大约2,000小时纯纯无杂质的视频。

有了这些数据，小伙伴们就开始对一个基于**多流卷积神经网络** (CNN) 的模型进行训练。**人工合成**一些嘈杂的“鸡尾酒会”给AI欣赏，目标是让它在酒会里，把每个人的音频流分离开来。
![](https://pic4.zhimg.com/v2-8af5498d0c07fb0aded800d73441e4e7_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='480' height='266'></svg>)△ 感觉有人，在背后截我图
视频里，演讲者的**大头贴**被一帧一帧地截下，计算机用它们的缩略图来提取**面部特征**。然神经网络食量非凡，它要吃的不止这些面部特征。加上**频谱图形式呈现的音轨**，才是神经网络美好的晚餐。

在训练过程中，AI学会了**编码视觉和听觉信号**，并且能够把它们组合成一种特殊的视听表现形式。在此基础上，AI还要为每个演讲者输出一个**时频掩膜** (time-frequency mask) 。

把这些掩膜与输入的噪音频谱相乘，再转换回**时域波形** (time-domain waveform) ，就能获得每个演讲者独立又清澈的语音信号了。




## **训练成果秀**

团队用**单音轨**的普通视频来做实验。观众要做的很简单，想要听到哪个人的声音，就把他/她的脸从视频里选出来，当然让计算机根据场景自动选择也是可以的。
![](https://pic1.zhimg.com/v2-bc1798135d6b4d3eb4344c0ce6153f78_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='408' height='230'></svg>)△ 安能辨我是...
除了被选中的人，其他人的声音都可以被完全压制，或者被**削弱到理想的程度**。

在多人发声的场景下，视觉信号除了有效提升语音分离的质量，还可以把分离之后的**音轨和视频里的人物对应**起来。
![](https://pic1.zhimg.com/80/v2-d1bf0660f1e86d6bf012a8554c36b36c_b.jpg)https://www.zhihu.com/video/967780759247958016
为了突显视觉信息的作用，团队从劈柴小哥哥 (谷歌CEO Sundar Pichai) 的同一段演讲视频里截取了两个不同的段落，**左右拼接**成一段视频。如果只靠音频，便**很难判断**是左边的劈柴还是右边的劈柴在说话。




## **不，是你的字幕**

语音识别的预处理，以及视频字幕的自动生成中，也能用上这个方法。遇到相互交叠的声音，视听模型或许可以解决，以往语音分离系统遇到的难题。

有翻墙技能的大家，可以使用油管的**字幕功能** (cc) ，对比整容前和整容后的视频，看看字幕有没有更准一些。
![](https://pic1.zhimg.com/v2-cdeaa4b3cf49f9cd47e705d581a92dd4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='333' height='151'></svg>)△ 刚才谁在说话
这是论文的传送门：

[https://arxiv.org/pdf/1804.03619.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1804.03619.pdf)




— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态




