# 把梯度下降算法变成酷炫游戏，这有一份深度学习通俗讲义 - 知乎
# 



> 晓查 发自 凹非寺 
量子位 报道 | 公众号 QbitAI

让小球滚下山坡，找到它们分别落在哪个山谷里。原来**梯度下降算法**还能变得像游戏视频一样酷炫：
![](https://pic3.zhimg.com/v2-114ae12e4eb14c9c83dddd9e93b31b56_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='267'></svg>)



以上是fast.ai创始人Jeremy Howard分享的一段视频，乍看就像就像是在复杂地形中作战的沙盘推演，其实揭示的是随机梯度下降（SGD）算法的本质。

谷歌大脑东京研究员hardmaru转发了视频对应的文章，评价它“像极了即时战略游戏”。
![](https://pic1.zhimg.com/v2-d558d15ec561096f826882775e7939bc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='496'></svg>)



可别光顾着好玩，视频还得配合文章一起“服用”才有效果。上面的热门视频就是摘自fast.ai成员Javier Ideami写的一篇科普文。


如果代码和公式让你感到枯燥，那么不妨从这段酷炫的SGD视频入手，再读一读这篇文章，它会帮你更直观地理解深度学习。

## **梯度下降算法的可视化**

到底什么是梯度？

深度学习的架构和最新发展，包括CNN、RNN、造出无数假脸的GAN，都离不开梯度下降算法。

梯度可以理解成山坡上某一点上升最快的方向，它的反方向就是下降最快的方向。想要以最快的方式下山，就沿着梯度的反方向走。

看起来像沙盘推演的东西，其实是我们撒出的小球，它们会沿着梯度下降的方向滚到谷底。

而梯度下降算法的最终目的，是找到整个“地形”中的最低点（全局最小值），也就是海拔最低的山谷。

但在这片地形中，山谷可能不止一处（局部最小值），所以我们需要撒很多球，让它们分别落入不同山谷，最后对比高度找到其中的海拔最低点。

以上就是随机梯度下降（SGD）算法的基本思想。

## **神经网络的输入输出**

说完梯度下降算法，下面开始介绍神经网络的基本知识。

从本质上讲，神经网络是通过一系列“权重” 将输入数据变成我们所需的输出。

我们先从最简单的2层神经网络说起，简单介绍一下神经网络的结构。实际上神经网络比这个要复杂得多，通常有几十层甚至上百层。



![](https://pic3.zhimg.com/v2-eaac0d3b338ffe19479a87593ce3ecca_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='962'></svg>)



**输入**：神经网络的输入是源数据，神经元数量与源数据的特征数匹配。上图以4个输入为例。


**第一层**：这是个隐藏层，包含有许多隐藏的神经元，它们又连接到周围层中的单元。

**第二层**：第二层也就是最后一层有1个单元，即网络的输出。

输入W和输出Z之间是线性关系：

对于第一层网络，Z1=W1X+b1，A1=ReLU（Z1），A1是Z1经过激活函数处理后的结果。

对于第二层网络，我们将第一层的输出A1作为第二层的输入，Z2=W2A1+b2，A2=Sigmoid（Z2）。

W将表示网络层的**权重**，它代表着网络不同单元之间连接的强度。b代表**偏置项**，可以为网络提供更大的灵活性。



![](https://pic3.zhimg.com/v2-d00e23eec5a0cc526163bc54ef490996_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='816'></svg>)



最终第二层的输出Yh=A2就是这个神经网络最终的输出。


Yh = A2 = Sigmoid（W2 ReLU（W1 X + b1）+ b2）

Yh将代表神经网络的预测结果，就是将X输入给网络后产生的输出。

在这个方程中，W1、b1、W2、b2是未知数，需要训练网络找到它们的正确值。

## **激活函数的选取**

你也许会注意到，在上面的神经网络中，每层的输出都是在线性运算的结果后加上一个“过滤”。为什么要这样处理呢？

现实中输入和输出之间的关系通常并非线性。如果神经网络的架构仅由线性算法组成，那么它很难计算出非线性行为。所以我们要在每层的结尾加上一个**激活函数**。

不同的激活函数有不同的特点。选取不当会导致梯度变得非常小，就是通常所说的**梯度消失**问题。

另外还存在一种相反的问题，就是**梯度爆炸**，当梯度值过大时，网络会变得非常不稳定。

常见的4种激活函数有：Sigmoid、tanh，ReLU、leaky ReLU，下面简单讨论一下它们的优缺点。



![](https://pic4.zhimg.com/v2-8d614109beee6de0e33edb8fea4c2fd3_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='829'></svg>)



## **Sigmoid**

**1/(1+e-x**)

这个函数非常适合将输入分为两类。它的形状很缓和，因此它的梯度能得到很好的控制。

主要的缺点是，在极端情况下，函数输出变得非常平坦。这意味着它存在梯度消失的问题。

## **tanh**

(2 / (1+e-2x)) - 1

它与Sigmoid非常相似。函数曲线更陡峭，因此它的输出也将更强大。缺点与Sigmoid类似。

## **ReLU**

max（0，x）

如果输入大于0，则输出等于输入。否则，输出为0。

它的输出范围从0到无穷大。这意味着它的输出可能会变得非常大，可能存在梯度爆炸问题。它还有个问题是左侧完全平坦，可能导致梯度消失。

ReLU计算简单，是神经网络内层最常用的激活函数。

## **Leaky ReLU**

将ReLU函数的前半段用0.01x代替。

## **softmax**

e-x / Sum(e-x)

输出范围介于0和1之间。

Softmax将输入归一化为概率分布。它将输入压缩为0到1范围，就像Sigmoid。

它通常在多分类场景中的输出层，Softmax确保每个类的概率之和为1。

实际上，神经网络是一系列函数的组合，有一些是线性的，有一些是非线性的，它们共同组成一个复杂的函数，将输入数据连接到我们需要的输出。

文章原作者将这个话题分成3部分讨论，更进一步的内容可以去他文末的链接中找到：
[https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504](https://link.zhihu.com/?target=https%3A//towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504)

— **完** —

量子位 · QbitAI

վ'ᴗ' ի 追踪AI技术和产品新动态

戳右上角「+关注」获取最新资讯↗↗

如果喜欢，请分享or点赞吧~比心❤


