# “你的深度学习框架包含15个漏洞”，360说 | 附论文 - 知乎
# 



注意！你的深度学习框架有漏洞！

这个警告来自360安全实验室（Qixue Xiao、Deyue Zhang）、佐治亚大学（Kang Li）和弗吉尼亚大学（Weilin Xu）的研究者，他们在一篇论文中，对TensorFlow、Caffe、Torch三个深度学习框架中的第三方软件包进行了研究，并在其中查找漏洞，最后得出最开头的研究结论。

尽管这还只是一项初步研究，但研究人员仍然在三个框架中发现了15个漏洞，类型包括：DoS拒绝服务攻击、躲避攻击等。

他们说：“深度学习框架是复杂的，重度依赖大量开源软件包。”这些依赖库，也正是漏洞的根源。

## **潜藏风险的依赖库**

一个典型的、用深度学习框架开发出来的AI应用，在部署时往往是这样的三层结构：



![](https://pic2.zhimg.com/v2-88ad7a886f3faaccd178245bdd5a9b49_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='333'></svg>)



最上面一层，是开发者看得见的深度学习应用，包含应用逻辑、深度学习模型和相应的数据；中间一层是TensorFlow、Caffe等深度学习框架；最下面一层，则是底层框架依赖，也就是深度学习框架所用到的那些组件，比如说OpenCV、GNU LibC、NymPy、以及Google的protobuf等等。

每个深度学习框架，都依赖着大量第三方软件包。比如说使用最广泛的TensorFlow，就有97个Python依赖库；Caffe背后，有137个依赖库，老牌框架Torch7，也有48个Lua模块。



![](https://pic2.zhimg.com/v2-fe6e6cbf9937d587151ad95f911f2ef9_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='220'></svg>)



## **△** 深度学习框架的依赖库

当你训练图像识别算法的时候，也许并不那么关心Caffe中用到的OpenCV开源库或者TensorFlow依赖的numpy，但是，风险正蕴含其中。

## **攻击从哪来？**

要了解深度学习应用可能面临的风险，就要先清楚它们的攻击面（attack surface），也就是这类软件可能受到怎样的攻击。

对于不同的应用，可能出现的攻击多种多样，不过几位研究员在论文中总结说，有三个攻击面，能够代表绝大部分情况。

他们以MNIST手写数字数据集为例对这三个攻击面做了说明：

**输入图像畸形**

深度学习应用在进行分类或识别等任务时，会从文件或者网络读取输入数据，在这个环节中，攻击者可能会构建畸形的输入。

需要特别说明的是，如果从摄像头等传感器直接获取输入数据，受到这种攻击的可能性会小很多，但也无法消除。

**训练数据畸形**

在构建深度学习应用的过程中，需要用数据对模型进行训练，而训练所用的数据集就可能被污染、打上错误的标签。这种攻击方式叫做数据下毒攻击（data poisoning
attack）。

**模型畸形**

如果深度学习开发者使用别人搭建的模型，就可能会遭受这种攻击。

## **都不安全**

以上面提到的第一个攻击面（输入图像畸形）为例，假设你的输入数据来自文件或者网络，TensorFlow、Caffe和Torch就有十几个漏洞，可能遭受DOS拒绝服务攻击、躲避攻击或者系统妥协攻击，比如说下表列出的这些：



![](https://pic4.zhimg.com/v2-938f2e8e2b23427b6343b251bd09f523_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='435'></svg>)



相比较而言，TensorFlow算是表现出色的，但其中也有两个Python软件包（NumPy和）存在DoS攻击风险。

开源计算机视觉代码库OpenCV中的漏洞最多，总共发现了11处。Caffe和Torch框架中都使用了OpenCV。另外，Caffe中还有图像处理库libjasper和图像浏览器OpenEXR的易受攻击版本。

有意思的是，研究人员还发现如何利用越界写入（out-of-bounds write）欺骗AI：在OpenCV中，数据指针可以设置为readData函数中的任何值，然后可以将指定的数据写入数据指向的地址。即可以用来改写分类结果。

OpenCV的例子如下：

我们来详细看一下这几类攻击：

**威胁一、DoS拒绝服务攻击**

我们在深度学习框架中发现，最常见的漏洞是软件错误，导致程序崩溃，或者进入死循环，或者耗尽所有的内存。

**威胁二、躲避攻击**

面对脆弱的深度学习框架，攻击者可以利用软件漏洞实施躲避攻击，例如：1、通过漏洞覆盖分类结果，修改特定内存内容 2、劫持控制流程以跳过或重新排序模型执行。

**威胁三、系统妥协**

攻击者可以利用漏洞劫持控制流，或者远程控制托管深度学习应用的系统。这个情况发生在深度学习应用作为云服务运行，并从网络输入馈送时。

这项研究，已经在敦促框架开发者对安全性进行改进了。作者们在论文中说：“我们的研究结果已经得到相关开发商的证实，其中很多已经根据我们的建议进行了修补。”

## **论文**

**最后，附上相关论文：**

Security Risks in Deep Learning Implementations
Qixue Xiao, Kang Li, Deyue Zhang, Weilin Xu

在量子位公众号(QbitAI)对话界面回复“**360**”，即可以直接下载。

—完—

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


