# 不许你歧视车道线，那也是路面的一部分 | 自动驾驶感知训练 - 知乎
# 



> 水栗子 编译自 Medium 
量子位 报道 | 公众号 QbitAI
![](https://pic2.zhimg.com/v2-d095047af873941835f761fe7c435769_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='749' height='496'></svg>)△ 神秘的视觉



**感知**，大概就是感受到周遭正在发生什么，的一种能力。这项技能对自动驾驶来说太重要了。

自动驾驶汽车依靠摄像头、激光雷达以及雷达等等传感器来感知周围的变化。

一位名叫凯尔 (Kyle Stewart-Frantz) 的大叔，准备了一份**指南**，鼓励大家在家训练自动驾驶系统的感知能力。



![](https://pic4.zhimg.com/v2-9c3c301b89510fa2889c46b0e1dfd93b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='868' height='473'></svg>)



当然，这个手册并不是他出于爱好写出来的，是随着Lyft和Udacity联合发起的**感知挑战赛** (Lyft Perception Challenge) ，而生的。

比赛考验的就是系统能不能准确地感受到，可以行驶的**路面**在哪里，周围的**汽车**在哪里。



![](https://pic4.zhimg.com/v2-74c4d9b5e543810ae86d1420b38fbe6b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1020' height='616'></svg>)



挑战赛中，能够倚仗的所有数据，都来自车载的**前向摄像头**。

## **摄像头不存在？**

这里的“**摄像头数据**”并非真实摄像头记录的影像，而是一个名为**CARLA**的**模拟器**生成的图景。

毕竟，自动驾驶汽车的软件开发**大多**是在模拟器中进行的，那里**快速**的原型设计和迭代，比在现实世界里使用真实硬件要高效得多。

那么，来看一下CARLA给的数据长什么样——



![](https://pic4.zhimg.com/v2-ff833d5ae9cdd7c708de14075122f4cb_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='367'></svg>)



左边是模拟摄像头捕捉的画面，右边则是与之对应的、**标记**好的图像。

用这样的数据来训练算法，让AI能够在**从未见过**的新鲜图像里，判断出哪些像素对应的是道路，哪些部分对应的是其他车辆。

这就是挑战赛的目标。

## **车前盖太抢镜？**

要完成比赛任务，自然会想到**语义分割**。用这种方式来训练神经网络，成熟后的AI便可以判断每个**像素**里包含的物体了。



![](https://pic4.zhimg.com/v2-da105617a35a1f337ead3a9c9cd9de6b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='608'></svg>)



第一步，是对标记好的图像做**预处理**。比如，因为设定是“车载前向摄像头”拍下的画面，每一幅图像都会出现**车前盖**，可是如果这样就把所有图像判定为“车”，就不太好了。

所以要把显示车前盖的那些像素的值设为**零**，或者贴上其他的“**非车**”标签。

第二步，**车道标识**和**道路**的值是不一样的，但我们希望这些标识，可以被识别为路面的**一部分**。



![](https://pic3.zhimg.com/v2-50514b616bb54f88f2490682e0703476_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='634' height='421'></svg>)△ 这不是给汽车的指示，但也太随性了



所以，要把车道标识和路面，贴上**一样**的标签。

用Python写出来，预处理功能就长这样——


```python
def preprocess_labels(label_image):
      labels_new = np.zeros_like(label_image)
      # Identify lane marking pixels (label is 6)
      lane_marking_pixels = (label_image[:,:,] == ).nonzero()
      # Set lane marking pixels to road (label is 7)
      labels_new[lane_marking_pixels] = 
 
      # Identify all vehicle pixels
      vehicle_pixels = (label_image[:,:,] == ).nonzero()
     # Isolate vehicle pixels associated with the hood (y-position > 496)
     hood_indices = (vehicle_pixels[] >= ).nonzero()[]
     hood_pixels = (vehicle_pixels[][hood_indices], \
                    vehicle_pixels[][hood_indices])
     # Set hood pixel labels to 0
     labels_new[hood_pixels] = 
     # Return the preprocessed label image 
     return labels_new
```


预处理过后的结果，就是**标记**和之前的不太一样了。



![](https://pic1.zhimg.com/v2-eec45b0dff9221afa88d81587ad85d98_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1055' height='275'></svg>)



准备活动做好了，神经网络的正式训练也就可以开始了。

## **谁是分类小公主？**

那么，大叔选的是怎样的神经网络？

定制一个**FCN-Alexnet**或许是个不错的选项，它擅长把每个像素分到不同的类别里。



![](https://pic3.zhimg.com/v2-41bc19bb986004ff36694eb6f61ccaba_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='550'></svg>)



循着以下链接，可以找到这个模型的详细信息——

代码：
[https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/voc-fcn-alexnet](https://link.zhihu.com/?target=https%3A//github.com/shelhamer/fcn.berkeleyvision.org/tree/master/voc-fcn-alexnet)论文：
[https://arxiv.org/pdf/1605.06211.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1605.06211.pdf)

大叔用一个**随机梯度下降**solver，把全部训练数据跑了**10次** (10 epochs) ，基础学习率设的是**0.0001**。

## **评估训练成果**

拿训练好的神经网络去跑验证数据，凯尔得到了0.6685的**F2**值，以及0.9574的**F0.5**值 (前者更重视召回率，后者更重视准确率) 。系统每秒处理6.06幅图像。

当然，视频会比这些数字更加生动——
![](https://pic2.zhimg.com/80/v2-7e7b80b0bc3f99b16870cdf5c6121681_b.jpg)https://www.zhihu.com/video/986328668230168576
## **然后还想怎样？**

大叔说，要让神经网络表现更好，将来会搜集更多数据，涉及更加**丰富**的路况。

另外，要进行一系列的**数据增强**，让数据和数据之间的差异更加明显。

关于神经网络的**结构**，也还有其他选择，比如为细粒度预测而生的**FCN-8**，值得尝试。

还有，可以引入**时态数据** (光流) ，来减少推断需要的帧数，同时保持比较高的准确度。

## **模拟器不够真？**

当然，只有模拟器也是不够的，自动驾驶系统终究要接受现实的考验。
![](https://pic2.zhimg.com/80/v2-1a9f7565541a35d79f39bffcbc1e2911_b.jpg)https://www.zhihu.com/video/986328763470258176

面对真实摄像头传出的画面，系统的辨识结果并没有**非常**理想。不过在许多帧里面，神经网络都能够在一定程度上，辨认出道路和车辆。

真实世界和模拟器里的驾驶场景，还是不一样的。

如果模拟器生成的图像和现实更加**接近**的话，可能结果就会好一些了。
![](https://pic1.zhimg.com/80/v2-c28480a71eff0ddb5a5e8ce630786ab8_b.jpg)https://www.zhihu.com/video/986328893925666816
不难看到，在和模拟器设定更为接近的路况下，系统的表现还是很不错的。

如此看来，这只AI还是很有前途。只要把模拟器造得更**贴近真实**，神经网络应该就能得到更有效的训练。

这里提供一段代码，可以用来查看，算法跑出的结果到底怎么样——


```python
from moviepy.editor import VideoFileClip, ImageSequenceClip
  import numpy as np
  import scipy, argparse, sys, cv2, os
 
  file = sys.argv[-]
 
  if file == 'demo.py':
      print ("Error loading video")
      quit

 def your_pipeline(rgb_frame):

     ## Your algorithm here to take rgb_frame and produce binary array outputs!

     out = your_function(rgb_frame)

     # Grab cars
     car_binary_result = np.where(out==,,).astype('uint8')
     car_binary_result[:,:] = 
     car_binary_result = car_binary_result * 

     # Grab road
     road_lines = np.where((out==),,).astype('uint8')
     roads = np.where((out==),,).astype('uint8')
     road_binary_result = (road_lines | roads) * 

     overlay = np.zeros_like(rgb_frame)
     overlay[:,:,] = car_binary_result
     overlay[:,:,] = road_binary_result

     final_frame = cv2.addWeighted(rgb_frame, , overlay, 0.3, , rgb_frame)

     return final_frame

 # Define pathname to save the output video
 output = 'segmentation_output_test.mp4'
 clip1 = VideoFileClip(file)
 clip = clip1.fl_image(your_pipeline)
 clip.write_videofile(output, audio=False)
```


用到的可视化数据在这里：
[https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/Lyft_Challenge/videos/Videos.tar.gz](https://link.zhihu.com/?target=https%3A//s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/Lyft_Challenge/videos/Videos.tar.gz)

## **你也一起来吧？**

当然，作为Lyft感知挑战赛的研发负责人，凯尔大叔这番苦口婆心的目的，还是吸引更多的小伙伴掺和进来。

**道路安全，人人有责**。大概就是这个意思，吧。



![](https://pic4.zhimg.com/v2-40a692861119e335b2eaaa716fef0ddf_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='380' height='285'></svg>)



入门选手的各位，感受一下Udacity的免费课程：
[https://www.udacity.com/course/self-driving-car-engineer-nanodegree—nd013](https://link.zhihu.com/?target=https%3A//www.udacity.com/course/self-driving-car-engineer-nanodegree%25E2%2580%2594nd013)

注：有善良的中文字幕

以及语义分割的详细步骤：

[https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef](https://link.zhihu.com/?target=https%3A//medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef)




— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


