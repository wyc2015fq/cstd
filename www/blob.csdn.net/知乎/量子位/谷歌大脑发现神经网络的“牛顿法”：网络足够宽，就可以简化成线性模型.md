# 谷歌大脑发现神经网络的“牛顿法”：网络足够宽，就可以简化成线性模型 - 知乎
# 



> 晓查 发自 凹非寺 
量子位 报道 | 公众号 QbitAI

来自谷歌大脑的研究者发现，对于宽神经网络，深度学习动态可以大大简化，并且在**无限宽度**限制条件下，它们由网络初始参数的一阶泰勒展开的线性模型所决定。

所谓的无限宽度(infinite width)，指的是完全连接层中的隐藏单元数，或卷积层中的通道数量有无穷多。



![](https://pic4.zhimg.com/v2-98a55e492ac0d15e5c3b3c990a6e135b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='416'></svg>)



虽然这些理论结果只是在无限宽度条件下成立，但谷歌发现原始网络的预测与线性版本的预测之间的一致性极好。而且对不同的体系结构、优化方法和损失函数都适用。

随着网络宽度变大，神经网络可以被其初始化参数的一阶泰勒展开项所取代。 而一阶线性模型动态的梯度下降是可解析的。

我们将这项研究的内容简单介绍如下：

## **实验方法**

本文采用完全连接、卷积和宽ResNet体系结构，使用足够小的学习速率，用完全和小批（mini-batch）梯度下降来训练模型，对比原始模型和线性模型之间的差异。

最后在CIFAR-10 (马和飞机)上进行二元分类，以及在MNIST和CIFAR-10上进行十种分类。

## **理论原理**

在高等数学中，有一种叫做“牛顿法”的方程近似解法。当所选区域足够小，就能用直线代替曲线，即一阶泰勒展开求近似解。在神经网络中，也有这么一种方法。

对于任何神经网络参数都可以做泰勒展开，即初始值加无限的多项式形式。当网络层的宽度趋于无限的时候，只需要展开中的第一个线性项，就成了线性模型。

假设D是训练集，X和Y分别表示输入和标注。这个完全连接的前馈网络有L个隐藏层、宽度为n。

在监督学习中，通过参数θ最小化经验损失，它是一个随时间变化的参数。f为输出的logits。



![](https://pic2.zhimg.com/v2-7afd459117dda96e0749fe2530186f31_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='786' height='142'></svg>)



## **线性化网络**

我们将神经网络的输出做一阶泰勒展开：



![](https://pic4.zhimg.com/v2-1489f5e946fd3b2d032a3bb37c264c17_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='788' height='84'></svg>)



ωt定义为θt − θ0，即参数θ相对于初始值的变化。输出包含两个部分，第一项是神经网络输出的初始值，在训练过程中不会改变；第二项是训练过程中的改变量。


ωt和ft随时间的变化满足以下常微分方程（ODE）：



![](https://pic2.zhimg.com/v2-140dc643e207cbd35ba0578e8622656d_b.png)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='788' height='134'></svg>)



## **无限宽度的高斯过程**

随着隐藏层宽度的增加，更加统计学中的**中心极限定理**，输出的分别将趋于高斯分布。

logits f(x)的平均值和标准差满足以下方程：



![](https://pic1.zhimg.com/v2-e45e80df3d9a3208df773ea31327bbc0_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='880' height='236'></svg>)



## **结果讨论**

当宽度无限且数据集有限时，神经网络的训练动态与线性化的训练动态相匹配。

在以前的实验中，选择足够宽的网络，能实现神经网络和较小数据集线性化之间较小的误差。



![](https://pic1.zhimg.com/v2-359c6e6df17d4a907b7c6fb439d2b350_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='876' height='524'></svg>)



谷歌研究了如何在在模型宽度度和数据集大小在相当大的一个取值范围内，让线性化动态和真实动态之间取得一致性。

最终，谷歌考察了测试集上网络的预测输出和真实输出之间的均方根误差( RMSE )。RMSE会随着时间的推移而增加，直到训练结束时达到某个最终值。



![](https://pic4.zhimg.com/v2-c1c4107cf0feb1c0e9978f49cf1775a7_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='856' height='758'></svg>)



我们可以看出，随着网络宽度的增加，二者之间的误差逐渐减小。对于全连接网络来说，这种下降的速率大约为1/N，对于卷积和WRN体系结构来说，则下降的比例更为模糊。



![](https://pic3.zhimg.com/v2-2cb2dc2e600826f6dc24f02218b4b13a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='858' height='824'></svg>)



此外，我们还可以看出，误差随着数据集大小，呈近似线性增长。尽管误差随着会数据集增加，但可以通过相应地增加模型大小来抵消这一点。

—**完**—
量子位 · QbitAI
վ'ᴗ' ի 追踪AI技术和产品新动态
戳右上角「+关注」获取最新资讯↗↗
如果喜欢，请分享or点赞吧~比心❤


