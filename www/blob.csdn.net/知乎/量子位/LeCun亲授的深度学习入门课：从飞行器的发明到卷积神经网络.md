# LeCun亲授的深度学习入门课：从飞行器的发明到卷积神经网络 - 知乎
# 



Root 编译整理

量子位 出品 | 公众号 QbitAI

深度学习和人脑有什么关系？计算机是如何识别各种物体的？我们怎样构建人工大脑？

这是深度学习入门者绕不过的几个问题。很幸运，这里有位大牛很乐意为你讲解。
![](https://pic4.zhimg.com/v2-c9d964be3f6ddf65ebd1a1c32404ff5b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='346'></svg>)
2月6日，UCLA（加州大学洛杉矶分校）与纯数学与应用数学研究所（IPAM）跨界组织主办的论坛上，“卷积神经网络之父”Yann LeCun操着一口浓重的法国口音，给数学界的科学家们介绍了深度学习。

LeCun从鸟类对飞行器发明的影响开始讲起，层层递进、逐步深入到深度学习的本质，可以说非常新手友好了。

机不可失，还不来围观这堂由大神亲自授课的深度学习入门指南？

快去搬凳抢座！

**量子位首先搬运了全程视频**（当然，是生肉），欢迎去量子位公众号（QbitAI）观看。

**当然，还整理了讲座的主要内容：**

今天在座的都是数学大神。

我呢，既不是数学家，也没拿诺贝尔奖。感觉站这儿份量不太够。

以前专业是工程学，不过炸过一些东西，后来就转软件了（不然世界怎么样就不好说了）。

今天来，给大家简单介绍一下深度学习。

但我们不从AI开始讲起，而是**从人类发明飞行器开始讲**。

依照达芬奇飞行器草图做的第一款飞行器，完全照搬了鸟类的外形。那时候根本不知道飞行底层的原理，所以只能从自然界的生物获得灵感，照葫芦画瓢。
![](https://pic2.zhimg.com/v2-570f0050d66fe74479b3fcc656933889_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='353'></svg>)
第一次飞行，只成功离地15公分，还是20公分的样子，飞行器就挂了。所以我们还是需要更系统的方法，就是后来莱特兄弟造飞行器用到的一套方法，飞行终于成为了现实。

一般来说，理论认知都是在实践积累之后才有的。

飞行就说这么多。

那么问题来了。

**人工智能可以从大自然里获得灵感吗？**

很明显，这个想法很旧啦。我们先看一下生物界的智能体。

人类的大脑，差不多有850亿个神经元。而每个神经元都有上万个突触，在一千到十万个之间不等。人脑相当高效，能耗才25瓦特，是PC的十分之一。

每个有大脑的动物都能学习，不同动物的学习方式可能不同，有些比较简单。它们并不需要特别好的视觉，或者其他智能体的教导，就能自己去学。

我们尝试找出动物学习的机制，然后用来训练机器学习。
![](https://pic4.zhimg.com/v2-5ffd819960cc7a503073ffd76eb85a53_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='336'></svg>)
慢慢地，从1940年起，就有了打造智能机器的想法。于是就冒出来了感知机（Perceptron）。

它不是一台计算机，而是一个计算机模拟器。输入值是电压，超过某个阈值，就打开。低于阈值，就关闭。而权重是可以被训练的，就像一个可旋转调节的钮。

尽管现在我们可以用三行Python代码实现它，但在那年代已经算是大型的计算机了。

这个感知机是怎么运转起来的呢？

原理是很简单的，你需要先集齐一堆训练数据。

比如说任务是图像识别，那么输入就是图像的一个个像素。当每个像素用0，1表示时，那么就可以组成一串数字。

你给机器一张图，字母A，然后输出应该是1。那么训练的时候，就读取图像中的像素，调高那些能增强最终结果是1，也就是判定字母是A的像素的权重，并调低偏离最终结果的像素的权重。
![](https://pic2.zhimg.com/v2-dc3b198527153dc06b7dc75a5b7558cd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='336'></svg>)
数学上只需要一行Python代码就可以搞定。

事实上，虽然这个办法是直觉上想出来的，但后来几年发现这个问题可以总结成几个方程，也是受到了生物学的启发。

我们回到人的大脑是怎么学习的。

每个神经元是通过突触来连接其他神经元，从而传递信号。
![](https://pic1.zhimg.com/v2-744992df5256d5f3787931a84b177a64_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='346'></svg>)
但数学上，这个概念被简化了，将感知机里的权重看成一个个的旋钮。

对于具体的输入，根据输出的错误再调参数，训练，重复，直到目标函数的值越来越小（目标函数的值，表示的是你得到的输出和你想要的输出的差值）。

这叫做梯度下降（gradient descent），依然是很简单的数学问题。

举个稍微复杂一点的例子，我们要做一个图片分类器，辨识汽车、飞机、椅子等物体
![](https://pic3.zhimg.com/v2-f81f4ca594646b16eaa519c7880bd2e6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='315'></svg>)
它们的外观千变万化，**我们怎样让计算机认出每一类物体呢？**

这需要依赖大量的手动调整。给系统一张车的照片，如果系统将它认成车，红灯亮起。如果红灯不亮，就调整这些按钮，让红灯的亮度增强；输入飞机的图片，调整按钮，让绿灯亮度增强。

输入足够多的训练数据不停调整按钮，直到机器能够辨认出来它从来没见过的相片为止，那么就算训练成功了。

你们肯定会问，这个能识别图像的神秘盒子里到底装了什么？

这个答案，在过去的几十年里，一直在变。

传统的模式识别，是给它一张图，然后过一个特征提取器。这个特征提取器是人工搭建的，把这些图像的像素变成一串数字，然后用简单的算法吸收消化，得到这张图的内容。这种方法在深度学习出现以前一直都在用。
![](https://pic3.zhimg.com/v2-d206d66d2b09aad215cbb955eba4aaae_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='320'></svg>)
而深度学习是把模块分成可以被训练的好几层。就像视觉信号的传递一样，需要多步来提取信息。

下一个我们要问的问题是，我们应该往这些可训练的模块箱里放什么东西？

“多层”的概念是50年代提出的，到80年代时用的人稍微多了起来。
![](https://pic3.zhimg.com/v2-ad793ebac539dcb5367f7dd421bb6316_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='300'></svg>)
每一层都是由简单的单元组成，而单元又是基于上一层的输入，经过不同程度的权重处理得到的。然后如果值超过阈值，就继续往下走，低于阈值就不取。

**那么，我们要如何训练机器呢？**

这其实是不断调小偏差的过程。问题的关键在于往什么方向调整参数、调整到什么程度，才可以拿到我们想要的输出。

1980年，这个问题才有了解决方案。

这个方案是一个复杂的数学概念的实际应用， 叫链式法则（Chain rule）。

当你有一个网络的时候， 你有的是连续的功能区块（Functional block）。

每一个区块或者做矩阵乘法，或者是给每个输入做一个非线性的运算。我们来看看系统之中分离出来的一个区块。
![](https://pic3.zhimg.com/v2-e05fb767b5991364e1fad8fc650b2b7e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='296'></svg>)
你可以简单地算出来输出值。比如说这是一个线性的矩阵乘法模块，参数乘以向量，这样你就能拿到输出的向量。这两个区块有不一样的维度。

现在假设，对于每个向量中任何元素的调整，我们都知道损失会往什么方向变化。

损失函数的斜率，表示的是我们得到的输出和我们想要输出的差值。通过计算，可以得出图中绿色的向量，从上到下计算一个递归公式，通过反向传播，就能得到cost和所有模块相关的梯度。

很多现在的平台，在你写程序定义网络后，都可以自动运行反向传播，计算梯度。

**这些问题都解决了之后呢，我们就可以建一个人工的大脑了吗？**

要知道，人脑每秒可以做10的17次方的运算，神经元数量达到10的11次方。

我们来看一款运算速度很快的芯片。右下角的英伟达Titan-V，这个GPU每秒可做10的15次方运算，比人脑要慢100倍。
![](https://pic2.zhimg.com/v2-b0434ac2f5c1e4f2217b2f1cf907c225_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='371'></svg>)
所以大家算算，即使芯片的速度翻一倍要18个月的话，那还要多长时间能达到和我们人脑一样？如果要让芯片在合理的大小范围的话，我认为我们还要等多几十年。

但这个不是主要问题。主要问题是我们不知道怎么编程它们、怎么训练它们、训练原则是什么。

这个GPU很便宜，才3000刀，但是现在大家都在买来挖矿，所以已经买不到了。

**我说过很多次了，如果在我职业生涯中，能够造一个智能体，像大鼠一样具有常识，我会感到很开心很满足。**我们现在也许有相应的算力了，但我们还没有搞清楚潜在原则。现在是这个底层原理限制住了。

好啦，**现在我们来跳出来看看生物还有没有给我们别的启发。**

Hubel & Wiesel 1962这个生物研究工作太有名了，大家都知道的，是70年代拿了诺贝尔奖的。工作本身是在60年代做的，是视觉信号传递的生理结构。

简单的细胞检测位置信息，复杂的细胞整合简单细胞受到刺激的信息
![](https://pic3.zhimg.com/v2-f5b9df6c4642b136672fb8558e213eb6_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='334'></svg>)
所以，如果有一个特殊的动机，稍微移动一点，复杂细胞都会被激活。

Fukushima 1982年造了一个计算模型，就是描述简单细胞和复杂细胞之间的层级关系。这个是80年代的工作，那时候还没有合适的学习算法。所以用了其他的非监督型算法。

后来，我受到这个算法启发，造了一个含有相似构造的网络，用反向传播算法来训练，就是我们平时说的**卷积神经网络（CNN）。**

下面是卷积神经网络的示意图。

图像中的像素会激活CNN中的单元。但我不敢称他们为神经元，不然神经科学家会不爽。因为比起神经元来说，这些单元实在是太简单了。
![](https://pic3.zhimg.com/v2-ecb76b9dac6f16b195a4125830ac673e_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='295'></svg>)
每个单元，看起来像patch。这些单元会和阈值比，比他们高，就打开。低的话就关上。

可以看到这个用激光笔指出的patch是系数。

左边这个是输入patch，把系数向量和输入向量乘在一起。用系数把整个输入刷一遍，然后你记录就能得到右边的结果。

如果它们能够匹配的话，就得到高度激活的结果，不匹配就得到非激活的结果。

这在数学上就叫做离散型卷积。

经过了层层卷积核的系数处理，最后得到的是最右边的压缩过的信息。
![](https://pic2.zhimg.com/v2-9b59c00c2a0cb34a63e584a1226d24a9_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='292'></svg>)
我们90年代中期的时候弄了一个很牛的模型。不仅能够认出来一个字母，还能认出多个字母，还不用先分割开。当时如果用经典数学算法就必须先分割。

等到无法从图像中分割出物体时，我们模型的重要性就显现出来了。

这个模型中每一层都是卷积的，同时进行分割和识别。

这是那个时候年轻的我，把一张纸条放在一个摄像头下面，然后按一下键盘。这是1993年的时候（嘴上说的是1992年）。

这是我在新泽西的时候贝尔实验室那会儿的电话号码，现在已经不用了。
![](https://pic3.zhimg.com/v2-812a8124e4ef9e3f3ee043be2c6ee46a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='285'></svg>)
在几秒钟之内，就可以处理图像，识别出数字。

训练数据量不用很多，哪怕是很小的、不同的手写体，都能识别成这样，效果很不错了。

为了以合适的速度跑起来这个程序，我们用了特殊的硬件DSP 32C，速度可到 20 FLOPS。最后，我们用这个造了一个可以识别支票的系统。于1996年左右开始铺开使用。

到90年代末，这套系统已经在处理10%到20%左右的支票了。如果你够老的话，也许你的支票被这套系统读取过。

这么看，这套系统还是挺成功的。可惜在90年代中期，在机器学习圈里，大家对神经网络的热情消失了。

很大一个原因是，这套系统需要大量的计算力投入才软件系统里。这样才有可能跑起来。

这一切都发生在MATLAB、微软系统、Linux出现之前，AT&T都还没公开相应的硬件资源。没有大型计算机，或大型数据集，大家做这个都只能靠直觉。

其实在那个时候，很多东西都很玄学，我们并不能从数学的角度去解释他们。

不能解释背后的原理的话，就没办法形成一套理论。没有理论就很混乱，都发不了文章。

哪怕事实上，这些方法是可用的，但是也被抛弃了。

不过我们当中的某些人，知道，这方法最终还是会回来的。因为在某些情况下，这套方法是更好的。

因为它们会自己学习，不仅仅是识别图像，还能够抽象地表示这个世界。它们能够找到事物的本质，然后找到不同部分之间的联系，然后组装成以个整体。它们做的事情很强大，所以也需要更多的数据。

在1996年和2002年之间，我几乎停止了这方面的研究，改做图像压缩。

2003年的时候我又开始搞回这个了。我们做了一辆有两个摄像头小车，让人来控制它，当距离障碍物2米的时候，我们会控制它向左转或向右转来避开障碍物。然后，我们训练一个CNN来看两个摄像头采到的画面，去预测方向盘转向的角度
![](https://pic2.zhimg.com/v2-1995633904ed967088f7e6698904f245_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='318'></svg>)
只需要20分钟的训练数据，这个CNN就可以自己开车了！遇到障碍物的时候，它会自行转向避开。

在这套系统的启发下，DARPA举办了LAGR（Learning Applied to Ground Vehicles），一个150万美元的项目。
![](https://pic2.zhimg.com/v2-8fb3fa0c72e2746e8423b67adaf59efd_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='402'></svg>)
你可以看到这个机器人有四个摄像头，内部装了三台计算机，可以在自然环境中自动行驶。我们训练了一个CNN，让它告诉我们在画面上，哪些区域是可以顺利通过的。

使用传统的立体视觉成像技术，也能实现这个功能。但是，立体成像很贵，工作范围也很有限，大概能做到10米的距离。

这就是CNN的一种用途。

很快我们就意识到，不能只是标记一个区域能不能通过，更有意思的是，看图中的某些像素属于哪个物体。（物体识别分类）。

举个例子来说，这些是天，树，窗，路等等。
![](https://pic4.zhimg.com/v2-cda73af39bcd175dd0f6a2a051f8db8f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='352'></svg>)
这是有人骑着自行车上路拍到的第一人称视角画面，这个算法不能说完美，它认为这里是沙漠，实际上在曼哈顿不可能有沙漠。

不过，它识别行人等主要目标的能力都不错，而且即使在普通电脑上跑，也比当时最领先的系统快100倍。这个算法让很多人产生了灵感，认为我们能把它用到无人驾驶上。

2014年，有两个公司很快就把这个技术拿过去用了。一个是MobilEye，另一个是Nvidia。
![](https://pic2.zhimg.com/v2-05182d508ae0b568f5343b254f5b96e5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='379'></svg>)
**2010年之前，这些研究都在低调地进行着，后来，事情有了变化。**

2011年的时候，深度学习在语音识别上有了重大的进展。

在2012年年底，深度学习在ImageNet比赛上一举成名。ImageNet数据集包含1000类物体的130万张照片，传统图像分类算法在这个数据集上取得的最低错误率大约是26%。

2012年，一个多伦多大学做出来的大型CNN，将错误率降到了15%。他们是第一个正式用GPU跑这么大的CNN的团队。

于是，突然之间，整个计算机视觉领域都开始使用这项技术。我从来没见过一个研究领域如此快速地从一种技术转向另一种。

其实就在2011年，我们还提交了一篇论文到CVPR。这篇论文打败了当时最好的记录，但是却被拒了。因为那个时候人们都不相信CNN能取得这么好的成绩。因为大家没见过，于是，他们就主观臆断地认为我们犯规了之类的。

但是3年之后，世道完全反过来了。你不用CNN，文章都不可能被接收。

不过这也不是一件好事。因为这样会灭杀多样性。讲这件事是想让大家知道，这在当时是一个多么重大的革命。

这些网络都特别大，有上百万个按钮、单元和权重。网络的第一层检测的都是一些基本motif，比如边缘、线条等等。

有的CNN多达50层，甚至更多。**为什么我们需要这么多层？**
![](https://pic3.zhimg.com/v2-5f0da77d3eea151dc4832b0867ef841a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='384'></svg>)
神经网络的多层架构对应着数据的组成型结构，不同层检测不同的特征，比如线条、边缘等底层特征，圆圈、弧线、角等中层特征，更接近图形的高层特征。

这个世界的所有事物呈现都是分层的。比如文本，就是从字母，字，词，从句，句子，段落组成的。

爱因斯坦曾经说过，这个世界最不可思议的事，是所有东西都是可以被理解的。

> 世界上最令人费解的事情是，世界是可以理解的。

**过去几年大量的公司做了很多努力，让这些技术落地，并规模化。**

开始列举最近各种研究进展

比如说，我们现在用256个CNN，1小时就能完成在整个ImageNet上的训练，错误率达到23.74。

计算机视觉的最前沿研究Mask R-CNN，可以做物体分割，关键点检测，人体姿态捕捉等等。用Sparse ConvNet还可以做3D语义分割——



![](https://pic1.zhimg.com/v2-ffd84e6b59db2a0d7e8dd554ca4cbbbc_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='386'></svg>)



另外，CNN还能用在和视觉没什么关系的领域，比如做翻译。这对于Facebook来说很有用，帮助用户翻译短篇的文章。

今天分享提到的很多资料，都是开源的。

卷积神经网络可以应用在很多领域，比如在无人驾驶上，可以帮机器用视觉感知环境。在医学影像、基因学、物理学等等各种领域都有应用，而且几乎每天都有新的落地领域出现。

深度学习不仅能感知，还能推理。

比如说，我们可以根据一张图片，提出问题，

> 下图中方块的数量比黄色的物体多吗？



![](https://pic4.zhimg.com/v2-97e5043db89f0f7efa362d483ae4f557_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='376'></svg>)
如果是人类来回答这个问题，需要分别数一数方块和黄色物体的数量，然后比较这两个数量的大小。

对于神经网络来说，就需要一个模块来分类出方块和黄色物体，另一个模块来数数，还需要一个模块比较大小给出答案。

这个神经网络是动态的会随着输入的变化而变化，输入会决定神经网络的架构。

另外，用记忆模块来增强神经网络也是一个很有意思的研究方向。

在讲座中，立昆老师又提到了他最近推崇的可微分编程。感兴趣的同学可以阅读之前的文章，以及自行看视频。

最后，立昆老师还强调了一点：目前，机器并没有通用的智能，也没有尝常识。
![](https://pic1.zhimg.com/v2-be4b0b3c8d82e83ecb32a6a47288ca38_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='373'></svg>)
— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动վ'ᴗ' ի 追踪AI技术和产品新动态


