# 谷歌助手超进化：任选两种语言即可自动识别，无需手动切换语种 - 知乎
# 



> 圆栗子 发自 凹非寺
量子位 出品 | 公众号 QbitAI

想象，你交往了一个日本女生。 
![](https://pic3.zhimg.com/v2-a90bf3579dfb6afe73e549abb6eedbda_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='600' height='394'></svg>)△ 千反田，日本语发音“吃蛋挞”
你家的智能音箱，大概就要听你们两个人说话了。

语音助手，如果要**中文日文来回调**的话，还挺累的。

不过，现在遇到这种情况，**谷歌助手**已经不用手动转换语言了：

预先设置好两种语言，比如中文日文。然后你说中文，谷歌助手就用中文答你，她说日语，谷歌助手就用日语回她。

多么顺滑。
![](https://pic2.zhimg.com/v2-8e21041de329dfffab70c3e404df8311_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='412' height='120'></svg>)△其实并不简单
上面说的都没错，除了**中文**还没支持……

目前支持的语言有：英语、西班牙语、法语、德语、意大利语，和日语。

至少，日语还是有的嘛。

而且，做个顺滑的**双语**小王子，对语音助手来说，并不是一件容易的事。
![](https://pic1.zhimg.com/v2-3656594f6b929ad9807671d3fb9b968c_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='806' height='560'></svg>)△ 空耳，是一座大山
谷歌团队也是经过多年努力，走过许多险要之地，才来到今天的。

从**识别多种语言**，到**理解多种语言**，再**优化识别过程**……

我们来仔细感受一下：

## **听出哪种语言**

听到别人在说日语的时候，你即便不会说日语，也很**容易听出**那是日语。

但让机器来做这样的判断，并不容易。就算有了全自动语音识别系统，也很难做到。 
![](https://pic2.zhimg.com/v2-c71941e91c547660f1723734003cc8a5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='980' height='368'></svg>)
所以，从前的语音助手，用之前都要设定好语言，省却这一步判断。

2013年，谷歌开始用深度神经网络，来开发口语辨别 (**LangID**) 技术。

如今，AI已经能在**2000多对**语言之间，辨别谁是谁：比如英语vs法语、英语vs日语、日语vs法语。

就像语音检测 (有没有人在说话) 、说话人识别 (谁在说话) 、语音识别 (ta在说啥) 的算法一样，这里用的也是**序列建模** (Sequence Modeling) 。

一个难点，就是要用**更大的音频数据集**来训练模型。

## **听懂说的是啥，要快**

要同时理解两种以上的语言，AI的工作流程变得很复杂。

多个进程要并行，每个进程都会产生**增量结果** (Incremental Results) ，让语音助手在分辨语种之外，还要分析人类在说些什么，创建命令然后执行。
![](https://pic4.zhimg.com/v2-8b31235b756be23486238e3b7dddd49f_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='291' height='286'></svg>)
比如，人类说“上个早晨6点的闹钟”，对语音助手来讲，就是把时钟应用打开，把6点的参数设好，另外还要设置闹钟**今天就工作**。当然，单语助手也是这样。

然而，单语容易做到的事，双语就未必了。启用LangID，两套语音识别系统同时工作，二者做出的判断最后要经过一轮**评估**。

这一步，是由**另外一个算法**完成的。这个算法会给两个语音识别系统的判断结果，做个排名，决定要输出怎样的命令。
![](https://pic1.zhimg.com/v2-5fd074471f35ce4a90cb10d3d787c788_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='714' height='202'></svg>)
当用户说完的时候，模型除了知道ta说了哪种语言，也要领会ta的意图。而评估步骤会增加**处理成本**，也会造成不必要的**延时**。

反应慢，可能是语音助手最大的缺点了。所以，算法还需要优化。

## **优化，优化**

要最大限度地解决运算成本与时间问题，就要在**分辨语种**的环节，加快速度。

如果，在用户说完**之前**，算法就能判断出语种。这时，另外一种语言的识别器，就不用继续听了。运算量减少了，用时也减少了。

听过一部分，算法就**初步猜测**一下语种。越早判断完成，就能越早把任务简化到**单语**。

但是，到底什么情况下，可以锁定一种预言，抛弃另一种？
![](https://pic3.zhimg.com/v2-a02c3c89763fe7c49425c555c09dc9b2_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='214'></svg>)
这里用的是**随机森林**，结合了背景信息，比如用户用的什么设备，算法给出的备选命令以前是不是经常出现，用户是不是经常用这种语言等等。

这些因素，都会坚定AI的判断。

## **从双语到三语**

现在，谷歌助手还只能支持两种语言同时识别。

但算法的优化还在进行，团队正在朝着**三语**进发。

不过，支持三语之前，最好还是先把汉语支持了吧。﻿
![](https://pic4.zhimg.com/v2-68db3e3f07d29bd38d939a82c62d04b3_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='472' height='268'></svg>)
— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai)· 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


