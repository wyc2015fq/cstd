# 刚刚，Google发布AI七原则：不开发武器，但会继续与军方合作 - 知乎
# 



> 夏乙 一璞 发自 凹非寺
量子位 出品 | 公众号 QbitAI

终于，Google亮明了态度。

在经历了数个月的风波之后，在数千员工发起联名抗议之后，在学界大牛纷纷联署反对之后，在各大媒体深入曝光内幕之后，Google终于调整了自己政策。

今天早间，Google CEO正式发布了使用AI的七项原则。其中包括不会将AI技术应用于开发武器，不会违反人权准则将AI用于监视和收集信息，避免AI造成或加剧社会不公等。

不过，Google也表示，将会继续与政府和军方展开AI合作，例如网络安全、培训以及征兵等领域。

“我们认识到，这种强大的技术引发了同样强大的问题，AI的开发和使用将会对未来多年的社会产生重大影响。”Google CEO皮查伊（Sundar Pichai）写道。

另外，Google制定的原则还要求公司职员及客户“避免给人带来不公正的影响”，尤其是与种族、性别、性向以及政治或宗教信仰有关的影响。

皮查伊称，对于违反上述原则的应用，Google保留封禁的权利。

同时，Google云CEO戴安·格林（Diane Greene）也在官方博客上发布了一篇文章，来介绍如何在Google Cloud上践行Google的AI原则。

她特别提到，Google将不再延续Project Maven的合同。但是“正在与‘我们的客户’合作，以求用对他们来说长期有效、又不和我们的AI原则冲突的方式，负责地履行我们的义务，”

## **各方评价**

看见这份《原则》，不少Google员工给出了很愉快的评价。

Google现任AI负责人Jeff Dean除了在Twitter上分享Google发表的这份原则之外，还提到为了真正践行这些原则，他们还提出了一套技术实践指南Responsible AI Practices，指导科学家和工程师们在构建AI产品的时候应该注意什么。就是这个：
[https://ai.google/education/responsible-ai-practices](https://link.zhihu.com/?target=https%3A//ai.google/education/responsible-ai-practices)

DeepMind联合创始人穆斯塔法（Mustafa Suleyman）自己没有发表观点，而是在Twitter上转发了人权观察组织（Human Rights Watch）Mary Wareham的话：各国政府应该关注到科技行业的最新动态，开始就禁止全自动武器展开协商，制定新的国际法规。
![](https://pic2.zhimg.com/v2-b8037ebe825e9767f6d1850d64e8f9e5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='800' height='450'></svg>)
Keras作者François Chollet说，对于AI创业公司来说，这个是一个很好的例子，表明你可以对产品中道德问题的潜在影响进行前瞻，每个AI创业公司都应该起草一份这样的东西，不要等到危机四起才动手。

Google大脑研究员Dumitru Erhan评价得非常简短，简单说了一句“看见这个出来了好开心”，大概代表了很多Googler的心声，二进宫的Ian Goodfellow也转发了。

但是对于这份原则，并不是所有人都买账。

《纽约时报》认为，由于Google此前一再强调自己为国防部所做的工作并非出于“进攻目的”，这些指导原则能否将该公司排除在类似Project Maven的合同之外，尚不清楚。

ZDNet也在评论中指出，即便Google说不让自己的技术用于武器开发，但一旦变成开源项目，这些技术和代码就有可能用于作恶，就像其他的工具与技术一样。

## **原则全文**
![](https://pic1.zhimg.com/v2-90a601a1811f96776eaeccedf0cdb280_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='720'></svg>)
皮查伊在名为《Google人工智能：我们的原则》的博文中写到：从垃圾邮件筛查到医疗诊断，人工智能可以帮助人们完成许多事情，这是我们致力于AI研发的原因。但这种强大的技术也会带来问题，人工智能的应用将会对未来社会带来深远影响。

然后他公布了七项人工智能指导原则：

**AI应用目标：**

我们相信AI应当

1、对社会有益。

2、避免制造或加剧偏见。

3、提前测试以保证安全。

4、由人类担责。

5、保证隐私。

6、坚持科学高标准。

7、从主要用途、技术独特性、规模等方面来权衡。

**我们不会将AI应用于：**

1.制造整体伤害之处。如一项技术可能造成伤害，我们只会在其好处大大超过伤害的情况下进行，并提供安全措施；

2.武器或其他用于直接伤害人类的产品；

3.收集使用信息，以实现违反国际规范的监控的技术；

4.目标违反被广泛接受的国际法与人权原则的技术。

在博文最后，Google CEO写道：我们相信这些原则是Google和AI未来发展的正确基础，这与我们2004年发布的价值观一致。我们在此明确表示Google更看重AI的长远发展，即便需要为此付出短期的权衡。

“**We said it then, and we believe it now.**”

## **风波回顾**
![](https://pic3.zhimg.com/v2-d6c48739662e94b585098086ab6f18fa_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='600' height='400'></svg>)
去年秋天，Google开始秘密与美国国防部接洽，合作Project Maven项目，根据双方的秘密协议，Google原本要为美军无人机提供顶尖AI技术。

包括李飞飞在内的高管团队，当时就此事展开了激烈的内部讨论。讨论的核心就是，不能让外界获知此事，否则会影响Google的声誉。

不过纸终究包不住火。

这一项目在内部开展后，Google员工们开始发起抵制。

3月，Google内部3000多名员工联名上书，抵制与五角大楼的合作。4月，事件进一步发酵，包括Bengio等上百名学者、科学家二次上书，数十名员工因此离职，要求Google退出Maven项目，并且起草政策声明Google永远不会开发战争技术。

随后媒体也介入，深扒此事。

详情请看我们此前的报道：

[Google被骂成筛子](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzNjc1NzUzMw%3D%3D%26mid%3D2247499397%26idx%3D1%26sn%3D0d6e5bc3defa4a680fb3864c11e3b709%26chksm%3De8d049f7dfa7c0e1de1f8424cf832d782b5f3e87d209b6d79f17d7a87928f8311903b4941205%26scene%3D21%23wechat_redirect)

[Google对与军方合作事件始末](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzNjc1NzUzMw%3D%3D%26mid%3D2247499306%26idx%3D3%26sn%3D5c0b3ad6cb665b15403454a9ae158de2%26chksm%3De8d04958dfa7c04e6f4f255931b60175cdfa055e18dce31b78dc6fbab4e93fbeeaaee51a207e%26scene%3D21%23wechat_redirect)

— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


