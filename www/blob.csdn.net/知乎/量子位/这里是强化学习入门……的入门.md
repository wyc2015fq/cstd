# 这里是强化学习入门……的入门 - 知乎
# 



> 原作：Thomas Simonini
墙化栗子 编译自 FreeCodeCamp
量子位 出品 | 公众号 QbitAI
![](https://pic2.zhimg.com/v2-b3dc44478a41de6442e79a2eb3fcd9b1_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='796' height='540'></svg>)
**强化学习**是机器学习里面非常重要的一个派别。智能体 (agent) 会不断执行一些操作，通过结果来学习，在不同的**环境**中分别应该采取怎样的行动。

在一系列教学文章里，我们可以了解不同的架构，来解决强化学习的问题。Q学习，深度Q网络 (DQN) ，策略梯度 (Policy Gradients) ，演员-评论家 (Actor-Critic) ，以及近端策略优化 (PPO) 都是将要涉及的算法。

这是本系列的第一篇文章，你可以抓住的**重点**有：

**·** 什么是强化学习，以及为什么奖励最重要

**·** 强化学习的三种方式

**·** 深度强化学习的“深度”是什么意思

以上几点，在进入强化学习的复杂世界之前，可能还是有必要了解一下。

## **这是个友好的引子**

强化学习的中心思想，就是**让智能体在环境里学习**。每个行动会对应各自的奖励，智能体通过分析数据来学习，怎样的情况下应该做怎样的事情。
![](https://pic1.zhimg.com/v2-4e966d941ae0d9e8d89413be4899d290_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='600' height='168'></svg>)
其实，这样的学习过程和我们自然的经历非常相似。**想象自己是个小孩子**，第一次看到了火，然后走到了火边。

你感受到了温暖。火是个好东西 (**+1**) 。
![](https://pic1.zhimg.com/v2-43946e1cf4646b4b706b6b518e1cf100_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='600' height='170'></svg>)
然后就试着去摸。卧槽，这么烫 (**-1**) 。
![](https://pic3.zhimg.com/v2-c34a26e5d8dc107606f4d55ce35964de_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='539' height='159'></svg>)
结论是，在稍远的地方火是好的，靠得太近就不好。

这就是人类学习的方式，与环境**交互**。强化学习也是一样的道理，只是主角换成了计算机。
![](https://pic4.zhimg.com/v2-e058a9d05a30d09edbbd0b70da685133_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='492'></svg>)
比如，智能体要学着玩超级马里奥。强化学习过程可以用一个循环 (**loop**) 来表示：

**·** 智能体在环境 (超级马里奥) 里获得初始状态**S0** (游戏的第一帧) ；

**·** 在state 0的基础上，agent会做出第一个行动**A0** (如向右走) ；

**·** 环境变化，获得新的状态**S1** (A0发生后的某一帧) ；

**·** 环境给出了第一个奖励**R1** (没死：+1) ；

于是，这个loop输出的就是一个**由状态、奖励和行动组成的序列**。

而智能体的**目标**就是让**预期累积奖励最大化**。

## **奖励假说为根基**

问题来了，目标**为什么**是预期累积奖励最大化？

因为，强化学习原本就是建立在**奖励假说**的基础之上。想表现好，就要多拿奖励。

每一个**时间步** (time step) 的累积奖励都可以表示为：
![](https://pic3.zhimg.com/v2-0b07905050aac6d2d575571ee2ee32fa_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='524' height='51'></svg>)
**或者**
![](https://pic1.zhimg.com/v2-daf2fbdd0e2bd0896be993fab3038f44_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='525' height='224'></svg>)
不过，我们**没有办法把奖励直接相加**。因为游戏里，越接近游戏开始处的奖励，就越容易获得；而随着游戏的进行，后面的奖励就没有那么容易拿到了。

把智能体想成一只**小老鼠**，对手是只猫。它的目标就是在被猫吃掉之前，**吃到最多的奶酪**。
![](https://pic2.zhimg.com/v2-8857915b23a2288e2f10817bf54aaaa5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='500' height='502'></svg>)
就像图中，**离老鼠最近的奶酪很容易吃**，**而从猫眼皮底下顺奶酪就难了**。离猫越近，就越危险。

结果就是，从猫身旁获取的奖励会**打折扣**，吃到的可能性小，就算奶酪放得很密集也没用。

那么，这个**折扣要怎么算**呢？

我们用γ表示折扣率，在0和1之间。

**·** γ越大，折扣越小。表示智能体越在意**长期**的奖励 (猫边上的奶酪) 。

**·** γ越小，折扣越大。表示智能体越在意**短期**的奖励 (鼠边上的奶酪) 。

这样，**累积奖励**表示出来就是：
![](https://pic4.zhimg.com/v2-91ec8e6fd0f2794e353d6ec9092e5167_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='984' height='328'></svg>)
简单来说，**离猫近一步**，**就乘上一个γ**，表示奖励越难获得。

## **片段性任务还是连续性任务**

强化学习里的任务分两种。

## **片段性任务 (Episodic Tasks)**

这类任务，有个**起点**，有个**终点**。两者之间有一堆状态，一堆行动，一堆奖励，和一堆新的状态，它们共同构成了一“集”。

当一集结束，也就是到达终止状态的时候，智能体会看一下奖励累积了多少，以此**评估自己的表现**。

然后，它就带着之前的经验开始一局新游戏。这一次，智能体做决定的依据会充分一些。
![](https://pic2.zhimg.com/v2-8857915b23a2288e2f10817bf54aaaa5_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='500' height='502'></svg>)
以**猫鼠迷宫**为例的一集：

**·** 永远从同一个起点开始

**·** 如果被猫吃掉或者走了超过20步，则游戏结束

**·** 结束时，得到一系列状态、行动、奖励和新状态

**·** 算出奖励的总和 (看看表现如何)

**·** 更有经验地开始新游戏

**集数越多**，**智能体的表现会越好**。

## **连续性任务 (Continuing Tasks)**

**永远不会有游戏结束的时候**。智能体要学习如何选择最佳的行动，和环境进行实时交互。就像自动驾驶汽车，并没有过关拔旗子的事。
![](https://pic4.zhimg.com/v2-b4e6eab11cec63a90cc7127baa04b433_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='408' height='230'></svg>)
这样的任务是通过时间**差分学习** (Temporal Difference Learning) 来训练的。每一个时间步，都会有总结学习，等不到一集结束再分析结果。

## **探索和开发之间的权衡**

在讨论强化学习的几种方法之前，必须讲到这件事。

**·****探索** (Exploration) 是找到关于环境的更多信息。

**·****开发** (Exploitation) 是利用已知信息来得到最多的奖励。

要记住，目标是将预期累积奖励最大化。正因如此，它有时候**会陷入一种困境**。
![](https://pic4.zhimg.com/v2-38fece19ddbf11811bc30b156d3996db_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='488' height='490'></svg>)
小老鼠可以吃到无穷多块分散的奶酪 (每块**+1**) 。但在迷宫上方，有许多堆在起的奶酪(**+1000**) ，或者看成巨型奶酪。

**如果**我们只关心吃了多少，小老鼠就永远**不会去找那些大奶酪**。它只会在安全的地方一块一块地吃，这样奖励累积比较慢，但它不在乎。

**如果**它跑去远的地方，也许就会发现大奖的存在，但也有可能**发生危险**。

程序猿需要设定一种规则，让智能体能够**把握二者之间的平衡**。

## **强化学习的三种方法**

前菜吃完了，我们终于要开始讲解决强化学习问题的方法了。三种方法分别是：基于价值（value-based）、基于策略（policy-based）以及基于模型（model-based）的方法。

## **基于价值 (Value-Based)**

这种方法，目标是**优化价值函数V(s)**。

价值函数会告诉我们，智能体在每个状态里得出的未来奖励最大预期 (maximum expected future reward) 。

一个状态下的**函数值**，是智能体**可以预期的未来奖励积累总值**，从当前状态开始算。
![](https://pic4.zhimg.com/v2-f40ce63e62a45a1637b19cfba5d1ae6f_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='692' height='118'></svg>)
智能体要用这个价值函数来决定，每一步要选择哪个行动。它会采取函数值 (就是**Q值**) 最大的那个行动。
![](https://pic2.zhimg.com/v2-63014e0e8b43ea3872640725f6cb8b81_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='748' height='426'></svg>)
在迷宫问题中，每一步我们都选取最大函数值：-7，-6，-5，以此类推，达到目标。

## **基于策略 (Policy-Based)**

这种方式，会**直接优化策略函数π(s)**，抛弃价值函数。

策略就是评判智能体在特定时间点的表现。
![](https://pic3.zhimg.com/v2-f1b3b43d01e144a3a5298096b46d2d6a_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='526' height='151'></svg>)
把每一个状态和它所**对应**的最佳行动建立联系。

策略分为两种，

**·****确定性**策略：某一个特定状态下的策略，永远都会给出同样的行动。

**·****随机性**策略：策略给出的是多种行动的可能性分布。
![](https://pic4.zhimg.com/v2-26e419f0c7269314fc886621b92b1b3b_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='580' height='209'></svg>)![](https://pic2.zhimg.com/v2-e3bb3eb801d3b168a4e505eb4e8d8e7d_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='748' height='426'></svg>)
从图中我们可以看到，策略**直接指出**了每一步的最佳行动。

## **基于模型 (Model-Based)**

这种方法是对环境建模。这表示，我们要创建一个模型，来表示环境的行为。

问题是，**每个环境**都会需要一个不同的模型 (马里奥每走一步，都会有一个新环境) 。这也是这个方法在强化学习中并不太常用的原因。

## **深度强化学习**

**所谓深度强化学习**，**就是在强化学习里**，**加入深度神经网络**。

如图，拿Q学习和深度Q网络 (DQN) 来举例。
![](https://pic3.zhimg.com/v2-0bdb5d258cb46f31cf0c5ad7f4377b86_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1000' height='753'></svg>)
**·****Q学习**，是利用一个传统算法创建Q-table，来帮助智能体找到下一步要采取的行动。

**·****DQN**，是利用深度神经网络来近似Q值。

恭喜你读到现在。这第一篇文章的信息量还是不小的。
![](https://pic4.zhimg.com/v2-1ef816ea338d97dd3186b9f1e3da5337_b.gif)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='334' height='171'></svg>)根本停不下来
有兴趣的同学，可以坚持服用一疗程。

这里是本系列**大纲**的传送门：
[Deep Reinforcement Learning Course​simoninithomas.github.io![图标](https://pic3.zhimg.com/v2-f5b1281b0b5cc206f53a70afe444648a_180x120.jpg)](https://link.zhihu.com/?target=https%3A//simoninithomas.github.io/Deep_reinforcement_learning_Course/)
— **完** —

欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)

诚挚招聘

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者

վ'ᴗ' ի 追踪AI技术和产品新动态


