# SQuAD2.0来了！新增5万人工撰写问题，且不一定有答案 | ACL最佳短论文 - 知乎
# 



允中 发自 凹非寺 
量子位 报道 | 公众号 QbitAI




SQuAD 2.0来了！


今日（6月13日），斯坦福NLP团队对外宣称，机器阅读理解数据集SQuAD（Stanford Question Answering Dataset）完成新一波更新，将由SQuAD 1.1版本迭代至SQuAD 2.0。

个中变化还是非常明显的。

## **SQuAD 2.0**

斯坦福NLP官方说，相较SQuAD 1.1中的10万问答，SQuAD 2.0又新增了5万个人类撰写的问题——而且问题不一定有对应答案。

于是同时迭代的SQuAD 2.0测试系统，不仅要求机器能从对应段落中找到问题答案，还测试机器在没有对应答案时可以say No，而不是瞎猜。

这算是进一步加大了机器在精准回答方面的难度。

目前，人类表现分别是EM——精准匹配结果：86.831分，F1——模糊匹配：89.452分。
![](https://pic3.zhimg.com/v2-fd2ffae02d4645528cb74e08ff8390be_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='225'></svg>)
需要指出的是，SQuAD推出之初，2016年，斯坦福大学从维基百科上随机选取了500多篇文章，并进一步细分成两万多个段落。随后采用众包的方式，由人类阅读这些文章后，为每个段落提出五个问题，并对段落内的答案进行人工标注。


最后，终于构成了包含10万多个问题的阅读理解数据集SQuAD。

但争论也隐藏其中，并在今年“机器阅读理解能力击败人类”事件中彻底吵开了。

## **SQuAD风云**

SQuAD数据集有两个衡量标准，EM和F1。

EM是精确匹配结果，也就是模型给出的答案与标准答案一模一样。

F1是模糊匹配，可以理解为机器答对了部分内容，是根据模型给出的答案和标准答案之间的重合度计算出来的

基于SQuAD的排名比拼，也是考察EM和F1两项成绩。

过去一年，大部分时间都是科大讯飞团队和微软不同团队的竞争。7月微软登顶，8月科大讯飞首次折桂，9、10两月基本是微软天下，11月讯飞再次创出最佳成绩。

然后风云突变。先是腾讯突然杀入，并成功在12月底霸榜。然而“好景不长”，微软亚洲研究院和阿里巴巴iDST团队今年初先后发力，再次创出历史最好成绩，并且首次“超越人类”——他们在EM成绩上都击败了“人类表现”。

于是就开始有声音说：人类已经在阅读理解上被机器超越了。

但也马上遭遇反驳。
![](https://pic4.zhimg.com/v2-aefb927318fdeba55342dcdc4d513eb7_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1000' height='667'></svg>)
学界一方面有人指出这种说法过于夸大不严谨。


另一方面也有人将矛头指向SQuAD数据集局限性的问题。

以色列巴伊兰大学的著名NLP研究者Yoav Goldberg，他专门写了个PPT，列出了SQuAD1.1的三大不足：
- 受限于可以选择span来回答的问题；
- 需要在给定的段落里寻找答案；
- 段落里保证有答案。

无独有偶，DeepMind也专门发布了一篇名为[NarrativeQA](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzNjc1NzUzMw%3D%3D%26mid%3D2247492423%26idx%3D2%26sn%3Dc02f90e09a49c5c41d7aac3472573b94%26chksm%3De8d05435dfa7dd235aeb66aae46bc619f3cb6940d88b5d00b34204c3e38757f48480de2c6f6f%26scene%3D21%23wechat_redirect)的论文谈论了这些问题。

他们认为，由于SQuAD问题的答案必须是给定段落中的内容，这就导致很多评估阅读理解能力应该用到的合情合理的问题，根本没法问。

同时，这种简单的答案通过文档表面的信号就能提取出来，对于无法用文中短语来回答、或者需要用文中几个不连续短语来回答的问题，SQuAD训练出来的模型无法泛化。

另外，SQuAD虽然问题很多，但其实用到的文章又少又短，这就限制了整个数据集词汇和话题的多样性。

因此，SQuAD上表现不错的模型，如果要用到更复杂的问题上，可扩展性和适用性都很成问题。

DeepMind的论文说，包括SQuAD在内的很多阅读理解数据集都“**不能测试出阅读理解必要的综合方面**”。

所以此次SQuAD 2.0的更新，一定程度上也可视为对上述问题的回应。

## **最新排名：猿辅导领队中国军团**

当然，哪里有AI数据集竞赛，哪里就有不断刷新榜单的中国军团。

之前在SQuAD，中国代表团中的常客是科大讯飞、微软亚洲研究院，不过去年以来，阿里达摩院旗下的iDST和腾讯也成了大军中一员，甚至有几次还是头号玩家。

但是，SQuAD 1.1最新榜单里，成为中国军团领头羊的团队，可能会令你陌生——YUANFUDAO。

没错，就是那个主打在线教育的猿辅导。

当前猿辅导以EM83.520，F189.612的成绩，微微微落后于Google大脑&CMU团队，排名全球第二。
![](https://pic3.zhimg.com/v2-f63a2dad933f6750965f98a9a2f52946_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='1182'></svg>)
不过猿辅导虽然是SQuAD的新面孔，但在另一项知名机器阅读比赛MSMARCO中，早已霸气外露。

在3月27日的最新排名中，猿辅导位列MSMARCO全球第一。
![](https://pic1.zhimg.com/v2-81bc59fd1501ec2bad29a047100aafe8_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='521'></svg>)
而且成绩还超过了人类水准，当时猿辅导团队的两项测试得分为：49.72、48.02。而人类基准为47、46。


所以现在猿辅导出现在SQuAD 1.1全球玩家前列，实际也不算特别意外。

可顺路一提的是，中国军团在SQuAD 1.1最新榜单中实力确实超强，前十排名中，随处可见中国团队。
![](https://pic1.zhimg.com/v2-a9f8ed1c8f6b6f7e838862b02946cfc4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='1125'></svg>)
这才叫：厉害了，我的国。

## **SQuAD2.0论文传送门：**

[https://arxiv.org/abs/1806.03822](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1806.03822)




— **完** —
欢迎大家关注我们的专栏：[量子位 - 知乎专栏](https://zhuanlan.zhihu.com/qbitai)
诚挚招聘
量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。
[量子位 QbitAI](https://zhuanlan.zhihu.com/qbitai) · 头条号签约作者
վ'ᴗ' ի 追踪AI技术和产品新动态




