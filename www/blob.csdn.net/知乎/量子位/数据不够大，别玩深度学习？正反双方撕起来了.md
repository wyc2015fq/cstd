# 数据不够大，别玩深度学习？正反双方撕起来了 - 知乎
# 



李林 问耕 发自 凹非寺

量子位 报道 | 公众号 QbitAI

争论，随时可能爆发。

比方当你看到一篇名为《数据不够大，别玩深度学习》（Don’t use deep learning your data isn’t that big）的博客时。

作者Jeff Leek在这篇博客中指出，深度学习已经进入一定程度的狂热状态，人们正试图用这个技术解决每一个问题。但真正的挑战在于：“只有极少数情况下有足够的数据进行深度学习”，不是每家都有科技巨头的数据。

深度学习与更简单的模型相比，优点在于有足够的数据来调整大量的参数，博主建议当数据集不是那么大的时候，应该采用一些更简单、更可解释的办法，而且不用担心过拟合等问题。

为了证明自己的论点正确，Leek还举了一个基于MNIST进行图像识别的例子，分辨0或者1。他用的两个方法一个是逻辑回归，一个是深度神经网络（5层，每层160个节点，Tanh过滤器，20个epoch）。

Leek把训练集大小分成10到80，每次增加5。重复5次以平均掉一些噪音。

想要看具体案例的，请移步这里：[Don't use deep learning your data isn't that big](https://link.zhihu.com/?target=https%3A//simplystatistics.org/2017/05/31/deeplearning-vs-leekasso/)
![](https://pic1.zhimg.com/v2-974abb3d71f6de9222bd36525b831ac4_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='457'></svg>)
总之结论是：对于低训练样本来说，更简单的方法略胜于更复杂的方法。随着样本量的增加，复杂的方法的精度逐渐上升。

博主想用这个简单的例子说明：

样本量很重要。对于Google、亚马逊或者Facebook来说，有着接近无限的数据量，这种情况下应用深度学习是有意义的。但是如果只是中等规模的样本量，深度学习可能不会提升准确度，而且还失去了可解释性。

传送门

Jeff Leak的文章：
[Don't use deep learning your data isn't that big](https://link.zhihu.com/?target=https%3A//simplystatistics.org/2017/05/31/deeplearning-vs-leekasso/)
很快，就有用户在Reddit上吐槽。这些人认同文章的标题，但不认同作者的论据。他指出原文作者使用的方法，甚至称不上是真正的深度学习：这个领域过去十年的进展一个都没用上，而这些技术对小数据量来说非常重要。

例如，dropout、数据增强等等。如果不使用数据增强，5层×160神经元的网络绝对是太大了，大小为80的训练样本根本不起作用。另外ReLU可能比Tanh更好。

还有其他各种吐槽，量子位就不一一搬运了。你以为这就完了？拿衣服。

Leek这篇文章火了不到一周，哈佛大学药学院的生物医药信息学专业博士后Andrew Beam写了篇文章来反驳：《就算数据不够大，也能玩深度学习》。

Beam开篇就说，之前Jeff Leek那篇文章看得他有点不爽。核心论点他大致同意，他也不觉得深度学习是万能良药。但是，你确定深度学习在数据量不足100的时候效果不好是因为过拟合？怎么看都是因为模型没调好啊……

于是，他带着暑期实习生写了个有12万参数的MLP(多层感知器)和20万参数的卷积模型，任务还是用MNIST数据集，区分0和1。然后，他们又重新构建了Leek文章用的简单回归方法Leekasso和深度学习MLP，做了个比较。
![](https://pic1.zhimg.com/v2-5f1b74d8fe99bea85eed1c547dde8480_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='480'></svg>)
△ 红线：Beam文的CNN；紫线：Beam文的MLP；绿线：Leekasso；蓝线：Leek文的MLP。横轴是样本数，纵轴是准确率，下图去掉了Leek文的MLP是为了更清楚地比较前三种

这一顿折腾，都是为了说明一个问题：你看我们写的MLP和CNN，表现都挺好的嘛，所以不是深度学习不行，是你不行嘛。

不过讨论来讨论去，这个“区分0和1”的任务可能根本不足以说明问题。StackOverflow的数据科学家David Robinson后来也掺和进来，发了条Twitter说：这是个靠识别图像中心/边缘像素颜色就能解决的问题。
![](https://pic3.zhimg.com/v2-540bf504d9f3f7eabb0bbd2af5041082_b.jpg)![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='740'></svg>)
除了表达“不服气”和“我更牛”之外，Beam的文章也很正经地谈了谈到底什么时候该用深度学习：当你没有大量数据的时候，用简单的模型通常是个更好的选择。

不过，就算你是个普通人，手里有100-1000个样本，也可以试试深度学习。在这种情况下，深度学习领域有很多缩减差异、防止过拟合的方法可供尝试，包括dropout与随机梯度下降相结合，或者将目标问题的特定参数融合到模型中去。比如说在建立CNN做图像处理的时候，我们其实会把图像的属性编码到模型中。

实在不行，还有迁移学习嘛。

传送门

Andrew Beam的文章：
[You can probably use deep learning even if your data isn't that big](https://link.zhihu.com/?target=http%3A//beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html)
Andrew Beam的代码：
[beamandrew/deep_learning_works](https://link.zhihu.com/?target=https%3A//github.com/beamandrew/deep_learning_works)
这篇文章一出现，立刻有人叫好。

然而俗话说喷人者，可能会被喷。

Beam这篇洋洋洒洒的雄文，被Hacker News用户rkaplan“嘲笑”说：“这个帖子甚至没提到没有大量数据而用深度学习的最简单方法：下载一个预先训练的模型，并用你的小数据微调最后几层神经网络”。（其实也提了一下）

例如在图像分类等领域，微调的效果非常棒，因为预训练的模型的前几层早已经学会了很多通用特征，适用于很多不同的数据集。甚至最好的皮肤癌分类器，也在ImageNet上进行了预训练。

量子位插一句，关于数据量不够大时怎么办，以及在ImageNet上预训练的方法，HTC负责研发及医疗的总裁、原Google中国工程院副院长张智威(Edward Y. Chang)博士，曾经在清华的一次分享中讲到过。

这里是张智威分享的传送门：《[研发医疗领域的AlphaGo](https://link.zhihu.com/?target=http%3A//m.toutiao.com/i6390142729560523265/%3Fgroup_id%3D6390140688105718018%26group_flags%3D0)》

没有大数据，也没有预训练，行不行？也有人说行。Beam的反驳文章提了两种方法，Hacker News用户autokad也举了个例子，说他有一个最常用的5000个英文单词数据集，5000也是一个挺小的数据。

在这个数据集上，他用RBF核函数的SVM获得了87-88%的精度；用直方图核函数加一点特征工程可以做到89.7%的精度。而用调整过一些参数的TensorFlow，也能达到大约89.7%的精度。

有人举例说明，有人现身说法。讨论甚至从数据大小，一直延伸到各种深度学习方法的比较，乃至应用领域等。

有网友留言说，不管这个讨论结果如何，都让关注深度学习的人有所收获。

不知道大家怎么看这个话题？

欢迎留言继续讨论。

想深度围观的朋友，在量子位微信公众号（QbitAI）对话界面回复：“越洋”两个字，我们再分享几个越洋讨论的传送门。

【完】


