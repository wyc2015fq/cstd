# 神经网络之作诗姬 v0.2 - 知乎
# 

最近这个月我都在研究CV中的超分辨重建

但是从上个礼拜开始, 卡死在2017年上半年了, 烦的不行

前面的都是纯CNN顶多加点残差, 接下去的就复杂了, 要不超深, 要不就疯狂加BN也不知怎么想的, 做都做不动....

随便撸了一个作诗姬换换口味.
[https://github.com/Moe-Net/Waifu-X​github.com](https://link.zhihu.com/?target=https%3A//github.com/Moe-Net/Waifu-X)
上面这个会自动下模型, 这里还有个离线版的不用额外配置: [releases/v1.0.0/Tang.zip](https://link.zhihu.com/?target=https%3A//github.com/Moe-Net/Waifu-SY/releases/download/v1.0.0/WaifuTang.zip)
![](https://pic3.zhimg.com/v2-64ece3ce9672bf34e5cfcb3ca72ddaca_b.jpg)
勉强学会仄起平收, 上下句似乎有点联系....

韵律乱七八糟, 整体不知所云...

## index 编码器

古诗数据用的这里的: [hujiaweibujidao/poetry](https://link.zhihu.com/?target=https%3A//github.com/hujiaweibujidao/poetry)

还有个全唐诗, 用的人挺多的, 但我用了下脏的不行, 数据洗了两个小时也没洗干净...

都是 json, 直接 Import[file,"RawJSON"] 遍历一下就行了.

```
all=Characters@StringJoin[.....]
encoder = NetEncoder[{"Characters", chars = Append[all, _]}]
```

![](https://pic3.zhimg.com/v2-391e4fc5478a89f350b53339fa71ecca_b.jpg)
编码器的作用是把字符串映射到 index

**注意这个下划线 _ 很重要, 它表示任意字符, 如果不加这个, 当遇到编码器没有的字符的时候会疯狂报错.**

编码器其实不是必须的东西, 编码器的作用是 IO 优化.

比较适合从硬盘上读数据, 这里其实用不着, 因为我是从内存读的数据.

## 前向网络

然后构建网络:

```
predictNet = NetChain[{
	UnitVectorLayer[Length@chars],
	GatedRecurrentLayer[],
	GatedRecurrentLayer[],
	NetMapOperator[LinearLayer[Length@chars]],
	SoftmaxLayer[]
}];
```

![](https://pic2.zhimg.com/v2-26ffabbfd9da467c8b96afad99518eed_b.jpg)
古诗也不用什么词向量降维了, 直接映射到单位向量就行.

然后接两层门控循环单元(GRU, **G**ated **R**ecurrent **U**nit)

GRU 比较复杂, 作用大概是依赖之前的序列信息, 学习当前时刻的信息, 然后忘记一部分没用的信息.

然后接一个降了一个Tensor维的线性层, 最后预测接下来的信息, 用 Softmax 激活

Softmax 输出的是各个 index 的概率.

## 后向网络

然后我们把预测网络嵌入到一个后向网络里去:

```
teacherForcingNet = NetGraph[<|
	"predict" -> predictNet,
	"rest" -> SequenceRestLayer[],
	"most" -> SequenceMostLayer[],
	"loss" -> CrossEntropyLossLayer["Index"]
|>, {
	NetPort["Input"] -> "most" -> "predict" -> NetPort["loss", "Input"],
	NetPort["Input"] -> "rest" -> NetPort["loss", "Target"]
},
	"Input" -> {Length@First[trainingData], "Integer"}
] // NetInitialize
```

![](https://pic1.zhimg.com/v2-19201d38e1ffb61248facc32378f3940_b.jpg)
我这里组了个 Teacher Forcing, 可以有效地加速训练

最后就是等...

调调超参数啥的, 终止条件设成 16 轮或半个小时.

```
result = NetTrain[teacherForcingNet,
	<|"Input" -> trainingData|>, All,
	MaxTrainingRounds -> , TimeGoal -> ,
	TrainingProgressCheckpointing -> {
		"Directory", "CheckPoints",
		"Interval" -> Quantity[, "Batches"]
	},
	BatchSize -> , TargetDevice -> "GPU",
	ValidationSet -> Scaled[0.01]
]
Export["Waifu-Tang-7.WMLF", generateNet]
```

因为再训练下去也没变化了....我咋知道的呢...因为我试过啊亲....
![](https://pic3.zhimg.com/v2-bd33af8b3c8b6b2b6526e532d4ad415e_b.jpg)
最终训练结果....error 是来搞笑的, forget that, 交叉熵太高了...

虽然交叉熵也不是啥合适的度量, 但还是有一定的参考意义的

主要还是中文太多了, 还是得做嵌入降维, 降到 300 才能玩的样子...

最后把预测网络从训练网络中剥出来.

```
teacherForcingNet = Import["result7.WXF"]["TrainedNet"];
generateNet = NetJoin[
	NetTake[NetExtract[teacherForcingNet, "predict"], ],
	{
		SequenceLastLayer[],
		NetExtract[ NetExtract[teacherForcingNet, "predict"], {, "Net"}],
		SoftmaxLayer[]
	},
	"Input" -> NetEncoder[{"Characters", chars}],
	"Output" -> NetDecoder[{"Class", chars}]
]
Export["Waifu-Tang-7.WMLF", generateNet]
one[char_] := Block[
	{choose, next},
	choose = RandomChoice[Sqrt[Values@#] -> Keys@#]&;
	next[str_] := choose[Rest@NetStateObject[generateNet][str, {"TopProbabilities",  + }]];
	Nest[StringJoin[#, next[#]]&, Nest[StringJoin[#, next[#]]&, char, ] <> "，", ] <> "。"
]
Text@*one /@ RandomSample[chars, ] // TableForm
```

![](https://pic3.zhimg.com/v2-02c6ef55f2e316d48779bef8547d3986_b.jpg)
训练结果可以从 [https://github.com/Moe-Net/Waifu-SY/releases](https://link.zhihu.com/?target=https%3A//github.com/Moe-Net/Waifu-SY/releases) 下载

比如这里 Import 的这个就是 [https://github.com/Moe-Net/Waifu-SY/releases/download/v1.1.0/result7.WXF](https://link.zhihu.com/?target=https%3A//github.com/Moe-Net/Waifu-SY/releases/download/v1.1.0/result7.WXF)

最后, 再宣传一波 QQ群1014125, 大多数时间是个水群就是了...

