# 机器学习中的矩阵求导技术 - 知乎
# 

在机器学习中常常需要处理矩阵函数的求导，这类函数通常具有如下形式：

![f(\textbf{X}) \in \mathbb{R} \quad 其中 \textbf{X}\in\mathbb{R}^{n}或 \textbf{X}\in\mathbb{R}^{n\times m}](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7BX%7D%29+%5Cin+%5Cmathbb%7BR%7D+%5Cquad+%E5%85%B6%E4%B8%AD+%5Ctextbf%7BX%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bn%7D%E6%88%96+%5Ctextbf%7BX%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes+m%7D)

即函数的输入为一个向量或矩阵，输出为一个标量。事实上矩阵函数的输出可以是一个向量或一个矩阵，但在机器学习中很少遇到，所以在本文中只处理输出为标量的函数的求导问题。（在神经网络中，函数的输出是一个向量，所以在那里推导逆向传播所需的梯度时，就需要一些其他的记号和技巧，这里暂时不管）

关于对这种函数进行求导，一种直接的方法是展开函数中的矩阵和向量，然后逐元素求导，但是这样会涉及较复杂的下标表示，以及需要较高的变形技巧。在张贤达老师的《矩阵分析与应用（第二版）》中介绍了一种较为简单，不需要太多技巧的推导方式，我认为十分实用，所以在这篇文章中我将简单介绍这种方法，并举几个例子。


首先，设 ![\textbf{X} \in \mathbb{R}^{m\times n}](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes+n%7D) ，那么，对 ![\textbf{X}](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D) 的微分就是对 ![\textbf{X}](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D) 中每个元素微分，即：

![d\textbf{X} = d[\textbf{X}_{ij}]^{m,n}_{i=1,j=1}](https://www.zhihu.com/equation?tex=d%5Ctextbf%7BX%7D+%3D+d%5B%5Ctextbf%7BX%7D_%7Bij%7D%5D%5E%7Bm%2Cn%7D_%7Bi%3D1%2Cj%3D1%7D)

由此结合一些有关矩阵的操作，就可以得到： 
- ![d(trX)=tr(dX)](https://www.zhihu.com/equation?tex=d%28trX%29%3Dtr%28dX%29)
- ![d(UV)=(dU)V+UdV](https://www.zhihu.com/equation?tex=d%28UV%29%3D%28dU%29V%2BUdV)
- ![d(X^T)=(dX)^T](https://www.zhihu.com/equation?tex=d%28X%5ET%29%3D%28dX%29%5ET)
- ![d(\alpha X)=\alpha dX](https://www.zhihu.com/equation?tex=d%28%5Calpha+X%29%3D%5Calpha+dX)

这些都不难证，比如第一个等式：

![d(trX)=d(\sum_{i=1}^{n} X_{ii})=\sum_{i=1}^{n}dX_{ii}=tr(dX)](https://www.zhihu.com/equation?tex=d%28trX%29%3Dd%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+X_%7Bii%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DdX_%7Bii%7D%3Dtr%28dX%29)

此外还有三个重要但需要较复杂推导的结论：
- ![d|X|=|X|tr(X^{-1}dX)](https://www.zhihu.com/equation?tex=d%7CX%7C%3D%7CX%7Ctr%28X%5E%7B-1%7DdX%29)
- ![dlogX=X^{-1}dX](https://www.zhihu.com/equation?tex=dlogX%3DX%5E%7B-1%7DdX)
- ![d(X^{-1})=-X^{-1}(dX)X^{-1}](https://www.zhihu.com/equation?tex=d%28X%5E%7B-1%7D%29%3D-X%5E%7B-1%7D%28dX%29X%5E%7B-1%7D)


还有个关键是矩阵的trace运算，这种运算具有一些非常好的性质，这些性质让我们可以对式子进行自由地变换：
- ![tr(A+B)=tr(A)+tr(B)](https://www.zhihu.com/equation?tex=tr%28A%2BB%29%3Dtr%28A%29%2Btr%28B%29)
- ![tr(cA)=ctr(A)](https://www.zhihu.com/equation?tex=tr%28cA%29%3Dctr%28A%29)
- ![tr(AB)=tr(BA)](https://www.zhihu.com/equation?tex=tr%28AB%29%3Dtr%28BA%29)
- ![tr(A^T)=tr(A)](https://www.zhihu.com/equation?tex=tr%28A%5ET%29%3Dtr%28A%29)

下面就可以来推导矩阵函数的微分了。


**对标量函数 ![f(\textbf{x})\quad \textbf{x}=[x_1,\cdots,x_m]^{T}\in\mathbb{R}^{m}](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%29%5Cquad+%5Ctextbf%7Bx%7D%3D%5Bx_1%2C%5Ccdots%2Cx_m%5D%5E%7BT%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bm%7D) 一阶微分**

![df(\textbf{x})=\frac{\partial f(\textbf{x})}{\partial x_1}dx_1 + \cdots + \frac{\partial f(\textbf{x})}{\partial x_m}dx_m = [\frac{\partial f(\textbf{x})}{\partial x_1},\cdots,\frac{\partial f(\textbf{x})}{\partial x_m}][dx_1,\cdots,dx_m]^{T}=\frac{df(\textbf{x})}{d\textbf{x}^{T}}d\textbf{x}](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7Bx%7D%29%3D%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial+x_1%7Ddx_1+%2B+%5Ccdots+%2B+%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial+x_m%7Ddx_m+%3D+%5B%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial+x_1%7D%2C%5Ccdots%2C%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial+x_m%7D%5D%5Bdx_1%2C%5Ccdots%2Cdx_m%5D%5E%7BT%7D%3D%5Cfrac%7Bdf%28%5Ctextbf%7Bx%7D%29%7D%7Bd%5Ctextbf%7Bx%7D%5E%7BT%7D%7Dd%5Ctextbf%7Bx%7D)

由于最右边的式子是两个向量的内积，是个标量，所以有：

![\frac{df(\textbf{x})}{d\textbf{x}^{T}}d\textbf{x} = tr(\frac{df(\textbf{x})}{d\textbf{x}^{T}}d\textbf{x})](https://www.zhihu.com/equation?tex=%5Cfrac%7Bdf%28%5Ctextbf%7Bx%7D%29%7D%7Bd%5Ctextbf%7Bx%7D%5E%7BT%7D%7Dd%5Ctextbf%7Bx%7D+%3D+tr%28%5Cfrac%7Bdf%28%5Ctextbf%7Bx%7D%29%7D%7Bd%5Ctextbf%7Bx%7D%5E%7BT%7D%7Dd%5Ctextbf%7Bx%7D%29)

而实值标量函数 ![f(\textbf{x})](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%29) 的梯度向量 ![\nabla_{\textbf{x}}f(\textbf{x})](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctextbf%7Bx%7D%7Df%28%5Ctextbf%7Bx%7D%29)为 ![m\times 1](https://www.zhihu.com/equation?tex=m%5Ctimes+1) 列向量，其定义为：

![\nabla_{\textbf{x}}f(\textbf{x})=[\frac{\partial f(\textbf{x})}{\partial x_1},\cdots,\frac{\partial f(\textbf{x})}{\partial x_m}]^{T}=\frac{\partial f(\textbf{x})}{\partial\textbf{x}}](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctextbf%7Bx%7D%7Df%28%5Ctextbf%7Bx%7D%29%3D%5B%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial+x_1%7D%2C%5Ccdots%2C%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial+x_m%7D%5D%5E%7BT%7D%3D%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7Bx%7D%29%7D%7B%5Cpartial%5Ctextbf%7Bx%7D%7D)

注意！ ![\frac{df(\textbf{x})}{d\textbf{x}^{T}}](https://www.zhihu.com/equation?tex=%5Cfrac%7Bdf%28%5Ctextbf%7Bx%7D%29%7D%7Bd%5Ctextbf%7Bx%7D%5E%7BT%7D%7D) 是Jacobian矩阵，其转置正是梯度！

所以总结一下，遇到函数 ![f(\textbf{x})](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%29) 时，直接对其进行微分，然后通过变换，将 ![df(\textbf{x})](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7Bx%7D%29) 变成 ![tr(Adx)](https://www.zhihu.com/equation?tex=tr%28Adx%29) ，则 ![f(\textbf{x})](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%29) 的梯度则为 ![A^T](https://www.zhihu.com/equation?tex=A%5ET) 。

下面用一个具体的例子来说明：

***Example***： ![f(\textbf{x})=\textbf{x}^TA\textbf{x}](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%29%3D%5Ctextbf%7Bx%7D%5ETA%5Ctextbf%7Bx%7D)

![df(\textbf{x})=d(\textbf{x}^TA\textbf{x})=d(tr(\textbf{x}^TA\textbf{x}))=tr[(d\textbf{x})^TA\textbf{x}+\textbf{x}^TAd\textbf{x}]](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7Bx%7D%29%3Dd%28%5Ctextbf%7Bx%7D%5ETA%5Ctextbf%7Bx%7D%29%3Dd%28tr%28%5Ctextbf%7Bx%7D%5ETA%5Ctextbf%7Bx%7D%29%29%3Dtr%5B%28d%5Ctextbf%7Bx%7D%29%5ETA%5Ctextbf%7Bx%7D%2B%5Ctextbf%7Bx%7D%5ETAd%5Ctextbf%7Bx%7D%5D)

注意到有 ![tr(A)=tr(A^T)](https://www.zhihu.com/equation?tex=tr%28A%29%3Dtr%28A%5ET%29) ，所以上式可变为：

![tr[(d\textbf{x})^TA\textbf{x}+\textbf{x}^TAd\textbf{x}]=tr[((d\textbf{x})^TA\textbf{x})^T+\textbf{x}^TAd\textbf{x}]=tr[\textbf{x}^TA^Td\textbf{x}+\textbf{x}^TAd\textbf{x}]=tr[\textbf{x}^T(A^T+A)d\textbf{x}]](https://www.zhihu.com/equation?tex=tr%5B%28d%5Ctextbf%7Bx%7D%29%5ETA%5Ctextbf%7Bx%7D%2B%5Ctextbf%7Bx%7D%5ETAd%5Ctextbf%7Bx%7D%5D%3Dtr%5B%28%28d%5Ctextbf%7Bx%7D%29%5ETA%5Ctextbf%7Bx%7D%29%5ET%2B%5Ctextbf%7Bx%7D%5ETAd%5Ctextbf%7Bx%7D%5D%3Dtr%5B%5Ctextbf%7Bx%7D%5ETA%5ETd%5Ctextbf%7Bx%7D%2B%5Ctextbf%7Bx%7D%5ETAd%5Ctextbf%7Bx%7D%5D%3Dtr%5B%5Ctextbf%7Bx%7D%5ET%28A%5ET%2BA%29d%5Ctextbf%7Bx%7D%5D)

所以原函数的梯度为：

![\nabla f(\textbf{x})=[\textbf{x}^T(A^T+A)]^T=(A+A^T)\textbf{x}](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Ctextbf%7Bx%7D%29%3D%5B%5Ctextbf%7Bx%7D%5ET%28A%5ET%2BA%29%5D%5ET%3D%28A%2BA%5ET%29%5Ctextbf%7Bx%7D)

下面这个例子是最小二乘估计，在这个问题中，我们实际上要解决如下的一个优化问题：

![\min_{\beta}. \lVert Y-X\beta\rVert^2](https://www.zhihu.com/equation?tex=%5Cmin_%7B%5Cbeta%7D.+%5ClVert+Y-X%5Cbeta%5CrVert%5E2)

可以对函数 ![f(\beta)=(Y-X\beta)^T(Y-X\beta)](https://www.zhihu.com/equation?tex=f%28%5Cbeta%29%3D%28Y-X%5Cbeta%29%5ET%28Y-X%5Cbeta%29) 直接求导来获得 ![\hat{\beta}](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbeta%7D) :

![\begin{align*}  df&= dtr((Y-X\beta)^T(Y-X\beta)) \\  &= tr[d((Y-X\beta)^T(Y-X\beta))] \\ &=tr[d(Y-X\beta)^T\cdot (Y-X\beta) + (Y-X\beta)^Td(Y-X\beta)]\\ &=tr[d(-X\beta)^T\cdot (Y-X\beta) + (Y-X\beta)^Td(Y-X\beta)] \\ &=tr[(d\beta)^T(-X)^T\cdot (Y-X\beta) + (Y-X\beta)^T(-X)d\beta] \\ &=tr[(Y-X\beta)^T(-X)d\beta + (Y-X\beta)^T(-X)d\beta] \\ &=tr[(Y-X\beta)^T(-2X)d\beta] \\ \end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D++df%26%3D+dtr%28%28Y-X%5Cbeta%29%5ET%28Y-X%5Cbeta%29%29+%5C%5C++%26%3D+tr%5Bd%28%28Y-X%5Cbeta%29%5ET%28Y-X%5Cbeta%29%29%5D+%5C%5C+%26%3Dtr%5Bd%28Y-X%5Cbeta%29%5ET%5Ccdot+%28Y-X%5Cbeta%29+%2B+%28Y-X%5Cbeta%29%5ETd%28Y-X%5Cbeta%29%5D%5C%5C+%26%3Dtr%5Bd%28-X%5Cbeta%29%5ET%5Ccdot+%28Y-X%5Cbeta%29+%2B+%28Y-X%5Cbeta%29%5ETd%28Y-X%5Cbeta%29%5D+%5C%5C+%26%3Dtr%5B%28d%5Cbeta%29%5ET%28-X%29%5ET%5Ccdot+%28Y-X%5Cbeta%29+%2B+%28Y-X%5Cbeta%29%5ET%28-X%29d%5Cbeta%5D+%5C%5C+%26%3Dtr%5B%28Y-X%5Cbeta%29%5ET%28-X%29d%5Cbeta+%2B+%28Y-X%5Cbeta%29%5ET%28-X%29d%5Cbeta%5D+%5C%5C+%26%3Dtr%5B%28Y-X%5Cbeta%29%5ET%28-2X%29d%5Cbeta%5D+%5C%5C+%5Cend%7Balign%2A%7D)

故梯度为 ![[(Y-X\beta)^T(-2X)]^T=-2[X^T(Y-X\beta)]](https://www.zhihu.com/equation?tex=%5B%28Y-X%5Cbeta%29%5ET%28-2X%29%5D%5ET%3D-2%5BX%5ET%28Y-X%5Cbeta%29%5D)

令梯度为0，则可得到 ![\beta=(X^TX)^{-1}X^TY](https://www.zhihu.com/equation?tex=%5Cbeta%3D%28X%5ETX%29%5E%7B-1%7DX%5ETY)

（这里 ![(X^TX)^{-1}X^T](https://www.zhihu.com/equation?tex=%28X%5ETX%29%5E%7B-1%7DX%5ET) 为 ![X](https://www.zhihu.com/equation?tex=X) 的伪逆，或称Moore-Penrose inverse）


**对标量函数 ![f(\textbf{X})\quad X\in\mathbb{R}^{m\times n}](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7BX%7D%29%5Cquad+X%5Cin%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes+n%7D) 一阶微分**

可以将 ![\textbf{X}](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D) 写成这种形式： ![[\textbf{x}_1,\textbf{x}_2,\cdots,\textbf{x}_n]](https://www.zhihu.com/equation?tex=%5B%5Ctextbf%7Bx%7D_1%2C%5Ctextbf%7Bx%7D_2%2C%5Ccdots%2C%5Ctextbf%7Bx%7D_n%5D) ，于是，对 ![f(\textbf{X})](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7BX%7D%29) 的微分为：

![df(\textbf{X})=\frac{\partial f(\textbf{X})}{\partial \textbf{x}_1}d\textbf{x}_1+\cdots+\frac{\partial f(\textbf{X})}{\partial \textbf{x}_n}d\textbf{x}_n](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7BX%7D%29%3D%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_1%7Dd%5Ctextbf%7Bx%7D_1%2B%5Ccdots%2B%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_n%7Dd%5Ctextbf%7Bx%7D_n)

注意到有：

![\frac{\partial f(\textbf{X})}{\partial \textbf{x}_i}d\textbf{x}_i = [\frac{\partial f(\textbf{X})}{\partial \textbf{x}_{1i}},\cdots,\frac{\partial f(\textbf{X})}{\partial \textbf{x}_{mi}}][d\textbf{x}_{1i},\cdots,d\textbf{x}_{mi}]^T](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_i%7Dd%5Ctextbf%7Bx%7D_i+%3D+%5B%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_%7B1i%7D%7D%2C%5Ccdots%2C%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_%7Bmi%7D%7D%5D%5Bd%5Ctextbf%7Bx%7D_%7B1i%7D%2C%5Ccdots%2Cd%5Ctextbf%7Bx%7D_%7Bmi%7D%5D%5ET)

所以有：

![df(\textbf{X})=[\frac{\partial f(\textbf{X})}{\partial \textbf{x}_{11}},\cdots,\frac{\partial f(\textbf{X})}{\partial \textbf{x}_{m1}},\cdots,\frac{\partial f(\textbf{X})}{\partial \textbf{x}_{1n}},\cdots,\frac{\partial f(\textbf{X})}{\partial \textbf{x}_{mn}}][d\textbf{x}_{11},\cdots,d\textbf{x}_{m1},\cdots,d\textbf{x}_{1n},\cdots,d\textbf{x}_{mn}]^T](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7BX%7D%29%3D%5B%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_%7B11%7D%7D%2C%5Ccdots%2C%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_%7Bm1%7D%7D%2C%5Ccdots%2C%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_%7B1n%7D%7D%2C%5Ccdots%2C%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7Bx%7D_%7Bmn%7D%7D%5D%5Bd%5Ctextbf%7Bx%7D_%7B11%7D%2C%5Ccdots%2Cd%5Ctextbf%7Bx%7D_%7Bm1%7D%2C%5Ccdots%2Cd%5Ctextbf%7Bx%7D_%7B1n%7D%2C%5Ccdots%2Cd%5Ctextbf%7Bx%7D_%7Bmn%7D%5D%5ET)

现在引入矩阵列向量化操作符 ![vec()](https://www.zhihu.com/equation?tex=vec%28%29) ，它可以将输入矩阵按列拼成一个向量，比如：

![vec(\textbf{X}) = [\textbf{x}_{11},\cdots,\textbf{x}_{m1},\cdots,\textbf{x}_{1n},\cdots,\textbf{x}_{mn}]^T](https://www.zhihu.com/equation?tex=vec%28%5Ctextbf%7BX%7D%29+%3D+%5B%5Ctextbf%7Bx%7D_%7B11%7D%2C%5Ccdots%2C%5Ctextbf%7Bx%7D_%7Bm1%7D%2C%5Ccdots%2C%5Ctextbf%7Bx%7D_%7B1n%7D%2C%5Ccdots%2C%5Ctextbf%7Bx%7D_%7Bmn%7D%5D%5ET)

于是：

![df(\textbf{X})=\frac{\partial f(\textbf{X})}{\partial vec^T (\textbf{X})}d(vec(\textbf{X})) = (vec({D_{\textbf{X}}^T f(\textbf{X})}))^T d(vec(\textbf{X}))](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7BX%7D%29%3D%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+vec%5ET+%28%5Ctextbf%7BX%7D%29%7Dd%28vec%28%5Ctextbf%7BX%7D%29%29+%3D+%28vec%28%7BD_%7B%5Ctextbf%7BX%7D%7D%5ET+f%28%5Ctextbf%7BX%7D%29%7D%29%29%5ET+d%28vec%28%5Ctextbf%7BX%7D%29%29)

上式中 ![D_\textbf{X} f(\textbf{X})](https://www.zhihu.com/equation?tex=D_%5Ctextbf%7BX%7D+f%28%5Ctextbf%7BX%7D%29) 是标量函数 ![f(\textbf{X})](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7BX%7D%29) 的Jacobian矩阵，后面用 ![A](https://www.zhihu.com/equation?tex=A) 表示，有：

![A=D_{\textbf{X}}f(\textbf{X})=\frac{\partial f(\textbf{X})}{\partial \textbf{X}^T} = \left[  \begin{matrix}    \frac{\partial f(X)}{\partial x_{11}} & \cdots &  \frac{\partial f(X)}{\partial x_{m1}}\\    \vdots & \cdots & \vdots \\     \frac{\partial f(X)}{\partial x_{1n}} & \cdots & \frac{\partial f(X)}{\partial x_{mn}}    \end{matrix}   \right]](https://www.zhihu.com/equation?tex=A%3DD_%7B%5Ctextbf%7BX%7D%7Df%28%5Ctextbf%7BX%7D%29%3D%5Cfrac%7B%5Cpartial+f%28%5Ctextbf%7BX%7D%29%7D%7B%5Cpartial+%5Ctextbf%7BX%7D%5ET%7D+%3D+%5Cleft%5B++%5Cbegin%7Bmatrix%7D++++%5Cfrac%7B%5Cpartial+f%28X%29%7D%7B%5Cpartial+x_%7B11%7D%7D+%26+%5Ccdots+%26++%5Cfrac%7B%5Cpartial+f%28X%29%7D%7B%5Cpartial+x_%7Bm1%7D%7D%5C%5C++++%5Cvdots+%26+%5Ccdots+%26+%5Cvdots+%5C%5C+++++%5Cfrac%7B%5Cpartial+f%28X%29%7D%7B%5Cpartial+x_%7B1n%7D%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f%28X%29%7D%7B%5Cpartial+x_%7Bmn%7D%7D++++%5Cend%7Bmatrix%7D+++%5Cright%5D)

所以：

![df(\textbf{X})= (vec({D_{\textbf{X}}^T f(\textbf{X})}))^T d(vec(\textbf{X})) = (vec(A^T))^T d(vec(\textbf{X}))](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7BX%7D%29%3D+%28vec%28%7BD_%7B%5Ctextbf%7BX%7D%7D%5ET+f%28%5Ctextbf%7BX%7D%29%7D%29%29%5ET+d%28vec%28%5Ctextbf%7BX%7D%29%29+%3D+%28vec%28A%5ET%29%29%5ET+d%28vec%28%5Ctextbf%7BX%7D%29%29)

注意到上式是一个标量，所以外面可以套上 ![tr()](https://www.zhihu.com/equation?tex=tr%28%29) 。同时又有 ![tr(U^T V)=vec(U)^T vec(V)](https://www.zhihu.com/equation?tex=tr%28U%5ET+V%29%3Dvec%28U%29%5ET+vec%28V%29) ，所以最终有：

![df(\textbf{X}) = tr(A d\textbf{X})](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7BX%7D%29+%3D+tr%28A+d%5Ctextbf%7BX%7D%29)

最后总结一些，遇到函数 ![f(\textbf{X})](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7BX%7D%29) 想求其梯度时，直接对其做微分 ![df(\textbf{X})](https://www.zhihu.com/equation?tex=df%28%5Ctextbf%7BX%7D%29) ，再通过一些变换将其凑成 ![tr(A d\textbf{X})](https://www.zhihu.com/equation?tex=tr%28A+d%5Ctextbf%7BX%7D%29) 的形式，于是原函数的梯度则为 ![\nabla_{\textbf{X}}f(\textbf{X})=A^T](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctextbf%7BX%7D%7Df%28%5Ctextbf%7BX%7D%29%3DA%5ET)

下面用一个具体的例子来说明：

***Example***：求 ![f(X)=tr(AX^{-1})](https://www.zhihu.com/equation?tex=f%28X%29%3Dtr%28AX%5E%7B-1%7D%29) 的梯度矩阵

![dtr(AX^{-1})=tr[d(AX^{-1})]=tr[AdX^{-1}]](https://www.zhihu.com/equation?tex=dtr%28AX%5E%7B-1%7D%29%3Dtr%5Bd%28AX%5E%7B-1%7D%29%5D%3Dtr%5BAdX%5E%7B-1%7D%5D)

又因为 ![d(X^{-1})=-X^{-1}(dX)X^{-1}](https://www.zhihu.com/equation?tex=d%28X%5E%7B-1%7D%29%3D-X%5E%7B-1%7D%28dX%29X%5E%7B-1%7D) ，故上式等于变为： ![-tr[AX^{-1}(dX)X^{-1}]](https://www.zhihu.com/equation?tex=-tr%5BAX%5E%7B-1%7D%28dX%29X%5E%7B-1%7D%5D)

应用 ![tr(AB)=tr(BA)](https://www.zhihu.com/equation?tex=tr%28AB%29%3Dtr%28BA%29) ，将 ![AX^{-1}(dX)](https://www.zhihu.com/equation?tex=AX%5E%7B-1%7D%28dX%29) 与 ![X^{-1}](https://www.zhihu.com/equation?tex=X%5E%7B-1%7D) 调换位置，最终可得：

![dtr(AX^{-1})=-tr(X^{-1}AX^{-1}dX)](https://www.zhihu.com/equation?tex=dtr%28AX%5E%7B-1%7D%29%3D-tr%28X%5E%7B-1%7DAX%5E%7B-1%7DdX%29)

由辨识规则，原函数的梯度矩阵则为：

![\frac{\partial tr(AX^{-1})}{\partial X}=-(X^{-1}AX^{-1})^{T}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+tr%28AX%5E%7B-1%7D%29%7D%7B%5Cpartial+X%7D%3D-%28X%5E%7B-1%7DAX%5E%7B-1%7D%29%5E%7BT%7D)


**对标量函数 ![f(\textbf{x})\quad \textbf{x}=[x_1,\cdots,x_m]^{T}\in\mathbb{R}^{m}](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%29%5Cquad+%5Ctextbf%7Bx%7D%3D%5Bx_1%2C%5Ccdots%2Cx_m%5D%5E%7BT%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bm%7D) 二阶微分**

在使用Newton法求解凸优化问题时，需要求解目标标量函数的Hessian矩阵，这就需要对原函数进行二阶微分，这里的辨识规则的推导和一阶微分类似，只不过最后要将原函数的微分表示成二次型。这里直接给出结论：

![d^{2}f(\textbf{x})=(d\textbf{x})^TBd\textbf{x} \Leftrightarrow \textbf{H}[f(\textbf{x})]=\frac{1}{2}(B+B^T)](https://www.zhihu.com/equation?tex=d%5E%7B2%7Df%28%5Ctextbf%7Bx%7D%29%3D%28d%5Ctextbf%7Bx%7D%29%5ETBd%5Ctextbf%7Bx%7D+%5CLeftrightarrow+%5Ctextbf%7BH%7D%5Bf%28%5Ctextbf%7Bx%7D%29%5D%3D%5Cfrac%7B1%7D%7B2%7D%28B%2BB%5ET%29)

![d^{2}f(\textbf{X})=(dvec(\textbf{X}))^TBdvec(\textbf{X}) \Leftrightarrow \textbf{H}[f(\textbf{X})]=\frac{1}{2}(B+B^T)](https://www.zhihu.com/equation?tex=d%5E%7B2%7Df%28%5Ctextbf%7BX%7D%29%3D%28dvec%28%5Ctextbf%7BX%7D%29%29%5ETBdvec%28%5Ctextbf%7BX%7D%29+%5CLeftrightarrow+%5Ctextbf%7BH%7D%5Bf%28%5Ctextbf%7BX%7D%29%5D%3D%5Cfrac%7B1%7D%7B2%7D%28B%2BB%5ET%29)

最后的 ![\textbf{H}[f(\textbf{X})]=\frac{1}{2}(B+B^T)](https://www.zhihu.com/equation?tex=%5Ctextbf%7BH%7D%5Bf%28%5Ctextbf%7BX%7D%29%5D%3D%5Cfrac%7B1%7D%7B2%7D%28B%2BB%5ET%29) 是为了确保Hessian矩阵为实对称矩阵。


**参考资料**

张贤达. 矩阵分析与应用（第二版）[M]. 北京：清华大学出版社，2013

