# 斯坦福大学 凸优化 引言 - 知乎
# 

在第二遍读Boyd的《Convex Optimization》时觉得应该把一些重要的东西记下来。有人说学习效率最高的方式是教别人，所以我写下这一系列的文章，不光提醒自己，同时也给正在学习这方面知识的同学一点参考。


优化问题通常具有如下的形式：

![\begin{align*} &minimize\quad f_0(x) \\ &subject\ to\quad f_i(x)\leq b_i\quad i=1,\cdots,m \end{align*} ](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26minimize%5Cquad+f_0%28x%29+%5C%5C+%26subject%5C+to%5Cquad+f_i%28x%29%5Cleq+b_i%5Cquad+i%3D1%2C%5Ccdots%2Cm+%5Cend%7Balign%2A%7D+)

这里 ![x=(x_1,x_2,\cdots,x_n)](https://www.zhihu.com/equation?tex=x%3D%28x_1%2Cx_2%2C%5Ccdots%2Cx_n%29) 称为**优化变量**，函数 ![f_0:R^n\rightarrow R](https://www.zhihu.com/equation?tex=f_0%3AR%5En%5Crightarrow+R) 称为**目标函数**，函数 ![f_i:R^n\rightarrow R](https://www.zhihu.com/equation?tex=f_i%3AR%5En%5Crightarrow+R) ， ![i=1,\cdots,m](https://www.zhihu.com/equation?tex=i%3D1%2C%5Ccdots%2Cm) ，称为**约束函数**。优化问题在处理问题时的应用程度之广，超乎绝大数人的想象。

线性函数是对任意 ![x,y\in R^n](https://www.zhihu.com/equation?tex=x%2Cy%5Cin+R%5En) 和 ![\alpha,\beta\in R](https://www.zhihu.com/equation?tex=%5Calpha%2C%5Cbeta%5Cin+R) ，有：

![f_i(\alpha x+\beta y)=\alpha f_i(x)+\beta f_i(y)](https://www.zhihu.com/equation?tex=f_i%28%5Calpha+x%2B%5Cbeta+y%29%3D%5Calpha+f_i%28x%29%2B%5Cbeta+f_i%28y%29)

而凸函数是指是对任意 ![x,y\in R^n](https://www.zhihu.com/equation?tex=x%2Cy%5Cin+R%5En) 和 ![\alpha,\beta\in R](https://www.zhihu.com/equation?tex=%5Calpha%2C%5Cbeta%5Cin+R) 且 ![\alpha +\beta=1,\alpha\geq0,\beta\geq0](https://www.zhihu.com/equation?tex=%5Calpha+%2B%5Cbeta%3D1%2C%5Calpha%5Cgeq0%2C%5Cbeta%5Cgeq0) ，有：

![f_i(\alpha x+\beta y)\leq\alpha f_i(x)+\beta f_i(y)](https://www.zhihu.com/equation?tex=f_i%28%5Calpha+x%2B%5Cbeta+y%29%5Cleq%5Calpha+f_i%28x%29%2B%5Cbeta+f_i%28y%29)

可以看出凸性是较线性更一般的性质！

有两类广为人知且应用广泛的特殊凸优化问题——最小二乘问题和线性规划问题。高中时就以对此有过介绍。事实上，可以认为这两类问题的求解已经是成熟的技术，可以快速地求解一些规模较大的问题。

凸优化问题中，要求函数 ![f_0,\cdots,f_m:R^n\rightarrow R](https://www.zhihu.com/equation?tex=f_0%2C%5Ccdots%2Cf_m%3AR%5En%5Crightarrow+R) 都是凸函数。注意这里并没有要求该优化问题的可行域是凸集，这是一个附带的结论。

凸优化问题是可以被有效解决的。在实际应用中可以使用内点法求解，可以证明，内点法可以在多项式时间内以给定精度求解这些凸优化问题（内点法将原凸优化问题转化为一系列Newton法可解决的问题，而Newton法的每一步相当于求解一系列的线性方程组）。可以认为，如果一个实际问题能够被表述为凸优化问题，那么事实上就已经解决了这个问题，因此，问题的关键就在于判别问题是否为凸优化问题以及如何表述问题。

非线性优化（Nonlinear optimization）一般指非凸优化，目前还没有有效的技术来解此类问题。在非线性优化中，凸优化技术仍然大有可为。比如在局部优化（local optimization）中利用凸优化寻找初始值，用Lagrange对偶法寻找全局优化的界。非凸优化问题也可以使用基于凸优化的启发式算法来寻找局部最优，比如SCP（Sequential Convex Programming），加入有如下问题：

![\begin{align*} &\min.\ f_0(x) \\ & s.t. \ f(x)=0 \\ &\quad \quad Ax\leq 0 \end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%5Cmin.%5C+f_0%28x%29+%5C%5C+%26+s.t.+%5C+f%28x%29%3D0+%5C%5C+%26%5Cquad+%5Cquad+Ax%5Cleq+0+%5Cend%7Balign%2A%7D)

其中 ![f(x)](https://www.zhihu.com/equation?tex=f%28x%29) 是非凸的。那么该问题可以转化为一系列具有如下形式的凸优化问题：

![\begin{align*} &\min.\ f_0(x) \\ & s.t. \ f(x^{(0)}) + Df(x^{(0)})(x-x^{(0)})=0 \\ &\quad \quad Ax\leq 0 \\ &\quad \quad \lVert x-x^{(0)} \rVert\leq \epsilon \end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%5Cmin.%5C+f_0%28x%29+%5C%5C+%26+s.t.+%5C+f%28x%5E%7B%280%29%7D%29+%2B+Df%28x%5E%7B%280%29%7D%29%28x-x%5E%7B%280%29%7D%29%3D0+%5C%5C+%26%5Cquad+%5Cquad+Ax%5Cleq+0+%5C%5C+%26%5Cquad+%5Cquad+%5ClVert+x-x%5E%7B%280%29%7D+%5CrVert%5Cleq+%5Cepsilon+%5Cend%7Balign%2A%7D)

