# 统计机器学习-1-函数空间 - 知乎
# 

本文来自我学习CMU的Statistical Machine Learning时的笔记整理。本系列大概每周一更（争取）。本文是第一篇，关于函数空间。


**动机--为什么要定义函数空间（Function Space）**

所谓函数空间，就是指一群具有特定结构的函数的集合。设想现在我们要处理一个非参数回归问题（nonparametric regression），我们观察到了一组数据点 ![(X_1,Y_1),\cdots,(X_n,Y_n)](https://www.zhihu.com/equation?tex=%28X_1%2CY_1%29%2C%5Ccdots%2C%28X_n%2CY_n%29) ，我们想要估计出一个最好的回归函数 ![m(x)](https://www.zhihu.com/equation?tex=m%28x%29) ，但我们不能无脑地最小化训练误差 ![\sum_{i}(Y_i-m(X_i))^2](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D%28Y_i-m%28X_i%29%29%5E2) ，因为这样的结果是![m(x)](https://www.zhihu.com/equation?tex=m%28x%29) 相对于对原来各离散的数据点进行插值后的结果，这样很可能会使泛化性能变差，或者说模型过拟合。一种合理的做法是把 ![m(x)](https://www.zhihu.com/equation?tex=m%28x%29) 限制在一个well-behaved的函数空间中，这样 ![m(x)](https://www.zhihu.com/equation?tex=m%28x%29) 便具有了所在函数空间的一些好的性质，比如平滑性。


![Hilbert\ Space](https://www.zhihu.com/equation?tex=Hilbert%5C+Space)

首先令 ![V](https://www.zhihu.com/equation?tex=V) 是一个向量空间（vector space），首先我们可以定义一个范数（norm），它是一个映射 ![\lVert \cdot\rVert:V \rightarrow[0,\infty)](https://www.zhihu.com/equation?tex=%5ClVert+%5Ccdot%5CrVert%3AV+%5Crightarrow%5B0%2C%5Cinfty%29) ，并且满足如下3个性质：
- ![\lVert x+y \rVert \leq \lVert x \rVert + \lVert y \rVert](https://www.zhihu.com/equation?tex=%5ClVert+x%2By+%5CrVert+%5Cleq+%5ClVert+x+%5CrVert+%2B+%5ClVert+y+%5CrVert)
- ![\lVert ax \rVert = a\lVert x \rVert\ for\ all\ a\in\mathbb{R} ](https://www.zhihu.com/equation?tex=%5ClVert+ax+%5CrVert+%3D+a%5ClVert+x+%5CrVert%5C+for%5C+all%5C+a%5Cin%5Cmathbb%7BR%7D+)
- ![\lVert x \rVert=0\Rightarrow x=0](https://www.zhihu.com/equation?tex=%5ClVert+x+%5CrVert%3D0%5CRightarrow+x%3D0)

称一个定义了范数的空间为赋范空间（normed space）。

有了范数之后便可以定义 ![V](https://www.zhihu.com/equation?tex=V) 中的Cauchy序列。一个序列 ![x_1,x_2,\cdots](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C%5Ccdots) 在某赋范空间（假设范数为 ![\lVert \cdot \rVert_{X}](https://www.zhihu.com/equation?tex=%5ClVert+%5Ccdot+%5CrVert_%7BX%7D) ）内是Cauchy序列如果 ![\lVert x_m-x_n \rVert_{X}\rightarrow0](https://www.zhihu.com/equation?tex=%5ClVert+x_m-x_n+%5CrVert_%7BX%7D%5Crightarrow0) 当 ![m,n\rightarrow\infty](https://www.zhihu.com/equation?tex=m%2Cn%5Crightarrow%5Cinfty) 。换言之，随着序列的增长，序列中的元素可以变得任意接近。

一个空间是完备的（complete）是指其中每个Cauchy序列都收敛到一个极限，及对 ![V](https://www.zhihu.com/equation?tex=V) 中每个Cauchy序列 ![\{v_n\}_{n=1}^{\infty}\subset V](https://www.zhihu.com/equation?tex=%5C%7Bv_n%5C%7D_%7Bn%3D1%7D%5E%7B%5Cinfty%7D%5Csubset+V) ，在 ![V](https://www.zhihu.com/equation?tex=V) 中都存在一个元素 ![v](https://www.zhihu.com/equation?tex=v) ，使得 ![\lim_{n\rightarrow\infty}v_n=v](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Crightarrow%5Cinfty%7Dv_n%3Dv) ，或等价地， ![\lim_{n\rightarrow\infty}\lVert x_n - x \rVert_{X}=0](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%5ClVert+x_n+-+x+%5CrVert_%7BX%7D%3D0) 。

一个完备的赋范空间称为Banach空间。

内积（inner product）是一个映射： ![\langle\cdot,\cdot\rangle: V\times V\rightarrow \mathbb{R}](https://www.zhihu.com/equation?tex=%5Clangle%5Ccdot%2C%5Ccdot%5Crangle%3A+V%5Ctimes+V%5Crightarrow+%5Cmathbb%7BR%7D) ，并对所有的 ![x,y,z\in V](https://www.zhihu.com/equation?tex=x%2Cy%2Cz%5Cin+V) 及 ![a\in\mathbb{R}](https://www.zhihu.com/equation?tex=a%5Cin%5Cmathbb%7BR%7D) ，下面4式成立：
- ![\langle x,x\rangle \geq0](https://www.zhihu.com/equation?tex=%5Clangle+x%2Cx%5Crangle+%5Cgeq0) 且 ![\langle x,x\rangle = 0\Leftrightarrow x=0](https://www.zhihu.com/equation?tex=%5Clangle+x%2Cx%5Crangle+%3D+0%5CLeftrightarrow+x%3D0)
- ![\langle x,y+z\rangle = \langle x,y\rangle+\langle x,z\rangle](https://www.zhihu.com/equation?tex=%5Clangle+x%2Cy%2Bz%5Crangle+%3D+%5Clangle+x%2Cy%5Crangle%2B%5Clangle+x%2Cz%5Crangle)
- ![\langle x,ay\rangle=a\langle x,y\rangle](https://www.zhihu.com/equation?tex=%5Clangle+x%2Cay%5Crangle%3Da%5Clangle+x%2Cy%5Crangle)
- ![\langle x,y\rangle=\langle y,x\rangle](https://www.zhihu.com/equation?tex=%5Clangle+x%2Cy%5Crangle%3D%5Clangle+y%2Cx%5Crangle)

有了内积后便可以定义范数： ![\lVert x\rVert = \sqrt{\langle x,x\rangle}](https://www.zhihu.com/equation?tex=%5ClVert+x%5CrVert+%3D+%5Csqrt%7B%5Clangle+x%2Cx%5Crangle%7D) 。

Hilbert空间就是一个完备的内积空间。由此可见，Hilbert空间都是Banach空间，因为Hilbert空间中的内积可以定义范数，但反过来Banach空间不都是Hilbert空间。

在Hilbert空间中， ![f_n\rightarrow f](https://www.zhihu.com/equation?tex=f_n%5Crightarrow+f) 的意思是 ![\lVert f_n-f\rVert \rightarrow 0](https://www.zhihu.com/equation?tex=%5ClVert+f_n-f%5CrVert+%5Crightarrow+0)当 ![n\rightarrow \infty](https://www.zhihu.com/equation?tex=n%5Crightarrow+%5Cinfty) 。注意并没有 ![\lVert f_n - f \rVert\rightarrow0 \Rightarrow f_n(x)\rightarrow f(x)](https://www.zhihu.com/equation?tex=%5ClVert+f_n+-+f+%5CrVert%5Crightarrow0+%5CRightarrow+f_n%28x%29%5Crightarrow+f%28x%29) 。

举个例子，比如有 ![f(x)=0](https://www.zhihu.com/equation?tex=f%28x%29%3D0) ， ![f_n(x)=\sqrt{n}\ \mathbb{I}(x\leq \frac{1}{n^2})](https://www.zhihu.com/equation?tex=f_n%28x%29%3D%5Csqrt%7Bn%7D%5C+%5Cmathbb%7BI%7D%28x%5Cleq+%5Cfrac%7B1%7D%7Bn%5E2%7D%29) ， ![x\in[0,1]](https://www.zhihu.com/equation?tex=x%5Cin%5B0%2C1%5D) ，可以将函数想象成一个连续的序列，其范数就正相关与在定义域上的积分，故 ![\lVert f_n - f \rVert ](https://www.zhihu.com/equation?tex=%5ClVert+f_n+-+f+%5CrVert+) 可以看成 ![\lim_{n\rightarrow \infty} \int_{0}^{1} \sqrt{n}\ \mathbb{I}(x\leq \frac{1}{n^2})dx](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Crightarrow+%5Cinfty%7D+%5Cint_%7B0%7D%5E%7B1%7D+%5Csqrt%7Bn%7D%5C+%5Cmathbb%7BI%7D%28x%5Cleq+%5Cfrac%7B1%7D%7Bn%5E2%7D%29dx) ，可以想象这就是一个矩形的面积，底为 ![\frac{1}{n^2}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%5E2%7D) ，高为 ![\sqrt{n}](https://www.zhihu.com/equation?tex=%5Csqrt%7Bn%7D) ，故当 ![n\rightarrow\infty](https://www.zhihu.com/equation?tex=n%5Crightarrow%5Cinfty) 时，其值趋近0，然而此时并没有 ![f_n(0)=f(0)](https://www.zhihu.com/equation?tex=f_n%280%29%3Df%280%29) 。

换言之，若函数按范数收敛，则未必有逐点收敛。若想要该条件为真，则需要Hilbert空间成为Reproducing Kernel Hilbert空间（RKHS）。


![L_{p}\ Space](https://www.zhihu.com/equation?tex=L_%7Bp%7D%5C+Space)

设 ![\mathcal{F}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D) 为一群将 ![[a,b]](https://www.zhihu.com/equation?tex=%5Ba%2Cb%5D) 映射到 ![\mathbb{R}](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D) 上的函数，则可以定义 ![\mathcal{F}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D) 的 ![L_{p}](https://www.zhihu.com/equation?tex=L_%7Bp%7D) 范数如下：

![\lVert f\rVert_{p} = (\int_a^b |f(x)|^p dx)^{\frac{1}{p}}](https://www.zhihu.com/equation?tex=%5ClVert+f%5CrVert_%7Bp%7D+%3D+%28%5Cint_a%5Eb+%7Cf%28x%29%7C%5Ep+dx%29%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D)

其中 ![0\lt p\lt \infty](https://www.zhihu.com/equation?tex=0%5Clt+p%5Clt+%5Cinfty) ，当 ![p=\infty](https://www.zhihu.com/equation?tex=p%3D%5Cinfty) 时，定义 ![\lVert f\rVert_{\infty}=\sup_{x}|f(x)|](https://www.zhihu.com/equation?tex=%5ClVert+f%5CrVert_%7B%5Cinfty%7D%3D%5Csup_%7Bx%7D%7Cf%28x%29%7C) 。

一般默认 ![\lVert f \rVert_{2} = \lVert f \rVert](https://www.zhihu.com/equation?tex=%5ClVert+f+%5CrVert_%7B2%7D+%3D+%5ClVert+f+%5CrVert) 。

有了 ![L_{p}](https://www.zhihu.com/equation?tex=L_%7Bp%7D) 范数，就可以定义空间 ![L_{p}(a,b)](https://www.zhihu.com/equation?tex=L_%7Bp%7D%28a%2Cb%29) ：

![L_{p}(a,b)=\{f:[a,b]\rightarrow\mathbb{R}：\lVert f\rVert_{p}\lt \infty\}](https://www.zhihu.com/equation?tex=L_%7Bp%7D%28a%2Cb%29%3D%5C%7Bf%3A%5Ba%2Cb%5D%5Crightarrow%5Cmathbb%7BR%7D%EF%BC%9A%5ClVert+f%5CrVert_%7Bp%7D%5Clt+%5Cinfty%5C%7D)

![L_{p}(a,b)](https://www.zhihu.com/equation?tex=L_%7Bp%7D%28a%2Cb%29) 是一个Hilbert 空间当且仅当 ![p=2](https://www.zhihu.com/equation?tex=p%3D2) 。


![H\ddot{o}lder \ Space](https://www.zhihu.com/equation?tex=H%5Cddot%7Bo%7Dlder+%5C+Space)

令 ![T\subset\mathbb{R}](https://www.zhihu.com/equation?tex=T%5Csubset%5Cmathbb%7BR%7D) ， ![\beta](https://www.zhihu.com/equation?tex=%5Cbeta) 是一个正整数， ![L](https://www.zhihu.com/equation?tex=L) 是一个固定的常数，则Holder空间的定义如下：

![H(\beta, L)=\{g:T\rightarrow\mathbb{R}:|g^{(\beta-1)}(y)-g^{(\beta-1)}(x)|\leq L|x-y|\} \ for\ all\ x,y\in T](https://www.zhihu.com/equation?tex=H%28%5Cbeta%2C+L%29%3D%5C%7Bg%3AT%5Crightarrow%5Cmathbb%7BR%7D%3A%7Cg%5E%7B%28%5Cbeta-1%29%7D%28y%29-g%5E%7B%28%5Cbeta-1%29%7D%28x%29%7C%5Cleq+L%7Cx-y%7C%5C%7D+%5C+for%5C+all%5C+x%2Cy%5Cin+T)

Holder空间还有多元函数的版本，这里暂且不论。

当 ![\beta=2](https://www.zhihu.com/equation?tex=%5Cbeta%3D2) 时，有：

![|g^{'}(x)-g^{'}(y)|\leq L|x-y|](https://www.zhihu.com/equation?tex=%7Cg%5E%7B%27%7D%28x%29-g%5E%7B%27%7D%28y%29%7C%5Cleq+L%7Cx-y%7C)

可以看出这个条件给出了函数二阶导的一个界，所以Holder空间表现了函数在局部的平滑特性。粗略地说， ![\beta](https://www.zhihu.com/equation?tex=%5Cbeta) 越大，函数越平滑。


![Sobolev\ Space](https://www.zhihu.com/equation?tex=Sobolev%5C+Space)

m阶Sobolev空间的定义如下：

![W_{m,p}=\{f\in L_{p}(0,1):\lVert D^m f\rVert \in L_{p}(0,1)\}](https://www.zhihu.com/equation?tex=W_%7Bm%2Cp%7D%3D%5C%7Bf%5Cin+L_%7Bp%7D%280%2C1%29%3A%5ClVert+D%5Em+f%5CrVert+%5Cin+L_%7Bp%7D%280%2C1%29%5C%7D)

这个约束的大致意思就是不仅函数 ![f](https://www.zhihu.com/equation?tex=f) 要有界，其m阶导的积分也要有界。

一般默认 ![W_{m,2}=W_{m}](https://www.zhihu.com/equation?tex=W_%7Bm%2C2%7D%3DW_%7Bm%7D) 。 ![p=2](https://www.zhihu.com/equation?tex=p%3D2) 时Sobolev空间的要求就是取g的m阶导，平方再积分，其值有限，正是由于积分，所以Sobolev空间表现了函数在全局的平滑特性。


![Reproducing\ Kernel\ Hilbert\ Space(RKHS)](https://www.zhihu.com/equation?tex=Reproducing%5C+Kernel%5C+Hilbert%5C+Space%28RKHS%29)

首先，为什么需要RKHS？因为RKHS中的函数具有好的性质（平滑）且易于计算。

其次，如何构造RKHS？利用核函数（Mercer kernel functions）。

首先定义核函数（Mercer kernels），它是一个连续函数 ![K：[a,b]\times [a,b]\rightarrow \mathbb{R}](https://www.zhihu.com/equation?tex=K%EF%BC%9A%5Ba%2Cb%5D%5Ctimes+%5Ba%2Cb%5D%5Crightarrow+%5Cmathbb%7BR%7D)，并且 ![K(x,y)=K(y,x)](https://www.zhihu.com/equation?tex=K%28x%2Cy%29%3DK%28y%2Cx%29) 。并且还要求 ![K](https://www.zhihu.com/equation?tex=K) 是半正定的，即：

![\sum_{i=1}^n \sum_{j=1}^n K(x_i,x_j)c_i c_j \geq 0](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En+%5Csum_%7Bj%3D1%7D%5En+K%28x_i%2Cx_j%29c_i+c_j+%5Cgeq+0)

上式对所有的点 ![x_1,\cdots,x_n\in[a,b]](https://www.zhihu.com/equation?tex=x_1%2C%5Ccdots%2Cx_n%5Cin%5Ba%2Cb%5D) 及所有实数 ![c_1,\cdots,c_n](https://www.zhihu.com/equation?tex=c_1%2C%5Ccdots%2Cc_n)成立。

两个值得注意的点：
- 在实际当中核函数的正定性通常难以验证，但可以从基本的核一步步组装得到复杂的核，利用一些规则可以保证每一步所得到核的正定性。
- 为什么需要核函数半正定？因为由Mercer theorem可知，如果核函数半正定，那么就存在从低维空间 ![x](https://www.zhihu.com/equation?tex=x) 到高维空间 ![\Phi(x)](https://www.zhihu.com/equation?tex=%5CPhi%28x%29) 的映射。

一个满足要求的核函数是高斯核（Guasssian Kernel）： 

![K(x,y)=e^{-\frac{\lVert x-y\rVert^2}{\sigma^2}}](https://www.zhihu.com/equation?tex=K%28x%2Cy%29%3De%5E%7B-%5Cfrac%7B%5ClVert+x-y%5CrVert%5E2%7D%7B%5Csigma%5E2%7D%7D)

有了核函数后便可以开始构造RKHS。对于核函数 ![K(x,y)](https://www.zhihu.com/equation?tex=K%28x%2Cy%29) ，可以将第一个变量固定成一个定值，于是 核函数变成![K_x(y)](https://www.zhihu.com/equation?tex=K_x%28y%29) ，令 ![\mathcal{H}_0](https://www.zhihu.com/equation?tex=%5Cmathcal%7BH%7D_0) 为所以的这样的核 ![K_x(y)](https://www.zhihu.com/equation?tex=K_x%28y%29) 的所有线性组合，即：

![\mathcal{H}_0 = \{f:\sum_{j=1}^{k} \alpha_j K_{x_j}(x)\}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BH%7D_0+%3D+%5C%7Bf%3A%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Calpha_j+K_%7Bx_j%7D%28x%29%5C%7D)

上式中的 ![\alpha_j](https://www.zhihu.com/equation?tex=%5Calpha_j) 和 ![x_j](https://www.zhihu.com/equation?tex=x_j)可以任选。

有了元素之后，便可以定义内积和范数，这样就像RKHS又迈进了一步。

内积的定义如下：假设有两个函数 ![f(x)=\sum_{j=1}^k \alpha_j K_{x_j}(x)](https://www.zhihu.com/equation?tex=f%28x%29%3D%5Csum_%7Bj%3D1%7D%5Ek+%5Calpha_j+K_%7Bx_j%7D%28x%29)和 ![g(x)=\sum_{j=1}^m \beta_j K_{y_j}(x)](https://www.zhihu.com/equation?tex=g%28x%29%3D%5Csum_%7Bj%3D1%7D%5Em+%5Cbeta_j+K_%7By_j%7D%28x%29)，内积定义如下： ![\langle f,g\rangle_{K}=\sum_{i}\sum_{j}\alpha_i\beta_j K(x_i,y_j)](https://www.zhihu.com/equation?tex=%5Clangle+f%2Cg%5Crangle_%7BK%7D%3D%5Csum_%7Bi%7D%5Csum_%7Bj%7D%5Calpha_i%5Cbeta_j+K%28x_i%2Cy_j%29) 。

于是，范数的定义为 ![\lVert f\rVert_{K}=\sqrt{\langle f,f\rangle}](https://www.zhihu.com/equation?tex=%5ClVert+f%5CrVert_%7BK%7D%3D%5Csqrt%7B%5Clangle+f%2Cf%5Crangle%7D) 。

为什么称为再生核呢？这是因为有如下的**The Reproducing Property**。假设有函数 ![f(x)=\sum_{i} \alpha_i K_{x_i}(x)](https://www.zhihu.com/equation?tex=f%28x%29%3D%5Csum_%7Bi%7D+%5Calpha_i+K_%7Bx_i%7D%28x%29) 及函数 ![K_x(\cdot)=\sum1\cdot K_x(\cdot)](https://www.zhihu.com/equation?tex=K_x%28%5Ccdot%29%3D%5Csum1%5Ccdot+K_x%28%5Ccdot%29) ，则根据内积的定义，有 ![\langle f, K_x\rangle=\sum_i\sum\alpha_i\cdot1\cdot K(x_i,x)=f(x)](https://www.zhihu.com/equation?tex=%5Clangle+f%2C+K_x%5Crangle%3D%5Csum_i%5Csum%5Calpha_i%5Ccdot1%5Ccdot+K%28x_i%2Cx%29%3Df%28x%29) ，即 ![f](https://www.zhihu.com/equation?tex=f) 与 ![K_x](https://www.zhihu.com/equation?tex=K_x) 的内积结果为 ![f](https://www.zhihu.com/equation?tex=f) 在 ![x](https://www.zhihu.com/equation?tex=x) 处的值。因此，核函数 ![K](https://www.zhihu.com/equation?tex=K) 称为再生核。

![\mathcal{H}_0](https://www.zhihu.com/equation?tex=%5Cmathcal%7BH%7D_0) 关于范数 ![\lVert\cdot\rVert_{K}](https://www.zhihu.com/equation?tex=%5ClVert%5Ccdot%5CrVert_%7BK%7D) 的completion（对原空间的拓展，包含了原空间中所有Cauchy序列的极限）记作 ![\mathcal{H}_K](https://www.zhihu.com/equation?tex=%5Cmathcal%7BH%7D_K)，称为由 ![K](https://www.zhihu.com/equation?tex=K) 生成的RKHS。

为什么RKHS具有好的性质？可以从Evaluation Functional的角度来看。Evaluation Functional ![\delta_x](https://www.zhihu.com/equation?tex=%5Cdelta_x) 的定义很简单： ![\delta_xf=f(x)](https://www.zhihu.com/equation?tex=%5Cdelta_xf%3Df%28x%29) 。一般来说， ![\delta_x](https://www.zhihu.com/equation?tex=%5Cdelta_x)不连续，即若 ![f_n\rightarrow f](https://www.zhihu.com/equation?tex=f_n%5Crightarrow+f) ，未必有 ![\delta_x f_n\rightarrow\delta_x f](https://www.zhihu.com/equation?tex=%5Cdelta_x+f_n%5Crightarrow%5Cdelta_x+f) （在Hilbert空间那里举了例子）。但是在RKHS中这是成立的：

![\delta_x f_n = \langle f_n,K_x\rangle \rightarrow  \langle f,K_x\rangle=\delta_x f](https://www.zhihu.com/equation?tex=%5Cdelta_x+f_n+%3D+%5Clangle+f_n%2CK_x%5Crangle+%5Crightarrow++%5Clangle+f%2CK_x%5Crangle%3D%5Cdelta_x+f)

中间箭头成立是因为 ![f_n](https://www.zhihu.com/equation?tex=f_n) 依范数收敛到 ![f](https://www.zhihu.com/equation?tex=f) 。

所以可以有如下结论：一个Hilbert空间是RKHS ![\Leftrightarrow](https://www.zhihu.com/equation?tex=%5CLeftrightarrow) Evaluation Functional连续。这样我们就建立了从范数收敛到逐点收敛的桥梁，这是个非常好的性质！（关于为什么RKHS中的函数具有这种平滑性，Larry有种直观的解释，可以对 ![K](https://www.zhihu.com/equation?tex=K) 进行特征函数与特征值的分解，然后![f\in \mathcal{H}_K](https://www.zhihu.com/equation?tex=f%5Cin+%5Cmathcal%7BH%7D_K) 可以表示成一系列特征值与其对应特征函数的线性组合，可以证明随着特征函数不平滑度的增加，其特征值会快速下降，所以 ![f](https://www.zhihu.com/equation?tex=f) 最后保持了较高的平滑度）

核技巧（The Kernel Trick）是一类相当通用的技巧。在许多算法中可以将 ![\langle x_i,x_j\rangle](https://www.zhihu.com/equation?tex=%5Clangle+x_i%2Cx_j%5Crangle) 替换为 ![K(x_i,x_j)](https://www.zhihu.com/equation?tex=K%28x_i%2Cx_j%29) 以得到该算法的非线性版本，这相当于将点 ![x](https://www.zhihu.com/equation?tex=x) 映射到高位空间中 ![\Phi(x)](https://www.zhihu.com/equation?tex=%5CPhi%28x%29) （参考Mercer Theorem），但利用核函数，计算会非常简便。


本文的最后给出一个神奇的结论——表示定理（Representer Theorem）

假设我们有数据 ![(X_1, Y_1),\cdots,(X_n,Y_n)](https://www.zhihu.com/equation?tex=%28X_1%2C+Y_1%29%2C%5Ccdots%2C%28X_n%2CY_n%29) ， ![f](https://www.zhihu.com/equation?tex=f) 是一个定义在RKHS上的预测函数，其预测结果为 ![f(X_1),\cdots,f(X_n)](https://www.zhihu.com/equation?tex=f%28X_1%29%2C%5Ccdots%2Cf%28X_n%29) ，令 ![\mathcal{l}](https://www.zhihu.com/equation?tex=%5Cmathcal%7Bl%7D) 为经验风险， ![g(\lVert f\rVert_{K}^2)](https://www.zhihu.com/equation?tex=g%28%5ClVert+f%5CrVert_%7BK%7D%5E2%29) 为结构风险（g是一个单调递增函数），那么该预测问题可以写作：

![\min_{f}\ l+g(\lVert f\rVert_{K}^2)](https://www.zhihu.com/equation?tex=%5Cmin_%7Bf%7D%5C+l%2Bg%28%5ClVert+f%5CrVert_%7BK%7D%5E2%29)

设其最优解为 ![\hat{f}](https://www.zhihu.com/equation?tex=%5Chat%7Bf%7D) ，那么 ![\hat{f}(x)=\sum_{i=1}^n \alpha_i K(x_i, x)](https://www.zhihu.com/equation?tex=%5Chat%7Bf%7D%28x%29%3D%5Csum_%7Bi%3D1%7D%5En+%5Calpha_i+K%28x_i%2C+x%29)

其中 ![\alpha_i](https://www.zhihu.com/equation?tex=%5Calpha_i) 是一些参数， ![x_i](https://www.zhihu.com/equation?tex=x_i) 是输入数据。

表示定理的强大之处在于大大简化了带正则项的经验风险最小化问题的难度。之前搜索这样的 ![\hat{f}](https://www.zhihu.com/equation?tex=%5Chat%7Bf%7D) 要在无穷多维的 ![\mathcal{H}_K](https://www.zhihu.com/equation?tex=%5Cmathcal%7BH%7D_K) 中，而现在只要在 ![\mathbb{R}^n](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5En) 中搜索 ![\alpha=(\alpha_1,\cdots,\alpha_n)](https://www.zhihu.com/equation?tex=%5Calpha%3D%28%5Calpha_1%2C%5Ccdots%2C%5Calpha_n%29) 即可。表示定理将一个通用的机器学习问题简化成了一个切实可解决的计算机问题。


（本文的参考资料来自CMU statistical machine learning课程的视频和讲义以及Wikipedia）

