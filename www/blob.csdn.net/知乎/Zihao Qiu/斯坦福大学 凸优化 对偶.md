# 斯坦福大学 凸优化 对偶 - 知乎
# 

假设现在有一个优化问题（**未必是凸优化问题**）：

![\begin{align*} &min.\ f_0(x) \\ &s.t.\ f_i(x)\leq0,\ i=1,\cdots,m \\         &\quad \quad h_i(x)=0,\ i=1,\cdots,m \end{align*} ](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26min.%5C+f_0%28x%29+%5C%5C+%26s.t.%5C+f_i%28x%29%5Cleq0%2C%5C+i%3D1%2C%5Ccdots%2Cm+%5C%5C+++++++++%26%5Cquad+%5Cquad+h_i%28x%29%3D0%2C%5C+i%3D1%2C%5Ccdots%2Cm+%5Cend%7Balign%2A%7D+)

设该问题的最优值为 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 。

可以通过一些方法来估计 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 的上界和下界。其中上界并不难得到，任何满足该优化问题约束的值都是 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 的上界，那么下界如何得到呢？

可用将约束条件考虑进来，构造新的函数：

![L(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^m \lambda_if_i(x)+\sum_{i=1}^p\nu_ih_i(x)](https://www.zhihu.com/equation?tex=L%28x%2C%5Clambda%2C%5Cnu%29%3Df_0%28x%29%2B%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_if_i%28x%29%2B%5Csum_%7Bi%3D1%7D%5Ep%5Cnu_ih_i%28x%29)

这里要求 ![\lambda_i\geq0,i=1,\cdots,m](https://www.zhihu.com/equation?tex=%5Clambda_i%5Cgeq0%2Ci%3D1%2C%5Ccdots%2Cm) 。由于 ![f_i(x)\leq0,h_i(x)=0,\ i=1,\cdots,m](https://www.zhihu.com/equation?tex=f_i%28x%29%5Cleq0%2Ch_i%28x%29%3D0%2C%5C+i%3D1%2C%5Ccdots%2Cm) ，所以总有 ![L(x,\lambda,\nu)\le f_0(x)](https://www.zhihu.com/equation?tex=L%28x%2C%5Clambda%2C%5Cnu%29%5Cle+f_0%28x%29) ，即函数 ![L](https://www.zhihu.com/equation?tex=L) 的取值是函数 ![f_0(x)](https://www.zhihu.com/equation?tex=f_0%28x%29) 的下界。函数 ![L](https://www.zhihu.com/equation?tex=L) 就是原优化问题的**Lagrange函数**， ![\lambda,\nu](https://www.zhihu.com/equation?tex=%5Clambda%2C%5Cnu) 称为原问题的**Lagrange乘子**。

现在我们已经有：

![L(x,\lambda,\nu)\leq f_0(x)](https://www.zhihu.com/equation?tex=L%28x%2C%5Clambda%2C%5Cnu%29%5Cleq+f_0%28x%29)

那么对两边同时对变量 ![x](https://www.zhihu.com/equation?tex=x) 进行最小化，我们就可以得到 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 的下界：

![\min_xf_0(x)=p^*\geq \inf_xL(x,\lambda,\nu)=g(\lambda,\nu)](https://www.zhihu.com/equation?tex=%5Cmin_xf_0%28x%29%3Dp%5E%2A%5Cgeq+%5Cinf_xL%28x%2C%5Clambda%2C%5Cnu%29%3Dg%28%5Clambda%2C%5Cnu%29)

![g(\lambda,\nu)](https://www.zhihu.com/equation?tex=g%28%5Clambda%2C%5Cnu%29) 即 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 的下界，称为**Lagrange对偶函数**。

进一步，我们想知道 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 最好的下界是什么。这个问题可以表述为如下的优化问题：

![\begin{align*} &max.\ g(\lambda,\nu) \\ &s.t.\quad \lambda_i\geq0,\ i=1,\cdots,m \end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26max.%5C+g%28%5Clambda%2C%5Cnu%29+%5C%5C+%26s.t.%5Cquad+%5Clambda_i%5Cgeq0%2C%5C+i%3D1%2C%5Ccdots%2Cm+%5Cend%7Balign%2A%7D)

这个问题就被称为原优化问题的**Lagrange对偶问题**。

一般来说，Lagrange对偶问题不仅仅是一种数学上的形式变化，在很多时候，对偶问题同原问题一样有着清晰的物理图景。比如求网络中的最大流问题，可以被表示成一个LP优化问题，而其对偶问题，正是求网络的最小割。这里我主要聚焦于Lagrange对偶在机器学习中的应用。

假设Lagrange对偶问题的最优值为 ![d^*](https://www.zhihu.com/equation?tex=d%5E%2A) ，由于其是 ![p^*](https://www.zhihu.com/equation?tex=p%5E%2A) 的下界，所以有：

![d^*\leq p^*](https://www.zhihu.com/equation?tex=d%5E%2A%5Cleq+p%5E%2A)

这是最一般的性质，故称**弱对偶性**。

假如有 ![d^*=p^*](https://www.zhihu.com/equation?tex=d%5E%2A%3Dp%5E%2A) 成立，则称为**强对偶性**成立。一般来说强对偶性不成立，但是假如原问题的目标函数是凸函数且满足Slater规则（原问题的约束有不在边界的严格可行解）的话，强对偶性成立。

当强对偶性成立时，假设 ![x^*](https://www.zhihu.com/equation?tex=x%5E%2A) 是原问题的最优解， ![(\lambda^*,\nu^*)](https://www.zhihu.com/equation?tex=%28%5Clambda%5E%2A%2C%5Cnu%5E%2A%29) 是对偶问题的最优解。那么有：

![\begin{align*} f_0(x^*)&=g(\lambda^*,\nu^*) \\ &=\inf_x(f_0(x)+\sum_{i=1}^m\lambda_i^*f_i(x)+\sum_{i=1}^p\nu_i^*h_i(x)) \\         &\leq f_0(x^*)+\sum_{i=1}^m\lambda_i^*f_i(x^*)+\sum_{i=1}^p\nu_i^*h_i(x^*)\\ &\leq f_0(x^*) \end{align*} ](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+f_0%28x%5E%2A%29%26%3Dg%28%5Clambda%5E%2A%2C%5Cnu%5E%2A%29+%5C%5C+%26%3D%5Cinf_x%28f_0%28x%29%2B%5Csum_%7Bi%3D1%7D%5Em%5Clambda_i%5E%2Af_i%28x%29%2B%5Csum_%7Bi%3D1%7D%5Ep%5Cnu_i%5E%2Ah_i%28x%29%29+%5C%5C+++++++++%26%5Cleq+f_0%28x%5E%2A%29%2B%5Csum_%7Bi%3D1%7D%5Em%5Clambda_i%5E%2Af_i%28x%5E%2A%29%2B%5Csum_%7Bi%3D1%7D%5Ep%5Cnu_i%5E%2Ah_i%28x%5E%2A%29%5C%5C+%26%5Cleq+f_0%28x%5E%2A%29+%5Cend%7Balign%2A%7D+)

所以有：

![f_0(x^*)+\sum_{i=1}^m\lambda_i^*f_i(x^*)+\sum_{i=1}^p\nu_i^*h_i(x^*) = f_0(x^*)](https://www.zhihu.com/equation?tex=f_0%28x%5E%2A%29%2B%5Csum_%7Bi%3D1%7D%5Em%5Clambda_i%5E%2Af_i%28x%5E%2A%29%2B%5Csum_%7Bi%3D1%7D%5Ep%5Cnu_i%5E%2Ah_i%28x%5E%2A%29+%3D+f_0%28x%5E%2A%29)

所以：

![\sum_{i=1}^m\lambda_i^*f_i(x^*)=0](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5Em%5Clambda_i%5E%2Af_i%28x%5E%2A%29%3D0)

又因为：

![\lambda_i f_i(x)\leq0,\ i=1,\cdots,m](https://www.zhihu.com/equation?tex=%5Clambda_i+f_i%28x%29%5Cleq0%2C%5C+i%3D1%2C%5Ccdots%2Cm)

所以：

![\lambda_i f_i(x)=0,\ i=1,\cdots,m](https://www.zhihu.com/equation?tex=%5Clambda_i+f_i%28x%29%3D0%2C%5C+i%3D1%2C%5Ccdots%2Cm)

上面的这条性质称为**互补松弛性**（complementary slackness）。直观含义就是假如原问题中的约束是紧的（ ![f_i(x)=0](https://www.zhihu.com/equation?tex=f_i%28x%29%3D0) ），那么其对应的对偶问题中的对偶变量就是松的（ ![\lambda_i>0](https://www.zhihu.com/equation?tex=%5Clambda_i%3E0) ）；反之若原问题中的约束是松的（ ![f_i(x)<0](https://www.zhihu.com/equation?tex=f_i%28x%29%3C0) ），那么其对应的对偶问题中的对偶变量就是紧的（ ![\lambda_i=0](https://www.zhihu.com/equation?tex=%5Clambda_i%3D0) ）。

在机器学习中使用最多的是下面提到的**KKT最优性条件**。这个条件描述的是原问题的最优解 ![x^*](https://www.zhihu.com/equation?tex=x%5E%2A) 和对偶问题的最优解 ![(\lambda^*,\nu^*)](https://www.zhihu.com/equation?tex=%28%5Clambda%5E%2A%2C%5Cnu%5E%2A%29) 所要满足的条件。你将会看到，假如原问题非凸，那么每个局部最优解（函数极小值）都会满足KKT条件；假如原问题是凸的，那么满足KKT条件的解一定是全局最优解！这就给出了一个求局部最优/全局最优的闭式解的方法！

KKT条件要求强对偶，且目标函数和约束函数可微。KKT条件如下：

![\begin{align*} f_i(x^*)&\leq 0,\quad i=1,\cdots,m\\ h_i(x^*)&=0,\quad i=1,\cdots,p\\ \lambda_i^*&\geq0,\quad i=1,\cdots,m\\ \lambda_i^*f_i(x^*)&=0,\quad i=1,\cdots,m\\ 0&=\nabla f_0(x^*)+\sum_{i=1}^m\lambda_i^*\nabla f_i(x^*)+\sum_{i=1}^p\nu_i^*\nabla h_i(x^*)  \end{align*}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+f_i%28x%5E%2A%29%26%5Cleq+0%2C%5Cquad+i%3D1%2C%5Ccdots%2Cm%5C%5C+h_i%28x%5E%2A%29%26%3D0%2C%5Cquad+i%3D1%2C%5Ccdots%2Cp%5C%5C+%5Clambda_i%5E%2A%26%5Cgeq0%2C%5Cquad+i%3D1%2C%5Ccdots%2Cm%5C%5C+%5Clambda_i%5E%2Af_i%28x%5E%2A%29%26%3D0%2C%5Cquad+i%3D1%2C%5Ccdots%2Cm%5C%5C+0%26%3D%5Cnabla+f_0%28x%5E%2A%29%2B%5Csum_%7Bi%3D1%7D%5Em%5Clambda_i%5E%2A%5Cnabla+f_i%28x%5E%2A%29%2B%5Csum_%7Bi%3D1%7D%5Ep%5Cnu_i%5E%2A%5Cnabla+h_i%28x%5E%2A%29++%5Cend%7Balign%2A%7D)

可以发现，前两条表示 ![x^*](https://www.zhihu.com/equation?tex=x%5E%2A) 满足原问题的约束，第三条表示对偶变量满足的约束，第四条表示complementary slackness。最后一条表示Lagrange函数 ![L(x,\lambda^*,\nu^*)](https://www.zhihu.com/equation?tex=L%28x%2C%5Clambda%5E%2A%2C%5Cnu%5E%2A%29) 在 ![x^*](https://www.zhihu.com/equation?tex=x%5E%2A) 处取到极小（因为 ![x^*](https://www.zhihu.com/equation?tex=x%5E%2A) 是原问题最优解）。

KKT条件的最后一条很关键，因为通过这个式子便可以得到原问题的全局最优解/局部最优解。这样的计算方法称为**拉格朗日乘子法**。下面举PCA的例子来说明。

可以从最大可分性的角度来推导PCA。假设有原空间的样本点 ![x_i](https://www.zhihu.com/equation?tex=x_i) ，新坐标系为 ![\{w_1,w_2,\cdots,w_d\}=W](https://www.zhihu.com/equation?tex=%5C%7Bw_1%2Cw_2%2C%5Ccdots%2Cw_d%5C%7D%3DW) 且 ![w_i](https://www.zhihu.com/equation?tex=w_i) 是标准正交基。那么 ![x_i](https://www.zhihu.com/equation?tex=x_i) 投影后的点为 ![W^Tx_i](https://www.zhihu.com/equation?tex=W%5ETx_i) 。投影后所有样本点的方差就是 ![\sum_iW^Tx_ix_i^TW=tr(W^TXX^TW)](https://www.zhihu.com/equation?tex=%5Csum_iW%5ETx_ix_i%5ETW%3Dtr%28W%5ETXX%5ETW%29) ， ![XX^T](https://www.zhihu.com/equation?tex=XX%5ET) 是中心化了的协方差矩阵。则PCA的优化目标可以写为：

![\begin{align*} &min_{W}.\ -tr(W^TXX^TW) \\ &s.t.\quad W^TW=I \end{align*} ](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26min_%7BW%7D.%5C+-tr%28W%5ETXX%5ETW%29+%5C%5C+%26s.t.%5Cquad+W%5ETW%3DI+%5Cend%7Balign%2A%7D+)

注意这是一个非凸优化，因为目标函数对 ![W](https://www.zhihu.com/equation?tex=W) 的二阶导（Hessian）正比于 ![-XX^T](https://www.zhihu.com/equation?tex=-XX%5ET) 。而协方差矩阵是半正定矩阵，所有目标函数非凸（由二阶条件可得）。

但此时仍可以使用拉格朗日乘子法（此法要求强对偶，但不要求凸）:

![L(W,\lambda)=-tr(W^TXX^TW)+\lambda(W^TW-I)](https://www.zhihu.com/equation?tex=L%28W%2C%5Clambda%29%3D-tr%28W%5ETXX%5ETW%29%2B%5Clambda%28W%5ETW-I%29)

![\frac{\partial L}{\partial W}=0 \Rightarrow XX^TW=\lambda W](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+W%7D%3D0+%5CRightarrow+XX%5ETW%3D%5Clambda+W)

所以此时实际上一系列的local optimal都以满足上式。在这个问题中，每个local optimal就是 ![XX^T](https://www.zhihu.com/equation?tex=XX%5ET) 的一个特征向量，其物理含义就是一个投影方向！最后这些local optimal组成了PCA的投影矩阵 ![W](https://www.zhihu.com/equation?tex=W) 。


