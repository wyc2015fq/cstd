# 【学界|机器学习】深度学习中消失的梯度 - 知乎
# 

> **文章作者：POLL**
文章已发布于**微信公众号【运筹OR帷幄】：**[【学界|机器学习】深度学习中消失的梯度](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/dagK_opxrXijJc6kcsjsGA)
*欢迎原链接转发，转载请私信*[@留德华叫兽](https://www.zhihu.com/people/961e8cc4f7512fda1ea6626ce9a05e8e)*获取信息，盗版必究。*
敬请关注和扩散本专栏及同名公众号，会邀请**全球知名学者**发布运筹学、人工智能中优化理论等相关干货、[知乎Live](https://www.zhihu.com/lives/users/961e8cc4f7512fda1ea6626ce9a05e8e)及行业动态：[『运筹OR帷幄』大数据人工智能时代的运筹学](https://zhuanlan.zhihu.com/operations-research)

## **0、前言**

了解深度学习的同学可能知道，目前深度学习面临的一个问题就是在网络训练的过程中存在**梯度消失问题**（vanishing gradient problem），或者更广义地来讲就是**不稳定梯度问题**。那么到底什么是梯度消失呢？这个问题又是如何导致的呢？这就是本文要分享的内容。

## **1、消失的梯度**

首先，我们将一个网络在初始化之后在训练初期的结果可视化如下：
![](https://pic2.zhimg.com/v2-addc0cf15c71fe72f10f6029c75b6a8d_b.jpg)
在上图中，神经元上的条可以理解为神经元的学习速率。这个网络是经过随机初始化的，但是从上图不难发现，第二层神经元上的条都要大于第一层对应神经元上的条，即第二层神经元的学习速率大于第一层神经元学习速率。那这可不可能是个巧合呢？其实不是的，在书中，Nielsen通过实验说明这种现象是普遍存在的。

我们再来看下对于一个具有四个隐层的神经网络，各隐藏层的学习速率曲线如下：
![](https://pic3.zhimg.com/v2-7d1416b4cb0cae0390934132ec85b1ba_b.jpg)
可以看出，第一层的学习速度和最后一层要差两个数量级，也就是比第四层慢了100倍。 实际上，这个问题是可以避免的，尽管替代方法并不是那么有效，同样会产生问题——在前面的层中的梯度会变得非常大！这也叫做激增的梯度问题（exploding gradient problem），这也没有比消失的梯度问题更好处理。**更加一般地说，在深度神经网络中的梯度是不稳定的，在前面的层中或会消失，或会激增，这种不稳定性才是深度神经网络中基于梯度学习的根本原因。**

## **2、什么导致了梯度消失？**

为了弄清楚为何会出现消失的梯度，来看看一个极简单的深度神经网络：每一层都只有一个单一的神经元。下面就是有三层隐藏层的神经网络：
![](https://pic1.zhimg.com/v2-fdd0e1e43f77fb933dcba605e9c87f74_b.jpg)
我们把梯度的整个表达式写出来：

![](https://pic3.zhimg.com/v2-3019582f77e7e61396d0206ab3e7e092_b.jpg)
为了理解每个项的行为，先看下sigmoid函数导数的曲线：
![](https://pic4.zhimg.com/v2-ed222c8d2a95431c87ee8e68ad17cdbf_b.jpg)
该导数在
![](https://pic2.zhimg.com/v2-ddc4c031e9d916d961c38d58f83fed3d_b.jpg)
时达到最高。现在，如果我们使用标准方法来初始化网络中的权重，那么会使用一个均值为0标准差为1的高斯分布。因此所有的权重通常会满足
![](https://pic4.zhimg.com/v2-04ac890365f3f6db31064e31abc08f13_b.jpg)
。有了这些信息，我们发现会有
![](https://pic3.zhimg.com/v2-6e303cd720b4883ae60a9364c723e76e_b.jpg)
，并且在进行所有这些项的乘积时，最终结果肯定会指数级下降：项越多，乘积的下降也就越快。

下面我们从公式上比较一下第三层和第一层神经元的学习速率：
![](https://pic4.zhimg.com/v2-bf15ae6903245dfaff347c8d32ca803f_b.jpg)
比较一下
![](https://pic3.zhimg.com/v2-635fb0e0ca11ba5f2dba86817f1a246e_b.jpg)
和
![](https://pic2.zhimg.com/v2-4b5bfcadd44f566cc4b5423b8a54d42d_b.jpg)
可知，
![](https://pic2.zhimg.com/v2-9eef04b454f5ed4df7705c8da7dc05c1_b.jpg)
要远远小于
![](https://pic2.zhimg.com/v2-4b5bfcadd44f566cc4b5423b8a54d42d_b.jpg)
。**因此，梯度消失的本质原因是：**
![](https://pic4.zhimg.com/v2-fa9ed2a1ba49a989413b8ab4f502315b_b.jpg)
**的约束**。

## **3、梯度激增问题**

举个例子说明下：

首先，我们将网络的权重设置得很大，比如
![](https://pic2.zhimg.com/v2-d1e64bce5c8d0c2dbb417ff0462f4b01_b.jpg)
然后，我们选择偏置使得
![](https://pic3.zhimg.com/v2-909d64d97d19ef902d20b85d6f8e8d82_b.jpg)
项不会太小。这是很容易实现的：方法就是选择偏置来保证每个神经元的带权输入是
![](https://pic3.zhimg.com/v2-67f1d7b6a842142b602b9c3daea3a28a_b.jpg)
（这样）。
![](https://pic1.zhimg.com/v2-afbb97ee19d7d05bab7de9e24c3062d0_b.jpg)


比如说，我们希望
![](https://pic2.zhimg.com/v2-072eeb750354bfbe602fd0ee36385091_b.jpg)
，我们只需要把
![](https://pic1.zhimg.com/v2-7b52a220ae0a54100c6aa1c9e824f3d0_b.jpg)
即可。我们使用相同的方法来获取其他的偏置。这样我们可以发现所有的项
![](https://pic1.zhimg.com/v2-8042ac46ddea2d5a19bf9a670402ef48_b.jpg)
都等于100*1/4=25。最终，我们获得了激增的梯度。

## **4、不稳定的梯度问题**

**不稳定的梯度问题：**根本的问题其实并非是消失的梯度问题或者激增的梯度问题，而是在前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。简而言之，真实的问题就是神经网络受限于不稳定梯度的问题。所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。

## **参考文献：**

1. Michael Nielsen,《Neural Networks and Deep Learning》

本文转载自**博客园：**[[Deep Learning] 深度学习中消失的梯度](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/maybe2030/p/6336896.html)

在本公众号后台回复关键词：“**学界**”获取大量由我平台编辑精心整理的学习资料！~

如果你是运筹学/人工智能硕博或在读，请在下图的公众号后台留言：**“加微信群”**。系统会进一步的提示，邀请您进全球运筹或AI学者群（群内学界、业界大佬云集）。

同时我们有：【**运筹学|优化爱好者**】【**供应链|物流**】【**人工智能**】【**数据科学|分析**】千人QQ群，关注下方公众号**点击“加入社区”按钮**，获得入群传送门。
![](https://pic2.zhimg.com/v2-95bfcc88cc182f0e1a6952413c78fe35_b.jpg)

