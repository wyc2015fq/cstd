# 语言模型_用户5706034127_新浪博客
||分类：[自然语言处理](http://blog.sina.com.cn/s/articlelist_5706034127_3_1.html)|
转载：https://sanwen8.cn/p/186HXN3.html
语言模型，简单的定义是：给定一句话（sentence)，这句话出现的可能性有多大；或给定连续的N个词 (words)，那么第N+1个词
(word) 是什么，概率是多少。这里我们介绍几个非常有名的 language models, 包括：n-gram Language
Model, Neural Net Language Model (NNLM), Recurrent Neural Net
Language Model (RNN) 和 skip-gram Language Model。
(1) N-gram Language Model
N-gram 语言模型是一种非参数的模型 (non-parametric
model)，它用于估计：给定前n个词，第n+1个词的概率。假设vocabulary中有N个词，那么这个条件概率就有N中可能，他们的概率和为1。N-gram
语言模型的的弊病是它根本无法描述词与词间的 semantic similarity，每个词都是独立于其他的词的。
举个例子：如果在训练样本中，我们有 eat a banana, eat a pear, eat a peach, 但是没有 eat a
mango, 那么我们就会得到 p(mango | eat, a) = 0, 也就是，eat + a
的后面不可能是mango，这显然是与事实不符的，因为 mango 和 banana, pear, peach 一样都是水果 (they
are semantically similar)，如果仅仅因为 eat a mango 在training数据中没出现，就断定
p(mango | eat, a) = 0 是不合理的。
(2) Neural Net Language Model (NNLM)
NNLM 是2003年 Yoshua Bengio
教授提出来的神经网络语言模型，它同样是为了build一个条件概率，即：给定n个连续的词，第n+1个词可能是什么，它的概率是多少。NNLM考虑了词与词间的语义相似度
(semantic
similarity)，它的基本思想是把所有的词都map到一个m维的空间，每个词由一个m维的向量表示。一个好的mapping是：语义相似的词，应该有相似的特征向量，不相似的词，他们的特征向量应该相差较大。Yoshua
Bengio 教授通过 neural network 来model这个条件概率，然后通过maximize
likelihood来同时解求neural network中的参数和vocabulary中每个词的m维的特征向量。NNLM不同于
n-gram模型，NNLM是一种参数模型 (parametric model)，条件概率是用neural
network来描述的。还是上面的那个例子：即使在training中没有出现 eat a mango，因为在maximize
likelihood 参数学**的过程的，mango和其他的水果会被mapping到相似的m维度特征向量，所以在 testing
datast 上， p(mango | eat, a) > 0, 并且和 p(banana | eat, a)
的值相似。
(3) Recurrent Neural Net Language Model (RNN)
前面两种 语言模型 都是基于马尔科夫假设(Markov assumption)
的，即句子中的某个词的概率只取决余它前面的n个词。那么，如果我们要考虑更多的 contextual information
呢？也就是说，如果我们扔掉 Markov
assumption，让这个词出现的概率取决于这句话它前面所有的词，而不仅仅是前n个词，结果会怎样呢？于是学者提出了RNN模型，用来model
句子：句子中的每个词出现的条件概率都取决于它前面的所有词，最终整个句子的概率就是每个词条件概率的乘积。RNN 和 NNLM
一样，是一种参数模型，是目前主流的 language model，RNN中的参数和每个词的embedding
(每个词被map到了一个m维的向量）同样使用maximize likelihood解求。由于RNN参数learning时：error
backpropagate 会有gradient vanishing / explosion 的问题，学者们引入了long-short
term memory (LSTM) unit (由 Jürgen Schmidhuber 教授及其学生在1997年提出) 和
gated recurrent units (GRU，由 Kyunghyun Cho 在2014年提出)，来减轻 gradient
vanishing / explosion 的问题。 实际中，学者们一般都会用 LSTM/GRU
来取代RNN进行sequence的modeling。
(4) Skip-gram Language Model
这是 Tomas Mikolov 在2013年提出来的 word representation 模型，它把每个word
映射到了一个m维的空间，skip-gram有个更著名的别名：word2vec，不过严格来讲： skip-gram
不算语言模型，因为它并没有考虑词的次序 (the order of words)，也不能描述句子的概率，它更准确来说是个 word 到
vector 的mapping模型。虽然NNLM和RNN同样也把每个word映射到了m维的空间，但是他们的 training time
complexity 都远远大于skip-gram，因为他们都有大量的 matrix multiplication
运算。Skip-gram有着类似于 NNLM 和 RNN 的思想：建立词的条件概率 (conditional
probability) 模型。不同点是：skip-gram用了非常简单的
log-linear model来建立条件概率模型，而NNLM和RNN都是用了非线性的条件概率模型。Skip-gram不同于 NNLM
和 RNN 的地方还在于：skip-gram
建立这样的条件概率，给定一个词，它的前后一定范围内，每个词出现的概率多少。Skip-gram同样使用maximize
likelihood来训练参数和word的mapping。word2vec
有公开的源代码，大家可以去试试看，有趣的是：很多词所对应的vector拥有很多有趣的线性关系，比如：北京
+ 中国 – 法国 = 巴黎 …
