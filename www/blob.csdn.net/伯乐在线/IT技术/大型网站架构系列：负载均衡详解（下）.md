# 大型网站架构系列：负载均衡详解（下） - 文章 - 伯乐在线
原文出处： [ITFLY8](http://www.cnblogs.com/itfly8/p/5080743.html)
《[大型网站架构系列：负载均衡详解（上）](http://blog.jobbole.com/97957/)》
## 一、软件负载均衡概述
硬件负载均衡性能优越，功能全面，但是价格昂贵，一般适合初期或者土豪级公司长期使用。因此软件负载均衡在互联网领域大量使用。常用的软件负载均衡软件有Nginx，Lvs，HaProxy等。本文参考大量文档，部分为直接拷贝，参考出处文末。
## 二、Ngnix负载均衡
Ngnix是一款轻量级的Web服务器/反向代理服务器，工作在七层Http协议的负载均衡系统。具有高性能、高并发、低内存使用等特点。是一个轻量级的Http和反向代理服务器。Nginx使用epoll and kqueue作为开发模型。能够支持高达 50,000 个并发连接数的响应。
操作系统：Liunx，Windows（Linux、FreeBSD、Solaris、Mac OS X、AIX以及Microsoft Windows）
开发语言：C
并发性能：官方支持每秒5万并发，实际国内一般到每秒2万并发，有优化到每秒10万并发的。具体性能看应用场景。
### 2.1.特点
1.模块化设计：良好的扩展性，可以通过模块方式进行功能扩展。
2.高可靠性：主控进程和worker是同步实现的，一个worker出现问题，会立刻启动另一个worker。
3.内存消耗低：一万个长连接（keep-alive）,仅消耗2.5MB内存。
4.支持热部署：不用停止服务器，实现更新配置文件，更换日志文件、更新服务器程序版本。
5.并发能力强：官方数据每秒支持5万并发；
6.功能丰富：优秀的反向代理功能和灵活的负载均衡策略
### 2.2.功能
#### 2.2.1基本功能
- 支持静态资源的web服务器。
- http,smtp,pop3协议的反向代理服务器、缓存、负载均衡；
- 支持FASTCGI（fpm）
- 支持模块化，过滤器（让文本可以实现压缩，节约带宽）,ssl及图像大小调整。
- 内置的健康检查功能
- 基于名称和ip的虚拟主机
- 定制访问日志
- 支持平滑升级
- 支持KEEPALIVE
- 支持url rewrite
- 支持路径别名
- 支持基于IP和用户名的访问控制。
- 支持传输速率限制，支持并发数限制。
#### 2.2.2扩展功能
#### 2.2.3性能
Nginx的高并发，官方测试支持5万并发连接。实际生产环境能到2-3万并发连接数。10000个非活跃的HTTP keep-alive 连接仅占用约2.5MB内存。三万并发连接下，10个Nginx进程，消耗内存150M。淘宝tengine团队测试结果是“24G内存机器上，处理并发请求可达200万”。
### 2.3架构
#### 2.3.1Nginx的基本工作模式
![](http://jbcdn2.b0.upaiyun.com/2016/02/cc9b1cddf8ac16c6eb97481303a13bb1.jpg)
一个master进程，生成一个或者多个worker进程。但是这里master是使用root身份启动的，因为nginx要工作在80端口。而只有管理员才有权限启动小于低于1023的端口。master主要是负责的作用只是启动worker，加载配置文件，负责系统的平滑升级。其它的工作是交给worker。那么当worker被启动之后，也只是负责一些web最简单的工作，而其他的工作都是有worker中调用的模块来实现的。
模块之间是以流水线的方式实现功能的。流水线，指的是一个用户请求，由多个模块组合各自的功能依次实现完成的。比如：第一个模块只负责分析请求首部，第二个模块只负责查找数据，第三个模块只负责压缩数据，依次完成各自工作。来实现整个工作的完成。
他们是如何实现热部署的呢？其实是这样的，我们前面说master不负责具体的工作，而是调用worker工作，他只是负责读取配置文件，因此当一个模块修改或者配置文件发生变化，是由master进行读取，因此此时不会影响到worker工作。在master进行读取配置文件之后，不会立即的把修改的配置文件告知worker。而是让被修改的worker继续使用老的配置文件工作，当worker工作完毕之后，直接当掉这个子进程，更换新的子进程，使用新的规则。
#### 2.3.2Nginx支持的sendfile机制
Sendfile机制，用户将请求发给内核，内核根据用户的请求调用相应用户进程，进程在处理时需要资源。此时再把请求发给内核（进程没有直接IO的能力），由内核加载数据。内核查找到数据之后，会把数据复制给用户进程，由用户进程对数据进行封装，之后交给内核，内核在进行tcp/ip首部的封装，最后再发给客户端。这个功能用户进程只是发生了一个封装报文的过程，却要绕一大圈。因此nginx引入了sendfile机制，使得内核在接受到数据之后，不再依靠用户进程给予封装，而是自己查找自己封装，减少了一个很长一段时间的浪费，这是一个提升性能的核心点。
![](http://jbcdn2.b0.upaiyun.com/2016/02/4c49442f28a81afb2b4499b8b5fbb257.jpg)
以上内容摘自网友发布的文章，简单一句话是资源的处理，直接通过内核层进行数据传递，避免了数据传递到应用层，应用层再传递到内核层的开销。
目前高并发的处理，一般都采用sendfile模式。通过直接操作内核层数据，减少应用与内核层数据传递。
#### 2.3.3Nginx通信模型（I/O复用机制）
开发模型：epoll和kqueue。
支持的事件机制：kqueue、epoll、rt signals、/dev/poll 、event ports、select以及poll。
支持的kqueue特性包括EV_CLEAR、EV_DISABLE、NOTE_LOWAT、EV_EOF，可用数据的数量，错误代码.
支持sendfile、sendfile64和sendfilev;文件AIO；DIRECTIO;支持Accept-filters和TCP_DEFER_ACCEP.
以上概念较多，大家自行百度或谷歌，知识领域是网络通信（BIO,NIO,AIO）和多线程方面的知识。
### 2.4均衡策略
nginx的负载均衡策略可以划分为两大类：内置策略和扩展策略。内置策略包含加权轮询和ip hash，在默认情况下这两种策略会编译进nginx内核，只需在nginx配置中指明参数即可。扩展策略有很多，如fair、通用hash、consistent hash等，默认不编译进nginx内核。由于在nginx版本升级中负载均衡的代码没有本质性的变化，因此下面将以nginx1.0.15稳定版为例，从源码角度分析各个策略。
### 2.4.1. 加权轮询（weighted round robin）
轮询的原理很简单，首先我们介绍一下轮询的基本流程。如下是处理一次请求的流程图：
![](http://jbcdn2.b0.upaiyun.com/2016/02/068434fddd630fa17fc278dcf67bf208.jpg)
图中有两点需要注意，第一，如果可以把加权轮询算法分为先深搜索和先广搜索，那么nginx采用的是先深搜索算法，即将首先将请求都分给高权重的机器，直到该机器的权值降到了比其他机器低，才开始将请求分给下一个高权重的机器；第二，当所有后端机器都down掉时，nginx会立即将所有机器的标志位清成初始状态，以避免造成所有的机器都处在timeout的状态，从而导致整个前端被夯住。
### 2.4.2. ip hash
ip hash是nginx内置的另一个负载均衡的策略，流程和轮询很类似，只是其中的算法和具体的策略有些变化，如下图所示：
![](http://jbcdn2.b0.upaiyun.com/2016/02/65c66b424577301ffbd9d0d4cf9dec9e.jpg)
### 2.4.3. fair
fair策略是扩展策略，默认不被编译进nginx内核。其原理是根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行分流。这种策略具有很强的自适应性，但是实际的网络环境往往不是那么简单，因此要慎用。
### 2.4.4 通用hash、一致性hash
这两种也是扩展策略，在具体的实现上有些差别，通用hash比较简单，可以以nginx内置的变量为key进行hash，一致性hash采用了nginx内置的一致性hash环，可以支持memcache。
### 2.5场景
Ngnix一般作为入口负载均衡或内部负载均衡，结合反向代理服务器使用。以下架构示例，仅供参考，具体使用根据场景而定。
#### 2.5.1入口负载均衡架构
![](http://jbcdn2.b0.upaiyun.com/2016/02/831dd8e7f4f531984bac576d3fcaf5ae.png)
Ngnix服务器在用户访问的最前端。根据用户请求再转发到具体的应用服务器或二级负载均衡服务器（LVS）
#### 2.5.2内部负载均衡架构
![](http://jbcdn2.b0.upaiyun.com/2016/02/1ec09963392bd8f07a07283a51fef2fb.png)
LVS作为入口负载均衡，将请求转发到二级Ngnix服务器，Ngnix再根据请求转发到具体的应用服务器。
#### 2.5.3Ngnix高可用
![](http://jbcdn2.b0.upaiyun.com/2016/02/c2bfd91f22e7fe7a9956295a3688f31e.png)
分布式系统中，应用只部署一台服务器会存在单点故障，负载均衡同样有类似的问题。一般可采用主备或负载均衡设备集群的方式节约单点故障或高并发请求分流。
Ngnix高可用，至少包含两个Ngnix服务器，一台主服务器，一台备服务器，之间使用Keepalived做健康监控和故障检测。开放VIP端口，通过防火墙进行外部映射。
DNS解析公网的IP实际为VIP。
## 三、LVS负载均衡
LVS是一个开源的软件，由毕业于国防科技大学的章文嵩博士于1998年5月创立，用来实现Linux平台下的简单负载均衡。LVS是Linux Virtual Server的缩写，意思是Linux虚拟服务器。
基于IP层的负载均衡调度技术，它在操作系统核心层上，将来自IP层的TCP/UDP请求均衡地转移到不同的 服务器，从而将一组服务器构成一个高性能、高可用的虚拟服务器。
操作系统：Liunx
开发语言：C
并发性能：默认4096，可以修改但需要重新编译。
### 3.1.功能
LVS的主要功能是实现IP层（网络层）负载均衡，有NAT,TUN,DR三种请求转发模式。
#### 3.1.1LVS/NAT方式的负载均衡集群
NAT是指Network Address Translation，它的转发流程是：Director机器收到外界请求，改写数据包的目标地址，按相应的调度算法将其发送到相应Real Server上，Real Server处理完该请求后，将结果数据包返回到其默认网关，即Director机器上，Director机器再改写数据包的源地址，最后将其返回给外界。这样就完成一次负载调度。
构架一个最简单的LVS/NAT方式的负载均衡集群Real Server可以是任何的操作系统，而且无需做任何特殊的设定，惟一要做的就是将其默认网关指向Director机器。Real Server可以使用局域网的内部IP(192.168.0.0/24)。Director要有两块网卡，一块网卡绑定一个外部IP地址 (10.0.0.1)，另一块网卡绑定局域网的内部IP(192.168.0.254)，作为Real Server的默认网关。
LVS/NAT方式实现起来最为简单，而且Real Server使用的是内部IP，可以节省Real IP的开销。但因为执行NAT需要重写流经Director的数据包，在速度上有一定延迟；
当用户的请求非常短，而服务器的回应非常大的情况下，会对Director形成很大压力，成为新的瓶颈，从而使整个系统的性能受到限制。
#### 3.1.2LVS/TUN方式的负载均衡集群
TUN是指IP Tunneling，它的转发流程是：Director机器收到外界请求，按相应的调度算法,通过IP隧道发送到相应Real Server，Real Server处理完该请求后，将结果数据包直接返回给客户。至此完成一次负载调度。
最简单的LVS/TUN方式的负载均衡集群架构使用IP Tunneling技术，在Director机器和Real Server机器之间架设一个IP Tunnel，通过IP Tunnel将负载分配到Real Server机器上。Director和Real Server之间的关系比较松散，可以是在同一个网络中，也可以是在不同的网络中，只要两者能够通过IP Tunnel相连就行。收到负载分配的Real Server机器处理完后会直接将反馈数据送回给客户，而不必通过Director机器。实际应用中，服务器必须拥有正式的IP地址用于与客户机直接通信，并且所有服务器必须支持IP隧道协议。
该方式中Director将客户请求分配到不同的Real Server，Real Server处理请求后直接回应给用户，这样Director就只处理客户机与服务器的一半连接，极大地提高了Director的调度处理能力，使集群系统能容纳更多的节点数。另外TUN方式中的Real Server可以在任何LAN或WAN上运行，这样可以构筑跨地域的集群，其应对灾难的能力也更强，但是服务器需要为IP封装付出一定的资源开销，而且后端的Real Server必须是支持IP Tunneling的操作系统。
#### 3.3.3LVS/TUN方式的负载均衡集群
DR是指Direct Routing，它的转发流程是：Director机器收到外界请求，按相应的调度算法将其直接发送到相应Real Server，Real Server处理完该请求后，将结果数据包直接返回给客户，完成一次负载调度。
构架一个最简单的LVS/DR方式的负载均衡集群Real Server和Director都在同一个物理网段中，Director的网卡IP是192.168.0.253，再绑定另一个IP： 192.168.0.254作为对外界的virtual IP，外界客户通过该IP来访问整个集群系统。Real Server在lo上绑定IP：192.168.0.254，同时加入相应的路由。
LVS/DR方式与前面的LVS/TUN方式有些类似，前台的Director机器也是只需要接收和调度外界的请求，而不需要负责返回这些请求的反馈结果，所以能够负载更多的Real Server，提高Director的调度处理能力，使集群系统容纳更多的Real Server。但LVS/DR需要改写请求报文的MAC地址，所以所有服务器必须在同一物理网段内。
### 3.3架构
LVS架设的服务器集群系统有三个部分组成：最前端的负载均衡层（Loader Balancer），中间的服务器群组层，用Server Array表示，最底层的数据共享存储层，用Shared Storage表示。在用户看来所有的应用都是透明的，用户只是在使用一个虚拟服务器提供的高性能服务。
LVS的体系架构如图：
![](http://images2015.cnblogs.com/blog/820332/201512/820332-20151227220009109-1768809526.png)
LVS的各个层次的详细介绍：
Load Balancer层：位于整个集群系统的最前端，有一台或者多台负载调度器（Director Server）组成，LVS模块就安装在Director Server上，而Director的主要作用类似于一个路由器，它含有完成LVS功能所设定的路由表，通过这些路由表把用户的请求分发给Server Array层的应用服务器（Real Server）上。同时，在Director Server上还要安装对Real Server服务的监控模块Ldirectord，此模块用于监测各个Real Server服务的健康状况。在Real Server不可用时把它从LVS路由表中剔除，恢复时重新加入。
Server Array层：由一组实际运行应用服务的机器组成，Real Server可以是WEB服务器、MAIL服务器、FTP服务器、DNS服务器、视频服务器中的一个或者多个，每个Real Server之间通过高速的LAN或分布在各地的WAN相连接。在实际的应用中，Director Server也可以同时兼任Real Server的角色。
Shared Storage层：是为所有Real Server提供共享存储空间和内容一致性的存储区域，在物理上，一般有磁盘阵列设备组成，为了提供内容的一致性，一般可以通过NFS网络文件系统共享数 据，但是NFS在繁忙的业务系统中，性能并不是很好，此时可以采用集群文件系统，例如Red hat的GFS文件系统，oracle提供的OCFS2文件系统等。
从整个LVS结构可以看出，Director Server是整个LVS的核心，目前，用于Director Server的操作系统只能是Linux和FreeBSD，linux2.6内核不用任何设置就可以支持LVS功能，而FreeBSD作为 Director Server的应用还不是很多，性能也不是很好。对于Real Server，几乎可以是所有的系统平台，Linux、windows、Solaris、AIX、BSD系列都能很好的支持。
### 3.4均衡策略
LVS默认支持八种负载均衡策略，简述如下：
#### 3.4.1.轮询调度（Round Robin）
调度器通过“轮询”调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。
#### 3.4.2.加权轮询（Weighted Round Robin）
调度器通过“加权轮询”调度算法根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。
#### 3.4.3.最少链接（Least Connections）
调度器通过“最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，采用“最小连接”调度算法可以较好地均衡负载。
#### 3.4.4.加权最少链接（Weighted Least Connections）
在集群系统中的服务器性能差异较大的情况下，调度器采用“加权最少链接”调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。
#### 3.4.5.基于局部性的最少链接（Locality-Based Least Connections）
“基于局部性的最少链接”调度算法是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用“最少链接” 的原则选出一个可用的服务器，将请求发送到该服务器。
#### 3.4.6.带复制的基于局部性最少链接（Locality-Based Least Connections with Replication）
“带复制的基于局部性最少链接”调度算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。它与LBLC算法的不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，按“最小连接”原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。
#### 3.4.7.目标地址散列（Destination Hashing）
“目标地址散列”调度算法根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。
#### 3.4.8.源地址散列（Source Hashing）
“源地址散列”调度算法根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。
除具备以上负载均衡算法外，还可以自定义均衡策略。
### 3.5场景
一般作为入口负载均衡或内部负载均衡，结合反向代理服务器使用。相关架构可参考Ngnix场景架构。
## 4、HaProxy负载均衡
HAProxy也是使用较多的一款负载均衡软件。HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，是免费、快速并且可靠的一种解决方案。特别适用于那些负载特大的web站点。运行模式使得它可以很简单安全的整合到当前的架构中，同时可以保护你的web服务器不被暴露到网络上。
### 4.1.特点
- 支持两种代理模式：TCP（四层）和HTTP（七层），支持虚拟主机；
- 配置简单，支持url检测后端服务器状态；
- 做负载均衡软件使用，在高并发情况下，处理速度高于nginx；
- TCP层多用于Mysql从（读）服务器负载均衡。 （对Mysql进行负载均衡，对后端的DB节点进行检测和负载均衡）
- 能够补充Nginx的一些缺点比如Session的保持，Cookie引导等工作
### 4.2.均衡策略
支持四种常用算法：
1.roundrobin：轮询，轮流分配到后端服务器；
2.static-rr：根据后端服务器性能分配；
3.leastconn：最小连接者优先处理；
4.source：根据请求源IP，与Nginx的IP_Hash类似。
## 五、本次分享总结
以上是本周的分享，从主要讲解了软件负载均衡的应用背景，Ngnix负载均衡，LVS负载均衡，Haproxy负载均衡。
因为时间关系，有些讲解的不细致，大家可以问下度娘/Google，希望本次分享对大家有帮助。
### 参考资料：
- Nginx负载均衡实现原理图解 http://www.server110.com/nginx/201403/7225.html
- Nginx架构及其web服务搭建优化配置详解
- http://linux.it.net.cn/e/server/nginx/2015/0102/11183.html
- Ngnix双主场景：http://network.51cto.com/art/201109/288597.htm
- 用LVS构架负载均衡Linux集群系统 linux lvs
- http://blog.chinaunix.net/uid-45094-id-3012037.html
- LVS基本介绍
- http://os.51cto.com/art/201202/317108.htm
