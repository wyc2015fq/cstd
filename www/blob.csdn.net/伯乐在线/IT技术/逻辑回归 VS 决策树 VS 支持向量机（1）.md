# 逻辑回归 VS 决策树 VS 支持向量机（1） - 文章 - 伯乐在线
本文由 [伯乐在线](http://blog.jobbole.com) - [土豆粉ss](http://www.jobbole.com/members/nightwishss) 翻译，[耶鲁怕冷](http://www.jobbole.com/members/feixuan) 校稿。未经许可，禁止转载！
英文出处：[Lalit Sachan](http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part1/)。欢迎加入[翻译组](https://github.com/jobbole/translation-project)。
当我们处理行业之间的一些标准业务问题时，分类问题是我们所需解决的主要难题之一。有许多方法都可以同样解决这个问题，在本文中，我们将会讨论其中三个主要方法：逻辑回归、决策树和支持向量机（SVM）。
这三个算法都可以用来分类（SVM 和决策树也可以用于回归，但今天我们这里不作讨论！）。我经常见到人们会问该选哪个方法来解决他们的某个特定问题。一个经典的通常也是正确的但又不怎么令人满意的回答是“这要视情况而定！”我同意这个回答确实十分烦人，所以我决定说明一下“视什么情况而定”。
这里将从二维角度给出一个很简单的解释，同时这个解释也可以推及到高维数据的情形，后面的麻烦就交给读者了。
我从这个最重要的问题开始讨论：我们到底要在分类中具体做什么？好吧，我们要试图（对数据）分类。（这真的是个严肃的问题吗？真的吗？）让我修改一下这个回答的措辞。为了分类，我们试图得到一个决策的边界或者曲线（并不需要一定是直线），来把我们的特征空间分成两类。
特征空间听起来是个非常花哨的词，会困扰很多以前没有遇到过这个词的人。我举个例子来解释一下它。我有一个带有三个变量的样本数据：x1，x2，target（目标值）. Target 的取值为 0 和 1 ，取值取决于自变量 x1 和 x2。如图所示：
![](http://ww2.sinaimg.cn/mw690/6941baebgw1f1qj23ghtcj20jg0fkq4o.jpg)
这个图正是你的特征空间，你的观察都是取自这里。这个例子里，因为我们只有两个自变量/特征，特征空间是二维的。这里你能看到你的 target 分成了两类，被不同的颜色标记。我们希望我们的算法能给我们一条直线或曲线，可以把这两类数据分开。
我们可以形象地看到，理想的决策边界（或者叫分离曲线）会是个圆。逻辑回归、决策树和 SVM 三者的区别就在于生成的决策边界的形状。让我们从逻辑回归开始说明。我们很多人都会对逻辑回归方法给出的决策边界的形状感到疑惑。产生这种疑惑的主要原因是我们在逻辑回归的文章中看到了太多次著名的 S 型曲线。
![](http://ww4.sinaimg.cn/mw690/6941baebgw1f1qj23i2wcj20jg0fkt9k.jpg)
图中你看到的蓝色曲线并不是决策边界。我们用逻辑回归的方法建立了二值响应模型，这个图仅仅是对它转换个形式表现出来。逻辑回归的决策边界总是一条线（或者一个面、或者一个在更高维上的超平面）。说服你的最好方法是通过你们都非常熟悉的逻辑回归方程来说明。
![3](http://www.edvancer.in/wp-content/uploads/2015/10/83662c8693f749deb8b5c6c7a51f5f65.png)
为了简化模型，我们假设 F 是一个所有自变量的线性组合。
![4](http://www.edvancer.in/wp-content/uploads/2015/10/ebbd5d2e58c32bb9632c505ac47a9151.png)
上面的等式也可以写作：
![5](http://www.edvancer.in/wp-content/uploads/2015/10/83662c8693f749deb8b5c6c7a51f5f65.png)
现在用逻辑回归来预测，你为（每个数据点分类的）概率值设置一个特定的阈值，这样预测值就会是 1，否则就是 0。我们设置阈值为 c。因此你的决策过程将会像这样：
如果 p>c,则 Y=1，否则 Y=0. 最终会给出一个决策边界 F>常量。
F>常量，这里只是一个线性决策边界。用逻辑回归处理我们的样本数据将会如图所示。
![](http://ww3.sinaimg.cn/mw690/6941baebgw1f1qj22rrdsj20jg0fkmz4.jpg)
可以发现，逻辑回归的效果并不怎么好。因为无论你怎么做，逻辑回归生成的决策边界总会是线性的，并不能取得我们需要的一个圆形的决策边界。因此，逻辑回归可以用于数据基本能线性可分的分类问题。（尽管在一些例子中可以通过变量转化使其线性可分，但是我们留着这个问题改天讨论）
现在让我们看看决策树是怎么处理这些问题的。我们知道决策树是由分级的变量规则构成的。下面给出依据我们样本数据所做的一个决策树例子。
![](http://ww3.sinaimg.cn/mw690/6941baebgw1f1qj219vhqj20jg0fkjrh.jpg)
仔细思考一下，这些决策规则– x1、x2 **大于或小于一个常数** –只不过是用同特征空间坐标轴平行的直线划分了特征空间，如下图所示：
![](http://ww4.sinaimg.cn/mw690/6941baebgw1f1qj20zcv0j20jg0fkgni.jpg)
我们增加树的规模让它更复杂，这会导致对特征空间进行更多的划分，从而尝试模拟一个圆形边界出来。
![](http://ww3.sinaimg.cn/mw690/6941baebgw1f1qj20013pj20jg0fkdhv.jpg)
哈~并不是一个万众期待的圆形，但是它尽力了。如果继续增加树的规模，可以注意到决策边界会用足够多的平行线仿真出一个圆形。所以，如果边界是非线性的，并可以通过把特征空间切割成矩形来近似边界（或者在高维空间中，用立方体或超立方体来逼近）那么决策树就是个比逻辑回归更好的选择了。
接下来，我们看看 SVM 的结果。把特征空间投射到核空间中，使数据的类别线性可分，这就是 SVM 的工作。用一个更简单的解释来说明这个过程，便是 SVM 给特征空间添加了一个额外的维度，这个变换方法可以让类别数据线性可分。当投射回原来的特征空间后，二维的平面决策边界生成的是非线性的决策边界。看下面的图，更清晰易懂。
![](http://ww2.sinaimg.cn/mw690/6941baebgw1f1qj1ztzgcj20jg0c0gob.jpg)
可以发现，一旦通过某种特殊方法给数据添加一个维度，我们就可以用一个平面（线性分离器）来分离两类数据了。这个屏幕一旦投射回原来的二维特征空间就就变成了一个圆形的边界。
看 SVM 在我们的样本数据上表现的多出色：
![](http://ww4.sinaimg.cn/mw690/6941baebgw1f1qj1yknbqj20jg0fkq53.jpg)
*注意：决策边界并不是一个饱满的圆，但却是圆的一个很好的逼近（多边形）我们只是画个简单的圆来展示，避免了用软件画一个复杂的多边形的麻烦而已。*
到现在为止，三个算法的区别可以明白了吧，但是还有一个问题没有解决。那就是当处理多维数据的时候，该怎么选择这三个算法呢？这是个挺重要的问题。因为当考虑超过三个自变量的情况时，你没有方便的做法把数据可视化。我们将在[第二部分](http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/)发表，不要走开！
