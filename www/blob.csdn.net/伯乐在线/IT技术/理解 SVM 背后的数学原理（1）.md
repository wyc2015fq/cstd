# 理解 SVM 背后的数学原理（1） - 文章 - 伯乐在线
本文由 [伯乐在线](http://blog.jobbole.com) - [方小达](http://www.jobbole.com/members/ufangd123) 翻译，[sunshinebuel](http://www.jobbole.com/members/sunshinebuel) 校稿。未经许可，禁止转载！
英文出处：[Alexandre KOWALCZYK](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/)。欢迎加入[翻译组](https://github.com/jobbole/translation-project)。
## 介绍
这是「SVM 背后的数学原理」系列文章的第一篇。虽然许多人说要了解 SVM，必须要有充分的数学背景知识，但我会尽量由浅入深慢慢地讲解，以便每一个细节都说清楚，甚至对于初学者来说也能够理解。
首先，我们能够从定义中看出SVM需要训练数据，也就是说它是一种监督学习算法。
知道 SVM 是一种分类算法也是十分重要的，这意味着我们将使用它去预测某个东西是否属于特定的类别。
## 支持向量机（SVM）的目标是什么？
> 
支持向量机的目标是找出能够最大化训练集数据间隔（margin）的最优分类超平面。
例如，我们拥有如下的训练数据：
![01_svm-dataset1](http://jbcdn2.b0.upaiyun.com/2016/05/e5def489d5c579e7bd980a60235affae.png)
我们已经绘制了人群的身高和体重散点图，也用不同的标记区分了男人和女人。
有了这些数据，我们将能够使用SVM回答下面几个问题：
> 
给定一个具体的数据点（身高和体重），这个人是男人还是女人？ 例如：如果知道某人身高175cm体重80kg，这个人是男人还是女人？
通过观察上图，我们能够发现分类这些数据是可能的。例如，我们可以描绘一条直线然后所有代表男人的点都在直线的上边，代表女人的点都在直线的下边。
## 什么是分类超平面？
这条直线被称为**分类超平面**，如下图所示：
![01_svm-dataset1-separated](http://jbcdn2.b0.upaiyun.com/2016/05/2d5ef46f46f23cb34b5c5135827b1e4d.png)
**如果它就是一条线，为什么我们称它为超平面呢？**
虽然这次我们是在二维空间使用一个十分简单的数据集例子，但是支持向量机能够在任意维度下工作！
超平面是平面的抽象。
- 在一维空间，超平面是一个点
- 在二维空间，它是一条线
- 在三位空间，它是一个面
- 在更高维度上，你能够称它为超平面
![separating-hyperplane](http://jbcdn2.b0.upaiyun.com/2016/05/cfbdf46223959abd09cf9a2d063252cf.png)
在一维空间中，点L是一个分类超平面。
## 什么是最优分类超平面？
你能够找到一个分类超平面，但这并不意味着它是最好的那个！在下面的例子中有几个分类超平面，每个都成功地将我们的数据集分类为男人和女人两部分。
![01_svm-dataset1-separated-2](http://jbcdn2.b0.upaiyun.com/2016/05/731573f4043a7270fdd6a580dac575b0.png)
存在许多的分类超平面
假设我们选择绿色的超平面并且使用它给真实数据分类。
![01_svm-dataset1-separated-bad](http://jbcdn2.b0.upaiyun.com/2016/05/0d642c9340ec1200b5c9b997bfe1d388.png)
这个超平面并不能够很好的分类数据。
这次，它分类出现了错误。明显，我们能够看出，如果我们选择了一个靠近某一类数据点的超平面，它也许并不能很好地分类数据。
因此我们将会尝试选择一个**尽可能远离每一种类别数据点的**超平面：
![01_svm-dataset-optimal-hyperplane](http://jbcdn2.b0.upaiyun.com/2016/05/e8b5ee1c3bcfde70d9abafce77f3a408.png)
这一个看起来更好。当我们用它分类真实数据中时，它仍然进行了完美的分类。
![01_svm-dataset1-separated-good](http://jbcdn2.b0.upaiyun.com/2016/05/8eb3c23b371d9d7c1e30c73d0e0f1ac2.png)
黑色的超平面比绿色的超平面分类更准确。
这就是为什么SVM的目标是**寻找最优分类超平面**：
- 因为它能够正确地分类训练数据
- 同时因为它能更准确地分类尚未出现的数据。
## 什么是间隔和它是如何帮助选择最优超平面？
![07_withMidpointsAndSeparator1](http://jbcdn2.b0.upaiyun.com/2016/05/7939f37f1078b20fe6606d0527858322.png)
我们的最优超平面的间隔
给定一个超平面，我们能够计算出超平面到最近的一个点的距离。一旦我们算出这个值，如果我们将距离乘以2我们就可以得到**间隔**（margin）。
基本上，间隔是一个“无人区”。在间隔内不存在任何数据点。
对于另一个超平面，间隔将看起来像这样：
![](http://ww1.sinaimg.cn/large/7cc829d3gw1f4nypboa11j20jf0etdha.jpg)
如图所示，间隔B比间隔A小得多。
我们能够观察到以下结果：
- 如果一个超平面十分接近某个数据点，它的间隔将很小
- 超平面距离数据点越远，间隔就越大
这意味着**最优超平面将是拥有最大边距的那个超平面**。
这也是为什么SVM的目标是**找到最大化训练集数据间隔的最优分类超平面**。
到这里关于SVM背后的数学原理的介绍就结束了，目前没有有多少公式，但是在下篇文章我们将增加一些数字然后试着从数学的视角（几何和向量）来进行理解。
