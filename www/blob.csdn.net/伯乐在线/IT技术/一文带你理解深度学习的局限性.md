# 一文带你理解深度学习的局限性 - 文章 - 伯乐在线
原文出处： [Francois Chollet](https://blog.keras.io/the-limitations-of-deep-learning.html)   译文出处：[CSDN - 聂震坤](http://www.iteye.com/news/32557)
### **深度学习：几何视图**
深度学习最令人惊讶的特点便是极易上手。十年以前，没有人可以猜得到经过梯度下降法训练过的简单参数模型可以在机器感知问题上实现如此惊人的结果。现在，事实证明，研究者所需要做的只是使用足够大的梯度下降素材来训练参数模型。正如 Feynman 曾经描述宇宙一样，“它并不复杂，只是里面包含的东西实在太多。”
在深度学习中，所有的东西都是一个向量（任何的东西都是几何空间中的一个点）。模型的输入（可能是文字，图片，等等）和目标首先被向量化（变成了一些初始输入向量空间和目标向量空间）。深度学习模型中的每个层都对通过它的数据进行一个简单的几何变换。同时，模型的层链形成一个非常复杂的几何变换，分解成一系列简单的几何变换。这种复杂的转换尝试将输入空间映射到目标空间，一次一个点。该变换按层级的比重进行参数化，这些层根据模型当前执行的程度进行迭代更新。这种几何变换的一个关键特征是它必须是可微分的，这是为了使我们能够通过梯度下降来学习来获得其参数。更直接一点，这意味着从输入到输出的几何变形必须是平滑和连续的——这是一个显着的约束。
这种复杂的几何变换应用到输入数据的整个过程可以理解为一个试图破坏纸球的人在3D中被可视化。褶皱的纸球可以理解为模型开始时的大量输入数据。每一次人的行为则是数据通过不同层级被处理的过程。完整的不起眼的人的手势序列是整个模型的复杂变换。深度学习模型是用于解决高维数据的复杂流形的数学机器。
这便是深度学习的奥妙：将含义转化为向量，几何空间，然后逐步学习将一个空间映射到另一个空间的复杂几何变换。所以开发者需要有足够高的维度的空间以便拥有原始数据关系的全部范围。
**深度学习的局限性**
可以通过这种简单策略实现的应用程序空间几乎是无限的。然而到目前为止，更多的应用程序对于当前的深入学习技术是完全无法实现的——即使是大量人为注释的数据。例如，你可以组装一个数据，数十万甚至数百万的英文语言描述软件产品的功能。由产品经理编写，由一个团队开发的相应的源代码并且有工程师满足这些要求。即使有这些数据，也不能训练出一个深入的学习模型来简单地阅读产品描述并生成相应的代码库。这只是其中一个例子。
一般来说，无论投入多少数据，任何需要推理式编程或应用科学方法（长期规划和类似算法的数据操作）都无法使用深度学习模式进行实现。 即使是学习具有深层神经网络的排序算法也是非常困难的。
这是因为一个深入的学习模型是“简单的”连续的几何变换链，将一个向量空间映射到另一个。他所可以做的是将一个数据包X映射到另一个数据包Y，并假设存在从X到Y的可学习的连续变换与X:Y的秘籍采用可用性作的数据。所以即使一个深入的学习模型可以被解释为一种程序，相反的大多数程序不能表示为深度学习模型 – 对于大多数任务，要么没有相应的实际大小的深层神经网络来解决任务，要么即使存在一个，它也可能不可学习，即相应的几何变换可能太复杂，或者可能没有可用的数据来学习它。通过堆叠更多层次和使用更多的培训数据来扩展当前的深度学习技术，只能表面缓解其中的一些问题。 深度学习模式在可以代表的方面是非常有限的，而且大多数希望学习的程序不能被表达为连续几何变形的数据流。
**拟人化机器学习模型的隐患**
现阶段人工智能的一个不得不面对的问题在于研究者们误解了深度学习模式的做法，并高估了自己的实力。人类思想的一个根本特征是我们的“主观意识”，我们倾向于对我们周围的事物投射意向，信仰和知识。在一个石头上画一个笑脸感觉石头都变高兴了。同样在深度学习中，当我们有能力成功地训练一个模型来创作描述图片的标题时，我们误认为该模型“理解”图片的内容以及它产生的标题。然而，当轻微改变训练数据中存在的图像时，研发者会非常惊讶的发现模型开始创作完全荒谬的字幕。
![](http://jbcdn2.b0.upaiyun.com/2017/08/80b358a0334bd3f848b9040a6ecc43b0.png)
### 通常，这类行为被强调为对抗例子，是通过向深度学习网络输出错误的样本来欺骗模型进行分类。通过梯度上升，可以稍微修改图像，以便最大化给定类的预测。 通过拍摄一只熊猫，并添加一个“长臂猿”梯度，我们可以得到一个神经网络将这只熊猫分类为长臂猿。 这证明了这些模型的脆弱性，以及它们运行的输入到输出映射与我们自己的人类感知之间的深刻差异。
![](http://jbcdn2.b0.upaiyun.com/2017/08/28bc6f2c7b510a1638a064d2f67bb284.png)
### 简而言之，深度学习模式对他们的输入没有任何理解，至少不是从人的意义上来说。我们对图像，声音和语言的理解是基于我们的运动感觉体验。然而机器人却没有这种经验，因此不能以任何以人为本的方式理解他们的驶入。通过输入大量的培训示例来训练我们的模型，我们让他们学习一个几何变换，将数据映射到这个特定的一组示例上的人类概念，但是这个映射只是我们思想中原始模型的简单草图， 从我们的体验中发展出来的假想——就像一面镜子中的一个昏暗的形象。
![](http://jbcdn2.b0.upaiyun.com/2017/08/1ed2a28a045dfe53e0409369570c5fb6.png)
### 作为一名机器学习从业者，始终要注意这一点，不要陷入陷阱，认为神经网络了解他们所执行的任务（并不会） – 至少不会对我们有意义。 他们接受了不同于我们教导，进行了不同的，更窄的任务的训练：即将训练投入与训练目标进行比较。 如果向他们展示偏离训练数据的任何东西，机器将以最荒谬的方式打破以前的结论。
**局部泛化与极端泛化**
在深度学习模型所做的输入与输出之间的简单的几何变形以及人类思考和学习的方式之间似乎存在着根本的区别。人类不仅仅是从亲身体验中进行学习，还提供了明确的学习方法。除了不同的学习过程外，底层的特性有着根本的却别。
人类的能力远不止对刺激做出回应，因为深网或者昆虫也可以。人会对目前的情况形成抽象复杂的模型，并根据这些模型来预测不同的未来，且进行长期的规划。人类有能力将已知的概念融合在一起，来表现以前从未经历过的事情，比如穿着牛仔裤的马，或者想象如果中了彩票会做什么。这种处理假设的能力，使我们的心理模型空间远远超过了我们可以直接体验到的一切。这种行为被称为极端泛化。
这与深层网络的局部泛化形成了鲜明的对比：如果训练时间内新的输入信息差异甚微，则停止从输入到输出的映射。假设使用机器来学习适当的发射参数以解决火箭登陆月球的问题，在使用深网进行此项任务时，无论是使用监督学习还是强化学习进行培训，都需要数以千万次的启动试验作为补充，以便学习从入空间到输出空间的可靠映射。相比之下，人类可以利用抽象能力来提出物理模型——火箭科学，并得到一个精确地解决方案，只需要一次或几次时间即可以让火箭成功登月。同理，如果开发一款控制人体的深网，并希望他学会安全的在城市里行驶。那么在在这种情况下则需要现在各种情况下死掉数千次直到可以推断出危险的行车路线并制定合适的回避行为。当进入一个新的城市，深网有需要重新学习大部分知识。相反，人类能够直接学习安全行为而不用经历死亡，这都要归功于人脑的抽象建模。
![](http://jbcdn2.b0.upaiyun.com/2017/08/cff17cac3a2dbd53c2f8a966899831d1.png)
### 简而言之，尽管我们在机器感知方面取得了不小的进步，但是我们仍然离人类级别的人工智能有很大区别。我们的模型只能执行局部泛化，在过去中学习来适应最新的情况。而人类可以极端泛化，迅速适应大胆新奇的情况，或为长期未来的情况进行规划。
**总结**
作为一个开发者应当记住：迄今为止，深入学习的唯一真正的成功是使用大量的人为注释的数据让机器使用连续的几何变换将空间X映射到空间Y。 做好这一切，则可以改变行业的游戏规则。但想让AI拥有人类的智慧，仍然有很长的路要走。
