# 机器是如何学会歧视的？机器学习中的偏见 - 文章 - 伯乐在线
本文由 [伯乐在线](http://blog.jobbole.com) - [maifans](http://www.jobbole.com/members/maifansnet) 翻译，[郑芸](http://www.jobbole.com/members/paradise_FD) 校稿。未经许可，禁止转载！
英文出处：[Jesse Emspak](https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/)。欢迎加入[翻译组](https://github.com/jobbole/translation-project)。
【导读】：数据的选用和人的主观想法往往会在机器学习中产生偏见，例如，用于训练的数据大多是白人而导致讲黑人的照片识别为猩猩，或是研究人员认为可以通过长相来识别这个人生是不是罪犯（然而被证实是完全错误的）。因此，选取合适的数据以及设计好的算法对于机器学习来说非常重要，否则只能是“垃圾进，垃圾出”。
> 
AI 是从人类创造者那里学到了偏见，与它那刻板，冰冷的逻辑无关
倘若 AI 接管了我们的生活，不大会发生人类陷入与冷酷使用 Spock 式逻辑的机器人军队的战争中，因为 AI 已在肉体上奴役我们了。相反， AI 程序已经通过机器学习算法为我们推荐喜欢的电影、在照片上认出哪个朋友可能会拒绝给你借钱、把警察带到你邻居家，或者告诉医生你需要节食。既然人类创造了这些算法，它们就容易产生偏见，从而导致糟糕的决策和更糟糕的结果。
补充：Spock  是《星际迷航》中的一个人物。如下图。
![](http://wx1.sinaimg.cn/mw690/63918611gy1ff7zteih3pj218g0xcjvv.jpg)
这些偏差也直接让人们对人工智能日益依赖的现状产生担忧。因为人类设计的绝对“中立”的 AI 系统仍可能固化人们的歧视，而不是识破它。例如，尽管程序本身没有明显的考虑种族因素，但[算法将黑人嫌犯标记为更有可能在将来犯罪](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)，而使用该软件的执法机构遭到了批评。
主要问题发生在两个方面：
- 一、用于校准机器学习算法的数据有时不足；
- 二、算法本身设计很糟糕。机器学习是软件开发人员利用大量与任务相关的数据训练 AI 算法的过程。
算法在初始提供的数据中获得模式，使其能在新的数据中识别类似的模式。但是它并不总是按我们认为的那样工作，而且有时结果很糟糕。例如，2015年6月， Google 的照片分类系统将两名非裔美国人识别为“大猩猩”。尽管谷歌很快解决了这个问题，但是微软 AI 研究员凯特·克劳福德在[《纽约时报》](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html)上表示这个错误反映了 AI 中更大的“白人问题”。也就是说，用于训练软件的数据太依赖于白人照片，降低了其准确识别具有不同容貌的人的照片的能力。
2017 年 1 月，Facebook 的用户新闻推送被大量虚假新闻淹没，也凸显了 AI 的偏差问题。 Facebook 的热门新闻算法是根据参与度（用户的点击或分享频率）来确定新闻排序的 ，没有考虑到新闻的真实性。 2016 年 11月初，有新闻媒体透露，一群马其顿青年在美国大选期间[愚弄了Facebook的“新闻推送”算法](https://www.buzzfeed.com/craigsilverman/how-macedonia-became-a-global-hub-for-pro-trump-misinfo)，以推送吸引右派选民的虚假新闻。 Facebook 表示已经修改了算法，并宣布了帮助 [Snopes](http://www.snopes.com/) 、[Factcheck.org](http://factcheck.org/) 、[ABC News](http://abcnews.go.com/) 和 [PolitiFact](http://www.politifact.com/) 清除明显的虚假新闻的计划。
马里兰大学计算机系副教授 Hal DauméIII 说：“这有点像「俄罗斯坦克问题」”。 这个典故产出自20世纪80年代，机器学习发展的早期。虽然它的真实性存在疑问，但它有一定的代表性，而且常常被计算机系的老师引用。这个故事说，美国军方试图训练一台电脑来区分俄罗斯和美国坦克的照片。 Daumé解释说：“它们的分类准确度非常高，但俄罗斯坦克的所有照片都模糊不清，而美国坦克是高清晰度的。” 该算法不是识别坦克，而是学习区分模糊和清晰的照片。
尽管有这些已知的缺陷，研究人员最近开发了一种算法，可以通过面部识别技术来辨认罪犯。 中国上海交通大学研究人员吴晓琳（音译，以下简称吴）和张希（音译，以下简称张）使用了一个有 1856 张面部照片的数据库训练机器学习算法，其中 730 人是罪犯，另外 1126 人不是。 在分析了 90％ 的图片后，AI 能够正确地识别剩余 10％ 照片中的罪犯。
该算法将具体的面部特征与罪犯相关联。例如罪犯的眼角、嘴唇曲率和鼻尖更有可能存在特定的空间关系。吴说，他也认同如果一个人有这些特征，不能说这个人更可能是罪犯。 吴同时发现罪犯的面孔差异较大，而非罪犯往往具有相似的面部特征。
吴继续使用以前没有看过的不同的照片来测试算法，发现它多半情况下可以正确识别罪犯。研究人员试图通过只使用没有面部毛发和疤痕的中国青年或中年男子的面部图片进行训练和测试其算法来避免偏差。
吴说：“我本来想证明面相学是错的”，他指的是基于面部特征评估性格的有数百年历史的伪科学。 “结果却令我们大吃一惊。”虽然这项研究似乎可以验证了面相学的某些方面，但吴承认，使用这种技术来识别罪犯将是“疯狂的”，没有计划在执法机关中应用。
其他科学家说吴和张的发现可能会强化现有的偏见。 Google 机器学习领域的首席科学家 Blaise Agüera y Arcas 指出，司法系统决定受试者的犯罪行为，但它执行着人类有偏见的决策（或许是潜意识的）。论文的核心问题是它依赖这一系统“作为标记犯罪分子的依据，然后基于人类的判断得出结论：机器学习是无偏见的。” Agüera y Arcas 补充说。
研究计算机视觉的华盛顿大学数学系助理教授 Kyle Wilson 如是说：「吴和他的同事“直接得出结论，他们发现了自然界的一个潜在的模式 —— 面部结构可以预示犯罪行为。这是一个非常愚蠢的结论。」他还表示，这个算法可能只是人类的偏见在一个特定的司法系统中的反映，并且在其他国家也可能有同样的问题。 Wilson 说：“基于偏见在刑事司法系统中的表现，相同的数据和工具可以用来更好地了解（人的）偏见。 而不是教一台电脑来重现这些人类的偏见。”
还有人说，通过解释电脑学习的模式中的错误，可以改进技术来消除人的偏见。瑞士 AI 实验室 Dalle Molle 人工智能研究所的科学主任 Jürgen Schmidhuber 说，人工智能系统在学习时会犯错。事实上这是肯定的，所以称之为“学习”。他指出，电脑只会从你给出的数据中学习。“你不能消除所有这些偏见的来源，就像你不能消除人类中的偏见来源，”但他补充说，首先承认问题，然后确保使用好的数据，并把算法设计好；提出正确的问题至关重要，或者记住程序员的一句老话：“垃圾进，垃圾出（Garbage in, garbage out）”。
