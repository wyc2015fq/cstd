# 如果你还不了解机器学习的简史，请看这篇文章 - 文章 - 伯乐在线
本文由 [伯乐在线](http://blog.jobbole.com) - [李大萌](http://www.jobbole.com/members/nemoland) 翻译，[艾凌风](http://www.jobbole.com/members/hanxiaomax) 校稿。未经许可，禁止转载！
英文出处：[Eren Golge](http://www.erogol.com/brief-history-machine-learning/)。欢迎加入[翻译组](https://github.com/jobbole/translation-project)。
【导读】：全面介绍机器学习的发展史，从感知机、神经网络、决策树、SVM、Adaboost到随机森林、Deep Learning。
![](http://www.erogol.com/wp-content/uploads/2014/05/test.jpg)
自科学技术和人工智能最初发展开始，科学家Blaise Pascal和Von Leibniz就思考着如何制造一台像人类一样具有智力的机器。 著名作家朱尔斯·凡尔纳，弗兰克·鲍姆（绿野仙踪），玛丽·雪莱（科学怪人），乔治·卢卡斯（George Lucas）（星球大战）梦想着具有人类行为的物种存在，甚至拥有在不同的环境中能战胜人类的技能。
![](http://www.erogol.com/wp-content/uploads/2014/05/Arts_et_Metiers_Pascaline_dsc03869.jpg)
帕斯卡机器执行减法和求和- 1642
机器学习是人工智能最重要发展分支之一，在科研领域和工业界都是非常热门的课题。企业和学校投入大量的资源来做提升知识面。最近，该领域在许多不同任务上取得了非常可观的结果，与人类的表现（交通标志的识别，机器学习达到了98.98％ – 已经超过了人类）相当。
在这里，我想分享一下机器学习发展一个粗略的时间表，并且标记了一些里程碑，但并不完整。此外，在阅读本文所有的论断时，都请在前自行脑补一句：“据我所知”。
普遍推广机器学习的第一人是Hebb，他在1949年提出了基于神经心理学习方法，被称为Hebbian学习理论。通过简单的解释，研究递归神经网络（RNN）节点之间的相关性。它记录网络上的共性，像记忆一样工作。这个论点正式地表达是这样：
> 
我们假设反射活动（或“踪迹”）的持续性或重复性会诱导细胞的持续性变化，增加其稳定性…当细胞A的轴突足够接近细胞B时，就能反复或持续刺激细胞B。此时，会在其中一个细胞或者两者中发生一些生长过程或代谢变化，来增加A的效率[1]。
![](http://www.erogol.com/wp-content/uploads/2015/11/us__en_us__ibm100__700_series__checkers__620x350.jpg)
Arthur Samuel
在1952年，IBM的亚瑟·塞缪尔（Arthur Samuel）开发了一个西洋棋的程序。 该程序能够通过棋子的位置学习一个隐式模型，为下一步棋提供比较好的走法。 塞缪尔与这个程序对局了很多次，并观察到这个程序在经过一段时间的学习后可以发挥得更好。
塞缪尔用这个程序驳倒了机器无法超越书面代码，并像人类一样学习模式的论断。 他创造并定义了“机器学习”：
> 
机器学习是一个能使计算机不用显示编程就能获得能力的研究领域。
![](http://csis.pace.edu/~ctappert/srd2011/photos/Rosenblatt-ratlab.jpg)
F. Rosenblatt
1957年，Rosenblatt再次提出以神经科学为背景的第二个机器学习模型——感知器，它与今天的机器学习模型更像。当时，这是非常激动人心的发现，实际上它比Hebbian的想法更容易实现。 Rosenblatt用下面几句话介绍了感知器模型：
> 
感知器模型的设计是针对于一般智能系统的一些基本特性，而不会纠缠于一些特定生物体通常情况下未知的特性。[2]
3 年后，Widrow [4]发明的Delta学习规则载入机器学习史册，该规则很快被用作感知器训练的实践程序。它也被称为最小二乘问题。这两个想法的结合创造了一个很好的线性分类器。 然而，感知器模型的热度在1969年被Minsky [3]所取代。他提出了著名的XOR问题，感知器无法对线性不可分割的数据进行分类。 这是Minsky对神经网络社区的反驳。 此后，神经网络的研究一直止步不前，到在80年代才有所突破。
![](http://www.cs.ru.nl/~ths/rt2/col/h10/draw-LTUdecis.GIF)
面向不可线性分割数据的 XOR 问题
尽管早在1973年由Linnainmaa [5] 以“反向自动差异化模式”为名提出了BP思想，但是直到1981年，Werbos [6]才提出将神经网络特定反向传播（BP）算法应用到多层感知器（MLP）。BP仍然是当今神经网络架构的关键组成部分。有了这些新想法，神经网络的研究再次加速。1985年至1986年间，神经网络研究人员相继提出了采用BP训练多层感知器（MLP）的理念（Rumelhart，Hinton，Williams [7] – Hetch，Nielsen [8]）
![](http://www.erogol.com/wp-content/uploads/2014/05/Hetch_Nielsen_NN.png)
From Hetch and Nielsen [8]
另一个学派，在1986年，J.R.Quinlan [9]提出了另一个非常著名的ML算法，我们称之为决策树，更具体地说是ID3算法。这是机器学习另一个主流的闪光点。此外，ID3以软件的形式发布，能够以简单的规则及其明确的推论更好地应用到实际生活中，与黑匣子神经网络模型相反。
在ID3之后，社区探索了很多不同的可用方案和算法改进（例如ID4，回归树，CART …），而且仍然是机器学习中的活跃话题之一。
![](http://www.erogol.com/wp-content/uploads/2014/05/Quinlan_ID3.png)
图：一个简单的决策树
机器学习领域最重要突破之一是Vapnik和Cortes [10]在1995年提出的支持向量机（网络）（SVM），它具有非常强的理论论证和实证结果。SVM将机器学习社区分为NN（神经网络）和SVM两个派别。 然而在2000年左右，SVM内核化版本提出之后，NN在这两个派别之间的竞争并不容易（我无法找到关于该主题的第一篇文章）。SVM在很多之前用NN模型解决的问题上得出了最佳结果。 此外，与NN模型相比，SVM能够充分利用凸优化，泛化边际理论和内核化的所有深奥知识。 因此，它可以从不同学科中获得巨大的推动力，促进理论和实践的快速发展。
![](http://www.erogol.com/wp-content/uploads/2014/05/SVM_Vapnik.png)
From Vapnik and Cortes [10]
NN模型受到的另一次重创是1 9 9 1年Hochreiter的论文[40]和2001年Hochreiter等人的论文[11]，表明在使用BP算法时，NN单位饱和后会发生梯度损失。简单来说，训练NN模型时由于单位饱和，在迭代超过一定数量后冗余，因此NN非常倾向于在短时间的时期过度拟合。
不久之前，Freund和Schapire在1997年提出了另一个实体机器学习模型，该模型采用增强的弱分类器组合，称为Adaboost。当时给这项工作的作者颁发了戈德尔奖。 Adaboost通过易于训练的弱分类器进行训练，给那些难的样本更高的权重。这种模型是许多不同任务的基础，如面部识别和检测。它也是PAC（Probably Approximately Correct）理论的一种实现。通常，所谓的弱分类器被Adaboost选为简单的判决树（单个决策树节点）。他们这样介绍Adaboost;
> 
我们研究的模型可以解释为一个广泛的，抽象的扩展的研究在线预测模型到一般决策理论的设置[11]
2001年，Breiman [2001]探索了另一个综合模型，该模型集合了多个决策树，其中每个决策树的训练集都是从实例集合中随机选择的，每个节点的特征都是从特征集合随机选择的。基于这些特性，该模型被称为随机森林（RF）。RF从理论上和实际上都证明不会产生过拟合。甚至AdaBoost在数据集中有过拟合和离群点时，随机森林能有更好的鲁棒性（有关RF的更多详细信息，请参阅我的旧帖子http://www.erogol.com/randomness-randomforests/）。随机森林在许多不同的领域，如Kaggle比赛中也有很出色的表现。
随机森林是树预测因子的组合，每棵树取决于独立采样的随机向量，并且森林中的所有树都具有相同的分布。森林的泛化误差a随着森林中树木数量增多而收敛[12]
随着时间向今天靠近，神经网络（NN）的一个新时代——深度学习已经被商业化了。这个短语只是指具有许多广泛连续层的NN模型。三层的NN模型崛起大致在2005年，由近期和现在的Hinton，LeCun，Bengio，Andrew Ng和诸多大师结合了许多不同的发现。下面我列举了一些机器学习领域重要的概念（我猜，我将专门为深度学习写一篇文章）;
- GPU编程
- 卷积NN [18] [20] [40]
- 解卷积网络[21]
- 算法优化
- 随机梯度下降[19] [22]
- BFGS和L-BFGS [23]
- 共轭梯度下降[24]
- 反向传播[40] [19]
- 整流器单元
- 稀疏性[15] [16]
- Dropout网 [26]
- Maxout网[25]
- 无监督的NN模型[14]
- 深度信念网[13]
- 堆叠式自动编码器[16] [39]
- 去噪NN模型[17]
结合所有这些想法和未列出的想法，NN 模型能够在对象识别、语音识别、NLP等不同的任务中击败之前的技术。但是应该注意的是，这并不意味着其他ML流派的结束。虽然深度学习的成功故事还在接二连三的上演，但是它在训练成本和调整模型的外部参数方面还有很多争议。此外，由于其简单性，SVM的使用依然非常普遍。 （据说可能会引起很大争议）
在结束之前，我需要再介绍一个比较前沿的 ML 趋势。随着WWW和社交媒体的发展，一个新的术语——大数据出现了，并大大影响了ML研究。由于大数据的数据规模都很大，许多强大的ML算法在相当多的系统中（当然对大型技术公司来说不是这样）是无用的。因此，研究人员提出了一套新的简单模型——dubbed Bandit Algorithms，被称为强盗算法[27 – 38]（通常都基于在线学习），使学习更容易，适应大规模问题。
我只是总结了机器学习的发展史。如果你发现错误，不足或没有添加引用的地方，请不要犹豫，可以通过各种方式告诉我。
